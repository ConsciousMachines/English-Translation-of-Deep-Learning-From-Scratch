

[[[00000000000000000000---da14299534ba28d101cb9da012463647af73411d71f4a60c134d52f8f1fa9369]]]Appendix C

[[[00000000000000000001---cb26d602ddc5c678094777463b1466968e55a180cb14bf7c01c89d882d048a23]]]Chapter 6 elaborated on LSTM as a “gated RNN”. LSTM is a very good layer, but it has many parameters and takes a long time to compute. Therefore, recently, many &quot;gated RNNs&quot; to replace LSTM have been proposed. Here we present a well-known and proven gated RNN called GRU (Gated Recurrent Unit) [42]. GRU keeps the concept of using LSTM gates to reduce parameters and reduce computation time. Now let&#39;s take a look inside the GRU.

[[[00000000000000000002---157c1fb20dae799dfe462fc2f1cd7d39db6c59719e3cd8491803a63592a97e5a]]]GRU interface

[[[00000000000000000003---39ed862485f36e24ea6a862a0dcac3e7823223d450df7ed509065e8941ad63c2]]]The key to LSTMs is the use of gates. This allows gradients to flow smoothly during training, thus mitigating vanishing gradients. GRU inherits this idea. However, there are some differences between LSTM and GRU. The big difference is in the layer interfaces, as shown in Figure C-1.

[[[00000000000000000004---06b8c760ec84a53afb1ccdb800376525b6c9937e4163d170cee91d7b642a0c11]]]
Figure C-1 Comparison of LSTM and GRU


[[[00000000000000000005---fc1c2b9ec2c842f22d6fc0f6bcf8c2aec698372c678b47ba6650a38761fb5cde]]]As shown in Figure C-1, LSTM uses two lines of hidden states and storage cells, whereas GRU uses only hidden states. By the way, this is the same interface as the &quot;Simple RNN&quot; dealt with in Chapter 5.

[[[00000000000000000006---60b584f4ff6115d91be1b9896ec95ac54d13f094ed322fcd8a0e9c0dbfe62e23]]]An LSTM storage cell is a private storage unit that is not exposed to other layers. LSTM recorded the necessary information in the memory cell and obtained the hidden state based on the information in the memory cell. In contrast, the GRU does not require additional storage such as storage cells.

[[[00000000000000000007---eee5bd1a37cad4c56203c0973c6229c152d6b5a16aa6b85a014f9c68258617ea]]]GRU Computational Graph

[[[00000000000000000008---2fb5a68376535f93ba0b11e2966a7d5cbbb3d8ef04ab4e520e3f6227799c4368]]]Now let&#39;s take a look at the calculations performed inside the GRU. Here, the calculations performed by the GRU are expressed in terms of formulas, and the corresponding calculation graphs are shown. The computation graph uses simplified nodes such as &quot;σ&quot; and &quot;tanh&quot; used in the LSTM computation graph in Chapter 6.

[[[00000000000000000009---ec427fe0db9c902144b8fecf77697815002565d2cef886031ddee448d82b7a41]]]The calculations performed by the GRU are represented by the four equations above (where xt and ht−1 are row vectors). And this computational graph looks like Figure C-2.

[[[00000000000000000010---2d36d4ec55a3a4e3528f508d3c21fb5671439a41461be883043047207b22b1ad]]]
Figure C-2 GRU Computation Graph: “σ” and tanh nodes have their own weights and perform affine transformations inside the nodes (“1−” node outputs 1−x when x is input)


[[[00000000000000000011---c330f0f4cb1ae50486d9bf23172a1c95bc2fa11879753c280b7ee2a48a6ad462]]]As shown in Figure C-2, the GRU has no memory cells and only one hidden state, h, propagates in time. And the gates used there will be 2 gates for r and z (by the way, LSTM used 3 gates). where r is called the reset gate and z is called the update gate.

[[[00000000000000000012---de82fc4864a255ba41cb2dae8b9fedc1e9f9ac1d9c820299338cd0ca761a09d6]]]The r in the reset gate determines how much past hidden states are &quot;ignored&quot;. If r is 0, then from equation (C.3) the new hidden state is determined from the input xt only. In other words, the past hidden state is completely ignored at this time.

[[[00000000000000000013---89084382d68aace658d2afeee93f059488774e2f813c417543988fbe5d05e315]]]An update gate, on the other hand, is a gate that updates the hidden state. It plays a dual role of LSTM&#39;s forget gate and input gate. It is the part of equation (C.4) that functions as a forget gate. This computation erases information that should be forgotten from the past hidden state. And it is the part that functions as an input gate. This weights the input gate to the newly added information.

[[[00000000000000000014---914784960570923bbc61fe556796c45c84f77c739e634b2a818458c24db76573]]]As such, GRU is a “simpler” architecture for LSTM. Therefore, compared to LSTM, the computational cost can be reduced and the number of parameters can be reduced. Note that we won&#39;t implement the GRU layer here, but it can be found in common/time_layers.py. Please refer to it if you are interested.

[[[00000000000000000015---1e4996292d84495e940742ff5bed12ed97b9bf0f0c76268a85a5f925ddbb6c14]]]Should I use LSTM or GRU? According to [32] and [33], the winner seems to fluctuate depending on the task and hyperparameter tuning. LSTMs (and variants of LSTMs) are often used in recent research. On the other hand, GRU is steadily gaining popularity. GRU&#39;s small number of parameters and low computational complexity make it particularly suitable for small dataset sizes and when model design requires iterative trials.

[[[00000000000000000016---26c6ff2fad7c7e7f39253adc724df20238ba4686447a74fc14408027340d7a52]]]Chapter 6

[[[00000000000000000017---7f43cc8c66ea7fe3c38fac2dac288480c93c4b0bef18c740b4610cfda386b3e4]]]Gated RNN

[[[00000000000000000018---26c9e76235208e64b30d92dcd1d931e5c5cbbf0bba464c04d8d08bac61a63425]]]Forgetting makes better progress.

[[[00000000000000000019---9d4c5e45b9adf7e1eb60f771be734413d8dd18c55d7095bb455040a615c478f3]]]—— Nietzsche

[[[00000000000000000020---4c4c28af9713972108614a15d103a6eec538d0b768254e69b75cafbcb5d7a6c9]]]The RNN we saw in the previous chapter had a looping path and was able to memorize past information. Moreover, its structure was simple and could be easily implemented. Unfortunately, the RNN in the previous chapter does not perform very well. The reason is that they (often) are not good at learning long-term dependencies in time series data.

[[[00000000000000000021---10f28ae49ad6f2d39dbb312524a84ed7d2ef222040c2ae228a27fa3daa92e8d8]]]Layers called LSTM or GRU are now often used in place of the simple RNN of the previous chapter. In fact, when we say &quot;RNN&quot;, we often see that the layer that it refers to is not the RNN in the previous chapter, but the LSTM. By the way, when explicitly referring to the RNN in the previous chapter, it is called &quot;Simple RNN&quot; or &quot;Elman&quot;.

[[[00000000000000000022---9a8746f1532403e2432966eded4482348a9d626a61e96e5f52694775f2b02b51]]]LSTM and GRU have a mechanism called &quot;gate&quot;. Its gates allow it to learn long-term dependencies in time series data. Here, I will point out the problems of the RNN in the previous chapter and introduce &quot;gated RNN&quot; such as LSTM and GRU as an alternative layer. In particular, we will spend a lot of time looking at the mechanism of LSTM, and will unravel the mechanism that enables &quot;long memory&quot;. We also build a language model using LSTM and show that it can be successfully trained on real data.

[[[00000000000000000023---56c3ca565249679823c4ca95f5861b956c1db79e991ba882056a0e66ba43d917]]]Problems of RNNs

[[[00000000000000000024---1cc71e438854eaffe958bc36f170d1693c729d9dca3d1df1045e92c793dbfddd]]]The RNNs described in the previous chapter are not good at learning long-term dependencies in time series data. The reason is that gradient vanishing or gradient explosion occurs in BPTT (Backpropagation Through Time). Here, we will start by reviewing the RNN layer that we learned in the previous chapter, and then we will explain why the RNN layer is not good at long-term memory using actual examples.

[[[00000000000000000025---1762a68faf927b58add4bfa65bd0cef41870dba5c2fe0bd71a98f5f6d964b22d]]]RNN review

[[[00000000000000000026---c4412c150be58e2565ede8642a7745bfce1b9cfd46795fea706e4f3183a0cc43]]]The RNN layer has looping routes. And when we unrolled the loop, it became a long horizontal network. This can be drawn graphically as shown in Figure 6-1.

[[[00000000000000000027---864695f91fd819b73713d8d07f4711009e56a9d4e92f1af8f96eade31d3531b7]]]
Figure 6-1 RNN layer: before and after loop unrolling


[[[00000000000000000028---575a4af5f7fff89a4dbb22a5fe4a10cee07fc506cde39741740f7d9795068274]]]As shown in Figure 6-1, the RNN layer outputs ht when inputting time series data xt. This ht is also called the hidden state of the RNN layer, and information from the past is stored in that state.

[[[00000000000000000029---4b85ba6f10ecc1f5c4aca5141408cf5e4abe1c1565f740f18c932a4fd50168df]]]The feature of RNN is to use the hidden state of the previous time. As a result, it was possible to take over the past information. By the way, the processing performed by the RNN layer at this time can be expressed as a computational graph as shown in Figure 6-2.

[[[00000000000000000030---ba120c96472f1c880cfaf544f8579664b8fded47fd5c5e132634276a6c41f8f2]]]
Figure 6-2 Computation graph of RNN layer (MatMul node represents matrix multiplication)


[[[00000000000000000031---4918fa813bfc95513f0b041d7789469361e6f25c58e231c09aaeb940509043d5]]]As shown in Figure 6-2, the calculations performed in the forward propagation of the RNN layer consist of matrix multiplication and summation, and transformation by the activation function tanh function. This is the RNN layer we saw in the previous chapter. Next, let&#39;s look at the problem with this RNN layer -- the problem of long-term memory.

[[[00000000000000000032---69afce367979757b4d5f762da6393daf59ccb766c4b3147fc7063e4a5f174a70]]]Gradient vanishing or gradient exploding

[[[00000000000000000033---a7e9b74fe52bd6df0fb7e2921e5a1089400f12d86b4afb55af99873d0686acc8]]]What the language model does is predict the next word given the words so far. In the previous chapter, we implemented a language model using RNN (we called it RNNLM). In pointing out the problems with RNNLM, I would like to reconsider the task shown in Figure 6-3 below.

[[[00000000000000000034---0ce02d58457caea75a004c4c7d2a48ae1ae5a992d3e5d75380731e2a2257c005]]]
Figure 6-3 What word goes in &quot;?&quot;? : Examples of problems that require (a certain amount of) “long memory”


[[[00000000000000000035---a905700e442bc51f09ac0240c52051c8fc4407e2f4a3c79160d72dea482a8abc]]]As I said before, the word that goes into the ``?&#39;&#39; is ``Tom&#39;&#39;. In order for RNNLM to answer this correctly, it needs to remember that &quot;Tom is in the room watching TV&quot; and &quot;Mary walked into the room&quot; as the current context. Such information must be encoded and stored in the hidden state of the RNN layer.

[[[00000000000000000036---b666efb4b893c2a2cd6bffab7ef679662ad5e2ef5ff6da7726817ffa26e7c423]]]Now, I would like to consider the above problem example from the standpoint of RNNLM learning. Here we see how the gradient propagates in the RNNLM given the word &quot;Tom&quot; as the correct label. Of course, we will be learning here with BPTT. Therefore, the gradient is transmitted in the past direction from the given location where the correct label is &quot;Tom&quot;. This can be drawn graphically as shown in Figure 6-4.

[[[00000000000000000037---5606d0eb50335b27c64f6e660a352a0a4b1a2318f6065f3724d69f9ccdda5b6b]]]
Figure 6-4 Gradient flow when learning that the correct label is &quot;Tom&quot;


[[[00000000000000000038---509b50245a15b5e256721ffb19997962e8829a3211724a6703fa8de630b34dfb]]]As shown in Figure 6-4, the existence of the RNN layer is important when learning that the correct label is &quot;Tom&quot;. By propagating &quot;meaningful gradients&quot; in the past direction, RNN layers can learn temporal dependencies. Gradients then contain meaningful information that should be learned, and by propagating it backwards it learns long-term dependencies. But if this gradient weakens along the way--it has almost no information--the weight parameter will not be updated. This means that long-term dependencies cannot be learned. Unfortunately, current simple RNN layers are often destined to either shrink (vanishing gradient) or grow (exploding gradient) over time.

[[[00000000000000000039---c2349b4987c3266856d9696624066f3a0ddd0001f0e7b4e1837fc209f3d2d35a]]]Causes of Gradient Vanishing or Exploding Gradients

[[[00000000000000000040---3f62d800908134beb2a88201e6ca82aa94b80fb2bc9850b068b969edd9b28e2f]]]Let&#39;s dig into what causes vanishing gradients (or exploding gradients) in RNN layers. Therefore, we focus only on the propagation of gradients in the time direction of the RNN layer, as shown in Figure 6-5.

[[[00000000000000000041---355344fd7831948c39d5b010ace34449b93efe2acc44b8025d12c7820a0e7691]]]
Figure 6-5 Propagation of temporal gradient in RNN layer


[[[00000000000000000042---11856caf9d26e8d6143f8f84fb76dac231be1eae7ef3205d6394d5d248c6e6e0]]]As shown in Figure 6-5, consider a time series of T lengths, and notice how the gradient coming from the T-th correct label changes. In the previous problem, this corresponds to the case where the Tth correct answer label is &quot;Tom&quot;. At this time, focusing on the gradient in the time direction, we can see that the gradient transmitted by backpropagation passes through the operations of &quot;tanh&quot;, &quot;+&quot; and &quot;MatMul (matrix product)&quot;.

[[[00000000000000000043---ac1257a6f0dddb6b279920635cdb0120b441b55e00575896d85fcc9a2f1e7c10]]]Backpropagation of &quot;+&quot; just flows the gradient transmitted from upstream as it is downstream. Therefore, the slope value does not change. So how does it change in the remaining two operations &#39;tanh&#39; and &#39;MatMul&#39;? First, let&#39;s look at &quot;tanh&quot;.

[[[00000000000000000044---4c7ab659f304e81ced65a8b5e31baf41c1f4d3ab39e414d1dab9d783cf4d02bd]]]As will be explained in detail in &quot;Appendix A Differentiation of sigmoid and tanh functions&quot;, when , the derivative is At this time, plotting the value of and the value of its derivative on a graph will result in Figure 6-6.

[[[00000000000000000045---d482ebf7701d5e3d91426100a028d698566b7060f578c7c76ab75cae7bda378c]]]
Graph of Figure 6-6 (broken line is differentiation)


[[[00000000000000000046---3ba25099234e550450229f148c29bdf2e81fea289827ff39e73f0b0aef518324]]]The dashed line in Figure 6-6 is the derivative of . As you can see, its value is less than or equal to 1.0 and decreases as x moves away from 0. What this means is that each time the gradient passes through a tanh node in backpropagation, its value gets smaller and smaller. So, after T passes through the tanh function, the gradient will be repeatedly weakened T times.

[[[00000000000000000047---b88efffffb8db7605213060e54d2178af2676b08aeeb0a8afa3b363812cc39cf]]]The tanh function is generally used for the activation function of the RNN layer, but by changing it to ReLU, we can expect to suppress gradient vanishing (when the input to ReLU is x, its output is ). This is because, in the case of ReLU, if the input x is greater than or equal to 0, backpropagation will flow the upstream gradient as it is downstream, and the gradient will not &quot;degrade&quot;. In fact, a paper [29] titled “Improving performance of recurrent neural network with relu nonlinearity” achieves performance gains using ReLU.

[[[00000000000000000048---d263f563227844992668a189737522ff14a56ada40667d26a5966e83d90a97d3]]]Next, turn your attention to the MatMul node in Figure 6-5. For simplicity, let&#39;s ignore the tanh node in Figure 6-5. Then the gradient of the backpropagation of the RNN layer will change only by the operation of &quot;MatMul&quot; as shown in Figure 6-7.

[[[00000000000000000049---8ef7128fbb7eeb349327b9850fb212d0e2944399064439b39d23c5825ae6cd8a]]]
Figure 6-7 Gradient of backpropagation when looking only at the product of matrices in the RNN layer


[[[00000000000000000050---9089a833e6fa99eb74c09ab79a0f63bb568b3f507f729a0ae3c5be544d7b79bc]]]In Figure 6-7, we assume that a gradient of dh comes from upstream. Backpropagation in the MatMul node then computes the gradient by multiplying matrices by dhWhT. After that, repeat this for the time size of the time series data. The point to note here is that the same weight Wh is used each time in this matrix multiplication calculation.

[[[00000000000000000051---2cc23b23148d449654389dd20b471ba9c98c664fbf03eb02c0f085025ca83062]]]So how do the gradient values during backpropagation change as they pass through the MatMul nodes? If you have any doubts, it&#39;s an experiment! Here, let&#39;s observe the change in the magnitude of the gradient with the following code (☞ ch06/rnn_gradient_graph.py).

[[[00000000000000000052---9e4c26e13f6282353c317c6a3b563f6b36db97ebce011aa295733a1e1823ef9c]]]# mini batch size

[[[00000000000000000053---28830f2bb22cb83dd09a6a385996e42a6f6ad7230f36580432a67e21beabaae5]]]# number of dimensions of hidden state vector

[[[00000000000000000054---f9d41b26a9bbb7ff2c718fa48695538e47f7f960a823202c464e732e31c9f5a0]]]# Length of time series data

[[[00000000000000000055---3d13ae40da4aab224baf96a5dc8798ad91c7f697cd909e71a108ceb1bb3a0cc8]]]# fixed random number seed for reproducibility

[[[00000000000000000056---4c5d6f2baab66f5ef0fcea1bd7aca46438a6a6221ab355ca765ddb5bc2bcaa98]]]Here dh is initialized by np.ones() (np.ones() is a matrix with all 1s). Then update dh by the number of MatMul nodes in backpropagation and add the magnitude (norm) of dh at each step to the array norm_list. Here, the average &quot;L2 norm&quot; in the mini-batch (N pieces) is obtained as the size of dh. The L2 norm is the square root applied to the sum of the squares of each element.

[[[00000000000000000057---704e2d589309aae12958962cb56de29ddb020ac302733862b0096c37660909ee]]]Now let&#39;s plot the result (norm_list) of running the above code on a graph. The result looks like this:

[[[00000000000000000058---ad3b3b7df8944fc487854968b9d7be284da1c20e87e524c6a79c795b3f0c9695]]]
Figure 6-8 Magnitude of gradient dh increases exponentially with time size


[[[00000000000000000059---dcb1176d1457086f5b3745b0ba0409b660b908c5bc9b5a58edb1b23186b9c491]]]As shown in Figure 6-8, we can see that the magnitude of the gradient increases exponentially over time. This is exploding gradients. Such a gradient explosion will eventually overflow and produce a value like NaN (Not a Number). As a result, neural network learning cannot be performed correctly.

[[[00000000000000000060---ed10af792326aa733539f946d0d916367f95cad5611f0f183c04d8c922f7840a]]]As a second experiment, I would like to change the initial value of Wh as follows.

[[[00000000000000000061---dbcf7471c2234e1f6aa82bfeb881c6f36d8db9ed94e0b05168380939f6a8ac06]]]Then, with this initial value, we perform the same experiment as before. The result is then:

[[[00000000000000000062---714564e3dc071797e16c6ade95613e4fba886f646700aabe1581ad6dc78549cb]]]
Figure 6-9 Gradient dh Magnitude Decreases Exponentially with Time Size


[[[00000000000000000063---0328b9d9861e97c3aad5795131dedd806afdedb94329cd9eb740e5161fc6cc83]]]As you can see from Figure 6-9, this time the result is an exponential decrease. This is vanishing gradients. When gradient vanishing occurs, the magnitude of the gradient decreases rapidly. Of course, when the gradient gets smaller, the weight parameters are no longer updated, so long-term dependencies can&#39;t be learned.

[[[00000000000000000064---51a149e191ffb2b8741b5a9a64f45e70851af4af3a62f8d634c535caa671e9aa]]]In the experiments performed here, the gradient magnitude increased or decreased exponentially. The reason for such an exponential change is, of course, that the matrix Wh is repeatedly multiplied T times. If Wh is a scalar here, things are simple. In that case, it increases exponentially when Wh is greater than 1, and decreases exponentially when it is less than 1.

[[[00000000000000000065---7bd8a6c491f7d1a2f042f24c26b35153110610a1c7f433acd4c48bf33d063ddf]]]But what if Wh is a matrix instead of a scalar? In that case, the &quot;singular values&quot; of the matrix are the indices. The singular values of a matrix are simply how spread out the data are. Depending on whether the value of this singular value—more precisely, the maximum value among multiple singular values—is greater than 1, we can predict the change in the magnitude of the gradient.

[[[00000000000000000066---9c3dd00e1cb287f1c957396dcc031a1e8640c0d6d7ed21adaa4d03d2580558f5]]]If the maximum singular value is greater than 1, we can predict that it is likely to grow exponentially. On the other hand, if the singular value is less than 1, we can conclude that it decreases exponentially. Note that not every singular value greater than 1 results in a gradient explosion. This means that this is a necessary but not a sufficient condition. A detailed discussion of vanishing/exploding gradients in RNNs can be found in [30]. Please refer to it if you are interested.

[[[00000000000000000067---b087edf074c146dd198dab4d10f87d31f4332a518ad043bef06761c20d8b4f26]]]Measures against gradient explosion

[[[00000000000000000068---aaf38b5d8985cd4311a910e2aac205ea1eaafb73d2e2d04529cb66f11a60349b]]]So far, we&#39;ve talked about RNN problems—gradient explosion and vanishing gradients. Now let&#39;s take a look at the workarounds. Let&#39;s start by looking at the gradient explosion.

[[[00000000000000000069---edd6ce99d06bc6ac3c0ac35a5d79f50fc684cdea3775509bced5f46307774e5b]]]There are standard methods for dealing with gradient explosions. It&#39;s a technique called gradients clipping. This is a very simple technique, and the algorithm in pseudocode looks like this:

[[[00000000000000000070---bfd571b36a0ed2ab2c11662ef92a7c71ae0608fb5168afa4823aa6e32d353c8c]]]Here we assume that we have a single gradient for all the parameters used in the neural network, so we denote this by the symbol . And set threshold as the threshold. At this time, if the L2 norm of the gradient (in the formula) exceeds the threshold, modify the gradient as above. This is gradient clipping. As you can see, it&#39;s a simple algorithm, but it works well in most cases.

[[[00000000000000000071---928f1039e7bb2d7b9b6c805e9c1bdc40e0c4aabd6cf0a6568c3e745b74b39669]]]is a collection of gradients for all parameters used in the neural network. For example, if a model has two parameters, weights W1 and W2, then combine the gradients dW1 and dW2 for those two parameters.

[[[00000000000000000072---71d344c0ed1aa1e90213331520ddc29d945530916343f98bbd454cda2f572856]]]Now let&#39;s implement gradient clipping in Python. Here, we implement gradient clipping with the function clip_grads(grads, max_norm). Argument grads is a list of gradients and max_norm is a threshold. Gradient clipping can then be implemented as follows (☞ ch06/clip_grads.py).

[[[00000000000000000073---3667f94ce747c68fddffcf1a61b4aef94ada80f7153915980976aca38ca252ba]]]Here&#39;s a gradient clipping implementation. As you can see, it shouldn&#39;t be too difficult. This clip_grads(grads, max_norm) will be used in the future, so prepare the same implementation in common/util.py.

[[[00000000000000000074---a092635d75f899f36de0deb4491af20b5aa5fa5d0f684081394ea7167483b42a]]]In this book, the class for training RNNLM is provided as the RnnlmTrainer class (common/trainer.py). Under the hood, it utilizes the gradient clipping function above to prevent gradient explosion. Gradient clipping with the RnnlmTrainer class will be explained again in &quot;6.4 Language model using LSTM&quot;.

[[[00000000000000000075---ca340265a21b387ed1908c778c00ce3e32f4396e8a615c40812c52d6cb32d2c8]]]The above is the explanation of gradient clipping. Next, we will look at how to deal with vanishing gradients.

[[[00000000000000000076---9ee9ab9f0bae5a0ff6e533df6be78b9a93ca7197d1bd09d9635990f9ad215a47]]]Vanishing gradients and LSTM

[[[00000000000000000077---e13ca92b1452c51237d8890c91017558465a6a4a7ddb7f28e81afe9a580ae57a]]]Vanishing gradients are also a big problem in RNN training. And to solve it, we need to fundamentally change the architecture of the RNN layer. This is where the main theme of this chapter, &quot;RNN with gates,&quot; comes into play. Many architectures (network configurations) have been proposed for this gated RNN, the representative ones being LSTM and GRU. In this section, we&#39;ll focus on LSTMs and take a closer look at how they work. We then show that it does not (or is unlikely to) experience vanishing gradients. GRU is explained in &quot;Appendix C GRU&quot;.

[[[00000000000000000078---a5e6f42d7cc7c4412621e165b591595cc1b0046c8c7baeb78fa909ff064a4a93]]]LSTM interface

[[[00000000000000000079---bbb5016ab549d4e3234f624f1ccd40bd365d5e8a4fe062528ecde9d929f02e62]]]We will now take a closer look at the LSTM layer. Before that, here, considering the future, I will introduce a &quot;simple projection&quot; on the computational graph. As shown in Figure 6-10, the projection shows matrix calculations etc. collectively as a single rectangular node.

[[[00000000000000000080---f3b26f4929a31cd6f6facbe866e14f234fc0a055743ebc02ad616b511e628690]]]
Figure 6-10 RNN layer with a simple projection: A simple projection is used in this section for ease of viewing


[[[00000000000000000081---11a208dbb1a5df9b1cfbafb2ee197e6ff846f2d173aa67b2d9fe3c63d11b6955]]]As shown in Figure 6-10, we will represent the computation by a single rectangular node &quot;tanh&quot; (where ht−1 and xt are row vectors). I think that this rectangular node contains the matrix product, the sum of the biases, and the transformation by the tanh function.

[[[00000000000000000082---3d9f1e5a88735a58d5cea063e721ee6cf44932553b6988acdc5957aae32643b8]]]You are now ready to go. Let&#39;s start by comparing the interface (input/output) of LSTM with RNN (Figure 6-11).

[[[00000000000000000083---2bc238f889b4c1829c3f10f65cd8a90c1b509a4b1dc5de984a51d799c2f58b1a]]]
Figure 6-11 Comparison of RNN and LSTM layers


[[[00000000000000000084---70e4a4ed931628317d1c0a4634d9ed88933e83b7b1a4c7bd89e645149d4cb527]]]The difference between the RNN and LSTM layer interfaces is that the LSTM has a path c, as shown in Figure 6-11. This c is called a storage cell (or simply &quot;cell&quot;) and corresponds to the storage part dedicated to LSTM.

[[[00000000000000000085---96b3a2a6caa467ffb552f7c0454ed010b5d7e51fc42911d7308df50ecdb43a82]]]A feature of the storage cell is that it only passes data to itself (only within the LSTM layer). In other words, it is completed only in the LSTM layer and does not output to other layers. On the other hand, the LSTM hidden state h is output to another layer (upward), similar to the RNN layer.

[[[00000000000000000086---455600e74bb2964ffae7eee177421473f659b3bbfb98fed71e80a8350da88fb5]]]From the perspective of receiving the output of the LSTM, the output of the LSTM is only h, the hidden state vector. The c of the storage cell is invisible to the outside. Or you don&#39;t have to think about its existence.

[[[00000000000000000087---ca7992d38f5528eda3cde4aaa909ea76ea57e4e765a83c9d00c71a951b91798a]]]Assembling LSTM layers

[[[00000000000000000088---80e1dbbda246b8f4ce4eaed63519809330e295c27fcf9d0187ccecdf9556fc48]]]Now let&#39;s take a look inside the LSTM layer. Here, I would like to take a closer look at the mechanism while assembling the parts of LSTM one by one. Note that the explanation here is based on the excellent LSTM commentary article &quot;colah&#39;s blog: Understanding LSTM Networks&quot; [31].

[[[00000000000000000089---8bd8cd643feb703ddb8aab73e8a8377f1edd14fab2377598fa2fc29128a0bcea]]]Now, as I said before, LSTM has a storage cell ct. This ct stores the memory of the LSTM at time t, which can be thought of as storing all the necessary information from the past up to time t (or learn to do so). . Then, based on the memory full of necessary information, it outputs the hidden state ht to the external layer (and to the LSTM at the next time). As shown in Figure 6-12, the calculation performed at this time outputs the memory cell transformed by the tanh function.

[[[00000000000000000090---fb825231eae1ddd11d5ddf0df74420936124f8e56f1a22f17f5234835bfafbe0]]]
Figure 6-12 LSTM layer that calculates hidden state ht based on storage cell ct


[[[00000000000000000091---32b5ba5aeca2cd3f3a2b109a60d453118774d5b93dd259d7e25b3eb305e58a91]]]As shown in Figure 6-12, we assume that the current storage cell ct is derived from three inputs (ct−1, ht−1, xt) by “some calculation” (details of “some calculation” will be I will explain). The important point here is that the updated ct is used to compute the hidden state ht. Note that this calculation is done by, which means applying the tanh function to each element of ct.

[[[00000000000000000092---9cc24af851dfd0dc26bd104d8847a67dbfdebd25c03d21b106e7c013cf4fc550]]]The relationship between the memory cell ct and the hidden state ht is (so far) just an element-wise application of the tanh function. What this means is that the number of elements in the storage cell ct and the hidden state ht are the same. For example, if the number of elements in storage cell ct is 100, the number of elements in state ht is also 100.

[[[00000000000000000093---6e72392d18299fe4fcebc8fa34ef3b77be4ee3acc02f688a0b33ed55c927ac94]]]Let&#39;s move on. Before that, here is a brief explanation of the function called &quot;gate&quot;. Gate is a word that means &quot;gate&quot; in Japanese. Gates control the flow of data, much like a gate &quot;opens/closes&quot;. As an image, as shown in Fig. 6-13, the role of the gate is to stop or release the flow of water.

[[[00000000000000000094---9a7f9fa00c7b781b616b18125e3b2fb5cd9ace87b89b9557240e3e9cdf48bda0]]]
Figure 6-13 Parable of a gate: controlling the flow of water


[[[00000000000000000095---9e8625c4136ef941de721e6a7d2f8a8d7fc0f680fbcc04f4e4f034925bd111de]]]Also, the gates used in LSTM are not binary &quot;open/close&quot;, they control how much the gate is opened and thus how much water is allowed to flow to the next. This means that it also controls the &quot;openness&quot;, such as 0.7 (70%) and 0.2 (20%), as shown in Figure 6-14.

[[[00000000000000000096---93c3e5135d9ffe414a9dc79500d42e555748a1fba27a03747ee2d7a9a031d46a]]]
Figure 6-14 Control the amount of water flowing in the range of 0.0 to 1.0


[[[00000000000000000097---3e51fd252c0576126484adae7e191f79ac02ee8ca3719bcbf6844ad53fcedda1]]]As shown in Figure 6-14, the degree of gate opening is represented by a real number between 0.0 and 1.0 (1.0 being fully open). And that number controls the amount of water that flows next. What is important here is that &quot;how much to open the gate&quot; is also learned (automatically) from the data.

[[[00000000000000000098---260bc5cfdf878d38ae045458775f9db681e858b0df99d6e92a49c800247b4158]]]The gate uses its own weight parameter to control how open the gate is. This weight parameter is updated with the training data. Also, the sigmoid function is used to find the degree of opening of the gate (the output of the sigmoid function is a real number between 0.0 and 1.0).

[[[00000000000000000099---7509fd999a4977e60387e664a1b5b2f10cf9bb640e08b0e7bab70f402ab2acc7]]]output gate

[[[00000000000000000100---c2906055415eaeae66fff139ace6204d1558a6be650602be4170da1651af3c88]]]Now let&#39;s get back to LSTM. In the previous explanation, the hidden state ht was simply applied to the memory cell ct with the tanh function. Now consider applying a gate to In other words, for each element of , we adjust how important they are as hidden states for the next time. This gate is called the output gate because it controls the output of the next hidden state ht.

[[[00000000000000000101---24f0dbcbd3d0d2e28ebb97950d828d09ac2e7643db45f66aba0e20b7ff78e35e]]]The degree of opening of the output gate--what percentage of the next pass--is obtained from the input xt and the previous state ht−1. The calculation to be done at this time is as follows. Note that o, which is the first letter of output, is added to the superscripts of the weight parameters and biases used here. In the following, the gates are similarly indicated with superscripts. Also, the sigmoid function is represented by σ().

[[[00000000000000000102---df29293023b6135a0a9364e92e0daf952d1271989b664791eab0fcba0027b08d]]]As shown in equation (6.2), there is a weight Wx(o) for the input xt and a weight Wh(o) for the previous state ht−1 (xt and ht−1 are row vectors). Then, the sum of the product of those matrices and the bias b(o) is passed through the sigmoid function, and it is the output o of the output gate. Finally, print the element-wise product of this with o as ht. Now, let&#39;s write the calculations performed here on a computational graph. The result should look like Figure 6-15.

[[[00000000000000000103---e713bfdeef57e25649017cbbc9481be7615c230b0c3ffb9ea6246428725cb718]]]
Figure 6-15 Add output gate


[[[00000000000000000104---33084354f66dfe9b691ea32c5bf791acbde064169595b8744ccd4f04ba6e686b]]]As shown in Figure 6-15, let us denote the calculation of equation (6.2) performed by the output gate by “σ”. And if its output is o, ht is calculated by multiplying it with o. Note that the &quot;product&quot; here is the element-wise product, also known as the Hadamard product. If we denote the Hadamard product by , then the computation we are doing is

[[[00000000000000000105---af5c02d5190dfdf2d4e3a803665b6ea90d52856b8b3f2a4bd1761a5cbbfba1f5]]]The above is the output gate of LSTM. Now the output part of the LSTM is complete. Next, let&#39;s look at the update part of the memory cell.

[[[00000000000000000106---3a7dc4d7d27aa5edadd9ac3c09f71164d79f33f55f736ee5128fe44e964e57e1]]]The output of tanh is a real number between −1.0 and 1.0. This numerical value between -1.0 and 1.0 can be interpreted as representing the strength (degree) of some encoded &quot;information&quot;. On the other hand, the output of the sigmoid function is a real number between 0.0 and 1.0. This represents the percentage of how much data is passed through. So (often) the sigmoid function is used for gates, and the tanh function is used for data with substantial &quot;information&quot; as the activation function.

[[[00000000000000000107---a93dc9c8dd80b889296cc96fe616fccf40219c4bebb9fa93255c6379073befba]]]forget gate

[[[00000000000000000108---7544e8e2f46f059f7ded4248b39b4af3194a2c876cc6ef61a370b07dc74a4daa]]]Forgetting makes better progress. The next thing we need to do is to explicitly tell the memory cell what to forget. To do that, of course we use gates.

[[[00000000000000000109---3bac12b05073afd4b9291ca433cf5301ae759c61858ef3e53ba645856276ff3c]]]Then, from the memory of ct−1, add a gate to forget unnecessary memory. Here we call it the forget gate. Now add a forget gate to the LSTM layer, and the computation graph will look like Figure 6-16.

[[[00000000000000000110---23cbf7c81bf480edd17e8b2f51f4405123ac1a689a538c0914c87cb10c9fb21c]]]
Figure 6-16 Add forget gate


[[[00000000000000000111---9b4c1ef2b8997cbc19b088e95669ec9f5a7d1d946f5931d578d1c4658703184c]]]In Figure 6-16, the series of calculations performed by the forget gate is represented by &quot;σ&quot;. In this &quot;σ&quot; there is a weight parameter dedicated to the forget gate. And the calculation performed at this time is expressed by the following formula.

[[[00000000000000000112---feb453816c88accffead3dc27bbe09cf0ecc3a62c168651cd90789f73335f2c5]]]Equation (6.4) gives the output f of the forget gate. Then the element-wise product of this f with the previous storage cell, ct−1, yields ct.

[[[00000000000000000113---1af2237180b7d7e7df68aec25930208530a639d5029b622dd8c286e6fb97a9a2]]]new storage cell

[[[00000000000000000114---6068efbd4436206c0ca512c8670e573634be1509657a33668a557e5ae9d270fc]]]The forget gate removed what was to be forgotten from the memory cells of the previous time. However, at this rate, the memory cell can only forget. Therefore, I would like to add new information to be remembered to the memory cell. To do so, add a new tanh node as shown in Figure 6-17.

[[[00000000000000000115---223ab2889fca6eff0af2793d9a5a82a70ff2b2ca1c2428a84757b182c75f89e4]]]
Figure 6-17 Add Required Information for New Storage Cell


[[[00000000000000000116---53fdfd3295c019ac5625c045bbae3a5a53bc190ecdb302a90f9c69c31da68151]]]The result computed by the tanh node is added to the previous time storage cell ct−1, as shown in Figure 6-17. New &quot;information&quot; is thereby added to the storage cell. This tanh node is not a &#39;gate&#39; but is intended to add new &#39;information&#39; to the storage cell. Therefore, the tanh function is used instead of the sigmoid function as the activation function. So here are the calculations we do in the tanh node:

[[[00000000000000000117---212f5e5810eb2a7a64eda6c46745c0220fb8a486e590088fd212aa43190c1f4e]]]Let&#39;s use g to represent the new memory that we add to the memory cell. A new memory is created by adding this g to ct−1 of the previous time.

[[[00000000000000000118---3fdf932874e2107dc07b7b1010764698a41ca8bcef59058c99793ee484e9f4e9]]]input gate

[[[00000000000000000119---2162a947d52469d022ecc74be443842519fdd40bbe692a397b641acea2b872df]]]Finally, consider adding a gate for g in Figure 6-17. Here, the newly added gate is called an input gate. After adding this input gate, the computation graph looks like this:

[[[00000000000000000120---33666007d45aa64450fbbbb4e4ac39adfd976f41f29f496d3c45689e7903b466]]]
Figure 6-18 Add input gate


[[[00000000000000000121---0e922dec0d5eb4c6ae6192e2e3526a2083ab64cf898746d2fbe28c5b8d1b39e0]]]The input gate determines how much additional information each element of g is worth. Rather than adding new information blindly, this input gate allows you to choose which information to add. Another way to look at this is to add new information weighted by the input gate.

[[[00000000000000000122---be279e1dd63530d7aaaad600daee35dea131fc183bb90d6e1b624b4ffdcc50ec]]]In Figure 6-18, the input gate is denoted by &#39;σ&#39; and its output is i. The calculation to be done at this time is as follows.

[[[00000000000000000123---8b4a0cfb658bd9d4e0dc45db0ddb32cd998671c6626cf95428a609f83cc1985e]]]After that, add the result of the element-wise product of i and g to the storage cell. The above is an explanation of the processing performed inside the LSTM.

[[[00000000000000000124---a06632b0c34ca1a799d88bb5e67f88e42d78591820bfdd59e03cc98f91abd48a]]]There are several &quot;variants&quot; of LSTM. The LSTM described here is a typical LSTM, but there are other layers that are (slightly) different in terms of how the gates are connected.

[[[00000000000000000125---7bc6b747c0c3aeaca1ebe111e94239abc5410a2ce7ce04751d1575f0f51f2e36]]]LSTM Gradient Flow

[[[00000000000000000126---d45fb8c43995dff759ed88ffc31b219e04f474dca09c9b8105ebe22a2e0cdb9a]]]I&#39;ve explained how LSTM works, but why doesn&#39;t this cause vanishing gradients? The reason for this can be seen by focusing on the backpropagation of memory cell c.

[[[00000000000000000127---374ed95629f421b1e6987b2d4116eeb156015cf726669682cfa9bf2b03092be0]]]
Figure 6-19 Backpropagation of Storage Cells


[[[00000000000000000128---34a727e5a3aa32e3c8e052ffc837f4eeab8a8d10bc050d5c6931563a774f83d6]]]Figure 6-19 focuses only on the storage cell and draws its backpropagation flow. At this time, the backpropagation of the memory cell will only pass through the &quot;+&quot; and &quot;x&quot; nodes. The &quot;+&quot; node simply passes the gradient from upstream. Therefore, no gradient change (degradation) occurs.

[[[00000000000000000129---5f04ed61088a8b8644a5dd39f46eb94d9f8136ff5d2647dd27e6593775f984f4]]]The rest is about the calculation of the &quot;x&quot; node, but this is not a &quot;matrix product&quot; but an &quot;element-wise product (Hadamard product)&quot;. By the way, in the RNN backpropagation we saw earlier, we used the same matrix of weights to perform repeated &quot;matrix multiplications&quot;. So that&#39;s what caused the gradient vanishing (or gradient exploding). On the other hand, the backpropagation of LSTM this time is an “element-by-element product” rather than a “matrix product” calculation. Then each time the element-wise product is calculated with a different gate value. This is the reason why vanishing gradients do not occur (or are unlikely to occur).

[[[00000000000000000130---3d09bf6337fd90b254696df0000dc6268bc8840c12aac16f08e1906ce1511552]]]The computation of the &#39;x&#39; node in Figure 6-19 is controlled by a forget gate (and it outputs a different gate value each time). For those memory cell elements that the forget gate should &quot;forget&quot; here, the elements of the gradient will be smaller. On the other hand, for the element that the forget gate led to &quot;must not forget&quot;, the element of the gradient is propagated in the past direction without deterioration. Therefore, the gradient of the memory cell can be expected to propagate without gradient vanishing (for information that should be remembered over a long period of time).

[[[00000000000000000131---c0beb4397890ab3e79521618a6e6754d6576eccd0ca4df88e3bc0bf663e3a624]]]From the above discussion, we can see that gradient vanishing does not occur (or is unlikely to occur) in LSTM storage cells. Therefore, memory cells can be expected to hold (learn) long-term dependencies.

[[[00000000000000000132---a9ba0f9f1c834da0789ef281c7f410a4ba505767ddf09cf8ea06164b46fafb0e]]]LSTM comes from the acronym for Long short-term memory. The term means short-term memory can be sustained for a long time.

[[[00000000000000000133---0a4399333b68e5dd10a997e51fb81da4d246b0c99ac60229aa3bca070f3d053c]]]LSTM implementation

[[[00000000000000000134---704eafff5a24e7d4c604625879cfd11f1517beb968aca74f6c3468f9544a37a1]]]Now let&#39;s implement the LSTM. Here we first implement the one-step process as an LSTM class. Then, implement a class that collectively processes T steps as TimeLSTM. First, we summarize the calculations performed by LSTM.

[[[00000000000000000135---6b8d4d2e605f7ce4479a50aa8c176fc0c2c32aeabfd696f2f4c9c895e5e85c5a]]]The above formula is the calculation performed by LSTM. What we want to focus on here are the four affine transformations in equation (6.7) (&quot;affine transformation&quot; here refers to equations such as xWx + hWh + b). In equation (6.7), four equations are used to perform affine transformations individually, but these can be calculated collectively in one equation. A graphical representation of how to do this is shown in Figure 6-20.

[[[00000000000000000136---245f62e01f0812a3cea1e00e7026709ed442fb3db3d603b2991e2ccc97e1f434]]]
Figure 6-20 Four calculations in one affine transformation by combining four weights


[[[00000000000000000137---d345a64917c5483670d17c2c6013153362f094278d0b763ca0183b9bd22c21e1]]]The four weights (or biases) can be combined into one, as shown in Figure 6-20. By doing so, the calculation of the affine transformation, which was normally performed four times, can be completed in one calculation. This can speed up calculations. This is because matrix libraries are often faster to compute together as &quot;big matrices&quot;. Furthermore, managing the weights collectively makes the source code concise.

[[[00000000000000000138---85798e738e7e6f02cca167aee8b9e969fa268ef37da9ae286073c45f06af59b3]]]Now, assuming that Wx, Wh, and b each contain four weights (or biases), I will show the computational graph of LSTM at this time. Then you can write something like Figure 6-21.

[[[00000000000000000139---a12fb20b57644da549d52e2c23e18ba704d7a71c05995b21ddaf3f479943baab]]]
Figure 6-21 Calculation graph of LSTM that performs affine transformation by combining four weights


[[[00000000000000000140---3aa401982025ed837cbd7e3cc2106b54eb2c6c385c79dbeecec9b3580853ac9c]]]As shown in Figure 6-21, we first perform four affine transformations together. Then the slice node retrieves the four results. This slice node is a simple node that divides the affine transformation result (matrix) into 4 equal parts. After the slice node, it passes through the activation function (sigmoid function or tanh function) and performs the calculation explained in the previous section.

[[[00000000000000000141---0003c0b987ee241b60ff4b04ba4c1d538ca4588a33d6a7090c677039785ae395]]]Let&#39;s implement the LSTM class by referring to Figure 6-21. First, here is the code for the initialization of the LSTM class (☞ common/time_layers.py).

[[[00000000000000000142---59eb0bf586c28d6b5be9d808cd5990052c44233294f5b2dc6e3f5cd46e1e1fa0]]]Here the initialization arguments are the weight parameters Wx and Wh, and the bias b. As I said before, those weights (and biases) are a bundle of 4 weights. The parameters given in this argument are set to the member variable params, and the gradients are initialized accordingly. Also, the member variable cache is used to hold intermediate results in the forward propagation, which are used in the backward propagation computation.

[[[00000000000000000143---fa0e500e3657387d8986e2d367b0a4b96d7763d9316e51878e96fda895a709d6]]]Next is the implementation of forward propagation. Forward propagation is forward(x, h_prev, c_prev) method. As arguments, we will receive the input x at the current time, the hidden state h_prev at the previous time, and the storage cell c_prev at the previous time (☞ common/time_layers.py).

[[[00000000000000000144---937d28414a8b6e29ce4b56e2d8d54945972c8ca0836e094796b5e7b5e51c36ef]]]Here, first of all, an affine transformation is performed. Again, at this time, the member variables Wx, Wh, and b each store four parameters, and the shape of the matrix changes as shown in Figure 6-22.

[[[00000000000000000145---c5ed437ec078c2a7ccf39ac66a380aa2acecdcf269b518c58190326316354511]]]
Figure 6-22 Transition of shape of affine transformation (bias omitted)


[[[00000000000000000146---373e91480d0ed733fae3f6ba1066ce3ab5264341e0254516a01f53a7e4e892f1]]]In Figure 6-22, let N be the number of batches, D be the dimensionality of the input data, and H be the dimensionality of both the storage cell and the hidden state. And the calculation result A stores the results of four affine transformations. Therefore, by slicing from there like A[:, :H] or A[:, H:2*H] and extracting it, it is distributed to the operation nodes after that. The rest of the implementation should not be particularly difficult if you refer to the LSTM formula and computational graph.

[[[00000000000000000147---370a144ca86ca8442bdee4135d464198dab1972a5c9d6a2384af920cd8058483]]]The LSTM layer keeps all four weights together. This means that the LSTM layer only has to manage 3 parameters: Wx, Wh and b. By the way, the RNN layer also holds three parameters: Wx, Wh and b. So even though the LSTM and RNN layers have the same number of variables in their parameters, their shapes will be different.

[[[00000000000000000148---0e225424aadd26967a4699976cdfc30a533872724273de50860c73b02dbdc0ab]]]LSTM backpropagation can be obtained by backpropagating the computational graph in Figure 6-21. With what I know so far, it shouldn&#39;t be difficult. However, since we are new to the slice node, we will only briefly explain its backpropagation.

[[[00000000000000000149---45d4bbc3bf3b8af8e6291f8d5cdc671ecbcff61eb4bf0b012f1bbcd31837c065]]]In the slice node, we divided the matrix into 4 parts and distributed them. So the backpropagation needs to combine 4 gradients. This can be drawn graphically as shown in Figure 6-23.

[[[00000000000000000150---8018ffc202285c23629c9aaaafe5aef49fef92ec8aedc2dfa885bf8f1930c0e2]]]
Figure 6-23 Forward propagation (top) and backpropagation (bottom) of the slice node


[[[00000000000000000151---fe3a93cd459531bf867d917589f2f0f19b9eec03ddd8e542c257f3a19f9c0f1f]]]Backpropagation of the slice node concatenates four matrices, as shown in Figure 6-23. In the diagram, we have four gradients (df, dg, di, do) that we concatenate to make dA. To do this with NumPy, you can use np.hstack(). np.hstack() horizontally concatenates the arrays given as arguments (np.vstack() concatenates vertically). So to do the above, just complete with the following line:

[[[00000000000000000152---28cef72ddec4916d0d7c5d0c0416912821cb2ab35a7be86b01423f0f3342e664]]]The above is the explanation of backpropagation of the slice node.

[[[00000000000000000153---80fc8b00e189fdcda4580e2e7acbf631a38c0c41207f8958a814a79c7e5180fb]]]Implementation of TimeLSTM

[[[00000000000000000154---5853cbd48bae24bc7e883ad0dba1f2bffa629d7961b6426b9eacd79a66cbe4a8]]]Now let&#39;s move on to the implementation of TimeLSTM. Time LSTM is a layer that collectively processes T time series data. The overall picture consists of T LSTM layers, as shown in Figure 6-24.

[[[00000000000000000155---a39c9acdf4e93a40d8a1ec9e165d33ed5c02dc54328ac5347dd11e227cebbeea]]]
Figure 6-24 Input and Output of Time LSTM


[[[00000000000000000156---cdc294094818abf26c0ad6355ec06f85c93e69ad1259240ecfb661d34068b3ab]]]By the way, as I mentioned before, Truncated BPTT is performed when training with RNN. Truncated BPTT cuts the backpropagation chain at an appropriate length, but the forwardpropagation flow must be maintained. Therefore, as shown in Figure 6-25, the hidden state and memory cells are stored in member variables. That way, the next time forward() is called, it will start from the previous hidden state (and memory cells).

[[[00000000000000000157---4b945368e1449796805e51b6b1f9b71e40dda74af808f0a2fc068c5ed6c469b2]]]
Figure 6-25 Backpropagation input and output for Time LSTM


[[[00000000000000000158---90306647f25e98c14b59d22445bfce8eda1c5edd4dc8b53483e8f87b8fd622e7]]]We have already implemented a Time RNN layer. Implement the Time LSTM layer in the same way here. This TimeLSTM can be implemented like this (☞ common/time_layers.py):

[[[00000000000000000159---80f2a91d066de632fe926363da8f9a544f65972609175d6d1bbd765c4f27c82b]]]The implementation of the TimeLSTM class is almost identical to that of the TimeRNN class, although the LSTM uses a storage cell c in addition to the hidden state h. Again, the stateful argument specifies whether to maintain state. Next, we will use this TimeLSTM to create a language model.

[[[00000000000000000160---cce0f90a3452a3daef209f9cfce9072667026e0392b60eefa7c4db2dc34d2dff]]]Language model using LSTM

[[[00000000000000000161---1d6995f742ec4968ca3d8a7e2c5b4c6563c75176a8475ade24abca55ae2e9d5f]]]Now that the Time LSTM layer has been implemented, we will implement the main subject, the language model. The language model implemented here is almost identical to the language model implemented in the previous chapter. The only difference is that we now use a Time LSTM layer where there was a Time RNN layer in the previous chapter, as shown in Figure 6-26.

[[[00000000000000000162---eaa419fda3d3b716eb603735fc84c9274f71cb9e1afb3865ee6f52a820fe408b]]]
Figure 6-26 Language model network configuration. The left figure is a model using Time RNN created in the previous chapter, and the right figure is a model using Time LSTM created in this chapter.


[[[00000000000000000163---b20118a87751538e1836dccd56c93907fd0d66b11718e4a41c8b5eb98a677a85]]]As shown in Figure 6-26, the difference from the language model implemented in the previous chapter is the use of LSTM. We will implement the neural network on the right in Figure 6-26 with the class name Rnnlm. The Rnnlm class is almost identical to the SimpleRnnlm class discussed in the previous chapter, but adds some new methods. So here is the implementation of the Rnnlm class using the LSTM layer†1.

[[[00000000000000000164---749c90d6cf3fc959584d481fd4a84ea6f995322ba69657a902117005c5c6e225]]][†1] The implementation code shown here corresponds to ch06/rnnlm.py, but ch06/rnnlm.py inherits a class called BaseModel, resulting in a more simplified implementation.

[[[00000000000000000165---d7f2704dda40d0528bed1e8e16c66e17b842f50172d7ba8554f23c7229aedc67]]]# Initialize weights

[[[00000000000000000166---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000000167---06f7dbb825277252d3f70b6a892a5aca619776d6c46333a516efdd8f89f3dda7]]]# Collect all weights and gradients in a list

[[[00000000000000000168---cbfab45d3414a300b0f066f0c1666e575af5138c24a3e9af256927a803c75215]]]The Rnnlm class implements a predict() method that processes right up to the softmax layer. This method will be used for sentence generation in Chapter 7. It also implements additional methods for writing/reading parameters—the save_params() and load_params() methods. The rest is the same implementation as the SimpleRnnlm class from the previous chapter.

[[[00000000000000000169---e9b5b66d07e3f3d4cf98e0d26194b75df48c56606b1db0639f82c2c58b765f37]]]In common/base_model.py there is a class called BaseModel. This class implements the save_params() and load_params() methods. Therefore, by inheriting the BaseModel class, you can also get the ability to read and write parameters. In addition, the implementation in the BaseModel class has optimizations such as GPU support and bit reduction (stored as 16-bit floating point numbers).

[[[00000000000000000170---b7b209e301b69a9181cc3f238d5ec093cd1600f105220e0565a310b77ec9390d]]]Now let&#39;s use this network to train the PTB dataset. This time, we will train using all the training data from the PTB dataset (in the previous chapter, we used part of the PTB dataset). Here is the code for training (☞ ch06/train_rnnlm.py).

[[[00000000000000000171---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000000172---70001ac4979f877d09f05ffad8684919f946c958040c421248f0baaee1fd4111]]]# number of elements in RNN hidden state vector

[[[00000000000000000173---c22a18d8b73c0d11312982ca7dc6df18e599577d5a63c5d7eb544f08296ddbd9]]]# Size to deploy RNN

[[[00000000000000000174---4f01c4043b8014c9e83e6792bb149425447a5726a4b0bff01d8bd515a46a874f]]]# load training data

[[[00000000000000000175---d2e487f751104844d65f8a00a849ef275b97f3f9f1fd61ddc98846db3788f6c7]]]# generate model

[[[00000000000000000176---050ca78bf6e581620830428316500ec8fb708ad99291a555120785e4c804ed9d]]]# ❶ Learn by applying gradient clipping

[[[00000000000000000177---f351a181db8c5dfa75189f4248f5d38925b476950b55c6404de8fc71db104bdc]]]# ❷ Evaluate with test data

[[[00000000000000000178---ca871a2aae8b0bdeca39b0865392f025d695987f7e9555da02ec55eb0acdbdc4]]]# ❸ Save parameters

[[[00000000000000000179---b9f99e74e809e0ba932516717ad3fdb3633ddb8c6ec7c0e5480a2577cb9a32f9]]]The source code shown here has much in common with the code in the previous chapter (ch05/train.py). Therefore, here we will focus on the differences from the previous chapter. First, in code ❶, the model is trained using the RnnlmTrainer class. The fit() method of the RnnlmTrainer class finds the gradient of the model and updates the model parameters. At this time, gradient clipping is applied by specifying the argument max_grad. By the way, inside the fit() method, the following implementation is done (pseudo code is shown here).

[[[00000000000000000180---4fa129145e8162559782acb2e95f9b1476fb32694c81ac76a603624f3ee581a5]]]# find the gradient

[[[00000000000000000181---43835c59a0d3b606f2b4661ba4ff8387e68128f2bf2470c4a58c06031e428872]]]# gradient clipping

[[[00000000000000000182---38f43c19da8bdd7e5866eb1f7c6ecbd90942f9b37066fd1c3a57f64ff215bc8c]]]# update parameters

[[[00000000000000000183---7fee81a50b99af2a23ce0f9ad1802c3a04a635e0e23c930592802b70fb42c017]]]We implemented gradient clipping as clip_grads(grads, max_grad) in ``6.1.4 Countermeasures against Gradient Explosion&#39;&#39;. Here we take advantage of that method to do gradient clipping.

[[[00000000000000000184---d0d21460b4a6d2bf801c0535b4cb2620f60842b495c13d9aba3dca7dbfe69773]]]Also, perplexity is evaluated every 20 iterations with the argument eval_interval=20 of the fit() method in ❶. Since the data size is large this time, instead of evaluating every epoch, we will evaluate every 20 iterations, and later draw the results as a graph using the plot() method.

[[[00000000000000000185---8a84303632b40503470287c26124fdf7e08cde6846e5c52a47cddf81b5fa7259]]]In ❷ part of the code, the perplexity is evaluated using the test data after the training is finished. Note that the model state (hidden state and storage cells of the LSTM) is reset before evaluation. The function to evaluate perplexity is implemented as eval_perplexity() in common/util.py, so use that (its implementation is very simple).

[[[00000000000000000186---5bbd59a2ca145b16fadcf0f637a32134b92b70762b94eb52eb1e3d964ccbef2b]]]Finally, in ❸, the parameters after learning are saved in an external file. This will be used in the next chapter when generating sentences using weight parameters after learning. The above is the code for learning RNNLM. Now let&#39;s run this code. This will print the following result in your terminal:

[[[00000000000000000187---e3e5c7b43aa2afa6e78e88ed91850aa142cda6cf8bebdafb192ba4643e10c70d]]]
Figure 6-27 Terminal output result


[[[00000000000000000188---4ce92878b8921d2d4c5728e20db2ad5f4fe6b1bf4a243d7bff68bb3152234ca7]]]In Figure 6-27, perplexity values are printed every 20 iterations. Looking at the result, the leading perplexity is 10000.84. This means (intuitively) we&#39;ve narrowed down the next words to about 10,000. Since our dataset has 10,000 vocabularies, this is a state in which nothing has been learned yet, so it is a guesswork. But as I continue to study, the perplexity improves as expected. In fact, the perplexity drops below 400 around 300 iterations. Let&#39;s take a look at the transition of perplexity on a graph. The result looks like this:

[[[00000000000000000189---c271d39a81632ed15e5284f2c8d02bfb71cc36b45fd58ab02d68208bc44bf036]]]
Figure 6-28 Transition of perplexity (result of evaluation every 20 iterations of training data)


[[[00000000000000000190---371361b175045f831161f91608f96a73f795b5722f6a1f4434c8a29342bdf029]]]In this experiment, a total of 4 epochs (1327 * 4 iterations) were learned. As shown in Figure 6-28, the perplexity decreased smoothly and finally reached around 100. And the final test data evaluation (point ❷ in the source code) is 136.07... This result will vary from run to run, but should be around 135. So our model has grown to narrow the next word down to about 136 (out of 10,000).

[[[00000000000000000191---c66aedf4f7bde34d667753688a05db2e94bf3e36f5abf95449c6276f5cae0f3e]]]So what does a perplexity value of 136 actually mean? To tell the truth, this is not a very good result. This is because the state-of-the-art research in 2017 shows that the perplexity of the PTB dataset is below 60 [34]. Our model still has a lot of room for improvement. Subsequently, we will further improve the current RNNLM.

[[[00000000000000000192---aeea31dc7f35806ced613ee469749b27b059dffd9f659d279b2480eba2c4beb0]]]Further improvement of RNNLM

[[[00000000000000000193---def21b15008c257c96b450445ec7b59aa29b1d5a26e349becd4087555f249df0]]]In this chapter, we will explain three points that should be improved for the current RNNLM. Then we would like to implement those improvements and evaluate how much the accuracy improves in the end.

[[[00000000000000000194---4710fa135613f48fbb39d3f65d96f163eb6ca46e3696cdfa04e5fd84876a8b37]]]Multiple LSTM layers

[[[00000000000000000195---a2985601288f1117ba6b84317c6f37c3bd578dfe0e801ae4d9f4897debe63642]]]Deepening the LSTM layer -- stacking many layers -- is often useful when trying to create a highly accurate model with RNNLM. So far, we have used only one LSTM layer, but we can expect to improve the accuracy of the language model by stacking multiple layers, such as two or three layers. For example, if we use two layers of LSTM to create an RNNLM, it will look like Figure 6-29.

[[[00000000000000000196---b9d4c4966a7712af42fcaa8e83768daef60d64270a0b6424d4e7ad594ebb5c14]]]
Figure 6-29 RNNLM when two LSTM layers are stacked


[[[00000000000000000197---2e2c1bdbd4242734e9d0b60bc8d268adac15f7fbcabc8981699dfd287c1571c6]]]Figure 6-29 shows an example of stacking two LSTM layers. At this time, the hidden state of the first LSTM layer is the input of the second LSTM layer. In a similar fashion, you can build on any number of LSTM layers, allowing you to learn more complex patterns. This is the same as the deepening story for feedforward neural networks. In the previous work &quot;Deep Learning from Scratch&quot;, we created a model with higher expressive power by stacking many layers such as Affine and Convolution layers.

[[[00000000000000000198---1105d81c1dcd019d34d09896ab7a12d0745de70c23a723b933bffb03456d9fa1]]]So how many layers should you add? Of course it&#39;s a question about hyperparameters. Since the number of overlapping layers is a hyperparameter, it should be determined appropriately according to the complexity of the problem to be tackled and the amount of prepared training data. By the way, in the case of the language model of the PTB dataset, it seems that good results can be obtained with the number of LSTM layers of about 2 to 4.

[[[00000000000000000199---579b94e2ce55bfa7bdb5d1ff544afa78539c4d14921118e9d95c7877b224b1f7]]]It is reported that the model called GNMT [50] used in Google Translate is a network with 8 layers of LSTM. As the example shows, if the problem to be tackled is complex and a large amount of training data can be prepared, making the LSTM layer “deeper” can be expected to improve accuracy.

[[[00000000000000000200---b775067d51557bb371ccd88571c44c9112a1a4941386af4f73f54230f069f18f]]]Suppression of overfitting by dropout

[[[00000000000000000201---06fc0be14c3c01a3be309e943ceb142d9b81a3957519a24db7e7f2eba904d269]]]By stacking LSTM layers, we can expect to learn complex dependencies of time series data. In other words, you can create a model with rich expressive power by making the layers deeper. However, such models often suffer from overfitting. The worse news is that RNNs overfit more easily than regular feedforward networks. Therefore, countermeasures against overfitting in RNNs are important and are still the subject of active research.

[[[00000000000000000202---843c5077107341b97f4dfb86a9e571076c0ef4521486a54ac1a4dda850416fe1]]]Overfitting refers to the state of over-learning on only the training data. In other words, the lack of generalization ability is overfitting. What we want is a model that generalizes well. For that reason, it is necessary to design the model by judging whether overfitting has occurred from the difference between the evaluation of the training data and the evaluation of the validation data.

[[[00000000000000000203---114a9fa037e09b3b4a23113b1810a0467f104bc02858b59d732ac0dd62618be4]]]There are standard methods to suppress overfitting. The first is to increase the training data. And to &quot;reduce the complexity of the model&quot;. These two are the first to come to mind. Another useful method is regularization, which penalizes model complexity. For example, L2 regularization penalizes weights that grow too large.

[[[00000000000000000204---8ded8ff1e5965937d822a2585b992b2af5284c87aada6fe4402e483756a6bf42]]]Also, like Dropout [9], randomly ignoring some of the neurons in the layer (for example, 50%) during training can be considered a kind of regularization (Figure 6-30). ). I would like to take a closer look at Dropout and apply it to RNNs.

[[[00000000000000000205---b80ee4cd5cf4efcd278fcce38fc268788b431af0528d7c1637c57b31d7dbfcad]]]
Figure 6-30 Conceptual diagram of Dropout (cited from reference [9]): Normal neural network on the left, network with Dropout applied on the right


[[[00000000000000000206---34bf986decb2d44737eac811b1e665dfb236a7ac218bf065223b0f9423b889af]]]As shown in Figure 6-30, Dropout randomly selects a neuron and by ignoring that neuron stops the signal transmission to its destination. This “random ignoring” becomes a constraint and can improve the generalization performance of neural networks. In the previous work &quot;Deep Learning from Scratch&quot;, we implemented a Dropout layer. There, we showed an example of inserting a Dropout layer after the activation function, as shown in Figure 6-31, and showed that it contributes to the suppression of overfitting.

[[[00000000000000000207---9886ec3ebdf141ba1c00d4f45243c44bb44a65f6c6cbbb48d2346b250cb635f3]]]
Figure 6-31 Example of applying a Dropout layer to a feedforward neural network


[[[00000000000000000208---2ca01be8ae54ff81cc56d8fdf33f4ac43175fe0b9a5a1579e3259007ea0c903b]]]So where should the Dropout layer be inserted in a model using RNN? The first possibility is the time series direction of the LSTM layer, as shown in Figure 6-32. But to answer first, this is not a good use.

[[[00000000000000000209---6930d239dba1318b1d6a76e41d0e52d87646467f5a064e0a04166cfb2c158c46]]]
Figure 6-32 Bad example: Insert Dropout layer in chronological direction


[[[00000000000000000210---31f54b5610b73bc9d6d7caeca371335f47ece6ef346a08be59dbd3532c929c78]]]If you put dropouts in the time series direction in RNN, information will be lost as time progresses (at the time of learning). In other words, the noise caused by Dropout accumulates proportionally as time progresses. Considering the accumulation of noise, it seems better to stop Dropout in the direction of the time axis. Therefore, consider inserting Dropout layers in the depth direction (vertical direction) as shown in Figure 6-33.

[[[00000000000000000211---ab126b65f5f84f70c8011587c9b87719da5fde952c2849e26c13e54911487559]]]
Figure 6-33 Good example: insert Dropout layer in depth direction (vertical direction)


[[[00000000000000000212---581d3fd5b10898975f1ffffad2314f31179c38608ccff3437bf1f2ffcfccc40d]]]In this case, information is not lost no matter how far the time direction (left and right direction) progresses. Dropout works effectively only in the depth direction (vertical direction) independently of the time axis.

[[[00000000000000000213---b094bc1093836fe0e3694a2bc80b86488ee3ee866f0d082eff43a3a057d4eca1]]]Compare Figure 6-31 and Figure 6-33. The example in Figure 6-31 shows the use of Dropout in a feedforward neural network, which applies Dropout in the depth direction. In the same way, by applying Dropout only in the depth direction in Figure 6-33, we can expect to suppress overfitting in the same way as with feedforward.

[[[00000000000000000214---bc127599bbc457c8ac3b78ee859a9adf2c5dc3e027db8f0a75d4078c5187f488]]]As we have discussed so far, &quot;regular Dropout&quot; is not suitable for the time direction. However, in recent research, various methods have been proposed for the purpose of regularizing RNNs in the time direction. For example, ``variational dropout&#39;&#39; has been proposed in [36] and has been successfully applied in the time direction.

[[[00000000000000000215---a007c4dce32a8c50a4960de901f293f5fbaa3c86f6f0339c2e550198f8eadc0c]]]Variational Dropout can be used not only in the depth direction but also in the time direction, further boosting the accuracy of the language model. The mechanism is that Dropouts in the same hierarchy use a common mask, as shown in Figure 6-34. The &quot;mask&quot; here refers to a binary random pattern that determines whether data is passed or not.

[[[00000000000000000216---5b4b6d0270be5f27ffd9ff76635b6d331cf7019080e0b2b8a369f745a4b00f11]]]
Figure 6-34 Variational Dropout Example: Dropouts with the same symbol use a common mask. By using a common mask for Dropouts in the same hierarchy, Dropouts in the time direction also work effectively.


[[[00000000000000000217---ef1985fee5bf3135e7a7b3132d7244d8cf22eaf39bf873e680d48b6544ec48c9]]]As shown in Figure 6-34, Dropouts in the same hierarchy share a mask so that the mask is &quot;fixed.&quot; This makes the loss of information &quot;fixed&quot;, avoiding the exponential loss of information that occurs with regular Dropouts.

[[[00000000000000000218---1efc2921cf7de55bdd1f261e969f643ff27a4ba1513539bcd07be1a1d138a2d0]]]Variational Dropouts have been reported to give better results than regular Dropouts. However, in this chapter, we will not use variational Dropout, but normal Dropout. The idea of variational Dropout is very simple, so if you are interested, you can implement it yourself!

[[[00000000000000000219---8a2509104cc4a496f06e425e569161fbc9e07731a7050e710c799e01022e0cea]]]weight sharing

[[[00000000000000000220---e4d5d28d51672cfb1779a4d02358de9ef6fc7799035e46e2bf4f74fc2e60daa8]]]A very simple trick to improve language models is weight tying [37][38]. Weight tying literally translates to &quot;tying weights together&quot;, which means sharing weights as shown in Figure 6-35.

[[[00000000000000000221---3b064d3180c1379140dbbbad1c404986ae8acf5de89c6ce724501fbe7f9c55d9]]]
Figure 6-35 Example of Weight Sharing in Language Model: Weight Sharing in Embedding Layer and Affine Layer Before Softmax


[[[00000000000000000222---bede38f938cdfa2312dfc153f080139a7f4c5044cfae37b6f5dc17de2c1bf205]]]Weight sharing is a technique that combines (shares) the embedding layer weights and the affine layer weights, as shown in Figure 6-35. By sharing the weights between the two layers, we can significantly reduce the number of parameters to learn. Still, it is a technique that kills two birds with one stone because it can improve accuracy!

[[[00000000000000000223---126535e7b70b24eddc01e0f42a1d9d43398c681bcac1db1b286c689d78bf2c24]]]Now let&#39;s think about it from the perspective of implementing weight sharing. Here, let the number of vocabulary be V and the number of dimensions of the hidden state of LSTM be H. Then the shape of the Embedding layer weights is (V×H) and the shape of the Affine layer weights is (H×V). To apply weight sharing then, simply set the transpose of the Embedding layer weights to the Affine layer weights. And this super easy trick gives great results!

[[[00000000000000000224---66c77bbe9a31c71f9074b63d19c80d874e619294a0442412751a7d00a1964cea]]]Why is weight sharing useful? Intuitively, sharing weights reduces the number of parameters to learn, which makes learning easier. Furthermore, reducing the number of parameters also has the benefit of suppressing overfitting. The rationale for the usefulness of weight sharing is theoretically described in paper [38]. Please refer to it if you are interested.

[[[00000000000000000225---01d79a9ad0820f4bcf8cf47ed2b5585e23da2c1846d0cea77954062f73c5e82e]]]A better RNNLM implementation

[[[00000000000000000226---b0ea85909adba0f84f8b6fd80b4f36b3df18f5166884c23f717c797680b388e1]]]So far, I&#39;ve explained three improvements to RNNLM. Let&#39;s see how effective those techniques are. Here, it is implemented as a BetterRnnlm class with the layer structure shown in Figure 6-36.

[[[00000000000000000227---ab345512658d39a01183870429372ad96ca3e52a91bac9db83b6e87aa9fbdae0]]]
Figure 6-36 Network Configuration for the BetterRnnlm Class


[[[00000000000000000228---8ccee2a0b5762fe1cdc06c5206e8347acb49f19819073e638ae1edf732a84e45]]]As shown in Figure 6-36, there are three points for improvement here:

[[[00000000000000000229---868778637d0ecbe637e1bf06226abfe75675898a0fcb0ff952cebfd078489f56]]]Multiple layers of LSTM layers (2 layers here)

[[[00000000000000000230---68ce147d642e8d803e352e225cac5c2d102beefbaba8c243c5d986d2a8198e98]]]Use Dropout (applies only in depth direction)

[[[00000000000000000231---132ce631787ae6f7c682d3c8c2def40edf7d026e9ef8f4856480f2bf8b43054c]]]Weight sharing (weight sharing between Embedding layer and Affine layer)

[[[00000000000000000232---f33bb7313b964a545f5cb51fe1c8bba9e575bc4fb8478a75e178bac38319fed4]]]Now, let&#39;s implement the BetterRnnlm class that incorporates these three improvements. This can be implemented like this (☞ ch06/better_rnnlm.py).

[[[00000000000000000233---67771630be2c55d5a7cd0d15dab7b431194cee52f81138427cb8b6febe5715c3]]]# 3 improvements!

[[[00000000000000000234---ebc72eeb5168baa2a77edd0f11ada4978d3fc509edcbeb0a59475be2b8c41812]]]# weight sharing!!

[[[00000000000000000235---bbba65981634a2143b52ef8e29fea4c943f3652f3d056e066e896a0f0ab09565]]]Here in the gray background of the code are the three improvements mentioned earlier. Specifically, two Time LSTM layers are stacked and a Time Dropout layer is used. Then share the weights between the Time Embedding layer and the Time Affine layer.

[[[00000000000000000236---bb4b484b69eeb345ad3692b3ee9e9289aff09d3934c8c9406be8db3eefb0dd53]]]Now let&#39;s train this improved BetterRnnlm class. Before that, I would like to add one trick to the learning code that we will be doing. The trick is to evaluate the perplexity on the validation data every epoch, and lower the learning coefficient only when the perplexity gets worse. This is a well-used technique in practice and often gives good results. The implementation here is based on the PyTorch language model implementation example [39]. Here is the code for training (☞ ch06/train_better_rnnlm.py).

[[[00000000000000000237---a79d601b43105e6197ef7cebeffbb5a98eb4364f8404f0e20200f7597facb926]]]# Remove the following comment out when running on GPU (requires cupy)

[[[00000000000000000238---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000000239---4f01c4043b8014c9e83e6792bb149425447a5726a4b0bff01d8bd515a46a874f]]]# load training data

[[[00000000000000000240---70b692c8495a455c86d2f874f10f2a37a31c46d54701b15976e2f87d22d40c42]]]We evaluate the perplexity on the validation data for each epoch of training, and if it is below the previous perplexity (best_ppl), we reduce the learning factor by a factor of 4. In order to do that, here we use the fit() method of the RnnlmTrainer class for one epoch of training, and then we repeat the process of evaluating perplexity on the validation data using a for statement. Let&#39;s run this training code.

[[[00000000000000000241---284acfa62b147c381dae5c3fcf2a4b9122ed5b0ccf4d884e133993df994ad495]]]This learning takes a lot of time. If you run it on a CPU, it will take about 2 days. On the other hand, if you use GPU, it will be completed in about 5 hours. The learned weights can be obtained from the following URL.

[[[00000000000000000242---63abc2f0e07980f6aa4052108a12c3b624a96b3f8d4c3cfa2da91caf2da7b26f]]]By executing the above code, the perplexity will decrease smoothly. And finally we get a perplexity of 75.76 on the test data (this result varies from run to run). Considering that the pre-improvement RNNLM had a perplexity of around 136, this is a significant improvement in accuracy. Layering LSTMs for greater expressiveness, Dropout for greater versatility, and weight sharing for efficient use of weights have allowed us to achieve such significant accuracy improvements!

[[[00000000000000000243---64ab00ec15f6d835d9fc5d8c789132b2fd34c8920b0e54993b62517edb409255]]]To cutting-edge research

[[[00000000000000000244---97abce841072984feda233a53992784cbc87b97b22a1fbc03cf31aa0f40608fa]]]This concludes our RNNLM refinement. We made some improvements to RNNLM and were able to achieve significant accuracy improvements. A perplexity of around 75 for test data in the PTB dataset is a reasonable result. But cutting-edge research goes further. Here, I would like to briefly introduce only the results of the latest research. Now let&#39;s take a look at Figure 6-37.

[[[00000000000000000245---2e1da701cfa5132e3aa241ac845d4748051eb0d5d2a385d45852dbb01495000f]]]
Figure 6-37 Perplexity results for each model on the PTB dataset (extracted from [34]). &quot;Parameters&quot; in the table represents the total number of parameters, &quot;Validation&quot; represents perplexity for validation data, and &quot;Test&quot; represents perplexity for test data.


[[[00000000000000000246---2fdcab4d05e49e5796433ac6bcd4316ab038477f30292993782ffea6d91482ab]]]Figure 6-37 is taken from reference [34]. This table summarizes the perplexity results for the best-ever language model PTB dataset. Focusing on the &quot;Test&quot; column of the table, we can see that perplexity has decreased as new methods are proposed. And the last line gives a result of 52.8. In fact, this number of 52.8 is such an impressive result that a perplexity approaching 50 for the PTB dataset was unthinkable until a few years ago.

[[[00000000000000000247---7845323a3c608f98e3a6683175f026943c67192e04f461b700e1cb7ec525b7de]]]Only the results of state-of-the-art research are presented here. Of course, our model is quite different. But the point is that the state-of-the-art model in Figure 6-37 and ours have much in common. For example, even state-of-the-art models use multiple layers of LSTMs. Then we do Dropout-based regularization—actually variational Dropout and DropConnect†2—and weight sharing. On top of that, several techniques of optimization and regularization are used, and the hyperparameters are tuned quite rigorously. This has resulted in a staggering figure of 52.8.

[[[00000000000000000248---eb56b611ff6fd4b7714fa4417c33864aa1e38618696f129dd680d8b7a0d52792]]][†2] DropConnect is a method that randomly ignores the weight itself.

[[[00000000000000000249---49c1824aa0040657b9a7d190c2cfa5bc57a157ebba372b610fbff5db0ef247ab]]]The model name in Figure 6-37 is &quot;AWD-LSTM 3-layer LSTM (tied) + continuous cache pointer&quot;. This continuous cache pointer technology is based on Attention, which we will learn more about in Chapter 8. Attention is a very important technology, and it is used for various purposes. It also contributes greatly to improving accuracy in the task of language modeling. It&#39;s a bit ahead, but let&#39;s look forward to Chapter 8 Attention!

[[[00000000000000000250---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000251---441b074b460be1c436088538de15fbe3bb3e80a490b7ab124c70d43780731e00]]]In this chapter, we have looked at gated RNNs. We point out that gradient vanishing (or exploding gradients) is a problem in the simple RNN of the previous chapter, and that gated RNNs—specifically, LSTMs and GRUs—are effective as an alternative layer. explained. These layers used a mechanism called gates, which allowed us to better control the flow of data and gradients.

[[[00000000000000000252---74a6441103454ab114a918c8411e2bf2025f959d925c9dd8f8e81045b8dc4c51]]]Also, in this chapter, we used the LSTM layer to create a language model. Then, we performed training on the PTB dataset and evaluated perplexity. In addition, using techniques such as LSTM multi-layering, dropout, and weight sharing, we have also succeeded in significantly improving accuracy. These techniques are in action in a cutting-edge study from 2017.

[[[00000000000000000253---cff31eb15b08ae24b59b24f0d0be88c7b5e5fd1e898b4f9d9b72318fc615531c]]]In the next chapter, we will use the language model to generate sentences. Then we&#39;ll take a closer look at models that transform one language into another, such as machine translation.

[[[00000000000000000254---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000000255---4e20b63c135b1ebc14c6c95bb729d1cf1026d3564ff74b6d8d0ff2e21b19e8fa]]]Gradient vanishing/gradient explosion is a problem in simple RNN training

[[[00000000000000000256---02119d2ea324c519c2b0e7674ed6ab6cedf032bba4c90d289c3de47814b033d3]]]Gradient clipping is effective for exploding gradients, and gated RNNs such as LSTM and GRU are effective for vanishing gradients.

[[[00000000000000000257---79af3cfa14e82aa8afe5fa6636a11a0e4820df1e3960d1936805cdb4c78fba0b]]]LSTM has three gates: input gate, forget gate, and output gate

[[[00000000000000000258---87a524a34e36c6f2e749ef8e73f701599db9f3ae8b9ce3ee7e7be322c2f278c8]]]Gates have dedicated weights and use the sigmoid function to output real numbers between 0.0 and 1.0

[[[00000000000000000259---f3dc4ee3f18fcdd668afb5c7f1a8b8cd1ca6d9e630292d3242291e51ef6f15ce]]]Techniques such as multiple LSTM layers, dropout, and weight sharing are effective for improving the language model.

[[[00000000000000000260---aa21f725ceb96b22a5fdfd95451b5299260806442aa07a361e12c8a9ad3bce23]]]RNN regularization is an important topic, and various Dropout-based methods have been proposed

[[[00000000000000000261---6763aa62211b44a20c71505eed18421683192c2f2f37668012e8aaa008367120]]]table of contents

[[[00000000000000000262---8436e0b0ef902ebd6da9f6f0ec21e99f841c4c2565715de6ee9ef693060fba69]]]　large door

[[[00000000000000000263---c4cf23daec95a26831302f10c2c2ac50a27c1900d0b8edb5d828fb23984bdae2]]]　credit

[[[00000000000000000264---a714161224255075961929dbba0d0997d0781ec435a517f55ecf5a7aee11f0e9]]]　Foreword

[[[00000000000000000265---34e370fd24c7c6c7ebc78ef248bcaa183f542be86ebe473d9a3d89ce7408b6c9]]]　Chapter 1 Neural Networks Review

[[[00000000000000000266---8add576cc147275e2663507992487a92862079a43428855043fbcdf2ecc0771e]]]　　1.1 Mathematics and Python review

[[[00000000000000000267---706335286d1e80c9484878882378e4d630a3a1a87eeb7850bbfd798274dd5261]]]　　　1.1.1 Vectors and Matrices

[[[00000000000000000268---e2643114275fd3fe5e880f8679e20756a609674617b4893ab2e28c37dddeb8e1]]]　　　1.1.2 Elementwise operations on matrices

[[[00000000000000000269---bb6e0a49f1b2d457bc5ca20235d675593f752c518584eb841b902834f24ed68a]]]　　　1.1.3 Broadcast

[[[00000000000000000270---f0e47d97e999d428917172571faa96008c755e0db97b9ce823fc4777459829c0]]]　　　1.1.4 Vector inner product and matrix product

[[[00000000000000000271---9f5b85abe4a164471d41228fd5fb90b9a4bbe7908171570ccb7f67f3305e8759]]]　　　1.1.5 Matrix shape check

[[[00000000000000000272---3c7a2ba8113ed466686de118c17d378805ccf6a869551264136ae27ca969a3af]]]　　1.2 Neural Network Inference

[[[00000000000000000273---d97c7c22bfe46232e09e2bf7acc4d4a7df0d5dcda6f3b652d1ac9ef94d2c497a]]]　　　1.2.1 Overall view of neural network inference

[[[00000000000000000274---0e6e53676a85a06f0bb5a8aa81e8ba63b5337f0af6f196edaee4ddb4b9d4a944]]]　　　1.2.2 Implementing Classification and Forward Propagation as Layers

[[[00000000000000000275---124782b765c41cbbf7d005a0b20be252dc8c9a85d1d7c60313e6955a08d4cc30]]]　　1.3 Training neural networks

[[[00000000000000000276---29572e62632a55d6f9c7c39e79fb9af1104856ee34bf7340ec1b014783659032]]]　　　1.3.1 Loss function

[[[00000000000000000277---63d54adef81ec11017010ec43d463252cc4d4490721afcf94aa4709bc9dbd9aa]]]　　　1.3.2 Derivatives and Gradients

[[[00000000000000000278---acdc4d68833e2aae7f1e4ed274c1fbd1d0c45027c7c12925ded7e314dd5645ce]]]　　　1.3.3 Chain Rules

[[[00000000000000000279---96cd2dd7725d6d9ecce1a2a3418f77ee7208a55dbe94339122c0d38ae8cb856e]]]　　　1.3.4 Computation graph

[[[00000000000000000280---13dc3d73fcc687906514b887e000ab8d7cc0294a6a91228cb90082a5f64b48dc]]]　　　1.3.5 Gradient derivation and backpropagation implementation

[[[00000000000000000281---6cc211e642b1003a978d3e10f3e22a3b295138524b02cc8db7b405a242e9ce4e]]]　　　1.3.6 Update weights

[[[00000000000000000282---bba5c808087b6aa8b1a96abee90f13de7208186879ba5bdddf2cee9dca96de15]]]　　1.4 Solving Problems with Neural Networks

[[[00000000000000000283---c63d8662588783c416ad30da20cd52fb737d0a6d874888ff15d9383a39ac657e]]]　　　1.4.1 Spiral dataset

[[[00000000000000000284---9e771990ff6490cc5f9309dcce406466d8f94ddb505b10e55dcdd813ba7b5185]]]　　　1.4.2 Neural network implementation

[[[00000000000000000285---19650f7c91ea140c4e7fe825f557caa7ba56ac5ee3e62fd941cc58bdb9a5e005]]]　　　1.4.3 Source code for learning

[[[00000000000000000286---c5de496b147849558cd02291a39454dcea10b61984302aaffcb2c277cdc540e2]]]　　　1.4.4 Trainer class

[[[00000000000000000287---da04c27389e19339df767981393dc63253d0ff1a2464c38b147d670e21af1178]]]　　1.5 Speeding up calculations

[[[00000000000000000288---ae3daf09f8c98010f28e789a128cc7f88086626eff508bda2670d47373356f45]]]　　　1.5.1 Bit precision

[[[00000000000000000289---ff1910a9f792a0584b6de42a82186714e8f92cae4499c12ee7e07dc5d51419e5]]]　　1.6 Summary

[[[00000000000000000290---af9fe5393fc52141728c343cd789741aa061ddece2ead41d1685e5670a04f729]]]　Chapter 2 Natural Language and Distributed Representation of Words

[[[00000000000000000291---3e5414ca9d261c1780628fb34ec5a4331a54e2d164b9e70accb7a5dfe1fafc90]]]　　2.1 What is natural language processing?

[[[00000000000000000292---f65a30a3f6da1bcd513e0d4350163342c03ff43e5d73ee922230ab2445cf51b8]]]　　　2.1.1 Word Meaning

[[[00000000000000000293---6d8eeab10bff86d221fa6ef1d873e4b06c617f5e7cea4d3357f3cd7734fa752f]]]　　2.2 Thesaurus

[[[00000000000000000294---3d2696bcda339f3f4bbc2cad30c04fc19b0893ebb9e3abccb9ef3c5302b65dbe]]]　　　2.2.2 Problems of the thesaurus

[[[00000000000000000295---0b6c17d6a272012089c930eee00feb0b6c694a8de888ce89fd92f3d6145e32d1]]]　　2.3 Count-based method

[[[00000000000000000296---37fae1d9010773e9274effe8a1ac2a6119569315c7297c1128666acb322aeefc]]]　　　2.3.1 Preparing a corpus with Python

[[[00000000000000000297---f1dace3a6e8f179891384874712db81f46a1ad898f489b99b0ce21c81ff27312]]]　　　2.3.2 Distributed Representation of Words

[[[00000000000000000298---03fd63e462080602a64127a51435e9b257a4f620e599d1335103e7b85e19ac10]]]　　　2.3.3 Distribution hypothesis

[[[00000000000000000299---0be643cddf32d4da945fb1c1d7ae386cfcc7d9535162fe676e7049cff31c8fbf]]]　　　2.3.4 Co-occurrence matrix

[[[00000000000000000300---956d2a07c6c26b499eee183160802ef176a8debf3bb9a488646f4a92a462c8d8]]]　　　2.3.5 Similarity between vectors

[[[00000000000000000301---fa20efced45bd3fac9a3898a6b9e12827d889f247a7e1c7c54f3aa72941a43ef]]]　　　2.3.6 Ranking display of similar words

[[[00000000000000000302---295aeeba4f09988366011576fc9077883da20d3f79514fb69cba8d70a8edaa66]]]　　2.4 Improving count-based methods

[[[00000000000000000303---f6155dd491506b94b2e79796ffea44a841d2254312380e13eaa2a867493a9216]]]　　　2.4.1 Mutual information

[[[00000000000000000304---f7ec5000caadfeb63d5df9af89d99fee5873566c8fdb877968402c52b64b4ce7]]]　　　2.4.2 Dimensionality Reduction

[[[00000000000000000305---d057d433554bdfe84c0bba1ad1c4249870aa0b58a999190ed71fa6939c9b7ad4]]]　　　2.4.3 Dimensionality Reduction with SVD

[[[00000000000000000306---6e54b7dd57cbefb621938039150429789aa4300fdb06387ea7b12094856f1e29]]]　　　2.4.4 PTB dataset

[[[00000000000000000307---8514b7e9638ce83773c9a231110eda384d2bb3435a65ac037e0250a44c5f2562]]]　　　2.4.5 Evaluation with PTB dataset

[[[00000000000000000308---ae0ab6766354c7b59938512b9ae8835e923b25f90b8d194270eadf7cebadd72a]]]　　2.5 Summary

[[[00000000000000000309---45431e4e29489400042ae85b16a8dfaeb898577f2787cef7676736cf01ce0ee5]]]　Chapter 3 word2vec

[[[00000000000000000310---d6aa20fd576d382b71fd04ec4c32a30a52d78fb0314444e1387a0909211f304e]]]　　3.1 Inference-based methods and neural networks

[[[00000000000000000311---31541ec42db773b4a1671afb4f78cb4a67e178c2a378d70b48cbdd18e23dbdfb]]]　　　3.1.1 Problems of the count-based method

[[[00000000000000000312---5ac0986917a630f0a163215f15f36ad38be4b9f6a3b427331adbf38e882279ce]]]　　　3.1.2 Overview of Inference-Based Techniques

[[[00000000000000000313---f280f8f9ab12efa04d8626fc10bad3153182f9fabb449cf672e6ec210dcdacc5]]]　　　3.1.3 How words are processed in neural networks

[[[00000000000000000314---78cb81bf76283fa12e9c6f62f93537336dc56516b6faf1872fb6167f1f473f5f]]]　　3.2 Simple word2vec

[[[00000000000000000315---af5cd4353b0210e5f4917fe3b001bfb11521c291a07bda8cb2776bcbeed4a2d5]]]　　　3.2.1 Inference processing of the CBOW model

[[[00000000000000000316---a09bae5815d060e943b09ac78897a1377d61d4014e4b0b50d31ec27014621f17]]]　　　3.2.2 Training the CBOW model

[[[00000000000000000317---3c4bb6494a89ca9279842913b87012b7a4d381d4e48dee6d1080d532ec385ae8]]]　　　3.2.3 Word2vec Weights and Distributed Representations

[[[00000000000000000318---0a60fc531d9dc57a1f42c114c60147a8d6a87803e11c3131dbd29d8816cd2cb2]]]　　3.3 Preparation of training data

[[[00000000000000000319---4c5e86bc19ba20ebd2e3f4e02455788fd1e1a9f5f3603a974eb88688e9d1656d]]]　　　3.3.1 Context and Target

[[[00000000000000000320---678c2677b1bb1de51edc90c38bc1243ae15f43d333298862e343785e57a6018d]]]　　　3.3.2 Conversion to one-hot representation

[[[00000000000000000321---1eeadc166b79166ddfc3616f18324e31112c670e0ff2df9475f08fd29281b528]]]　　3.4 Implementation of the CBOW model

[[[00000000000000000322---c9e9e892e69fbd624d06c168f1d48d3ff251b22ad7a5b68a461818c905bf3747]]]　　　3.4.1 Implementation of training code

[[[00000000000000000323---095be0be30b87fd12e9904b9a401cb953359caa86601f4d0c6829f540cd75d85]]]　　3.5 Supplement about word2vec

[[[00000000000000000324---14655ab68c30ab71a1f1f7500285c70728bdfdb6d2df74432fe7ffc433992733]]]　　　3.5.1 CBOW model and probability

[[[00000000000000000325---a69832133bd84efdabb3db3dc524dc5bf1e43aa44fd98eeda63b80c26f862aee]]]　　　3.5.2 skip-gram model

[[[00000000000000000326---2760bfadb8c88e87e8cc6e5fe20d9da254de8f41dff57b14142f96271073c830]]]　　　3.5.3 Count-based vs Inference-based

[[[00000000000000000327---eb1470deb618b8c50848fc501cd76cfaf2c022236de44babcc48446467413729]]]　　3.6 Summary

[[[00000000000000000328---a319e10d1883cd267ade68af957f8a910c1db87a2f252cd55870cb1b68d2d35c]]]　Chapter 4 Speeding up word2vec

[[[00000000000000000329---5c0d3fc69d9abf336f717f8314072c317b6f40937ff6b0ce07bca6cc1600511b]]]　　4.1 Improvement of word2vec ①

[[[00000000000000000330---ec8bb3fad2f83923a0552b7e88ac04b2c522342ce868f03f9af8210b2d5deec4]]]　　　4.1.1 Embedding layer

[[[00000000000000000331---4587632d9e5baaaf98c6f56f717174fc662fd9a02a6c44c330305ae247ba4b47]]]　　　4.1.2 Implementing the Embedding Layer

[[[00000000000000000332---8fa86aca077427e02734a719a38fec185960404db81e51d6a67bfaedefdaf104]]]　　4.2 Improvement of word2vec②

[[[00000000000000000333---26bd63345e458d4fb8f36066691fd7846d3975918f312456492ff2690282eb6c]]]　　　4.2.1 Problems of computation after the intermediate layer

[[[00000000000000000334---4f10fa28741c0e1bbc47abd91c881695f52dd0fc3a2c27363049bbc4d7e7bcc3]]]　　　4.2.2 From multi-class classification to binary classification

[[[00000000000000000335---503645936af46448a2adc8640dd66b4f596bb18820e9a49e1a7c98f41e3757d1]]]　　　4.2.3 Sigmoid function and cross-entropy error

[[[00000000000000000336---67f73fa94679e809826720e1e9b309a8ea8ef1c053aeff75e9c8f1744204f3ef]]]　　　4.2.4 From multi-class classification to binary classification (implementation)

[[[00000000000000000337---53182f8a4c8957ba17576e3102608ff7003746bfd3eb4f7c0de4b13a8c9f4e80]]]　　　4.2.6 Sampling method of Negative Sampling

[[[00000000000000000338---1acdd8afc6ea11e10f5711aae041617247fda98100982c07a91b63c651db4216]]]　　　4.2.7 Implementation of Negative Sampling

[[[00000000000000000339---271db7a81a7a7696e7d9e5ff69d857556362081b64960f77d07e67e5e106f62e]]]　　4.3 Learning improved word2vec

[[[00000000000000000340---2aeaea758cbbed772b1463fdddd6c427d0e51b9c7ee5151bd829f11eb98899c2]]]　　　4.3.1 Implementation of the CBOW model

[[[00000000000000000341---6326f7dbf91bc514b3356cc1765793703055b532e2ad3c79f954e53630942926]]]　　　4.3.2 Training code for CBOW model

[[[00000000000000000342---1e189635521177f904d0a932a7547137c081cf7bdaeb21d7134c43e5b508380b]]]　　　4.3.3 Evaluation of the CBOW model

[[[00000000000000000343---ffb0b14cfd6ac3913b6daad07c42aa18dc94d2b78df147c0e1b58ae89d527cc8]]]　　4.4 The rest of word2vec themes

[[[00000000000000000344---8fb75a516a1bb3896d2e6f26a97fd28bbad1254c57c3042aaa3364f2d1040c95]]]　　　4.4.1 Application example using word2vec

[[[00000000000000000345---a6c7b917a50f2686a627dbec07186dbd462ace4bc1321275514ae0ec62496ea4]]]　　　4.4.2 How to evaluate word vectors

[[[00000000000000000346---fed7b7fecc46989a1e9f47f0d4ffc95cffd33b4b0fbf121f66b70d0eb94da5bd]]]　　4.5 Summary

[[[00000000000000000347---82b1d8b50bba37fe835a3cbc47fd2c6d8745560aef2041057f19b0a6af41e39b]]]　Chapter 5 Recurrent Neural Networks (RNN)

[[[00000000000000000348---ca3c11512d3c6ca59a1a93020e5ac41b4ee9781306a4309fbcce1b35823d8b60]]]　　5.1 Probability and Language Models

[[[00000000000000000349---9458704cc89af277bdaf41fc14c0f54997b0d4798705f00960642f763ce86f1a]]]　　　5.1.1 Looking at word2vec from a probability perspective

[[[00000000000000000350---3e2dd29c5cbe534277a35eae3f92d6c39fc6262563798bd91ccc03be3619684d]]]　　　5.1.2 Language model

[[[00000000000000000351---997a0b8bc12bb2e8db02adad2ffc704b128f3a7c218684bf42e9cd9a8e32cd81]]]　　　5.1.3 CBOW model to language model?

[[[00000000000000000352---82d04f67c78204303d4a2047e00005d0c6b5d5a0f3f72046a008349eea378f9c]]]　　5.2 What are RNNs?

[[[00000000000000000353---af7a271d1ef0547e610161adfdeae247aabd0f9c5a83e69cbdb6c2e5aad6a3df]]]　　　5.2.1 Cyclic Neural Network

[[[00000000000000000354---a07d530a03182ac2acdb33593787922f34ec33898bb87a9e90ba32652ff043a9]]]　　　5.2.2 Unrolling loops

[[[00000000000000000355---857c01b02a552bd9af0151d502431722d0be9711c71493e3f41ee927d914dedb]]]　　　5.2.5 Mini-batch training of Truncated BPTT

[[[00000000000000000356---ac94e775316e97910f4ede8aaefd9ebd142be9267e8c94d2745021a41b5e46bf]]]　　5.3 RNN implementation

[[[00000000000000000357---ecdf9a060ebbddae1f967a9b1af504ae09d66670da22441da10d97da79bcbce0]]]　　　5.3.1 Implementation of RNN layer

[[[00000000000000000358---0169cdfd7ba0bebde8319164804a69a990dab5fce84b23e6e204426391bb71dc]]]　　　5.3.2 Implementation of Time RNN layer

[[[00000000000000000359---4f5c788aa8e95d5277d0df626c4acc80bbb34ca1765cd03df708a8585bd1e07f]]]　　5.4 Implementation of layers that handle time series data

[[[00000000000000000360---6afafd37ea761febc3932f616c911fa3bc6e968481c1010286a1cbcecfb07938]]]　　　5.4.1 Overview of RNNLM

[[[00000000000000000361---677984c021c3f3c78bf7b3f33ee44e7d8f18a5e48ebf234ecd2fdcba198663b5]]]　　　5.4.2 Implementation of the Time layer

[[[00000000000000000362---2889a930bfcc924b50362d4a6c111b360ebeb5d1f7b95f0de5153c58d95438b6]]]　　5.5 Training and Evaluation of RNNLM

[[[00000000000000000363---1e13f53b0457ac45d4c5809819fd2f4bd3f2f17f7fe52f189811db8f3a6c3e53]]]　　　5.5.1 Implementation of RNNLM

[[[00000000000000000364---78f45a1be5a5af7da3de7380bff605c664abb2f9f61f599dffe17f6fd08c2cf7]]]　　　5.5.2 Evaluation of language models

[[[00000000000000000365---33775ec4b198042f3e66a08fdec92a143b161a06bd8860071df09e9ddf588ece]]]　　　5.5.3 Training code for RNNLM

[[[00000000000000000366---8d2a453acb9e4a061e759135085492874133b9abe749d5d4f1e39c3fd02083ae]]]　　　5.5.4 Trainer class of RNNLM

[[[00000000000000000367---8d969011b3f261a966e4292f30aa359130f937178406e5c028980bc020ca46e1]]]　　5.6 Summary

[[[00000000000000000368---21beb13bc3e8cc16298596d16a98a9208bc4b7f75fea9568f40140d48eae98ea]]]　Chapter 6 Gated RNN

[[[00000000000000000369---7f3c14308e057e6c52c39011f6a3975bd4509009b97725f713cc2dbf72689b87]]]　　6.1 Problems of RNNs

[[[00000000000000000370---1bc848bd7e75212760adf3638baf526e2f219409669c391edff6875d88c37254]]]　　　6.1.1 RNN review

[[[00000000000000000371---05a4d834acc5b490971032f2fdc1a1a1827c12796434162926049b985d7b959c]]]　　　6.1.2 Vanishing or Exploding Gradients

[[[00000000000000000372---eeee2026a46e943b467e920627da16d304ee97703f51a13c243ef11b49e8591a]]]　　　6.1.3 Causes of vanishing or exploding gradients

[[[00000000000000000373---f6adb893c729689accb437be024f2fc7c6762a286d0b2c78ebf02af4bb0cdd6f]]]　　　6.1.4 Measures against Gradient Explosion

[[[00000000000000000374---661f4d12d69b3afa48e003a385e7614bde0e3bf73ec3a74675ad0743d1a20eb4]]]　　6.2 Vanishing Gradients and LSTM

[[[00000000000000000375---bd614057ce12b5f245ef45314a06f87d021e843903b368ea38cd12d8e42d356e]]]　　　6.2.1 LSTM interfaces

[[[00000000000000000376---9b4f92bda592140e832c51f6bc8ae66e9cf9d2a4b9aa02f3171948be90931472]]]　　　6.2.2 Assembling LSTM Layers

[[[00000000000000000377---f76cdba7030a53e8907bae36511041e2540cd4d2c34bb5ebed90240702f85698]]]　　　6.2.3 output gate

[[[00000000000000000378---5f472f365e5ff737f8d1199e53f4e4ea94a5a406886ed4092a9d48e623c4fec0]]]　　　6.2.4 forget gate

[[[00000000000000000379---7bfff52bbdd4afa9c74facc0678aec5691fe86c361e347a5b9ce0834b7eecf7d]]]　　　6.2.5 New storage cells

[[[00000000000000000380---020ca2bb30b77f71e3485f52517dfed5d41bd56fe989a4ec0e8755a9c7569eef]]]　　　6.2.6 input gate

[[[00000000000000000381---35f1201f9cb99ef35fef31915c6d8a120729751b91ab78559043ced52e7f84a9]]]　　　6.2.7 LSTM gradient flow

[[[00000000000000000382---fbd655dd5ddb12fbc94e9e8230366b5060a77992485477f1074b5ebc81e6dc62]]]　　6.3 LSTM implementation

[[[00000000000000000383---796e3d9d49a3730af758a57d76b6973142cd516fe35b6a9bdb2839e3308f4343]]]　　　6.3.1 Implementation of TimeLSTM

[[[00000000000000000384---8aeaffe406a72059b055513a829bd624019d7a73b97fbacf89785762cfe1484b]]]　　6.4 Language model using LSTM

[[[00000000000000000385---3980389f9e697ba6e9da873bb9e8571c1ecef0a6258b86399a6d5e3dda2b6423]]]　　6.5 Further Improvements to RNNLM

[[[00000000000000000386---fab03f4a0a9af5c8c8eb170591279616ae757dd899f0191474526ebb79df4030]]]　　　6.5.1 Multiple LSTM layers

[[[00000000000000000387---0ad0d22ce20e745f0ac2d89d1dede8738abaffa1b34802724679208bfd0321e7]]]　　　6.5.2 Suppression of overfitting by Dropout

[[[00000000000000000388---992055fcf73828058ca7fbcd65eb87266256c45cd428089c6fd3eb1e76468d40]]]　　　6.5.3 Weight sharing

[[[00000000000000000389---5107fdbd5ab6ba461921b455f8d9c7ab7f0020324c13d32dd5af0711d016f961]]]　　　6.5.4 Better RNNLM implementation

[[[00000000000000000390---d8dfda7b4c25e25c0e3ccfc29b84fd51a854eeae363e4d5d7c854ab9817ae027]]]　　　6.5.5 To the cutting edge research

[[[00000000000000000391---bd6b7f11ef97f57139e54084fce81459a867cbfd6f4e912773a73333286fccc4]]]　　6.6 Summary

[[[00000000000000000392---494bb0856f981f02aaa0de0329efa9ff8d185e5df31e3395728084d786a84250]]]　Chapter 7 Sentence generation by RNN

[[[00000000000000000393---373c3faa052339ff1454eedceae03f1ca608373404f3234c4bfcfc14e23ec000]]]　　7.1 Sentence generation using language model

[[[00000000000000000394---b49cad8f89e4adde0053247ba906191e510232ee6864c21522d6b62aaf479e0e]]]　　　7.1.1 Procedure of sentence generation by RNN

[[[00000000000000000395---bf4ab3694542aeaa47f0c41510ca3cddd8cf2496d558cf8548028682bda0b362]]]　　　7.1.2 Implementation of sentence generation

[[[00000000000000000396---bc79c878426a36d5f7b25931b246b68a4c01586132c0ce70b99f37c65b46e9a2]]]　　　7.1.3 Towards better writing

[[[00000000000000000397---82112a533c839e32806f0c68fbe8666fb1732593423400a615ff8a09c7338f03]]]　　　7.2.1 Principle of seq2seq

[[[00000000000000000398---a22322159df3d3845e01be66348374c55aac85e34d6b3ffd8b402e71532bc96a]]]　　　7.2.2 Toy Problems for Transforming Time Series Data

[[[00000000000000000399---ca1baaa2658f5f7b5d11d404caf1437b035fb7272a43df838f25bba3395a68ef]]]　　　7.2.3 Variable length time series data

[[[00000000000000000400---0b94a34cada5dd24aad429719971166d3ce43c32786cee694032e72323b5f3ac]]]　　　7.2.4 Addition dataset

[[[00000000000000000401---7a687682eb7af3b9423ac62dff13c6be7716ac2f42f2e1c1c59bc01f78c1657e]]]　　7.3 Implementation of seq2seq

[[[00000000000000000402---86fbf6914aed38e4bab5d6ee20344e211c6d38512b42a5fad3359422a8659494]]]　　　7.3.1 Encoder class

[[[00000000000000000403---a6c08dd250df37660c6c3622ab301a4efd10143c8579f638011acafda09831e4]]]　　　7.3.2 Decoder class

[[[00000000000000000404---40aef2c74cf7c9c4b52bf194a10ddd43b71c5d38feb14c76653c1da775394ba5]]]　　　7.3.3 Seq2seq class

[[[00000000000000000405---0f2fb5b532c902e7cf1547cb9a0d3f8f14590d5b50225237a976b8a8f9269980]]]　　　7.3.4 Evaluation of seq2seq

[[[00000000000000000406---936a4207f3ae0d33125998c7b18c731acef504a0d5a09e9a3914b94948ab3f78]]]　　7.4 Improving seq2seq

[[[00000000000000000407---dffb2d48f677df8387599ccf2c4789ceff1be0afb9b566b6eae9a2441e33d3c5]]]　　　7.4.1 Reversing input data (Reverse)

[[[00000000000000000408---6d1eccebc5e7827df5ead3f843d0c0e998d93e012ca971616c14b6754a6ac8fd]]]　　　7.4.2 Peeky

[[[00000000000000000409---c5d4b1696424e99e591aa396b6bfee955243346f88e8ca7418a298b578ca226b]]]　　7.5 Applications using seq2seq

[[[00000000000000000410---7d5b9428ad179b456264df56281ac1156821c86a719064d3e1900b2ba358ff28]]]　　　7.5.1 Chatbot

[[[00000000000000000411---dab33ef57a592054eb8bdfa7ca6728feea3b7afb2354099f54b666a988b77dd0]]]　　　7.5.2 Learning Algorithms

[[[00000000000000000412---410b1402b1b116a3166e79b42dac5d0c28f42b6801748b24740f57ccbdb0ff76]]]　　　7.5.3 Image Caption

[[[00000000000000000413---b7b481b38a049b690afc0fb4c84b879f968ded875a1548887c967d9cfe6881ca]]]　　7.6 Summary

[[[00000000000000000414---3a846ecc4304927f067af2b91f4592b3ebc6f25d125965a6c3801e41406e6f8c]]]　Chapter 8 Attention

[[[00000000000000000415---c7f3cd8a5401e09cdd7a91dd89de94383dedd090d25f592d0be572ba14c5e9f8]]]　　8.1 Mechanism of Attention

[[[00000000000000000416---7d361e01a751e8f8555cd7354f3208f8b4327bd1cf43ad01020e93fd4d352ca8]]]　　　8.1.1 Problems with seq2seq

[[[00000000000000000417---58eea65297d959e6f9877e8b6e79bb6e3606d154d86072c5339ba8748b496b79]]]　　　8.1.2 Improved Encoders

[[[00000000000000000418---cac46716492b68dee27324b8a8696a678deeb8b8e25cb5949445ff78c3405e3d]]]　　　8.1.3 Improvement of Decoder (1)

[[[00000000000000000419---bda13332f523c97cf18d11d73c604641ac4772c51b10b412fb16d8077ad64ad4]]]　　　8.1.4 Improvement of Decoder ②

[[[00000000000000000420---96624800916f4bfc2635f2b003bc7dd66e4e2462990f776db3804b19fd0dc2e9]]]　　　8.1.5 Improvement of Decoder (3)

[[[00000000000000000421---39befe8b6f796bd78a523320f4024f91cb7d15670b9a12856c81de466d172d96]]]　　8.2 Implementation of seq2seq with Attention

[[[00000000000000000422---969bafd524a1007a154388dd5b707ed12f3a03a549c9a127b315293b328533b0]]]　　　8.2.1 Encoder implementation

[[[00000000000000000423---88797b82fbc86bf18c60dc5eaf315c76ccb60be0987f70cf1fc53fb612116935]]]　　　8.2.2 Implementation of Decoder

[[[00000000000000000424---9bd0432b95db3eeb4e0bc999975a80d8de394032723aa75dcf447463aa189bcf]]]　　　8.2.3 Implementation of seq2seq

[[[00000000000000000425---c2d07cf27142019a61e8e3de857e8041d2618f97147a85d58c2f6d2a2c4715d2]]]　　8.3 Evaluation of attention

[[[00000000000000000426---3de0f480b0e385498e6caaac5eeaeb88f7cc53c391457e5da8d2eaac06ca1d6d]]]　　　8.3.1 Date Format Conversion Issues

[[[00000000000000000427---661a88be6d87c8c4c168af48de74594e1d3d0b871ee4be50fb4d7f3aa664a633]]]　　　8.3.2 Learning seq2seq with Attention

[[[00000000000000000428---7bbea3e3bfeb8171dcefddab536e71d6d6fa4f08aac39672a9508be4815413ac]]]　　　8.3.3 Attention visualization

[[[00000000000000000429---e113a4f56d3d7d259e5d517586e6bb1a2c25edae2c14e8b8d99263e2357b0575]]]　　8.4 Remaining topics on Attention

[[[00000000000000000430---089875a4496df2ed6a0e58dcab743cf1c2d2500911ed68f8ef71b5e854abc4b4]]]　　　8.4.1 Bidirectional RNN

[[[00000000000000000431---2c6d81b40579f087c1d0cedfd8d914b3897706fd113ef6e9ddf57a10a5153c8b]]]　　　8.4.2 Usage of Attention Layer

[[[00000000000000000432---e0652a3bc1e0b2972ced06e710ed5c6bfefe3ed13a51820e710976922dde1de8]]]　　　8.4.3 seq2seq deepening and skip connection

[[[00000000000000000433---475c099160c070a87a6de9df2101e1c748f442d8a177ce6c8560c886f0e30c82]]]　　8.5 Application of Attention

[[[00000000000000000434---163b44d324a4f96a7d26f3a401651d9ee1bf47459e2be6ff9e72d7b7db71de66]]]　　8.6 Summary

[[[00000000000000000435---9013ab1989d3834d8ee1bcd8bca01044bbd815e04612e55fbaa55969c1e6fcd2]]]　Appendix A Differentiation of sigmoid and tanh functions

[[[00000000000000000436---4be7ecd4078a851c81f331b8e2fe4cfb7e6c28dfe3cedbf3977d7d2e8f1ad8e4]]]　　A.1 sigmoid function

[[[00000000000000000437---3baeac11482c4783f881c34fc2b0c2cee3c73015642f523fa4d8e024c90b9b14]]]　　A.2 tanh function

[[[00000000000000000438---f8b983cb70d98b65b764061746464bcc5b884f4c6eeb93bfa40928555da8566d]]]　　A.3 Summary

[[[00000000000000000439---f0277ff1f9f1758be2069bf26d9ca351817c71d85ae10a04d5e3d5d35980653c]]]　Appendix B Running WordNet

[[[00000000000000000440---5abf70426d93542a2a2399e93c4d6335775ba3a333cc88df7d4f56233b951b99]]]　　B.1 Installing NLTK

[[[00000000000000000441---3dbaa77b95457981ad7a82e26d2e619f8e45825d63e229df87cff7b846a6058b]]]　　B.2 Getting synonyms with WordNet

[[[00000000000000000442---e36593de71fccafd3480af9e6426f87ff2d2265c52912cb4ffc8664fc14064db]]]　　B.3 WordNet and word networks

[[[00000000000000000443---b0604330d3575c4b15adb749658dad45c4a12ebce607038e9135c0dd498702eb]]]　　B.4 WordNet Semantic Similarity

[[[00000000000000000444---fd2134c0f46f5537b10c9f30dd7e1f237ab9a1b6135ad6c939a437fa0cb08166]]]　Appendix C GRUs

[[[00000000000000000445---947c361a33e58a8f869b5b75ac316bf427870ad7f57369a394302ff1fa8b676a]]]　　C.1 GRU Interface

[[[00000000000000000446---45cd993d638a2f10bf809890d866dbdd086dbb5c09e2b43ea04dc5d508d02440]]]　　C.2 GRU Computational Graph

[[[00000000000000000447---011a6f34bca1b3aaaf230e390693587d87b64453210ac759a29c1b071eec532c]]]　in conclusion

[[[00000000000000000448---7d1b2ce73e1cc85cd8553303b8d164b9926a375fcfcc2b2f9c962f6f08260cd6]]]　References

[[[00000000000000000449---78c748b750c5565a6695bfce45017c02cb814212b08090570bb3e600dcbfc48f]]]　About the author

[[[00000000000000000450---cbd795697fe2923c1bfa79339686212ecf39db44832ae3daabbbe8854954d406]]]　colophon

[[[00000000000000000451---f38d5404a52e44f974500f76196c1d802178e13b6617ddbd703916329e976cb1]]]Chapter 4

[[[00000000000000000452---147da4a65d375590bcaa6bcfa9d7cdba629cd7eb76eef0063df0c2cceb1df7d9]]]speeding up word2vec

[[[00000000000000000453---44409dc2e305fe2c28f516e716037162a3da1de24bf6664ab06ad2b93875f3f9]]]Don&#39;t struggle to know everything.

[[[00000000000000000454---8f1a055bb6c03fd48adbd43835ad7548c2f3cf0c3bf6db651bff14022c4370b4]]]Otherwise you won&#39;t remember anything.

[[[00000000000000000455---12e28e349416bee5df1f74bf7a04a47f10b307b23c5e0a6c01cc93ba3cf65252]]]—— Democritus (ancient Greek philosopher)

[[[00000000000000000456---8d4fdf005516e26480fa59bb11cae8e51eaaf3bdcd62ea6a90d8f466b462652f]]]We learned how word2vec works in the previous chapter and implemented the CBOW model. Since the CBOW model was a simple two-layer neural network, it was easy to implement. However, there are some problems with its implementation. A major problem is that the amount of computation increases as the number of vocabularies handled in the corpus increases. In fact, once we reach a certain vocabulary size, the CBOW model from the previous chapter takes too much time to compute.

[[[00000000000000000457---68c457f1afab1fda6a9d4d8112e1166a58d8dffd383db2ed00bd82b88291bd29]]]Therefore, in this chapter, we will focus on speeding up word2vec and work on improving word2vec. Specifically, we will add two improvements to the simple word2vec in the previous chapter. The first improvement is the introduction of a new layer called the Embedding layer. And as another improvement, we introduce a new loss function called Negative Sampling. This allows us to complete the &quot;real&quot; word2vec. When the real word2vec is completed, it will be trained on the PTB dataset (a corpus of practical size). And I would like to actually evaluate the goodness of the distributed representation of that word.

[[[00000000000000000458---a9ac1c3f079b10aaa1fbf38b11d15d9e7cd5f59dcd9df168265d95c98566cb0c]]]Improvement of word2vec ①

[[[00000000000000000459---73fe5861df0f92466e6e957b7d9721fe72dc03cc8dab7056cfbc8cf710c0436f]]]Let&#39;s start by reviewing the previous chapter. In the previous chapter, we implemented the CBOW model represented in Figure 4-1.

[[[00000000000000000460---3cf8ab6e32fff6de6ee0db0aceef4258f62810a40e70bc049b6ace7f5a51525b]]]
Figure 4-1 CBOW model implemented in the previous chapter


[[[00000000000000000461---a2fdd3beb8e0bf08d2bfab87fc5739f8d4b540f0ce699c1303cbc3fc4db8297f]]]As shown in Figure 4-1, the CBOW model in the previous chapter processes two words as contexts and guesses one word (target) based on them. At this time, the middle layer is calculated by multiplying the matrix with the weight on the input side (Win), and the score for each word is obtained by multiplying the matrix with the weight on the output side (Wout). That score was then run through a softmax function to get the probability of occurrence of each word, which was then compared to the correct label—more precisely, by applying the cross-entropy error—to find the loss. .

[[[00000000000000000462---7fd3c91cdf467bfb0d3c86a96394d22c2414bcc43c064f3d31b841d5fc1b5d8f]]]In the previous chapter, we limited the context window size to 1. This meant that we used only the words before and after the target as context. Later in this chapter, we will add the ability to handle contexts of arbitrary size.

[[[00000000000000000463---7c89a1fa0b61015cb380c80f394588e86cb503d12c13ad4a1002abda1bf01fc8]]]The CBOW model in Figure 4-1 has no particular problems when dealing with small corpora. In fact, the total number of vocabularies handled in Figure 4-1 was 7, but that scale can be handled without any problems. However, some problems arise when dealing with huge corpora. To point out the problem, here, as an example, I would like to consider a CBOW model in which the number of vocabularies is 1 million and the number of neurons in the middle layer is 100. The processing performed by word2vec at this time is as follows.

[[[00000000000000000464---948ee43281666d45bccb6ab416677d86669fbba7231978e82dd41234ecb8b62d]]]
Figure 4-2 CBOW model assuming 1 million vocabularies


[[[00000000000000000465---1106bd1e3908808ddb8f47ccd720a341b664c9bdbe990767fb27cd95fce401e1]]]As Figure 4-2 shows, there are 1 million neurons in the input and output layers. This huge neuron takes a lot of time in intermediate calculations. Specifically, at this time, the following two calculations become bottlenecks.

[[[00000000000000000466---03ac54130aa27c6f156ffeb1ebcd8f85c7826da1050b7f13c40eadda52d3bf08]]]Computation by product of one-hot representation of input layer and weight matrix Win (solved in Section 4.1)

[[[00000000000000000467---d1c6ed8e24c8c537096adf353c6868ddf829e58c2138ed8ed3e153e9799aea68]]]Product of hidden layer and weight matrix Wout and calculation of Softmax layer (solved in Section 4.2)

[[[00000000000000000468---cb4eaa1cf908e5df0439cc421290a94f98ed8806fe8a42f08c0e8b3342b1e5b3]]]The first problem concerns the one-hot representation of the input layer. This is because words are treated as one-hot expressions, so the size of the one-hot expression vector increases as the number of vocabulary increases. For example, if you have a vocabulary of 1 million, the one-hot representation alone will require a memory size of 1 million elements. Furthermore, we need to calculate the product of the one-hot vector and the weight matrix Win, which alone requires a lot of computational resources. This problem is solved by introducing the new Embedding layer in section 4.1.

[[[00000000000000000469---e00c03354d65bf69e5aa6eea7bdd9a4ff57396b7e830068fb4be209a0e2f70e8]]]The second problem is the calculation after the middle layer. First of all, many calculations are required for the product of the hidden layer and the weight matrix Wout. Also, in the areas related to the Softmax layer, the problem is that the amount of computation increases as the number of vocabularies to be handled increases. We solve this problem in Section 4.2 by introducing a new loss function called Negative Sampling. Now, let&#39;s make improvements to eliminate these two bottlenecks.

[[[00000000000000000470---faf1141594e3f3855bc2b7ee3a853c766d118feadfcc555a610f4d6cdaca9e45]]]The file before improvement (implementation of word2vec in the previous chapter) is in simple_cbow.py (or simple_skip_gram.py) in the ch03 directory. The improved word2vec file is in cbow.py (or skip_gram.py) in the ch04 directory.

[[[00000000000000000471---160b00c5aef0c267ea12d4800efb7796b9695012f130cedd228ae6ce31257fbe]]]Embedding layer

[[[00000000000000000472---38b198fef35298d47a38abb4b5675ca36050a36dd04585421008c99caf279977]]]In the implementation of word2vec in the previous chapter, we converted words into one-hot representations. We then fed it into the MatMul layer, where we multiplied the one-hot vector with the weight matrix. Now imagine a case where the vocabulary of words is 1 million. At this time, if the number of neurons in the hidden layer is 100, the matrix product in the MatMul layer can be written as shown in Figure 4-3.

[[[00000000000000000473---481c3b47aa17c5397973cda8e00598f7385eb407e280fb768b1b3800ea52f883]]]
Figure 4-3 Calculation by product of context (one-hot representation) and weight of MatMul layer


[[[00000000000000000474---1aba0f9f45a50eeca4454b705ebe5c8c0961c7636b144cc41c6403578b68dbc8]]]As shown in Figure 4-3, if we have a corpus with a vocabulary of 1 million words, the one-hot representations of the words will also have a dimensionality of 1 million. And we need to multiply such a huge vector by the weight matrix. But what we&#39;re doing in Figure 4-3 is simply extracting a particular row of the matrix. Therefore, if you think honestly, it seems that conversion to one-hot representation and matrix multiplication in the MatMul layer are unnecessary.

[[[00000000000000000475---54572483767c2d2e2e26f55612a9c8f235672886701c8c7489d0a7a2366c78eb]]]Let&#39;s create a layer to extract &quot;rows (vectors) corresponding to word IDs&quot; from the weight parameter. For now, let&#39;s call that layer the Embedding layer. Incidentally, embedding comes from the term word embedding. In other words, word embeddings (distributed representation) are stored in this Embedding layer.

[[[00000000000000000476---154655472fdb2967533f0848f996de1a3434d133d0f061ddfbe9521cc33c6443]]]In the field of natural language processing, dense vector representations of words are called word embeddings or distributed representations of words. As a minor difference, word vectors obtained by count-based methods have been called distributional representations, and word vectors obtained by inference-based methods using neural networks have been called distributed representations in the past. However, in Japanese, both are translated as &quot;distributed representation&quot;.

[[[00000000000000000477---8ca498cd9d770601d4fbc5e08951037c80cfb0ed42304d1b5b7dcf993d32ce90]]]Embedding layer implementation

[[[00000000000000000478---4c16770343cb341b6f3753b668363803500e6d7bd91009305a9245b01e127d27]]]Extracting rows from a matrix is easy. For example here let&#39;s say the weights W are a NumPy 2D array. Then, to extract a particular row from this weight, simply write W[2], W[5], and so on. When actually written in Python, it looks like this:

[[[00000000000000000479---8d5aeae7f947230192b4c4117916cdc83f8f4899023da2fb2a16455a257c3423]]]It is also easy to extract multiple rows from the weight W at once. All you have to do is specify the row number by means of an array. In practice, it looks like this:

[[[00000000000000000480---751c9bacd45a106a5bc7f0057450903e84b5bca6d4a24fc9a04d767c13ee9309]]]In this example, 4 indices (1st, 0th, 3rd, 0th) are extracted together. By giving an array to the argument, you can extract multiple lines at once. By the way, this is an implementation assuming mini-batch processing.

[[[00000000000000000481---ae42bf0db7b10b7f223fbddadaeddcdca635f7f8863c9ac64b8b3a9daf79e308]]]Now let&#39;s implement the embedding layer&#39;s forward() method. Given the examples so far, the implementation looks like this (☞ common/layers.py).

[[[00000000000000000482---3aedca72a7a483acaaa02473e64445c42f81162964c98e21923456d02b1804d7]]]Use params and grads as member variables according to the implementation rules of this document. Also, store the index (word ID) of the line to be extracted as an array in the member variable idx.

[[[00000000000000000483---814d2f57de94672624c429d5017d55135015582b626939e94189a327eb203403]]]Next, consider backpropagation. Forward propagation of the embedding layer only pulled out certain rows of weight W. This simply allowed the neurons in a particular row of weights to flow—untouched—to the next layer. Therefore, in backpropagation, the gradient transmitted from the previous layer (output side layer) is simply passed to the next layer (input side layer) as it is. However, we want the gradient coming from the previous layer to be set to a specific row (idx) of the gradient dW of the weights. This can be represented graphically as shown in Figure 4-4.

[[[00000000000000000484---2f3b00d01e86092b3a25284006f754a262088caf2b2cab6557f44e174cdc96c7]]]
Figure 4-4 Overview of forward and backward processing of the embedding layer (embedding layer is denoted as Embed)


[[[00000000000000000485---a397326b082a1ff009f5d0f813c753f30b89316ce4b4cc5851e4bad19f885d8d]]]Based on the above, let&#39;s implement backward(). This could be written like this:

[[[00000000000000000486---226df70e0ea77c432791e6cc178fd708bc61d62b4cd2539b2cb560b96d22d113]]]# actually a bad example

[[[00000000000000000487---45b1b9e3e9853e6cdcbdb01a63c60eff4229641a7a302924857b3f98f49de3f4]]]Here we take the gradient of the weights as dW and overwrite the elements of dW with 0 with dW[...] = 0 (instead of making dW 0, we keep the shape of dW and replace the elements with 0). Then, the gradient dout transmitted from the previous layer is assigned to the row specified by idx.

[[[00000000000000000488---cb123b95c196332664c67f3348a8cd22675f43922c275e859af76f013c962c43]]]Here we created a matrix dW of the same size as the weights W and put the gradients in the corresponding rows of dW. However, since what we ultimately want to do is update the weights W, we don&#39;t need to bother creating a matrix like dW (which is the same size as W). Instead, if you keep the row number (idx) you want to update and its gradient (dout), you can use that information to update only that particular row with weight (W). However, in this case, I thought about using it in combination with the already implemented update class (Optimizer), so I implemented it like this.

[[[00000000000000000489---d3cb869cc549a6fa8a0e6272727cc15305428bd1cd0f113917f6aa52b7a8370b]]]Now, there is actually one problem with the previous implementation of backward(). That problem arises when elements in idx are duplicated. For example, let&#39;s say your idx is like [0, 2, 0, 4]. At this time, the problem shown in Figure 4-5 occurs.

[[[00000000000000000490---88c9cae631aaa0b9a6fea11a89282743eabe1d506215eb6c4f37665580af69e8]]]
Figure 4-5 If the idx array elements have the same row number, just substituting the dh row causes problems


[[[00000000000000000491---26ec0979b400ab25be43cc07752257d24daf73a605333eaa42729525d7fa5c76]]]Try substituting the value of each row of dh into the location specified by idx, as shown in Figure 4-5. So in this case, the 0th row of dW will be assigned two values. Either value will be overwritten.

[[[00000000000000000492---45d53f362e1c5503fe0aecfcf710cc4c83086affa495281a6949e781ea34ae18]]]To deal with this duplication problem, we need to do &quot;addition&quot; instead of &quot;assignment&quot; (let&#39;s think about why we do addition). That is, each row of dh is added to the corresponding row of dW. So here&#39;s the correct backpropagation implementation:

[[[00000000000000000493---525d3a2d581ef845c4d0f5599ffbc4f2d664cac71677b5c4f62ca19d645a5fbd]]]# or

[[[00000000000000000494---518fb53e0d1b85978a19b9f87dd47debcdc691d666a95f26a25570c6026b3174]]]Here we use a for statement to add the gradient to the appropriate index. Now even if there are duplicate indexes in idx it will be handled correctly. Note that the implementation using the for statement here can also be done with NumPy&#39;s np.add.at(). np.add.at(A, idx, B) adds B to A, where idx specifies the row of A to add.

[[[00000000000000000495---a28d9a51357e651780738b9bf4121ebedfa4015c98899649cfecf0b4ead67f9d]]]In general, using NumPy&#39;s built-in methods is faster than using the for statement in Python. This is because NumPy methods are tuned for speed and efficiency in the lower layers. Therefore, in the source code above, using np.add.at() is much more efficient than using the for statement.

[[[00000000000000000496---75351459d15e932264994420a24df8204e8ee15d74a08c1778425843e0d8b84c]]]This completes the implementation of the Embedding layer. Now in the word2vec (CBOW model) implementation, we can switch the input side MatMul layer to Embedding layer. This reduces memory usage and eliminates unnecessary calculations!

[[[00000000000000000497---ed5f36ea76e66d51e15109012c1ef202fe18e17d608f43ac5b98e4ba3aea0730]]]Improvement of word2vec②

[[[00000000000000000498---6d066331dd91690cc4d26d52bcecfde1a3c75972f45e78873eefa1181abaf52b]]]Next, we will work on the second improvement of word2vec. As I said before, the remaining bottleneck is post-hidden processing—matrix multiplication and softmax layer computation. Eliminating this bottleneck is the goal of this section. As a solution here, we use a technique called Negative Sampling. By using Negative Sampling instead of Softmax, the amount of computation can be kept small and constant no matter how large the vocabulary is.

[[[00000000000000000499---b7112b1387848f69f998140e141761ebb7b5b2a30d5a8bee0d9def6a0a2342d7]]]The story in this section is somewhat complicated. In particular, the mounting surface becomes somewhat complicated. Therefore, here, I would like to proceed step by step while checking one by one.

[[[00000000000000000500---d80829c74098ca079292bbbefef0964d18eb9a52411bb1f15b3d6482410c7430]]]Problems of computation after the middle layer

[[[00000000000000000501---61d7110cec5ae6fe3b32c1a3c5b3a43375b6331783bd56dd712888f52ed6513b]]]To point out the problem of calculation after the middle layer, let us consider word2vec (CBOW model) with 1 million vocabulary and 100 neurons in the middle layer as in the previous section. The processing performed by word2vec at this time is as follows.

[[[00000000000000000502---d0d8019297f624f016a1a35a0fae0d79053393d66432e1931648c076407078f8]]]
Figure 4-6 word2vec assuming a vocabulary of 1 million: &quot;you&quot; and &quot;goodbye&quot; as contexts, and &quot;say&quot; as the target (word to be predicted)


[[[00000000000000000503---20df83e9ecb2f261a521ab3329be76ff7f765f4afe8c94833a4e87462906e9da]]]As Figure 4-6 shows, there are 1 million neurons in the input and output layers. By introducing the Embedding layer in the previous section, we were able to eliminate waste in the computation of the input layer. The remaining problem is processing from the middle layer onwards. This requires a lot of computation time in two places:

[[[00000000000000000504---d6b893956bc7cb6d97c03d6a9cf42dec98b902756c037d500a948753478c614a]]]Product of middle layer neurons and weight matrix (Wout)

[[[00000000000000000505---d3d40adc86a2206fdb105070e2874db72a69e81077e535c65026ad0ba2f9794f]]]Softmax layer calculation

[[[00000000000000000506---ac57b44cb2f6a6160a9489788788a6932cc1a3964c9c8fbf28738a76946a7803]]]The first problem is about multiplication of large matrices. In the example above, the size of the hidden layer vector would be 100 and the size of the weight matrix would be 100×1 million, but computing such a huge matrix product would take a lot of time (and a lot of memory). In addition, similar calculations need to be performed during backpropagation, so the calculation of the matrix product should be &quot;light&quot;.

[[[00000000000000000507---9bc86fe1445e4712a70f427fef78e2d5e690e9a2bb05f7120c456b7a0a8c2c86]]]The same problem occurs for the second Softmax. In other words, as the number of vocabularies increases, the computational complexity of Softmax also increases. This will become clearer if we look at the following Softmax formula:

[[[00000000000000000508---311bade9ce2412f5a95bd80186be92a47c115b15b1e1a2a847e480b1e570291d]]]Formula (4.1) is the softmax formula for the k-th element (word) (score elements are s1, s2, ...). Here, we assume that the number of vocabulary is 1 million, so the calculation of the denominator of formula (4.1) requires calculation of exp 1 million times. Since this calculation also increases in proportion to the number of vocabulary, a &quot;light&quot; calculation that replaces Softmax is required.

[[[00000000000000000509---aee60f15fc97d170734b199e3df624a830c12ff021a4c1164f48b55d7df9c20d]]]From multi-class classification to binary classification

[[[00000000000000000510---cee3d6f9db8df3f998aaa90429864c83f1f2eb49426a0d4dc4283ab5ea248836]]]From now on, I will explain a technique called negative sampling. The key idea of this method is &quot;binary classification&quot;. More precisely, approximating “multi-class classification” with “binary classification”—this is an important point in understanding negative sampling.

[[[00000000000000000511---74acdba2134e024636f3c8eb34a967e8fa5464297f2d1953c46bf4a60c6b53a4]]]So far we have dealt with the problem of &quot;multi-class classification&quot;. In our previous example, we thought of it as a problem of choosing the correct word out of a million. So, can such a problem be treated as a &quot;binary classification&quot; problem? More precisely, can&#39;t we approximate the &quot;multiclass classification&quot; problem with the &quot;binary classification&quot; problem?

[[[00000000000000000512---4389bd382356a5d1689a80e0ff4acba64a82bd20cae92f18c2e7238325aac7d5]]]Binary classification deals with yes/no questions. For example, questions such as &quot;Is this number 7?&quot;, &quot;Is this a cat?&quot;, and &quot;Is the target word &#39;say&#39;?&quot; are problems addressed by binary classification. All such questions can be answered with a yes/no answer.

[[[00000000000000000513---37fd615237eaf07cfc72672f549e2a6341534dd4095b13e09a31b76af5edf73b]]]What we&#39;ve done so far is to make it possible to guess the correct word with a high degree of probability given the context. For example, we trained a neural network to increase the probability of the correct word &quot;say&quot; given &quot;you&quot; and &quot;goodbye&quot; as context. And if trained well, the neural network will be able to make correct guesses. In other words, the neural network can give the correct answer to the question, &quot;What are the target words when the context is &#39;you&#39; and &#39;goodbye&#39;?&quot;

[[[00000000000000000514---f0935a73f27d4dcccdd6d68c3f786fb08bca5f358ed88ff0eb67fa60745f34b5]]]What we should think about here is to solve the problem of &quot;multi-class classification&quot; as &quot;binary classification&quot;. To do this, think of questions that can be answered with “yes/no”. For example, consider a neural network that answers the question, &quot;Is the target word &#39;say&#39; when the context is &#39;you&#39; and &#39;goodbye&#39;?&quot; In such a case, it is sufficient to prepare only one neuron in the output layer. You can think of the neurons in that output layer as outputting a score that is &quot;say&quot;.

[[[00000000000000000515---05fe6b269176884e271d53a7f5b3c6e56f762d53fe6768e93d60a4a896826bce]]]So what does the CBOW model do in this case? If you actually represent it graphically, it can be written as shown in Figure 4-7.

[[[00000000000000000516---aa67148ec4a07c1d1e319aa5d07d90f64fade9551ec1118fa6725b18b0c723b5]]]
Figure 4-7 Neural network for obtaining scores only for target words


[[[00000000000000000517---64be8482ccd06a923ecac78abc763ae90ab775ab3cc14aa90d6bf06b3a857094]]]The output layer has only one neuron, as shown in Figure 4-7. Therefore, the product of the middle layer and the weight matrix on the output side can be obtained by extracting only the column (word vector) corresponding to &quot;say&quot; and calculating the inner product of the extracted vector and the middle layer neuron. increase. This calculation can be written in detail as shown in Figure 4-8.

[[[00000000000000000518---5c54f0da409f1ca248dcf960f977eb0afd7836d649eefacda410827ceb9cb881]]]
Figure 4-8 Calculate the inner product of the column vector corresponding to &quot;say&quot; and the hidden layer (&quot;dot&quot; in the figure is the inner product calculation)


[[[00000000000000000519---79441ceabc6caeb8ec1e15109a551e6a86956191c354ee80227c5e9849610a0c]]]As shown in Figure 4-8, the weight Wout on the output side stores the word vector for each word ID in each column. Here we extract the word vector &quot;say&quot;. Then, find the inner product of that vector with the neurons in the hidden layer. This will be your final score.

[[[00000000000000000520---dffd52719f5821d327bb033d24ca922501bd5142bb2b5750b62137a7aad696d9]]]In the output layer so far, we performed calculations on all words. Here we focus only on the word &quot;say&quot; and calculate only its score. Then apply a sigmoid function to convert that score to a probability.

[[[00000000000000000521---02dcdd4b2d8c31e6183d3df54dd790a7ba1e5db31824502e0ace50f44bf6ed43]]]Sigmoid function and cross-entropy error

[[[00000000000000000522---890eb015966853feb3ec83b8b530141a9893c25295a337be400d3c122a34e6b2]]]To solve a binary classification problem with a neural network, we apply a sigmoid function to the scores and get the probabilities. And to find the loss, we use the &quot;cross-entropy error&quot; as the loss function. This is a neural network convention for binary classification.

[[[00000000000000000523---7ee5946ed93e9abedd4ad2575dd6dd70b05cc7fa75317ce3c1f5e6591acbe376]]]For multi-class classification, we use the &quot;softmax function&quot; for the output layer (to convert scores to probabilities) and the &quot;cross-entropy error&quot; for the loss function. In the case of binary classification, we use “sigmoid function” for the output layer and “cross-entropy error” for the loss function.

[[[00000000000000000524---0af49ffd687b0a01b5777a5985e9486f07b2876639294d3d3ac3e45ffd65471e]]]Here, I would like to review the sigmoid function. As soon as possible, the sigmoid function could be written as shown in equation (4.2).

[[[00000000000000000525---c7a7366a47e6896478b85f4ea4b4dde08e3c67075df06eb921d19f273daf4244]]]If we write equation (4.2) as a graph, it will look like the right figure in Figure 4-9. As you can see from the figure, the shape of the graph is an S-shaped curve, and the input value (x) is converted to a real number between 0 and 1. The point here is that the output of the sigmoid function (y) can be interpreted as a &quot;probability&quot;.

[[[00000000000000000526---c10a45b38b14b82ad54b0e29c6699a14cd6c5205ff0634a1d3e9e4d4d388d730]]]The sigmoid function has already been implemented as a Sigmoid layer. This can be written as the left diagram in Figure 4-9.

[[[00000000000000000527---f7f5302edc4a766610303005befee6ee93e827dd325ac3500a0193b36a32b129]]]
Figure 4-9 Left: Sigmoid layer. Right: Graph of sigmoid function


[[[00000000000000000528---f1472dc3a5a6eed06abefe49ad071e674e1f06a8838624a2520e4db887db8d48]]]Once you get the probability y from the sigmoid function, find the loss from this probability y. The loss function used for the sigmoid function is the &quot;cross-entropy error&quot;, the same as for multi-class classification. This cross-entropy error can be written as

[[[00000000000000000529---c5a6903c3ca095ef644e3474de24e72dfb4c8126fa61fc82034e5306d6eb3e93]]]where y is the output of the sigmoid function and t is the correct label. This correct answer label t takes a value of either 0 or 1. When t is 1, the correct answer is &quot;Yes&quot;, and when t is 0, the correct answer is &quot;No&quot;. So when t is 1, is output, and vice versa when t is 0.

[[[00000000000000000530---7ce55d440bff366a9ad7e718840d78c015b64b158e007f26018030413dbb1630]]]Both binary and multi-class classification use the &quot;cross-entropy error&quot; as the loss function. Representing them in mathematical formulas, they are formulas (4.3) and (1.7), but they express the same thing, just in different ways of writing the formulas. To be precise, when using two neurons in the output layer for multi-class classification, the equation (4.3) for binary classification is exactly the same. Therefore, the implementation of the Sigmoid with Loss layer is completed with a few tweaks to the implementation of the Softmax with Loss layer.

[[[00000000000000000531---b60d4954240ff4ee30788851783ad0874851e4e1ceaa4f51a3a1821981b7e6d8]]]Next, the Sigmoid layer and the Cross Entropy Error layer are represented graphically. These can be drawn graphically as shown in Figure 4-10.

[[[00000000000000000532---40b1b851aed3364cbdbcedbdc223b4320e4a3a1f2369cde1a05e72b93acda69b]]]
Figure 4-10 Computation graph of Sigmoid layer and Cross Entropy Error layer. The right figure is described collectively as a Sigmoid with Loss layer


[[[00000000000000000533---527a2b90bafbe962cedce156ad0b1e055522983654eb15617a483d8240c69ba5]]]One thing to notice about Figure 4-10 is the y−t value of backpropagation. where y is the probability output by the neural network and t is the correct label. And y−t is just the difference between those two values. This means, for example, that if the correct label is 1, the error will be small as y approaches 1 (100%) as much as possible. Conversely, as y moves away from 1, the error increases. Then, as the error flows to the previous layer, it learns “larger” in case of large error and “smaller” in case of small error.

[[[00000000000000000534---034ce33928d99f3e55d27b467ed82af5971ff3862c467b31651745c8f2a7248c]]]The combination of the &quot;sigmoid function&quot; and the &quot;cross-entropy error&quot; resulted in a &quot;beautiful result&quot; of backpropagation value y−t. Similarly, the combination of the &quot;softmax function&quot; and the &quot;cross entropy error&quot;, and the combination of the &quot;identity function&quot; and the &quot;sum of squares error&quot; also propagate the value of y−t during backpropagation.

[[[00000000000000000535---a589053e681b2ded74103a4c4a69b89f607599fc72f0494909d9d46da233a9d9]]]From multi-class classification to binary classification (implementation)

[[[00000000000000000536---92c22c128ffbf9ce20aecadadee58ce5be5cf115a47f3a0e5e09d83e85676a91]]]I would like to organize the story so far from the viewpoint of implementation. So far we have dealt with multi-class classification problems. There, we prepared as many neurons as the number of vocabulary in the output layer and passed them through the Softmax layer. The neural network used at this time can be illustrated as shown in Figure 4-11, focusing on &quot;layers&quot; and &quot;operations&quot;.

[[[00000000000000000537---5f08a03780326a5fde0441d6db401343a6ce5473f38a51d9fd316f64e9fe7adc]]]
Figure 4-11 Overall view of the CBOW model that performs multilevel classification (embedding layer is denoted as Embed)


[[[00000000000000000538---6700cc15c75b1bc0ce9cc277efcf0e6a40daff86ab1074020d1fd82d574f8b84]]]Figure 4-11 shows an example where the contexts are &quot;you&quot; and &quot;goodbye&quot; and the correct target (word to be predicted) is &quot;say&quot; (for word IDs, &quot;you&quot; is 0, (assuming &quot;say&quot; is 1 and &quot;goodbye&quot; is 2). In addition, the input layer uses an embedding layer to extract the distributed representation of the corresponding word IDs.

[[[00000000000000000539---cb68a63884f2e4c677b999ae670cce7df209d977bd0861f82b1bd05d1d90b6db]]]In the previous section, we implemented the embedding layer. This layer extracts the distributed representation (word vector) of the target word ID. Previously this was replaced with a MatMul layer.

[[[00000000000000000540---df29918368efcb1c4522557510954ce3f5d7148cd8bc50031bf22783444f4d8b]]]Now, let&#39;s transform the neural network in Figure 4-11 into a network that performs binary classification. If we quickly show the network configuration, it will look like Figure 4-12.

[[[00000000000000000541---e0245b45499caea30f3e11e0e4b68fc29b43ccd40321255804866eb3ae95cb9b]]]
Figure 4-12 Overall view of word2vec (CBOW model) for binary classification


[[[00000000000000000542---8274d97fec74adc959156067e036485d3330c518e9b08253f8439eb5d7057cb5]]]Here, the middle layer neuron is h, and the inner product is calculated with the word vector corresponding to the word &quot;say&quot; of the weight Wout on the output side. And we get the final loss by putting that output into a Sigmoid with Loss layer.

[[[00000000000000000543---5a1d629b2def1eed565df25234ce1ca2d1595a0c65c13e77f3cf918707de06be]]]In Figure 4-12, the Sigmoid with Loss layer is entered with &quot;1&quot; as the correct label. This means that the answer to the question at hand is &quot;yes&quot;. If the answer is &#39;No&#39;, enter &#39;0&#39; as the correct answer label in the Sigmoid with Loss layer.

[[[00000000000000000544---f52e52ca3531c38329aee3af5b383736f648318a08f2d8acae87184c03352a38]]]Now, for the sake of clarity, I would like to simplify the second half of Figure 4-12. For that we introduce the Embedding Dot layer. This layer is a layer that combines two processes, the Embedding layer and &quot;dot operation (inner product)&quot; in Figure 4-12. Using this layer, the second half of Figure 4-12 can be written as:

[[[00000000000000000545---fc671487043f2025f2f433ee67600c03c37528c835c7f829678ff9171b503dba]]]
Figure 4-13 Focus only on the processing after the middle layer in Figure 4-12 and draw. Use the Embedding Dot layer to combine the embedding layer and the inner product calculation


[[[00000000000000000546---bcfafcba723358a753adc78296f1dacd86a38d0407334f4a0bd18864de70bd5b]]]The intermediate layer neuron h passes through the Embedding Dot layer and then through the Sigmoid with Loss layer. As you can see, by using the Embedding Dot layer, we were able to write the processing after the middle layer simply.

[[[00000000000000000547---df84f2199d7e209c0415f691c54d8c39cd5db49f89767c413ae676f3ac11c9f4]]]Let&#39;s take a quick look at the Embedding Dot layer implementation. Here, we implement this layer as an EmbeddingDot class as follows (☞ ch04/negative_sampling_layer.py).

[[[00000000000000000548---8f7e52bffd4b285f494a36bb10297ae16cf301ef48cfb5c81232e389cec6b824]]]The EmbeddingDot class has a total of 4 member variables -- embed, params, grads and cache. As per the implementation rules of this book, params stores parameters and grads stores gradients. In addition, embed uses the embedding layer, and cache is used as a variable to temporarily hold the result calculated during forward propagation.

[[[00000000000000000549---1c6f590037aa0f30a3bd1580633a4c96af0cb8b54e00703dae0a8b77acdfa1bb]]]The forward propagation forward(h, idx) method receives the intermediate layer neuron (h) and the NumPy array of word IDs (idx) as arguments. Here, idx is an array of word IDs, because it assumes &quot;mini-batch processing&quot; that processes data collectively.

[[[00000000000000000550---72027d31aecc35c837123ca825aac08190a19223584f044e0b6f04a80580e366]]]The forward() method in the above code first calls forward(idx) of the Embedding layer and then computes the inner product. The dot product calculation is done by a single line of np.sum(self.target_W * h, axis=1). To understand this implementation, it&#39;s easiest to see concrete values. A specific example is shown in Figure 4-14.

[[[00000000000000000551---7f00155839a602b9f8d3de20384211fc2ee4d3ad7d4b58e77ff3974c5df1b956]]]
Figure 4-14 Concrete Values for Each Variable in the Embedding Dot Layer


[[[00000000000000000552---04e744913f8e39e028d16a48cb72afc97774d1530c5c8541dac34a8cee43b8a5]]]Prepare appropriate W and h and idx as shown in Figure 4-14. Here idx is [0, 3, 1], which represents an example of processing three data together as a mini-batch. idx is [0, 3, 1], so target_W is the result of extracting the 0th, 3rd, and 1st rows of W. And in target_W*h, the elementwise product is computed (&quot;*&quot; in NumPy is elementwise product). Then sum the results row by row (by axis=1) to get the final result out.

[[[00000000000000000553---2742163093e2b410f38eacf0c15daf3b88603e98480af2bc6bae646c04ebf19f]]]The forward propagation of the Embedding Dot layer is explained above. Backpropagation is obtained by propagating gradients in the reverse order of forward propagation. I won&#39;t explain the implementation here (it&#39;s not a difficult problem, so let&#39;s think for ourselves).

[[[00000000000000000554---5583c7a79f53d1e0f38e7db0aa39366af19b5fdccd82632bb7b30044c744a0e9]]]Through the discussion so far, we have been able to convert the problem to be solved from &quot;multi-class classification&quot; to &quot;binary classification&quot;. However, unfortunately this does not solve the problem. This is because, as it stands now, we have only learned about positive examples (correct answers). So for negative examples (wrong answers), I&#39;m not sure what the outcome will be.

[[[00000000000000000555---11f50bc8ce8dc0af20a833251d3b9d54ea9a5228f403e59da598b543d70f046a]]]Now consider the previous example again. The previous example is when the context is &quot;you&quot; and &quot;goodbye&quot; and the correct target is &quot;say&quot;. So far, we&#39;ve done binary classification only for positive ``say&#39;&#39;s. If we have &quot;good weights&quot; here, the output (probability) of the Sigmoid layer will be close to 1. Figure 4-15 shows the processing at this time as a calculation graph.

[[[00000000000000000556---426beb4c8abf89d6a773a63de62b0b652f5b9fba842bfc5af6e38f659e38732e]]]
Figure 4-15 Example of processing after the middle layer of the CBOW model: Here, the contexts are &quot;you&quot; and &quot;goodbye&quot;. At this time, the probability that the target is &quot;say&quot; is 0.993 (99.3%)


[[[00000000000000000557---44b3d80246df0dcd39340703aa588cf0347c63c974c2df35c9087f1042a443af]]]In the current neural network, it will only learn about the positive example &quot;say&quot;. But I don&#39;t know anything about negative examples—words other than &quot;say.&quot; What do we really want to do here? That is, for positive examples (“say”), the output of the Sigmoid layer approaches 1, and for negative examples (words other than “say”), the output of the Sigmoid layer approaches 0. This can be represented graphically as shown in Figure 4-16.

[[[00000000000000000558---7e6dea6f258784fd0d27c94abc386aec2935879b9cccf3578c07b124a5d1583b]]]
Figure 4-16 Assuming that the positive example (correct answer) is &quot;say&quot;, the output of the Sigmoid layer is close to 1 when &quot;say&quot; is input, and the output is close to 0 when words other than &quot;say&quot; are input Become. such a weight is required


[[[00000000000000000559---4b5fa9020e76eed59a6011280da2539a9aabcce6a158a4a61e49cda67023b811]]]For example, when the contexts are &quot;you&quot; and &quot;goodbye&quot;, the probability that the target is &quot;hello&quot;—the probability of the wrong word—is desired to be low. In Figure 4-16, the probability when the target is &quot;hello&quot; is 0.021 (2.1%), but a weight that outputs a value close to 0 is required.

[[[00000000000000000560---0f804cc20f972757d466b28a7a76580ac2731b86d8e4b833abb38e30b270051e]]]In order to handle multi-value classification problems as binary classification, it is necessary to be able to correctly (binary) classify both &quot;correct answers (positive examples)&quot; and &quot;wrong answers (negative examples).&quot; Therefore, the problem must be considered for both positive and negative examples.

[[[00000000000000000561---452da4e199a479c66732e0c90adbdc933934dcd72d19582801ebfc9c4c3d7c16]]]So, are we going to learn binary classification for all negative examples? The answer is of course &quot;no&quot;. Targeting all negative examples would be unwieldy as the vocabulary increases (the purpose of this chapter was to deal with the increase in vocabulary in the first place). So, pick up some negative examples—five, ten, etc.—as an approximation (we&#39;ll talk about how to choose them later). In other words, a small number of negative examples (negative examples) are used. This is what is meant by the technique of &quot;negative sampling&quot;.

[[[00000000000000000562---58098f55a66513b95c1bcd7b122b11eb4ce5c27f527ae72601dc87a694264b7b]]]To summarize, the method of negative sampling finds the loss when positive samples are targeted. At the same time, some negative examples are sampled (picked out) and the loss is similarly calculated for the negative examples. Then, add the losses for each data (positive and sampled negative examples) and take the result as the final loss.

[[[00000000000000000563---fc521bca71a868ac6f067d4c6353f09c2c37cd66b3da4e098c29bc42ce44a7fc]]]Let&#39;s explain the story so far with a concrete example. Here we have the same example as before (positive target is &quot;say&quot;). And, assuming we sampled two negative example targets, let&#39;s say the sampled words are &quot;hello&quot; and &quot;I&quot;. In such a case, if we focus only on the middle layer and beyond of the CBOW model, the computational graph of Negative Sampling can be drawn as shown in Figure 4-17.

[[[00000000000000000564---e7d0c466531b876480bc441ec0db0241926f85a20e55ee203e354b5c3be71aa9]]]
Figure 4-17 Negative sampling example (Focus on the processing after the intermediate layer and illustrate the calculation graph by layer)


[[[00000000000000000565---fd18944bc5c0f191b11e8a66a72b0a00b75a01953941b53041d37a222bbb5dba]]]The point to note in Figure 4-17 is the handling of positive and negative examples. For positive examples (“say”), enter “1” as the correct answer label into the Sigmoid with Loss layer as before. On the other hand, for the negative examples (“hello” and “I”), enter “0” as the correct label for the Sigmoid with Loss layer, since they are incorrect examples. After that, add the losses in each data and output the final loss.

[[[00000000000000000566---3b7639ece51439590e807705edb7f0a839ca648a86e2d7895796837a45190622]]]Sampling method of Negative Sampling

[[[00000000000000000567---e54ec23ae53dcf732c0459956abd6d8ac0c0444710cb3c1a252f2c22aad776a3]]]There&#39;s only one thing left to explain about Negative Sampling. That is how to sample negative examples. There are known better ways to do this than sampling at random. It does the sampling based on the corpus statistics. Specifically, words that are frequently used in the corpus are more likely to be extracted, and words that are less frequently used in the corpus are less likely to be extracted.

[[[00000000000000000568---b182bef3678e61a4ce98a4cc7be95751b60d1337980f4b01f6166ec27cc3aa37]]]To sample based on the frequency of word usage in the corpus, find the number of times each word appears in the corpus and represent this as a &quot;probability distribution&quot;. Then we sample words from that probability distribution (Figure 4-18).

[[[00000000000000000569---39c8afcbb3f7b27f965eb48de12c5a582cd2d4aff9cf29614c26a72097b641ba]]]
Figure 4-18 Example of sampling multiple times according to probability distribution


[[[00000000000000000570---a19562c77636f365041b108e172025e0b75896ca0ba99eb57154efb0811aa765]]]Once the probability distribution is obtained based on the number of occurrences of each word in the corpus, all that is left is to perform sampling according to that probability distribution. Sampling based on probability distribution makes it easier to extract words that appear frequently in the corpus. On the other hand, &quot;rare words&quot; are difficult to extract.

[[[00000000000000000571---1d9d409157a27035527c5ee71da3598aeed8ee4c6098b3dfbdfb4e639cd4a0e1]]]In Negative Sampling, it is desirable to cover as many words as negative examples if possible. However, due to computational complexity, it is necessary to limit the number of negative examples to a small number (5, 10, etc.). As a negative example, what if only &quot;rare words&quot; were selected? That would have bad consequences. This is because even in real-world problems, rare words rarely appear. In other words, the importance of dealing with rare words is low. Rather, being able to deal with high-frequency words will lead to better results.

[[[00000000000000000572---11476c6218912bb787584e3a05e703a86141fea313948b55e922194a117050a8]]]Now, I will explain an example of sampling according to a probability distribution using Python. You can use NumPy&#39;s np.random.choice() method for this purpose. Here are some examples to demonstrate how to use that method.

[[[00000000000000000573---8abdd480022f636dc3af1447ebc854a1af43eeb39b2b39078f01ffbfdf266f0c]]]

# Randomly sample one number from 0 to 9 &gt;&gt;&gt; 

[[[00000000000000000574---14393bff6f310b1319e20f693c4ba653fc25c0d197357d4e8165860cb5205672]]]
2 # randomly sample only one word &gt;&gt;&gt; 

[[[00000000000000000575---c2c8bc7e8d2c0127a8e287a170c3e0e87ee2ae2fad07dd204fe3e2ca6ad2e305]]]
&#39;goodbye&#39; # random sampling of only 5 (with duplicates) &gt;&gt;&gt; 

[[[00000000000000000576---e84d4dfc33aa7de1af7658901d64c54a803ee3cbbebe94cb311c40ff8198650c]]]
array([&#39;goodbye&#39;, &#39;.&#39;, &#39;hello&#39;, &#39;goodbye&#39;, &#39;say&#39;], dtype=&#39;<U7')

# 5つだけランダムサンプリング（重複なし）
> &gt;&gt; 

[[[00000000000000000577---5300390f1c5215d6db3b737dfc3eed08ba9bb3ff99bb28518b46c08f5d535d54]]]
array([&#39;hello&#39;, &#39;.&#39;, &#39;goodbye&#39;, &#39;I&#39;, &#39;you&#39;], dtype=&#39;<U7')

# 確率分布に従ってサンプリング
> &gt;&gt; 

[[[00000000000000000578---314ac5baf14ebc14c49a6add25dc655fff555bf7cde54384c8555b26843c44f4]]]As shown here, np.random.choice() can be used for random sampling. At this time, if size is specified as an argument, sampling will be performed multiple times at once. Also, if you specify replace=False in the argument, it will sample without duplication. Then, by specifying a list representing the probability distribution for the argument p, sampling is performed according to the probability distribution. All that&#39;s left now is to use this function to sample negative examples.

[[[00000000000000000579---a5a4d73af4851248b6e6406cc1432979624133dbda8561bae2a9127cc96e4cde]]]Now, in Negative sampling proposed by word2vec, it is proposed to add one step to the above probability distribution. It raises the underlying probability distribution to the power of 0.75, as shown in equation (4.4).

[[[00000000000000000580---2ac0ffd0f55dc14f9004dcfef62efbe18fa1ef5be331e7758595f79d8f9e68ba]]]Here we represent the probability of the i-th word. Equation (4.4) simply raises each element of the original probability distribution to the power of 0.75. However, the &quot;sum of the probability distribution after transformation&quot; is required in the denominator so that the sum of the probabilities is 1 even after transformation.

[[[00000000000000000581---7503b0c28a4a2a6415bceab0528802855dc4a20cdf62ea92467881e6b91fac18]]]Then, why do we perform the conversion as shown in formula (4.4)? This is to avoid &quot;throwing away&quot; words with a low probability of appearance. To be more precise, you can increase the odds of low-probability words by a small amount by raising them to the power of 0.75. Here&#39;s an example in action:

[[[00000000000000000582---9d54416bc62bca4e77278cc50541e89df36dc950b89b3004c206af29a75527e9]]]This example shows that an element with a probability of 0.01 (1%) before the transformation is now 0.026... (2.6...%) after the transformation. In this way, as a remedy to make low-probability words (slightly) more likely to be sampled, we perform the &quot;0.75th power&quot;. Note that the number 0.75 has no theoretical meaning, and it is possible to set it to a value other than 0.75.

[[[00000000000000000583---2eb53032b259b2e1e28fdb4f05a54710756fff328bd45a26bc870993067c7eef]]]As we saw above, negative sampling creates a word probability distribution from the corpus, raises it to the power of 0.75, and uses np.random.choice() to sample negative examples. This document provides a class named UnigramSampler for such processing. Here, only the usage of UnigramSampler is explained briefly. The UnigramSampler class is in ch04/negative_sampling_layer.py, so if you&#39;re interested in implementing it, check it out.

[[[00000000000000000584---9b16d0c99e53f8f82dc96607129023859e9f3f53afa2ad967348ecb033d121ef]]]A unigram means &quot;a single (consecutive) word&quot;. In the same vein, a bigram means &quot;two consecutive words&quot; and a trigram means &quot;three consecutive words&quot;. The reason we named the class UnigramSampler here is to create a probability distribution for a single word. If this were a &quot;bigram&quot;, it would create a probability distribution for combinations of two words such as (&#39;you&#39;, &#39;say&#39;), (&#39;you&#39;, &#39;goodbye&#39;)... .

[[[00000000000000000585---40c5da014bcaeb82f59138bedd8e3acea83fe26e30cd5b2bbf8bb3ee89926c55]]]The UnigramSampler class takes three arguments upon initialization. It is the corpus of the list of word IDs, the power of the &quot;power&quot; to the probability distribution (default 0.75), and the sample_size of the number of negative samples to sample. Also, the UnigramSampler class has a method called get_negative_sample(target). This performs sampling of word IDs other than those specified by the argument target as positive examples. So here&#39;s an excerpt of an example that uses the UnigramSampler class.

[[[00000000000000000586---2db7efdcf2aa1fde508881d2567f5df4514b7a1473154ede9a2362232c5dae35]]]Here we consider three data [1, 3, 0] as a mini-batch as a positive example. At this time, two samples are taken as negative examples for each data. In the example above, you can see that the negative example for the first data is [0 3], the second is [1 2], and the third is [2 3]. Negative samples are now sampled.

[[[00000000000000000587---585d2c7b41d0cc13b7c90c9452cd1400b9b75928609aedd3a45d8a4421ccd5e9]]]Implementation of Negative Sampling

[[[00000000000000000588---179a12174f2c2265824d177a9756a368b761dec725302299b0e5955b987a90dd]]]Finally, we will implement Negative Sampling. Here, we will implement it with a class called NegativeSamplingLoss. Let&#39;s start with the initializer (☞ ch04/negative_sampling_layer.py).

[[[00000000000000000589---e1e0af6e428ee0f1b4531e7005f52c6ae41cd78c3af2d5553b5e2d0a1b5e9052]]]The initialization arguments are W, the output weight, the corpus of the corpus (list of word IDs), the power of the probability distribution, and sample_size, the number of samples of negative examples. Here, the UnigramSampler class described in the previous section is generated and stored in the member variable sampler. Also, set the number of samples for negative examples to the member variable sample_size.

[[[00000000000000000590---0a0c6e623b6c6839770b43aa64e83846a8b7e1154fa67175b13fc8b9c5371c60]]]The member variables loss_layers and embed_dot_layers hold the list of required layers. At this time, sample_size + 1 layers are generated in the two lists, because one layer for positive examples and sample_size layers for negative examples are generated. Here we assume that the first layer in the list deals with positive cases. In other words, loss_layers[0] and embed_dot_layers[0] are the layers that handle positive examples. After that, we put the parameters and gradients used in this layer into arrays. Next, we show the implementation of forward propagation (☞ ch04/negative_sampling_layer.py).

[[[00000000000000000591---f548d9ea909e1da461cf2b7aa7fdcb31a17647170d1d2453d193a1f087440766]]]# positive forward

[[[00000000000000000592---8f9056c9737dcc6fbee62d033d22d831713ad539434dde0cbfa0cfd35ee1118d]]]# Forward negative examples

[[[00000000000000000593---220f9e19082720a2680d6e7d70497f8cb6de2a23dabc22f449104bb6d7ec5c0b]]]The arguments received by the forward(h, target) method are the hidden layer neuron h and the target of the positive examples. The process here is to first sample negative examples with self.sampler and call it negative_sample. After that, forward propagation is performed for each data of positive and negative examples, and the loss is added. Specifically, output the score in forward of the Embedding Dot layer, then put the score and label in the Sigmoid with Loss layer to find the loss. Note that the correct labels are &quot;1&quot; for positive examples and &quot;0&quot; for negative examples.

[[[00000000000000000594---c146c437d359239973052cd618d059314a2d6151bea9ef0f3ac15d0878dddbb1]]]Finally, here&#39;s an implementation of backpropagation.

[[[00000000000000000595---211ac3a4c5cbc4b9cb1fede8c85b58c2cd7633fd28ba10d073f58f71c10ec92b]]]Implementing backpropagation is straightforward. This just calls backward() for each layer in the reverse order from forward propagation. Intermediate layer neurons were copied multiple times during forward propagation. This corresponds to the Repeat node described in &quot;1.3.4.3 Repeat node&quot;. So in that backpropagation you end up adding multiple gradients. The above is the explanation of the implementation of Negative Sampling.

[[[00000000000000000596---b3bb4a239c4491f2bba1b1593e0cfd2793694f3dab3173066fef68581dbac520]]]Learning improved word2vec

[[[00000000000000000597---20c5f1aa78f211941b80f11d9553145532f28d8814241058529c58a5f3fe2fb8]]]I&#39;ve been improving word2vec so far. I first explained the Embedding layer, followed by a technique called Negative Sampling. And I&#39;ve actually implemented both of them. Now let&#39;s implement a neural network that incorporates those improvements. And I would like to learn using the PTB dataset to obtain a more practical distributed representation of words.

[[[00000000000000000598---c9557fa260fd872ff836c81a2254be6aa0a429ae9832eedffc7143382551d0a8]]]CBOW model implementation

[[[00000000000000000599---ddb5001a918b4e0644835ba8deaade51a49444dcb6bf66d757a772cf9451af67]]]Here, in implementing the CBOW model, we will improve the simple SimpleCBOW class from the previous chapter. A refinement is the use of Embedding and Negative Sampling Loss layers. In addition, as a context, extend it to handle arbitrary window sizes.

[[[00000000000000000600---3e944f75a914cd9ca2247a2678024c251864fd523e14c31321b6028db54caa06]]]Here&#39;s an improved CBOW class implementation: First, here is the initializer (☞ ch04/cbow.py).

[[[00000000000000000601---d7f2704dda40d0528bed1e8e16c66e17b842f50172d7ba8554f23c7229aedc67]]]# Initialize weights

[[[00000000000000000602---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000000603---c6621dd98a52b254b658cf9d0c144907f70f54ee8dc15065658c31f65f2f5401]]]# use embedding layer

[[[00000000000000000604---1c25c69046eebacf8e2d056b24a9eaca53c9066df616a1e5033e837bfe7958c9]]]# Gather all weights and gradients into an array

[[[00000000000000000605---33a07c0284817444aaa0c98a6c115ec58d5a0921f2a2ee323a7ec261fbb73efa]]]# Set distributed representation of word to member variable

[[[00000000000000000606---41baad0724352287933381f34cf2c94b12ef5f5968722086a120f0203dd64173]]]This initializer accepts four arguments. vocab_size is the number of vocabularies, hidden_size is the number of neurons in the hidden layer, and corpus is the list of word IDs. And the size of the context—how much of the surrounding words to include as context—is specified by window_size. For example, if window_size is 2, the context is 2 words to the left and right of the target word (4 words total).

[[[00000000000000000607---2141483037ef9f1b00e213ce87ed347a39456739743832101f17f3575a06fe19]]]In the SimpleCBOW class (implementation before improvement), the shape of the weights on the input side and the weights on the output side differed, and the weights on the output side arranged word vectors in the column direction. On the other hand, the weights on the output side of the CBOW class have the same shape as the weights on the input side, and the word vectors are arranged in the row direction. This is due to the use of Embedding layers within the NegativeSamplingLoss class.

[[[00000000000000000608---d35e0b3df7e0b681c72b8251b1e380b26e5e17654632c3305d4783281acfbb7b]]]After initializing the weights, it&#39;s time to create the layers. Here we create 2 * window_size embedding layers and store them in an array in the member variable in_layers. Then create a Negative Sampling Loss layer.

[[[00000000000000000609---8feb89614c5540bb180f4070b3bff96573406c8a91fd35406cfce6b4ae432c53]]]After creating the layer, all the parameters and gradients used in this neural network are grouped into the member variables params and grads. We also set the weight W_in to the member variable word_vecs so that we can access the word&#39;s distributed representation later. Next, I will show forward() method for forward propagation and backward() method for backward propagation (☞ ch04/cbow.py).

[[[00000000000000000610---9ef9cdc4cdbadb34433a931d6d9de7c0b0f55122b9d3e7e129cdef6a8b7089cf]]]Our implementation simply calls the forward (or back) propagation of each layer in the appropriate order. This is a natural extension of the SimpleCBOW class from the previous chapter. However, the forward(contexts, target) method takes a context and a target as arguments, which consist of word IDs (in the previous chapter it was a one-hot vector, not word IDs). A concrete example of this is shown in Figure 4-19.

[[[00000000000000000611---671ce47304e53b45a93646d3a5a6d3884e89e41ae8d7893a4b19fbc5cc3532ff]]]
Figure 4-19 Example of expressing context and target by word ID: Here, the context is shown with a window size of 1


[[[00000000000000000612---af87421d541821f9a2b1998f0b56bc46f00fffaa7c89b652543a21ba28960de3]]]The array of word IDs shown on the right side of Figure 4-19 is an example of contexts and targets. As you can see, contexts is a 2D array and target is a 1D array. Such data is entered in forward(contexts, target). The above is the description of the CBOW class.

[[[00000000000000000613---fe50c8ed4224e085e6e44ba63918a5b1b26acda9d958a48213fde5db0cd637be]]]Training code for CBOW model

[[[00000000000000000614---071e948b1337864f08aea2331b598dfd0caa819c757e5284cdd5a246d1b17890]]]Finally, we implement the training of the CBOW model. Here we are simply training the neural network. Here&#39;s the quick code (☞ ch04/train.py).

[[[00000000000000000615---0782d07c8326e3ada616c74db4f78ad8b4d68df88b9a02d1a5e777d6520eec20]]]# Remove the following comment out when running on GPU (requires cupy)

[[[00000000000000000616---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000000617---05c3b68912d36a95e530e768bb3c99298575fcfd017e4b2f5fabaf04b0aaf74c]]]# load data

[[[00000000000000000618---769161ec200580925ce9495d507ece581d4b56eaf3476d6fcea5cf12fd995539]]]# Generating models etc.

[[[00000000000000000619---3fc6a637a64115653cbb2fb5046cc785ce044599592328e07a423a28465cb5af]]]# start learning

[[[00000000000000000620---6ee8330b8d6a80de4f84bde698cb0962007ee8f5520eab73ea2fd29da3966e82]]]# save necessary data for later use

[[[00000000000000000621---0663823170845a48b74606df46ffa8c076b6079f42476b4dd4f9034b74f9bfc3]]]Our CBOW model has a window size of 5 and the number of neurons in the hidden layer set to 100. Although it depends on the target corpus, a window size of 2 to 10 and the number of neurons in the intermediate layer (the number of dimensions of distributed representation of words) of 50 to 500 seem to produce good results. We will discuss such hyperparameters later.

[[[00000000000000000622---a157f2c887238514bc20adfaf3c71037a0600943d93eac8785fcceb0a6149ca1]]]Now, the PTB corpus we are dealing with this time is much larger than before. Therefore, it takes a lot of time (about half a day) to study. Here, as an option, we have prepared a mode that allows it to run using the GPU. To run on GPU, enable &quot;# config.GPU=True&quot; at the top of the file. However, to run on a GPU you need a machine with an NVIDIA GPU and CuPy installed.

[[[00000000000000000623---b9631d453975c6634f34aed1556a77ae6d347bcb086427ded9d955e5ac26e94d]]]Once trained, we extract the weights (here only the input weights) and save them to a file (with a dictionary for word and word ID conversion) for later use. In order to save it to a file, we use a function called &quot;pickle&quot; in Python. Pickle can be used to save (or load) objects in Python code to a file.

[[[00000000000000000624---911e1ac95042ce1f92d8a12a5f3094040e8d4e7bfff39b67b279dfd15517ff97]]]Learned parameters are prepared in ch04/cbow_params.pkl. If you can&#39;t wait for the training to finish, use the pre-trained parameters provided in this book. Note that the learned weight data will differ depending on the environment in which each person learned. This is due to the random initial values used to initialize the weights, the random selection of the mini-batches, and the randomness of the sampling in negative sampling. Due to their randomness, the final weights will be different in your environment, but in the big picture you will get similar results (trends).

[[[00000000000000000625---86d2ef4d52b38ed64439bfddb6e9dad6ef437ae118e9b43fa98a3f7ccd52ffd1]]]Evaluation of the CBOW model

[[[00000000000000000626---19abd438d3d9b8c2d6903217172ba85a42214f1c5466bd3aa1816a10dbab038c]]]Now, let&#39;s evaluate the distributed representation of the words learned in the previous section. Here, I would like to use the most_similar() method implemented in Chapter 2 to display the closest word to some words (☞ ch04/eval.py).

[[[00000000000000000627---8309c8bad862860280c1e0d3ff21651f6853a40af5e6eec7e33752821fd56cdc]]]Running the code above produces the following results (your results here will vary slightly depending on your learning environment):

[[[00000000000000000628---8ecfd32458863e63e00bb8a36b1d02e96369c3cea355ded686556ed9f67d3daf]]]Let&#39;s see the result. First, when the query is “you”, similar words include the personal pronouns “i (=I)” and “we”. If you query for the following &quot;year&quot;, you will see words with the same nature that express periods such as &quot;month&quot; and &quot;week&quot;. And if you query &quot;toyota&quot;, you&#39;ll get car manufacturers such as &quot;ford&quot;, &quot;mazda&quot;, and &quot;nissan&quot;. Looking at these results, it seems that the distributed representation of words acquired by the CBOW model has good properties.

[[[00000000000000000629---9f085ef6c32595c49d5f3c3c5363d3a0dd88a6dbfbbdf47e9531fe4640ed0e04]]]Furthermore, it is known that the distributed representation of words obtained by word2vec not only gathers similar words close together, but also captures more complex patterns. A typical example of this is the analogy problem famous for &quot;king-man+woman=queen&quot;. More precisely, this means that using word2vec&#39;s distributed representation of words, we can solve the analogy problem by vector addition and subtraction.

[[[00000000000000000630---c03f637f9d2dc2daa4ffd069c2aa6819ab4c9d88fc9986eb4d07e333158a904e]]]To actually solve the analogy problem, as shown in Figure 4-20, search for words in which the &quot;man → woman&quot; vector and the &quot;king → ?&quot; vector are as close as possible in the space of word vectors.

[[[00000000000000000631---5f2511aa2fb98d795fe4fa12ae5c9c531f5de9cdeda03e2627c6966a87ae2d32]]]
Figure 4-20 In solving the analogy problem &quot;man : woman = king : ?&quot;, showing the relationship of each word in the space of word vectors


[[[00000000000000000632---f02729bbe73772bf3f31040acf366e1a905d58a94cde26634d76f973167904ea]]]Here, let&#39;s denote the distributed representation (word vector) of the word &quot;man&quot; by &quot;vec(&#39;man&#39;)&quot;. Then, if we express the relationship we want to find in Fig. 4-20 as a formula, it will be &quot;vec(&#39;woman&#39;) - vec(&#39;man&#39;) = vec(?) - vec(&#39;king&#39;)&quot;. vec(&#39;king&#39;) + vec(&#39;woman&#39;) - vec(&#39;man&#39;) = vec(?)&quot;. In other words, the problem we have to solve is to find the word vector closest to the vector &quot;vec(&#39;king&#39;) + vec(&#39;woman&#39;) - vec(&#39;man&#39;)&quot;. This book provides a function implementing such logic in common/util.py as analogy(). With this function, analogy problems like the one above can be answered in one line like analogy(&#39;man&#39;, &#39;king&#39;, &#39;woman&#39;, word_to_id, id_to_word, word_vecs, top=5). And then the following result will be printed to the terminal:

[[[00000000000000000633---c0022de4c68f7828bd92cd6256c9bdef4f80e2be71c59da70d9f87d87aae9ab2]]]As shown above, the problem sentence is displayed on the first line, and the five words with the highest score are output. At this time, the score is displayed next to each word. Now let&#39;s try to solve some analogy problems. Here, we will ask the following four questions (☞ ch04/eval.py).

[[[00000000000000000634---7a21ebdfafd204e575c0aeac268d03577084b0fdcecbd8f775d4833dbea406dd]]]And when I run this I get results like this:

[[[00000000000000000635---84aeabf0cdaa31c77d8d86ff158f10a6fa0163a4bbfc69f1ab1e289af0783182]]]The result is exactly what we expected! The first question is ``king : man = queen : ?&#39;&#39;, to which the correct answer is ``woman&#39;&#39;. The second question is &quot;take : took = go : ?&quot;, and the answer is &quot;went&quot; as expected. This is evidence of a pattern of present and past tenses, and can be interpreted as information about tenses encoded in the word&#39;s distributed representation. And you can see that the third topic correctly captures the combination of singular and plural forms of words. Unfortunately, I could not answer &quot;worse&quot; to the fourth question &quot;good: better = bad: ?&quot;. However, the fact that they answered comparative words such as ``more&#39;&#39; and ``less&#39;&#39; suggests that such properties are also encoded in the word&#39;s distributed representation.

[[[00000000000000000636---4f6a9cee85fdf674f8817b9f666befb38c980c9c1ee68dfe6949f7abe0b9570f]]]In this way, if we use the distributed representation of words obtained by word2vec, we can solve the analogy problem by vector addition and subtraction. Furthermore, it captures patterns not only in the meaning of words, but also in grammatical information. In addition to this, a number of interesting results have been found in word2vec&#39;s distributed representation of words, such as the relationship that &quot;better&quot; exists between &quot;good&quot; and &quot;best&quot;.

[[[00000000000000000637---829a1f333ef4a026ab14d032ba62037bcf1628126d449384f2f6f9e1184ab5ac]]]The result of the analogy problem here will feel very good. Unfortunately, this is just my pick of the problems that can be solved well. In practice, you will not get the results you expect in many problems. This is mainly due to the small size of the PTB dataset. Training on a large corpus will yield more accurate and robust distributed representations of words, and will greatly improve the accuracy rate of analogy problems.

[[[00000000000000000638---72831978b8202c65c4549fb63e282bc1fbb1773f5e08ff820c383ba1948a8131]]]The rest of the subject about word2vec

[[[00000000000000000639---633efcd8a07eb2a83a7f75ad2486e8c0d3e667f6e6d94adfd2d4435b0d89950b]]]We&#39;ve almost finished explaining the mechanism and implementation of word2vec. In this section, I would like to introduce a theme about word2vec that I have not been able to handle so far.

[[[00000000000000000640---cf69e19559e1938f7177f4c41721dcab1c2b6d5e33dbd0c44b2532488af75aaa]]]Application example using word2vec

[[[00000000000000000641---cc5b3cd62a7e936203a5ffce7caa1cdef972dba6362f88515dfb833b5fb3a167]]]Distributed representations of words obtained by word2vec can be used to find similar words. But the benefits of distributed word representations don&#39;t stop there. Transfer learning is the reason why distributed representations of words are important in the field of natural language processing. This means that knowledge learned in one field can be applied to another.

[[[00000000000000000642---8360c34b27a086d14bb45bc9d4490e53157dd49df3efd8bc492aed5a24da7426]]]When solving natural language tasks, we rarely learn word2vec distributed representations of words from scratch. Instead, it trains on a large corpus (such as text data from Wikipedia or Google News) first, and then uses that trained distributed representation for individual tasks. For example, in natural language tasks such as text classification, document clustering, part-of-speech tagging, and sentiment analysis, the first step in converting words to vectors can take advantage of distributed representations of trained words. And for almost all of the wide variety of natural language processing tasks, distributed representations of words give great results!

[[[00000000000000000643---b4be0c365b311d897a7be3a62efefba06ba63b63160d118069cf72324cf437b3]]]Another advantage of the distributed representation of words is that they can be transformed into fixed-length vectors. Furthermore, sentences (sequences of words) can also be converted into fixed-length vectors using distributed representation of words. How to convert sentences to fixed-length vectors has been actively researched. The simplest method would be to convert each word in the sentence to a distributed representation and find the sum of them. This is called a bag-of-words model (idea) that doesn&#39;t consider word order. We can also use a recurrent neural network (RNN), which we will discuss in Chapter 5, in a more sophisticated way to transform sentences into fixed-length vectors using word2vec&#39;s distributed representation of words.

[[[00000000000000000644---f1a2533949dc804dc9944a122cd6c082d90bfafe60438c9f89751f77da36dd2c]]]Being able to convert words and sentences into fixed-length vectors is very important. This is because if natural language can be converted to vectors, general machine learning methods (neural networks, SVM, etc.) can be applied. This can be represented graphically as shown in Figure 4-21.

[[[00000000000000000645---b1dcd7c01c869db6699efe6c3d239e3d94740e9f246a3e5e9475fa4b53cecc70]]]
Figure 4-21 Processing flow of a system using distributed representation of words


[[[00000000000000000646---d31f651d7643ab4c999cdab4340aec1fc5f1b99f2646780146d661f515599518]]]As shown in Figure 4-21, if a question written in natural language can be converted into a fixed-length vector, the vector can be used as input for another machine learning system. By converting natural language into vectors, it becomes possible to output (and learn) the desired answer within the framework of a general machine learning system.

[[[00000000000000000647---278c02a00cf80e659e23459d26cd8f5d32a9ae1b0742f1ab9e2fa2ea9d81bf0c]]]In a pipeline like the one shown in Figure 4-21, it is common to use separate datasets for training the distributed representation of words and for training the machine learning system. For example, for the distributed representation of words, we use a general-purpose corpus such as Wikipedia to complete training first. Then, train a machine learning system (such as SVM) on the data collected for the problem you are currently facing. However, if there is a large amount of training data for the problem we are currently facing, we may consider doing the distributed representation of words and training the machine learning system from scratch at the same time.

[[[00000000000000000648---6ddcb995c2e43e1d5888571bddff0cfd3eca23d001f8df3c046bcd392c0a5bc4]]]Let&#39;s take a look at how to use the distributed representation of words with specific examples. Suppose you are developing and operating a smartphone app with over 100 million users. Your company gets an unmanageable amount of emails from users every day (or you can find lots of tweets on Twitter etc.). While there are positive opinions, there will also be voices of dissatisfied users.

[[[00000000000000000649---55053c15889612317f2625ee63cbe98738b69b71f57071bd2f7dc2c41aa26e71]]]So you wonder if you could create a system that could automatically classify incoming emails (and &quot;tweets&quot; etc.). For example, as shown in Figure 4-22, I think it would be possible to classify the user&#39;s emotions into three stages based on the contents of the email. If you can correctly categorize user sentiment, you can read the emails of dissatisfied users first. That way, you may be able to discover fatal problems in your app and take action early. This will increase user satisfaction.

[[[00000000000000000650---e35a6534b5f3649238f9208edfba9622c4a75910c974aa534b08d92745f12fdd]]]
Figure 4-22 Example of automatic mail classification system (sentiment analysis)


[[[00000000000000000651---a514a0ccceb0cbfab4be822681c9edae6ffdbfb917436c56f9dfd608a5cb9330]]]To create an automated email categorization system, you start by collecting data (emails). In our example, we collect emails sent by users and manually label the emails. For example, assign labels representing three levels of emotion ——positive/neutral/negative——. Once the labeling work is done, we convert the emails into vectors using pre-trained word2vec. The rest is to train some sort of sentiment analysis classifier (SVM, neural network, etc.) with vectorized emails and sentiment labels.

[[[00000000000000000652---9cbfe7be9aa39ae3c4103aaa2ae7e04853f79f94974b3479fa787c350fb56d67]]]Problems dealing with natural language, like this example, can be transformed into vectors by distributed representations of words. Then, it becomes possible to solve it with normal machine learning methods. Plus, you get the benefit of word2vec&#39;s transfer learning. In other words, in many natural language tasks, using the word2vec distributed representation of words can be expected to improve accuracy.

[[[00000000000000000653---0511b90e17643dd0ded7312a2ec8681e5b93f7d566d3f958316828c51e21eeea]]]How to evaluate word vectors

[[[00000000000000000654---9b5c5c6d17648cddcc9f91a4dba6118b52dea915a4d1adbba14509650ea1d95d]]]I was able to get the distributed representation of the word by word2vec. Then, how should we evaluate the goodness of the distributed representation? Here, we briefly describe how to evaluate distributed representations of words.

[[[00000000000000000655---e88611f3cf7cdf1ee1d9273777f47cd789f7b447291cc28eb0c0225c8e4fbf9e]]]Distributed representations of words are practically used in some kind of application, like the sentiment analysis example above. In that case, what we ultimately want is a system with good accuracy. What we have to think about here is that the system (for example, a system that performs sentiment analysis) consists of multiple systems. In the previous example, multiple systems are a system that creates a distributed representation of words (word2vec) and a system that performs classification for specific problems (such as SVM that classifies emotions).

[[[00000000000000000656---5838431f919b88ed1026d02a66fc9605210b5f76b020ff1f947d1a7b98cf3967]]]Training a distributed representation of words and training a classification system may be done separately. In that case, for example, to investigate how the dimensionality of the distributed representation of words affects the final accuracy, we first train the distributed representation of words, and then use that distributed representation to train another machine You have to do the learning system learning. In other words, it is necessary to evaluate after performing two stages of learning. Furthermore, in that case, tuning for optimal hyperparameters is also required for the two systems, which takes a lot of time.

[[[00000000000000000657---868ff1d2c39721b6db8245fd1c0d13f7dc6e752bf8395e6ee03cf244ecfc8f80]]]Therefore, in evaluating the goodness of distributed word representations, it is common practice to separate the evaluation from the actual application. The evaluation index that is often used at that time is the evaluation by &quot;similarity&quot; of words and &quot;analogous problem&quot;.

[[[00000000000000000658---5f9814df1e2132097e448a293a14ee4c2a12bd7b94c90eda1ce270d9e0e6dcec]]]Word similarity evaluation is often performed using human-generated word similarity evaluation sets. For example, given a score between 0 and 10, ``cat&#39;&#39; and ``animal&#39;&#39; have a similarity of 8, and ``cat&#39;&#39; and ``car&#39;&#39; have a similarity of 2, and so on. score). Then, compare the human score with the word2vec cosine similarity score to see the correlation.

[[[00000000000000000659---565b819a43b9b99c61bff55f2163a5c289b24bf66ef57aa098f6e6d72cf4c35e]]]In the analogy problem evaluation, analogy problems such as &quot;king : queen = man : ?&quot; For example, the paper [27] presents the results of evaluation by analogy problem. Here, some excerpts from the results are shown in Figure 4-23.

[[[00000000000000000660---8362f54d72569ef67467b160331f2ab673b1cbf53bec424f8576e63a5abe4073]]]
Figure 4-23 Evaluation results of word vectors by analogy problem (Table created by excerpt from paper [27])


[[[00000000000000000661---b643cc1e43e1c253728b7d43af95dd3fcb0d660e1c4036cc1ee2e094bf1047e6]]]In Fig. 4-23, a comparative experiment was conducted using the word2vec model, the number of dimensions of the word distributed representation, and the corpus size as parameters. Each result is in the three columns on the right. The &quot;Semantics&quot; column in Figure 4-23 shows the accuracy rate for the analogy problem of inferring the meaning of words. For example, this is a question of the meaning of words like &quot;king : queen = actor : actress&quot;. On the other hand, &quot;Syntax&quot; is a problem that asks for morphological information of words, and corresponds to a problem such as &quot;bad : worst = good : best&quot;.

[[[00000000000000000662---cf957dfc80911e70d3d32e5ff35e646b3c1c88ee5048f2c3954b4adb5662510b]]]From the results in Figure 4-23, we can see the following:

[[[00000000000000000663---d91151642ff5909fea33e7960ae2ad1ddf581b9626bd83e3113988c64df51287]]]Different models have different accuracy (choose the best model according to the corpus)

[[[00000000000000000664---366eb371c697b0d09f2b3c481689856b1554ea05a7a386f85803753ff53a70b3]]]The bigger the corpus, the better (big data is always desired)

[[[00000000000000000665---d33175474475082326be65c1a75ce690ec109206ce3193c8fc3d69cf69e26d17]]]The number of dimensions of the word vector must be moderately large (accuracy will be poor if it is too large)

[[[00000000000000000666---9f63432c83598725bfb09a2ea902ccedeb9e2d7c26d2928ddfe7363ba4c5732c]]]By analogy problems, it is possible to measure (to some extent) whether you have correctly understood the meaning of words and grammatical problems. Therefore, if the distributed representation of words can solve the analogy problem with high accuracy, we can expect good results in applications that handle natural language. However, how much the goodness of the distributed word representation contributes (or does not contribute) to the target application changes depending on the situation of the problem to be handled, such as the type of application and the content of the corpus. In other words, a high score on the analogy problem does not guarantee that the target application will always produce good results. It is necessary to pay attention to this point.

[[[00000000000000000667---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000668---d316305f5f80518389e31794343861547aa11681bd7ab5af4a1ac1c10b9a139a]]]In this chapter, we improved the CBOW model in the previous chapter with the theme of speeding up word2vec. Specifically, we implemented an Embedding layer and introduced a new technique called Negative Sampling. The background to this is that the amount of computation increases in proportion to the increase in the number of vocabulary words in the corpus.

[[[00000000000000000669---148570fc559620db3b6cff559777c706a672a3dbdc15201fb156d3a20e8f1550]]]An important theme in this chapter is dealing with ``some&#39;&#39; rather than ``all&#39;&#39;. After all, just as humans cannot know everything, it is not realistic for computers (with their current capabilities) to process all data. Instead, it&#39;s more rewarding to focus on a few things that are important to you. In this chapter, we have taken a closer look at a technique based on that idea—Negative sampling. Negative sampling achieved computational efficiency by targeting only &quot;some&quot; words instead of &quot;all&quot; words.

[[[00000000000000000670---0df77062480453e9780b7da304176ffc8a23f9aae62365a6f10e18e828eb37c2]]]In the previous chapter and this chapter, the series of stories on the theme of word2vec is over. word2vec has had a huge impact on the field of natural language. The resulting distributed word representations are used for various natural language processing tasks. Furthermore, the idea of word2vec has been applied not only to natural language but also to other fields such as voice, image, and video. If you have a solid understanding of word2vec in this chapter, that knowledge should be useful in many areas.

[[[00000000000000000671---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000000672---f0a6e62f5dbf6414a0ca1b67764c1c3f7528dd74cdfc8e9d9000cb86ca699545]]]The embedding layer stores the distributed representation of words and extracts the corresponding word ID vector in forward propagation.

[[[00000000000000000673---36deff93b0679602b19fde64e516a883d2c7d55c83d7226f0f22dd082b75f1e5]]]In word2vec, the amount of calculation increases in proportion to the increase in vocabulary, so it is better to use a fast method that performs approximate calculation.

[[[00000000000000000674---fc7a4bcf4797b0758f737ea90cd901447a5b30931d90c17cc5b7479475613fe3]]]Negative sampling is a method of sampling several negative examples, and by using this, multi-value classification can be treated as binary classification.

[[[00000000000000000675---67f62303d666b5a23575c5936d0d272913c16771437f6130f85a09137cddbc7a]]]The distributed representation of words obtained by word2vec embeds the meaning of words, and words used in similar contexts are located close to each other in the space of word vectors.

[[[00000000000000000676---df1004b2b406c68df4810fa5922d8eac6d27412f11012a1883e7de7930bd16b6]]]Word2vec&#39;s distributed representation of words has the property that analogy problems can be solved by addition and subtraction of vectors.

[[[00000000000000000677---84a69f6e94d3fcf822f9f8c4ac0d111aa1d542f7be39cefd7fa6ecf46222bd37]]]word2vec is particularly important for transfer learning, and its distributed representation of words can be used for various natural language processing tasks.

[[[00000000000000000678---066c6a41a8ed489b43ec07285dc093bd8202eacf454506394df85a55c7652bd4]]]Appendix A

[[[00000000000000000679---39e63a71320d51401dbe2a8b5a09eee9d2854d3beda9006d4d3f05a68179713e]]]Differentiation of sigmoid and tanh functions

[[[00000000000000000680---2cd1872348f5d1a0f9acf74fc714047b4fbd9069b39ccce8fac2ec31911d1762]]]Various functions are used as activation functions in neural networks. Here, we take up the representative sigmoid function and tanh function, and explain the process of obtaining the differentiation of these two functions using two different approaches. Specifically, the derivative of the sigmoid function uses a computational graph, and the derivative of tanh is obtained by expanding the formula. Understanding each approach will help you become more familiar with computing derivatives.

[[[00000000000000000681---9c9d333a66d039b79ee4ea1b3bae8157a87d457dbcd001347e082b4a0f683231]]]sigmoid function

[[[00000000000000000682---e45d89423581f6ffcceb6f0ef6a0589aa1995a8b62e629c9d22906086aecb78b]]]The sigmoid function is represented by the following formula.

[[[00000000000000000683---8cec54c1fe5b2762f8989e53de9465c0152849aeff3795677d1ab139b590d50d]]]At this time, the calculation graph of formula (A.1) looks like Figure A-1.

[[[00000000000000000684---ad942049e1d29c98a0b4321fe8a8bda3a9ce1bff23b3b63667385a6b9fc564fa]]]
Figure A-1 Computation graph of sigmoid layer


[[[00000000000000000685---650178310d5220939d3e4b1cb3a1c305053097a3105de81f5e77532a8108b9d5]]]In Figure A-1 there are &quot;x&quot; and &quot;+&quot; nodes, as well as &quot;exp&quot; and &quot;/&quot; nodes. The exp node does the calculation y = exp(x) and the &#39;/&#39; node does the calculation. Now, let&#39;s use the computation graph to perform backpropagation while checking one by one.

[[[00000000000000000686---1e1a14c33f065ef0105b18caef6d183e1d0a0b539979e9cf4e1d923c6da00334]]]The &#39;/&#39; node represents , but this derivative is analytically represented by the formula: (A.2) According to formula (A.2), in the case of back propagation, the upstream gradient is multiplied by −y2 (the square of the output of forward propagation with a negative value) to flow downstream. increase. If you write it as a calculation graph, it will look like Figure A-2. Figure A-2 Backpropagation Step 1




[[[00000000000000000687---9a862938a68f2549ebdbbe04a12785c620b08db531722b869b3983fcf2e029e6]]]The &quot;+&quot; node just passes the upstream value downstream. The calculation graph looks like Figure A-3. Figure A-3 Backpropagation Step 2




[[[00000000000000000688---bb3e5b75cf312ea7ecbcafa839c857ee0edf56583bfed6a97ae67dcacaa6f917]]]The exp node represents y = exp(x) and its derivative is given by (A.3) In the computational graph, the upstream gradient is multiplied by the forward propagation output—exp(−x) in this example—to flow downstream (Figure A-4). Figure A-4 Backpropagation Step 3




[[[00000000000000000689---697a025c98c0851cb25da1c535d5bce558791861d012e0d27a810477e339eb2a]]]The &quot;x&quot; node multiplies the values that swapped the inputs during forward propagation. So here we multiply by -1. Figure A-5 Computation graph of sigmoid function (backpropagation) From the above, backpropagation of the sigmoid layer was successfully performed as the computation graph of Figure A-5. From the results in Figure A-5, the output of backpropagation is , and this value is propagated to downstream nodes. Furthermore, this can be organized and written as: (A.4) From this expansion, we can see that the backpropagation of the sigmoid function can be calculated only from the output of the forward propagation. Summarizing the above, the computational graph of the sigmoid function can be written as shown in Figure A-6. Figure A-6 Computation Graph of the sigmoid Function



[[[00000000000000000690---6e112f34910214e5d99e0484a5ee21365fa51b36d562f94af3dfc24b12277fb0]]]The above is the differentiation of the sigmoid function. Here, we used the computational graph to find the derivative of the sigmoid function. Next, we show the process of analytically obtaining the derivative of the tanh function.

[[[00000000000000000691---a5e1193fc3bc03bc76be9106e88e03bbe2439bce6ce1e1eb87b1909576650f5d]]]tanh function

[[[00000000000000000692---d517a35171ecd3d729040e585c781486a002d8085995df8fea7ac2ff3c19a06b]]]The tanh function is called the hyperbolic tangent function or hyperbolic tangent. This tanh function is represented by equation (A.5).

[[[00000000000000000693---42bbe197350e2d4ac11c54a15cc45d0372b5ae75f8a548a1d3d1abee466a02b0]]]Our goal here is to find for equation (A.5). To do so, we use the following differentiation formula:

[[[00000000000000000694---d1bbb27efe1779cf32ea00a3e2ee40882d4e2b18db5245552399d32414d860c8]]]Equation (A.6) is the formula for the differentiation of fractional functions. Here, for ease of viewing, the derivative of with respect to x is represented by . Similarly, the derivative of f(x) with respect to x is denoted by f&#39;(x).

[[[00000000000000000695---0d309de4466d472da5678d35835f77709f41b3a20319b824bf065c1173d8ed8d]]]Also, the following derivative can be derived analytically for the Napier number (e).

[[[00000000000000000696---857f5d6f81eb72ebb66c5610ca75fb865796099eb1b1116825ec32008ad25641]]]By using the above formulas (A.6), (A.7), and (A.8), the derivative of the tanh function can be obtained as follows.

[[[00000000000000000697---de1f6b41a42037bb0db3ce5d0adcc653d36f742b1f8d20624e89bce41c8dde59]]]As shown in equation (A.9), the derivative of the tanh function can be obtained by simple formatting using the &quot;formula for differentiation of fractional functions&quot;. And the result is 1−y2. From this result, the computational graph of the tanh function can be drawn as shown in Figure A-7.

[[[00000000000000000698---b4169c877085bc714e4230491454d1a357ee4e517b778ac0223f68cbf944add9]]]
Figure A-7 Computation Graph of the tanh Function


[[[00000000000000000699---254092c030245ac39630627e53a8a63ce6884d6bc0cf12db3e9117e8ccbac08b]]]The above is the derivation of the derivative of the tanh function. By expanding the formula, we were able to obtain the differentiation concisely and clearly.

[[[00000000000000000700---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000701---92c4666aa40295c9eab7f678ff0911eede4c5b79c4166d373955fe9b826ae61b]]]Here we have solved the derivative in two ways: the computational graph and the analytical way. These two methods both reach the same goal. Therefore, depending on the problem, you should use one or the other as appropriate. I think that the formula is probably more convenient when you get used to it. But initially—especially if you&#39;re wondering—the visual method of computational graphs might help. Also, being able to solve a problem in multiple ways is sometimes very important. As you&#39;ve learned here, looking at a problem sometimes as a formula and sometimes as a computational graph will give you a better understanding.

[[[00000000000000000702---25776fc8b2e92d3d94c0cea93957def950b0a750b090f7c92cc11575b1ce6318]]]Chapter 2

[[[00000000000000000703---8871acd718965cae689bcaaa9af4fe7f91edf6f718bc14ea7be40720b869342e]]]Natural Language and Distributed Representation of Words

[[[00000000000000000704---29caa3754be5281e8a24a9e7064222c153d9f682d2fb3693063e2fdba6a325ba]]]Marty: &quot;This is heavy&quot;

[[[00000000000000000705---dd569b72ab42128a1b14ecb4463f7a61c62d493b7d166baebf8895abbeab7be5]]]Doc: &quot;Will things be that heavy in the future?&quot;

[[[00000000000000000706---58566dd2d795c99e654d8482b2a57520771e76b6e0ede3273f9c39a1ccfba8dd]]]——Movie “Back to the Future”

[[[00000000000000000707---52abbdc3e5147180e4571c992e7b38ad10c57b9ebcf54a70f4905bd1a50ad429]]]From now on, we will finally step into the world of natural language processing. Natural language processing covers a wide range of fields, but the essential problem is getting computers to understand our words. In this chapter, we will focus on what it means to make computers understand words, and what kinds of approaches exist. In particular, we&#39;ll take a closer look at classical methods—methods that predate the advent of deep learning. From the next chapter, we will move on to methods based on deep learning (more precisely, neural networks).

[[[00000000000000000708---66dfab3f0e77cf7751db923ea2670ba73a74ff5eb63ad8e9fa4bac4ec7f27a56]]]In this chapter, you will also practice working with text in Python. It implements processes such as dividing text into words and converting words into word IDs. The functions implemented in this chapter will also be used in the following chapters. In other words, this chapter also serves as a preliminary preparation for text processing in the future. Now let&#39;s move on to the world of natural language processing!

[[[00000000000000000709---1e1e52d94f82ea2fb03488e7f9edddc8b705d1bd41f847c120316c6df9b70edc]]]What is natural language processing?

[[[00000000000000000710---e692ed291d79c5502071961f5aefaf70f712f18ba584e1f689c5da87e3f19544]]]The words that we usually use, such as Japanese and English, are called natural languages. Natural Language Processing (NLP) literally means “the field of processing natural language”. To put it simply, it means &quot;technologies (fields) for making computers understand our words.&quot; In other words, the goal of natural language processing is to make computers understand human speech and do things that are useful to us.

[[[00000000000000000711---b5bf64d25edf6dfbb1d7a76001daa23e1fe231867c710997fad15aad40424ce5]]]By the way, when you think of languages that computers can understand, you may think of things like &quot;programming languages&quot; and &quot;markup languages.&quot; These languages have grammars defined so that the meaning of the code can be interpreted uniquely, and computers analyze the code according to fixed rules.

[[[00000000000000000712---8da318780bae485ee90b9e06b53fc946fcaa0fc8b33b3b3cd309c750754a587e]]]As we all know, programming languages in general are mechanical and inorganic languages. This is what you might call a &quot;hard language&quot;. On the other hand, natural languages such as English and Japanese are “flexible languages”. “Flexible” means that the meaning and form can change flexibly, such as the possibility of various expressions even in sentences with the same meaning, or the ambiguity of sentences. Also, new words and new meanings are born (or become obsolete) with the times. This is also a manifestation of the flexibility of natural language.

[[[00000000000000000713---cc96332920934c8037b86f5653ded0c7a2ff354405cd39c0f0e5b7d774fbcd41]]]In this way, natural language is a living language, and there is &quot;softness&quot; in it. Therefore, getting a hard-headed computer to understand natural language is a difficult problem. But if we can overcome that conundrum—if we can make computers understand natural language—we can make computers do things that are useful to humans. In fact, we can see many such examples. For example, search engines and machine translation are easy to understand examples. In addition, many natural language processing technologies are already used around us, such as question answering systems, IME (Kana-Kanji conversion), automatic summarization of sentences, and sentiment analysis.

[[[00000000000000000714---7d850b53e6a40f4d94ad006879223849472a4cf62104234bec0a5f5d0bf1827e]]]One of the applications of natural language processing is &quot;question answering system&quot;. IBM&#39;s Watson is a well-known example. Watson made a name for himself on the 2011 American quiz show Jeopardy! &quot;was. On that show, Watson answered the quiz more accurately than anyone else and beat the previous champion (although Watson had the advantage of being given text questions). This “incident” has attracted a lot of attention from the public, and I feel that since around this time, public expectations and concerns about artificial intelligence have increased. IBM also calls Watson a &quot;decision-making support system.&quot; Recently, a case was reported in which a patient&#39;s life was saved by proposing the correct treatment method for a patient with an intractable disease by utilizing a huge amount of past medical data. It has been.

[[[00000000000000000715---3f84cdfc83ed58c7a0acbd1207b22b22a9461bbeea4ad0f08d3eda074e33c3fc]]]the meaning of the word

[[[00000000000000000716---7131bbbd312afbbcdb77e00e89de89d759fada977ccd00e489507afa6d334f8c]]]Our language is made up of letters. And the meaning of our language is composed of &quot;words&quot;. A word is, so to speak, the smallest unit of meaning. Therefore, in order to make a computer understand natural language, it seems to be said that it is important to make a computer understand &quot;the meaning of words&quot;.

[[[00000000000000000717---dce0941de89602531c172f763b3d5f668894359117b4da163948f801793a0de6]]]The theme of this chapter is to make the computer understand the &quot;meaning of words&quot;. To be more precise, I will think about how to express the &quot;meaning of words&quot; well. Specifically, we will look at the following three techniques in this and the next chapter.

[[[00000000000000000718---3b036300c3af1f582c11ff038c982dd214a8a2c5a539abf7b4d5ad4cbb1dd0ef]]]Thesaurus method This chapter

[[[00000000000000000719---8976edaad853b77382e383ed45209266ca6df7c06a698ca0682f22968a7e9270]]]Count-based method This chapter

[[[00000000000000000720---acc09477230068d631f501635e29d827090d6384771b49efd5b4d8ba3ad99520]]]Inference-based method (word2vec) Next chapter

[[[00000000000000000721---15796db5d4afdd2a6e4c090cf151e67ec2ab998bdbba0eafba61fdbe80de8e07]]]First, let&#39;s take a quick look at how to use a hand-made thesaurus (thesaurus). Next, we describe a technique for representing words from statistical information—here, called the &quot;count-based technique.&quot; That&#39;s all you&#39;ll learn in this chapter. And in the next chapter, we will deal with an &quot;inference-based&quot; method with neural networks (specifically, a method called word2vec). The structure of this chapter is based on the Stanford University course &quot;CS224d: Deep Learning for Natural Language Processing&quot; [10].

[[[00000000000000000722---470716975fb21ce7a6bc6e4d18ce2d6f11caa810d76422b72aa53a303887493b]]]thesaurus

[[[00000000000000000723---68cf985a5dea6e2a1042bc1c455c809f1c89092fc97afa9f09e77026ab92eedb]]]In order to express the &quot;meaning of a word&quot;, it is conceivable to define the meaning of the word manually. One way to do this is to explain the meaning of each word, like in a dictionary such as &quot;Kojien&quot;. For example, if you look up the word &quot;automobile&quot; in a dictionary, you will find explanations such as &quot;vehicles or vehicles that are fitted with wheels and are propelled by them...&quot;. If we define words that way, computers might be able to understand what they mean.

[[[00000000000000000724---6b28df7820e19ac598307daf5cbf4bf0d4b3e0de78c12f729a3d941853a01d98]]]Looking back at the history of natural language processing, there have been many attempts to manually define the meaning of words. However, instead of general dictionaries used by people like &quot;Kojien,&quot; a type of dictionary called thesaurus has been used in many cases. A thesaurus is (basically) a thesaurus, in which &quot;words with the same meaning (synonyms)&quot; and &quot;words with similar meanings (synonyms)&quot; are classified into the same group. For example, if you use a thesaurus, you can find that there are synonyms for car, such as automobile and motorcar (see Figure 2-1).

[[[00000000000000000725---6f67082c711e927580a95cd22a208306b620c097620a3acab578aadb7b499871]]]
Figure 2-1 Examples of synonyms: car, auto, automobile, etc. are synonyms for &quot;car&quot;


[[[00000000000000000726---a2ff0720debd0a8faa948502ed91234f6e892be5670a1f1e693711cbdcb492e2]]]In addition, the thesaurus used in natural language processing sometimes defines more detailed relationships between words, such as &quot;upper and lower&quot; and &quot;whole and part&quot;. Specifically, as shown in Figure 2-2, the relationship between words is defined by the graph structure.

[[[00000000000000000727---60bb19b510a3b4e2e808a06707ab03485a3b9381224b9e1f04582bad06d63299]]]
Figure 2-2 Form a graph based on the relationship between the upper and lower levels of the meaning of each word (the figure is created with reference to [14])


[[[00000000000000000728---ba94ac51aafcb386cbd4b495c95020a5ae16f06db6051df859679aa3697788d5]]]In Figure 2-2, the word &quot;motor vehicle&quot; is positioned as a superordinate concept of the word &quot;car&quot;. In addition, it shows that there are more specific car models such as &quot;SUV&quot;, &quot;compact&quot;, and &quot;hatch-back&quot; in the subordinate concept of &quot;car&quot;.

[[[00000000000000000729---9b4d51eaae37a84e5df6d4a34e1ac47839e95f691cf86af4137dd8054b0060b9]]]By creating a set of synonyms for all words in this way and expressing the relationship between each word in a graph, we can define the connections between words. By using this &quot;word network&quot;, we can teach the computer the relationships between words. This could be said to have given the computer (even indirectly) the meaning of words. And we can use that knowledge to make computers do things that are useful to us.

[[[00000000000000000730---46524a6b4e30b05ebf956d497db8429fbc6a42aa7b3882402b416476e5512d76]]]How you use a thesaurus depends on your natural language processing application. For example, in an information retrieval scene, if you know that automobile and car are synonyms, you can include search results for automobile in search results for car.

[[[00000000000000000731---999e99dd85e7e4d0e2c3a8cb02eafaab31831449506bdcd1478060f3be1e365f]]]In the field of natural language processing, the most popular thesaurus is WordNet[17]. WordNet is a traditional thesaurus that was developed at Princeton University in 1985 and has been used in many research projects. It is also very active in various natural language processing applications.

[[[00000000000000000732---8e5347607ff2203c303a5124bac5594a0799ffda3af204de2f484599ed80d6d1]]]With WordNet, you can get synonyms and use &quot;word networks&quot;. It is also possible to calculate similarity between words using word networks. I won&#39;t go into detail about WordNet here. For those interested in implementing Python using WordNet, see Appendix B, Running WordNet. In Appendix B, we install WordNet (installing a module called NLTK to be exact) and do some quick experiments.

[[[00000000000000000733---4e0a06ac27f9ebf20fc4f99ea944305eeedee22cd860981fee77e5043020d107]]]In &quot;Appendix B Running WordNet&quot;, we will actually use WordNet to conduct an experiment to find the degree of similarity between words. Here, we will look at an example where similarity between words can be calculated based on a &quot;word network&quot; defined by humans. If it is possible to calculate the degree of similarity between words (correctly to some extent), it can be said to be the first step toward understanding the &quot;meaning of words.&quot;

[[[00000000000000000734---40262a357f0ff1484a87bf52ec88faa46fa5ed264450e411214bf1890d7ecc41]]]Problems with thesaurus

[[[00000000000000000735---9b784fedde3d46e3cc9ba0561a7fd83021af7649da31097332742888034893e4]]]A thesaurus like WordNet defines relationships such as synonyms and hierarchical structures for many words. And if we use that knowledge, it seems that we can (indirectly) give the computer &quot;the meaning of words.&quot; However, there are major drawbacks to their manual labeling. Below, I list the problems with the thesaurus approach and add a brief explanation for each.

[[[00000000000000000736---f94d59dab6784a365f5d68362817c7282ac1d45a9d37975f749a815b4dfee307]]]Difficulty adapting to changing times

[[[00000000000000000737---645b0cb67a0a3552931dfb1e72a1acfbf9fceb09c3430ff37327edbb9a7b67cd]]]The words we use are alive. New words are born over time, and the dusty old words will one day be forgotten. For example, the term “crowdfunding” is a new coined word that has recently started to be used. There are also cases where the meaning of words changes over time. For example, the English word &quot;heavy&quot; means &quot;(something) is serious&quot; (this is mostly used as slang). However, in the past it was not used that way. In the movie Back to the Future, there is a scene where Marty from the future in 1985 and Doc from 1955 don&#39;t understand what &quot;heavy&quot; means. To keep up with such word changes, the thesaurus must be continually updated manually.

[[[00000000000000000738---751c6b5310ec5e7f37471d1e964175a9c983605918b27e729b74c3b4b8cb8a01]]]high labor cost

[[[00000000000000000739---4833c3f69ecaad3c5b21cafa88d3fe85a2666f1e497ef8c31da674c2ad1ef204]]]Creating a thesaurus incurs significant human costs. Taking English as an example, the total number of existing English words is said to exceed 10 million. So, ideally for such a huge number of words, we need to do word associations. By the way, WordNet seems to have more than 200,000 words registered.

[[[00000000000000000740---27b19f463eb322468cdc774c02026cb9932be21b784d8a78b50cf4c5fa1d6c25]]]Inability to express subtle nuances of words

[[[00000000000000000741---1844ec64d2202346d3141df712e7b9fc7ec9257161500fdca33e1f2f10566ab7]]]A thesaurus groups similar words together as synonyms. But in reality, even similar words have different nuances. For example, the words &quot;vintage&quot; and &quot;retro&quot; mean the same thing, but are used differently. A thesaurus cannot represent such nuanced differences (if done manually, it would be extremely difficult).

[[[00000000000000000742---6bd01f9b5a4038ecc34e7a966a41645bc2973df007e21ea62483d1ce126bc7b9]]]Thus, the method of using a thesaurus—in other words, of manually defining the meaning of words—has a major problem. In order to avoid such problems, we proceed to &quot;count-based method&quot; and &quot;inference-based method&quot; using neural networks. These two methods automatically extract &quot;word meanings&quot; from large amounts of text data. This frees you from the heavy lifting of manually connecting words!

[[[00000000000000000743---781a1b60dab7c811a7a652528b1cef201627e014d2b46542cf0b9aef092f0da9]]]Not only in natural language processing, but also in the field of image recognition, the task of manually designing feature values has been done for many years. However, deep learning has made it possible to obtain the desired results directly from raw images, greatly reducing the need for human intervention. A similar phenomenon occurs in the field of natural language processing. In other words, we are shifting from manual thesaurus and features (features) design to a paradigm that obtains the desired results from text data with as little human intervention as possible.

[[[00000000000000000744---45bc0a55ef99eaff3e8e5f4415d496256d595e8734983822811985193093ae29]]]count-based method

[[[00000000000000000745---bcb515f7e103394ecfd504eaa709f2694a831890fbcd1bbe7de606b77dc99f18]]]As we move forward to count-based methods, we use a corpus. A corpus is simply a large amount of text data. However, text data collected purposefully for NLP research and applications, rather than blindly collected text data, is generally called a &quot;corpus.&quot;

[[[00000000000000000746---e2fc2ab8864f494519339aa26940f0c3dee19def087102ac7f93402d33ae3932]]]After all, a corpus is just text data. However, the text contained therein was written by human hands. Another way of looking at this is that the corpus contains a wealth of human &quot;knowledge&quot; of natural language. A corpus contains knowledge of a person&#39;s natural language: how to write sentences, how to choose words, and what words mean. The goal of count-based methods is to automatically and efficiently extract the essence from such a corpus of human knowledge.

[[[00000000000000000747---b40395194f0c8b2bed8f8699916f8d510d513dbb4224f6ecf328a1cad03507e2]]]Corpora used in the field of natural language processing may have additional information given to text data. For example, a case where each word in the text data is labeled with a &quot;part of speech&quot;. In that case, the corpus is generally given in a structured form (for example, in a data format such as a tree structure) so that computers can easily handle it. We assume that the corpus we use here is given as plain text data—one large text file—without such additional labels.

[[[00000000000000000748---3c980b3b73e43372b31a234f8d709b9bba0a113f2a932a5040c70531108a560b]]]Corpus preparation with Python

[[[00000000000000000749---569b9537e5bac084f406d36d353cb824f668ce8509b13154673ae3754aad423f]]]There are various types of corpora used in the field of natural language processing. Famous examples include text data such as Wikipedia and Google News. The works of great authors such as Shakespeare and Soseki Natsume are also used as a corpus. In this chapter, we first use a simple text consisting of one sentence as a corpus. After that, we will deal with more practical corpora.

[[[00000000000000000750---8e1c76bbc2deae7087c4e5d56811d50e58691eb3224b9024748e142f7c96a7dd]]]Now use Python&#39;s interactive mode to preprocess a very small text data (corpus). Preprocessing here means splitting the text data into words and converting the split words into a list of word IDs.

[[[00000000000000000751---2ddc9db02090b453cf00df2f6431e9e8e81f65a78ffb970e5967d55270d03972]]]Now, let&#39;s check them one by one and assemble them step by step. First, let&#39;s start with the sample sentences that will be used as the corpus this time.

[[[00000000000000000752---8940a0d5c0689db4817572548dbdf335fd63725ea61bedb32f0aea8bfbee77e1]]]Here, we use a text consisting of a single sentence as a corpus. Originally, this text would contain thousands or tens of thousands of sentences (concatenated). However, here we will give priority to simplicity and perform preprocessing on this small text data. Now let&#39;s split the above text into words.

[[[00000000000000000753---0abd061b01afb472d32069b11504e1b82ad5414fc7ba50235ae56aac7a7ffbff]]]Here we first use the lower() method to convert all letters to lowercase. This is a measure to treat even the first word of a sentence as a common word. And split(&#39; &#39;) splits with spaces as &#39;delimiters&#39;. However, here, considering the period (.) at the end of the sentence, we insert a space before the period——correctly, replace “.” with “.”——and then divide.

[[[00000000000000000754---de5482d8934bfead877eaee3f0f29c15d2b63a4a6e91692f3daa2ec5efc7780f]]]In doing the word splitting here, I did a &quot;kick-off implementation&quot; that puts a space before the period. There is a smarter and more versatile way to do this. It&#39;s a way to use regular expressions. One way is to import the regular expression re module and run re.split(&#39;(\W+)?&#39;, text) to split by word. For details on regular expressions, please refer to the book &quot;Detailed Explanation of Regular Expressions 3rd Edition&quot; [15].

[[[00000000000000000755---f70fd8c5e17cd0b8a64bbb2a710960cbc20923a467cba9dca28609223dd12d4a]]]The original sentence is now available as a list of words. Splitting the words makes it easier to handle, but it is somewhat inconvenient to manipulate the words as they are. Therefore, we assign IDs to words and modify them so that they can be used as a list of IDs. As a preliminary preparation, create a correspondence table of word IDs and words in a Python dictionary.

[[[00000000000000000756---ef56e0722a7c2b9ffc803c49859e5affa50d6c007e8ece097a5b39d87b15f5cf]]]The variable id_to_word is responsible for converting word ids to words (key is word id, value is word) and word_to_id is responsible for converting word to word id. Here we go through each element of words split into words one by one, and if the word does not exist in word_to_id, we add a new id and word to word_to_id and id_to_word respectively. Note that since the length of the dictionary is set as the new word ID, the word IDs will increase from 0, 1, 2, and so on.

[[[00000000000000000757---6a5b4db7b88d20be4ff35a0db480dcf6994ab14adff08ba791269a2e69c0584f]]]With this, I was able to create a correspondence table between word IDs and words. Now let&#39;s take a look inside.

[[[00000000000000000758---5c729fc575c22c826b5f488e637f8c25d591f3173dd2379016e603d1a937a142]]]With these dictionaries, you can search for word IDs from words and vice versa for words from word IDs. In practice, it looks like this:

[[[00000000000000000759---59d82c602017eac4169419fb2a4720c9a4b7160fbdb57df9f513a04b46f91d14]]]So finally, let&#39;s convert &quot;list of words&quot; to &quot;list of word ids&quot;. Here we use a Python comprehension to convert the word list to a word ID list. And convert it to a NumPy array.

[[[00000000000000000760---e5cd16d5568ba447e8ddb94e0fbc79a9a8b576f461746e17ff4562b86741410b]]]A comprehension is a notation for writing simple loop processing such as lists and dictionaries. For example, if you wanted to square each element of the list xs = [1,2,3,4] to make a new list, you could write [x**2 for x in xs].

[[[00000000000000000761---83b6c464c40942d05daaefe6cdc8919a2f5b23127253eb496cd6ae112a33e356]]]Now you are ready to use the corpus. Let&#39;s implement the above processing collectively as a function named preprocess() (☞ common/util.py).

[[[00000000000000000762---dfa7f51654cbb739a7f44852de5fa0e29dde007bfa5f08e796191302b511f58c]]]With this function, corpus preprocessing can be done as follows:

[[[00000000000000000763---b6021d86ec1d3658eb288d824f368f15a77c8c29c080f9bb7e1a30f538a09274]]]This completes the preprocessing of the corpus. The three variable names prepared here, corpus, word_to_id, and id_to_word, will appear in many places in the rest of this book. corpus is a list of word ids, word_to_id is a dictionary from word to word ids, id_to_word is a dictionary from word ids to words.

[[[00000000000000000764---584a21ff2bb8ad05dabef61d907362f2411b1128f18b3fc25ca6292ceac8e976]]]Now you are ready to work with the corpus. Our future goal is to use the corpus to extract &quot;word meanings&quot;. To that end, we will look at the “count-based method” in this section. This technique allows us to represent words as vectors.

[[[00000000000000000765---e433a2caad6307706f227311f74f8eefd79ff18d9b24e8b00a3c123baef5005c]]]distributed representation of words

[[[00000000000000000766---3cc06160d8072c4027179b6c4e2685b33926a7c305b469a698f6ea3f6ef33c01]]]Suddenly, there are various “colors” in the world. Colors have specific names, for example, &#39;cobalt blue&#39; and &#39;sink red&#39;. On the other hand, colors can also be expressed in terms of how many RGB (Red/Green/Blue) components are present. While the former names names that differ by the number of colors, the latter represents colors as three-dimensional vectors.

[[[00000000000000000767---0e62d1733e350a73e114526cec7c115dd33aa596f64bb23252a7d2d79d6d9a79]]]The point to be noted here is that vector representations such as RGB can specify colors more accurately. In addition, it has the advantage that it can be expressed compactly with only three components, and (in many cases) it is easy to create a color image. For example, even if you don&#39;t know what kind of color &quot;Kokiake&quot; is, if you say (R, G, B) = (201, 23, 30), it&#39;s a reddish color. I understand that. In addition, it is easier to determine the relationship between colors—whether they are similar or not—and quantify them more easily with vector representation.

[[[00000000000000000768---9935f61b47cb8888f90fddaba469d1439999f197e9deea3234f76a8dcbe35616]]]Then, is it possible to do something like the vector representation of &quot;color&quot; that I talked about here with &quot;words&quot;? More precisely, could we construct a compact and sensible vector representation for the field &quot;word&quot;? What we should aim for from now on is a vector representation that accurately captures the &quot;meaning of words.&quot; In the field of natural language processing, this is called a distributed representation of words.

[[[00000000000000000769---66cc82db8f9b61807ebb0bfa2128e9120b1bd5c610e65165bafe1ccc77c37a55]]]A distributed word representation represents a word as a fixed-length vector. And the vector is characterized by being represented by a dense vector. A dense vector means that (many of) each element of the vector is represented as a non-zero real value. For example, a 3D variance representation would be [0.21, -0.45, 0.83]. An important future topic is how to construct such a distributed representation of words.

[[[00000000000000000770---22ab7890bbb9ea82584f80d91de4bf3514e8fba2834d2c4ccce83e68c6807d42]]]distribution hypothesis

[[[00000000000000000771---ea4396fc209606be378f4a3d88792c52358be3dd8d7c350864b3040aaa6c3814]]]In the history of natural language processing, many studies have been done to express words as vectors. If you look at such studies, you will find that almost all important methods are based on one simple idea. The idea is that the meaning of a word is formed by the words around it. This is called the distributional hypothesis, and much of the recent research into word vectors is based on this hypothesis.

[[[00000000000000000772---0c85ce37a683229d4206a35707c9aaef3dca646d433020305b589ce0caf9a12c]]]The meaning of a word is formed by the words that surround it—what the Distribution Hypothesis says is very simple. Words themselves have no meaning, they say that the meaning of a word is formed by the &quot;context&quot; of the word. Indeed, words that are semantically the same occur many times in similar contexts. For example, &quot;I drink beer.&quot; and &quot;We drink wine.&quot; Also, sentences like &quot;I guzzle beer.&quot; It also leads to the fact that guzzle and drink are words with similar meanings (by the way, guzzle is a word that means &quot;to drink guzzle&quot;).

[[[00000000000000000773---ad9bb2e857f97e1f79a1e5bebe3e77b826d907070448f93f2035558ac67c1de8]]]Now, I will use the word “context” a lot from now on. When we say &quot;context&quot; in this chapter, it refers to the words that exist around it (relative to the word of interest). For example, in the example in Figure 2-3, the two left and right words correspond to &quot;context.&quot;

[[[00000000000000000774---720c3442076cc191dc8d47c88093dd60217b00d794dd4f731019d8c02125efb5]]]
Figure 2-3 An example of a &quot;context&quot; with a window size of 2. Focus on the word &quot;goodbye&quot; and use the two words to the left and right of it as context


[[[00000000000000000775---7a6c8c45ae9cfc67c2a9c036fe7b1cb868ac73c49128b2362eaa9910eb6dedcb]]]As shown in Figure 2-3, &quot;context&quot; refers to the surrounding words for a central word. We also use the term &quot;window size&quot; to describe the size of the context—that is, how much of the surrounding words it contains. A window size of 1 means 1 word left and right, a window size of 2 means 2 words left and right, and so on.

[[[00000000000000000776---c75e81f8063c700d7b0185c46de0cd87f670560013b649a04b581fcec4863938]]]Here I&#39;ve even included the left and right words for context. However, in some situations, you might want to use just the left word or just the right word as context. You can also think of a context that considers sentence breaks. In the interest of simplicity, this book does not consider sentence breaks and only treats left-right contexts.

[[[00000000000000000777---22c801a391878b2b680531716422cc9b9241785e462b98559188576f046baf83]]]co-occurrence matrix

[[[00000000000000000778---95d665d9b2c4631c981def53f73ee190095149b7189f264c85552c2b3a8e39bd]]]Now let&#39;s think about how to represent words as vectors based on the distribution hypothesis. A naive way to do this is to &quot;count&quot; the surrounding words. Specifically, when we focus on a certain word, we count how many words appear around that word and tally it up. We will refer to this as the &quot;count-based method&quot;. Note that this is sometimes called a &quot;statistical method&quot; depending on the literature.

[[[00000000000000000779---01b9cb0373ea61e250d20b22cd5de2125363811682afb70b49fc0be62d17b68a]]]Now let&#39;s look at count-based techniques. Here, we will start from preparing again using the corpus and the preprocess() function in &quot;2.3.1 Preliminary preparation of the corpus with Python&quot;.

[[[00000000000000000780---917bb39549541ca68f91613d9a9d68fdd19080616fda8325cd3ad1255679d8b8]]]From the above result, you can see that there are 7 vocabularies in total here. Now, for each word, count the frequency of words in that context. Here, the window size is set to 1, and we start with &quot;you&quot;, which has a word ID of 0.

[[[00000000000000000781---f778f30de11de6f7ee693c4b47c400b75724bbdeb028d0b8a2b90257f734c5cc]]]
Figure 2-4 Counting Contexts for the Word &quot;you&quot;


[[[00000000000000000782---82ebe5509c3e11882ec06862117f8371ec17d2a957e96d4ffcc2e9ad13c11fa9]]]As you can see from Figure 2-4, there is only one word &quot;say&quot; in the context of the word &quot;you&quot;. Therefore, if you represent this in a table, it will look like Figure 2-5.

[[[00000000000000000783---7be57ad4a863c10b1598df9bc82bd7f03081f9fc5ddd9a1197b20bcf4cce8bc5]]]
Figure 2-5 A table representing the frequency of words in the context of the word &quot;you&quot;


[[[00000000000000000784---d0fb433d8d30a2f71ca4b217cf3c7efbcaafaca39f0191ddd72b1ee65590fa7a]]]Figure 2-5 shows the frequency of words co-occurring as context for the word &quot;you&quot;. And this also means that the word &quot;you&quot; can be represented by the vector [0, 1, 0, 0, 0, 0, 0].

[[[00000000000000000785---4c665a3261a484988b14a3b935053619bc11b51c43743824b86e49f8e5e9c335]]]Now, let&#39;s do the same thing for &quot;say&quot; with a word ID of 1. The result should look like Figure 2-6.

[[[00000000000000000786---9a28e41c9cd43e177cbd9a5b6ca2790f2f5e8da5f284e73428f896578d6b8332]]]
Figure 2-6 A Table Representing the Frequency of Words in the Context of the Word &quot;say&quot;


[[[00000000000000000787---d95f40ad20db1461330594a0eec6f41635363629b41607fc7a7670d531ab1701]]]From the result above, we can see that the word &quot;say&quot; can be represented by the vector [1, 0, 1, 0, 1, 1, 0]. Do this for all words—here, seven words. The result should look like Figure 2-7.

[[[00000000000000000788---b60d650c6a153504d9669f8431731536e4109675880694ca78c4b3736982aefa]]]
Figure 2-7 For each word, count the frequency of words in its context and summarize in a table


[[[00000000000000000789---f47916bfd1d4f55caf495b2754e79c2af080d67f26b4f3a7f3de6f1bb498b648]]]Figure 2-7 summarizes co-occurring words in a table for all words. Each row in this table corresponds to a vector of words in question. Since the table in Figure 2-7 is in the form of a matrix, it is called a co-occurrence matrix.

[[[00000000000000000790---146d232e0b0886f43cdcaba62bff63da91fb7958bb995f77d43ff9094e01907d]]]Let&#39;s actually create the above co-occurrence matrix. Here, we will manually type in the results in Figure 2-7.

[[[00000000000000000791---21cacf4f00970fe95f4676d80fcae8d0747d6bd4efc232d6ee6275246744acb6]]]Now we have a co-occurrence matrix. Using this co-occurrence matrix, the vector for each word can be obtained as follows.

[[[00000000000000000792---382c017cf5bbb53f7e438150ea6ab9d160e335762029d90a60ee4cfa0fbad1d7]]]# vector with word id 0

[[[00000000000000000793---67db047c2a30b8d9932ceda90b674702b9662200319eec7422a8dcbf44ba6807]]]# vector with word id 4

[[[00000000000000000794---926999d57f2366b4114bfa4c633e6aab044adfd5ae6fafcb3099a9a0d40d7e24]]]# &quot;goodbye&quot; vector

[[[00000000000000000795---8bbd92630b2463656478b305492c7d9b35113e8fdec6e95e29d7580518ef153f]]]As shown here, we were able to vectorize the words by the co-occurrence matrix. I created the co-occurrence matrix manually here, but of course this can be automated. Now let&#39;s implement a function that creates a co-occurrence matrix from the corpus. Here, it is implemented with the function name create_co_matrix(corpus, vocab_size, window_size=1). The argument corpus is a list of word IDs, vocab_size is the number of words, and window_size is the window size (☞ common/util.py).

[[[00000000000000000796---4d0bcaf19f9759fa29474346ec5ac6f815cf518914c9a1c448875542b0019a17]]]Here, we first initialize the co_matrix with a two-dimensional array with 0 elements. Then, for each word in the corpus, count the words in that window. At this time, the left and right edges of the corpus are checked for protruding.

[[[00000000000000000797---57205c9038771b760f6c1e99fbc826a8c83d748390e8944882cfd4b174d31e7d]]]Now, no matter how big the corpus becomes, you can automatically create a co-occurrence matrix. From now on, we will use this function to create the corpus co-occurrence matrix.

[[[00000000000000000798---d970d2ab07c2175bbfa0f6313b2a8d4dbc7903e50c236fdcee1771c353e1b50a]]]Similarity between vectors

[[[00000000000000000799---73aceb091bd05aefaa8a90958ccf0fc66893fae0d28824395e800b5bc7090334]]]We were able to represent the words in vectors by the co-occurrence matrix. Next, let&#39;s look at how to measure the similarity between vectors.

[[[00000000000000000800---1841406e7a866938d302f4079075b46567f92cb34c42100562bfcac633cfdd5f]]]There are many possible ways to measure the similarity between vectors. Typical examples include inner product of vectors and Euclidean distance. There are many other methods, but cosine similarity is often used for the similarity of vector representations of words. Cosine similarity is defined by the following formula when we have two vectors with

[[[00000000000000000801---a96d9537aaa18c8679424ed4971760fc2de3f336cae88b7b7dfecd07bfed1649]]]In equation (2.1), the numerator is the inner product of the vectors, and the denominator is the &quot;norm&quot; of each vector. The norm is the magnitude of the vector, and here we calculate the &quot;L2 norm&quot; (the L2 norm is calculated as the square root of the sum of the squares of each element of the vector). The point of equation (2.1) is to normalize the vectors and then take the inner product.

[[[00000000000000000802---19e8afb9d80338da72362c0bd07d987ba87765fc02f99c53f11207d3f54cabd7]]]Cosine similarity intuitively expresses &quot;how much two vectors point in the same direction&quot;. The cosine similarity is 1 when the two vectors are pointing in exactly the same direction, and -1 when they are pointing in exactly the opposite direction.

[[[00000000000000000803---c65ea00c656eceb6d8c866866322e01bf2efac096cf74a50932e239ba1b7a051]]]Now let&#39;s implement cosine similarity. Implemented according to formula (2.1), we can write:

[[[00000000000000000804---44f3570794c7a1bdb9aa127ee02e048963f09f614321403e7a0ce6487ede516c]]]# normalize x

[[[00000000000000000805---d68606c7bb0f9db3c9c08a4b2263ca9e24b85d06cbd820fb9cfa6cca8ca90471]]]# Normalize y

[[[00000000000000000806---f14f8f9c2e9ea2dc1d6e1b20b5fd4e00944b0bfcb3c1c1ab08a63420e415202a]]]Here we expect the arguments x and y to be NumPy arrays. The underlying implementation first normalizes the vectors and then finds the inner product of the two vectors. This completes the cosine similarity implementation, but there is one problem with this implementation. That is, if a zero vector (a vector whose elements are all 0) is entered as an argument, &quot;division by 0&quot; will occur.

[[[00000000000000000807---f0742e824857d63429e25f37a3ce05988831a8acc3f36e5ed7752a8361a60122]]]A common practice for problems like this is to add small numbers when doing division. Here, eps is specified as a small value in the argument, and if nothing is specified, eps=1e-8 (= 0.00000001) is set (eps is an abbreviation for &quot;epsilon&quot;). . So here&#39;s an improved implementation (☞ common/util.py).

[[[00000000000000000808---4d3da625e044509e061f6118bd5f64ae6fbdb26f060113464d69f7e04868d391]]]I used 1e-8 as a small value here, but values this small are usually &quot;absorbed&quot; by other values due to &quot;rounding errors&quot; in floating point numbers. In the implementation above, the addition of eps will have no effect on the final result in most cases, as that small value will be &quot;absorbed&quot; into the norm of the vector. On the other hand, when the norm of the vector is 0, the small value is left as is, preventing &quot;divide by 0&quot;.

[[[00000000000000000809---cc9660cac16e7b04ae0e404fd856df05d28414e954b6b26568470f5aa1d51424]]]Using this function, the similarity of word vectors can be calculated as follows. Here, we will calculate the similarity between &quot;you&quot; and &quot;i (=I)&quot; (☞ ch02/similarity.py).

[[[00000000000000000810---f9012a6ada1de5ed8cc834cf7d10d1a371bc1db485f014da48988cc333f47642]]]# &quot;you&quot; word vector

[[[00000000000000000811---9676c426389e5231bd8266309c6f9e28225e573e9229ceca67fd7613f9cbc72b]]]# the word vector for &quot;i&quot;

[[[00000000000000000812---89286b0ea232f6a584dc82b9d86e26c04ff22f093d2d0f7d902025747aed5d52]]]From the above results, the cosine similarity between &quot;you&quot; and &quot;i&quot; is 0.70... Cosine similarity ranges from 1 to -1, so this value seems to be a relatively high value (similar).

[[[00000000000000000813---2c138e6d4c8ea526357986388be2f1d59f5ca38f2457c91ff2cbabcb56679dea]]]Ranking display of similar words

[[[00000000000000000814---14bc60319bfa673e13a67841b32c82f3b0f75eabce1aa52389530ab99c43f041]]]Now that I have the cosine similarity implementation complete, I would like to use that function to implement another convenience function. It is a function that, given a word as a query, displays words that are similar to that query in descending order. Here, we will call the function name most_similar() and implement it with the following arguments (Table 2-1).

[[[00000000000000000815---41aa7e8ae5f1bccf119132f131d34d551af80f008d4354e420287877be82c7e4]]]Table 2-1 Arguments for the most_similar() Function

[[[00000000000000000816---383e050186f4e0257fc9064bb388e492b72a6228dad5f3c373282a1f44644598]]]Argument name

[[[00000000000000000817---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000000818---1447d8c28f727838f80c46f046b371180a2ca6232ae9c9c0fe71c3399eaba64b]]]query (word)

[[[00000000000000000819---28a2bd71bd44e5b4c8d630324caf78b76af1903437229a2a2b8e2a6969eff2ca]]]dictionary from word to word id

[[[00000000000000000820---663c10a7e77e870c8cb6e0074bc31dd055c2591227a6411ad8f396884f2d7293]]]dictionary from word id to word

[[[00000000000000000821---0136d2383bfdf21d916f0d79cb39efbc8c8647c490b813822fb9298376e5f5cf]]]A matrix of word vectors. Assume that each row contains a vector of corresponding words

[[[00000000000000000822---8108fe974b1dfccf96d76f1a89e296eefb4b40712bc0545c3e1e04f9d9d7295a]]]How many top positions to display

[[[00000000000000000823---3d0efdf14d9b75d250fde8bfc1c84c0b8a58a9fd72af1a21eaf32c92008f17ad]]]Here&#39;s a quick implementation of the most_similar() function (☞ common/util.py).

[[[00000000000000000824---5a6fbd23c014f32aedbab37ca7afe0f0e0a1cdcd8454c1d01f988b1a5c85545c]]]# ❶Extract the query

[[[00000000000000000825---61d05ebc659ac7f00792f7cd16f6992335ebe1e8b773724376d2b5f5dab4daa5]]]# ❷ Calculation of cosine similarity

[[[00000000000000000826---a3413b7036b510f7a133efd688cc0ec1da3aadeddb687bef42e083019ada9442]]]# ❸ From the results of cosine similarity, output the values in descending order

[[[00000000000000000827---08ed78da2aa62da1bcff65e1464ac20e23ab147b3eb8f6260fb18a29a4b13034]]]The above implementation is done through the following steps:

[[[00000000000000000828---b8ef1b35175c7ccb22eb558b261e9f28989b10856eb27906d83ca04e97fd400b]]]❶ Take out the word vector of the query

[[[00000000000000000829---0e8318044eeab6a408997cf8d8444a6d143427027f41f711143a8dbf32d9f173]]]❷ Find the cosine similarity between the query word vector and all other word vectors

[[[00000000000000000830---92a6c6418f518e7366a71dc1fc25d639f2b2406acc057c368e262afffb1ec7e0]]]❸ Display cosine similarity results in descending order of value

[[[00000000000000000831---b2d1ff40060cc18428077071eb62dd36a4970b55187f42fe4c7fcc86303a7d7a]]]Here, only the code of ❸ will be supplemented. In 3, sort the indices of the elements in the similarity array in descending order and output the higher order. At this time, the method argsort() is used to sort the array indices. This argsort() method sorts the elements of the NumPy array in ascending order (however, the return value is the array index). An example would look like this:

[[[00000000000000000832---8c76ae7185c64f8f974070fc4d43b4657ef45c08e498c68c2127fb4dd438d076]]]Here, for the NumPy array [100, -20, 2], the values of each element are sorted in ascending order. Each element in the returned array then corresponds to an index in the array. The result above is in the order &quot;1st element (-20)&quot;, &quot;2nd element (2)&quot;, &quot;0th element (100)&quot;. What we want to do here is sort the word similarity in descending order. Therefore, use the argsort() method after each element of the NumPy array is negatively multiplied. Continuing the example above, we get:

[[[00000000000000000833---54252eeef483ed11e735bcf13a4f0dbbb05fd4b24d1a2a84ee0543d349728797]]]By using this argsort(), you can output words in descending order of similarity. The above is the implementation of the most_similar() function. Now let&#39;s use the most_similar() function. Here, we will use the word &quot;you&quot; as a query and display similar words. Shown in code, it looks like this (☞ ch02/most_similar.py).

[[[00000000000000000834---1ac60dc820878405b31c368ab959370b5c078205b4dfd22bcb9bfd123334d64c]]]Running this gives the following result:

[[[00000000000000000835---e3023bbaca6705e44f9bf30f42f42bbd0882cf2149cc5099459f1a6e4431c96c]]]This result shows only the top 5 similar words for the query &quot;you&quot;. Then the value next to each word is the cosine similarity. Looking at the result above, there are three words that are closest to &quot;you&quot;, which are &quot;goodbye&quot;, &quot;i (=I)&quot;, and &quot;hello&quot;. Indeed, both &quot;I&quot; and &quot;you&quot; are personal pronouns, so the result makes sense. However, the high cosine similarity values of &quot;goodbye&quot; and &quot;hello&quot; are very different from our perception. Of course this is due to the extremely small size of the corpus. Later we will do the same experiment with a larger corpus.

[[[00000000000000000836---36dcaeeacfd31f13ac7ae2d079deadef7a77d44c07417fa201fb3ed502d951d9]]]Now, as we&#39;ve seen, we can represent words in vectors by co-occurrence matrices. This concludes the &quot;basics&quot; of count-based techniques. There are still many things left to talk about as it is called &quot;basic&quot;. In the next section, we will explain ideas for further improving the current method and actually implement the improvement proposals.

[[[00000000000000000837---34f684501bba2b37d642702c0e0c553df13bb8e3f5170f3439f9f301b3473d64]]]Improving count-based techniques

[[[00000000000000000838---98d819b0602bf210a0a70fc6f6fa2d736442816001d7706a2579862616da0b35]]]In the previous section we created a word co-occurrence matrix. It succeeded in representing words as vectors. However, there is still room for improvement in the co-occurrence matrix. In this section, we will work on improving it. And once that improvement is done, I would like to have a distributed representation of &quot;real&quot; words using a more practical corpus.

[[[00000000000000000839---c189c7bf7062f6035c5ebfb52f789526938c3bd5f408aa60e7a05c0405cb93c8]]]mutual information

[[[00000000000000000840---81cd95bb483726e99183bea46bd765ffaf12d565b336553c7658484d2174ecca]]]The elements of the co-occurrence matrix in the previous section represent the number of times two words co-occur. However, this number of “lives” does not have very good properties. The reason becomes clear when you look at high-frequency words (words that appear a lot).

[[[00000000000000000841---e5cba63839110467ebb427abe3340eeec549195ac2adc6d722e6b81eac8c8218]]]For example, consider the co-occurrence of ``the&#39;&#39; and ``car&#39;&#39; in a corpus. In that case, you&#39;ll see a lot of the phrase &quot;... the car...&quot;. Therefore, the number of co-occurrences is a large value. On the other hand, the words &quot;car&quot; and &quot;drive&quot; are clearly strongly related. But if we just look at the number of occurrences, ``car&#39;&#39; would have a stronger association with ``the&#39;&#39; than with ``drive.&#39;&#39; This means that the word &quot;the&quot; is a high-frequency word, so it is evaluated as having a strong association with &quot;car&quot;.

[[[00000000000000000842---e267ca51001ac31a2e0feaa491df4f096c126a051c371f1d5937f1fb2d1d0e28]]]To solve such problems, an metric called Pointwise Mutual Information [19] (hereafter abbreviated as PMI) is used. It is defined for random variables x and y by

[[[00000000000000000843---e42aaec930c8f806307146846d095c4496ae9c49f59ec55eb46e189f9ea62c02]]]Equation (2.2) is the definition of PMI. where P(x) is the probability that x occurs and P(y) is the probability that y occurs. And P(x, y) represents the probability that x and y occur at the same time. This PMI indicates that the higher the value, the higher the relevance.

[[[00000000000000000844---eec2aa4c703246fe9fa080116a32ac2b481da55c85e8a9321c5b5adc45d820f2]]]Applying this to our natural language example, P(x) refers to the probability that the word x appears in the corpus. For example, let&#39;s say the word &quot;the&quot; occurs 100 times in a corpus of 10,000 words. Then it becomes Also, P(x, y) represents the probability that words x and y co-occur. Again, for example, if ``the&#39;&#39; and ``car&#39;&#39; co-occur 10 times.

[[[00000000000000000845---8e4d5d0c245bcb4ea1fe3739ac94d52e96416dc1d51eb9eaa098792d2160fb3d]]]Let&#39;s rewrite equation (2.2) using a co-occurrence matrix (each element is the number of co-occurring words). Here, the co-occurrence matrix is C, and the number of co-occurrences of words x and y is represented by C(x, y). Also, the number of occurrences of words x and y is represented by C(x) and C(y), respectively. At this time, if the number of words included in the corpus is N, equation (2.2) can be rewritten as follows.

[[[00000000000000000846---40f367e297bd1790544fb061b447fa9b253499df8f9c79793f3fae00448f3e63]]]PMI can be obtained from the co-occurrence matrix by formula (2.3). Now, let&#39;s use formula (2.3) to calculate it concretely. Here, let&#39;s assume that the number of words (N) in the corpus is 10,000, and ``the&#39;&#39; appears 1,000 times, ``car&#39;&#39; 20 times, and ``drive&#39;&#39; 10 times. And let&#39;s assume that ``the&#39;&#39; and ``car&#39;&#39; co-occur 10 times, and ``car&#39;&#39; and ``drive&#39;&#39; co-occur 5 times. At this time, from the viewpoint of the number of co-occurrences, &quot;car&quot; is more related to &quot;the&quot; than to &quot;drive.&quot; What about PMI&#39;s perspective? This can be calculated as

[[[00000000000000000847---b8fbe01f62fbe4d235cd4ebdfd42b882d88cc7deb8a8c2c20fc206bf2f857dd4]]]As the results show, using PMI, “car” became more relevant to “drive” than “the”. This is the result we wanted. This result is due to the fact that the number of single word occurrences was taken into account. In this example, the PMI score was lowered due to the high occurrence of &quot;the&quot;. Note that ≈ in the formula is the symbol “nearly equal” and means approximately equal.

[[[00000000000000000848---91c87246a1eb5d5da661fb48436a366846010ff3a0d0aa0b87b426fa84dd7d8d]]]Now we have a good indicator called PMI, but there is one problem with this PMI. The point is that if the number of co-occurrences between two words is 0, log20 = -∞. To deal with this, in practice the following positive mutual information (Positive PMI) is used (hereafter abbreviated as PPMI).

[[[00000000000000000849---023e34e75bd6a1f27b3394afe4f892ac15ae68ce9fd2abd1572a7169ecca777a]]]By equation (2.6), when PMI is negative, it is treated as 0. Now, the degree of relevance between words can be represented by a real number greater than or equal to 0. Now let&#39;s implement a function to convert the co-occurrence matrix to a PPMI matrix. We implement it under the name ppmi(C, verbose=False, eps=1e-8) (☞ common/util.py).

[[[00000000000000000850---e50c0978776fc95f71d930995d130c456005e274bbe2fe5c21f79172be2fb299]]]Here, the argument C is a co-occurrence matrix, and verbose is a flag that determines whether to output the progress. This verbose is used to check the progress by setting verbose=True when dealing with a large corpus. This code implements a simple implementation so that the PPMI matrix can be obtained from only the co-occurrence matrix. Specifically, when the number of co-occurrences of words x and y is C(x, y), we implement such an approximation. Also, the code above uses a tiny value of eps to avoid np.log2(0)=-inf.

[[[00000000000000000851---7ede8df0d54bdb3b2e03786ca7a99965ea7e4a10546a546aeb175169c4a1dca7]]]In &quot;2.3.5 Similarity between vectors&quot;, a minute value was added to the denominator to prevent &quot;division by 0&quot;. Again, the calculation np.log(x) is np.log(x + eps) to prevent the logarithm calculation from diverging to infinity.

[[[00000000000000000852---b68611b28abf98992e16da11dd80a9d787f4b09b0b71aac7ca41327a581e25d3]]]Now let&#39;s convert the co-occurrence matrix to a PPMI matrix. This can be implemented as follows (☞ ch02/ppmi.py).

[[[00000000000000000853---a48f5a9931b68d1375d3194fb1a3536e9051df0fe9249afc7b5064407899d57f]]]# Display with 3 significant digits

[[[00000000000000000854---a628f55d0f62ea33701c4dde429638f44eba2b91cea93cb6b86977d9243a4aa3]]]Running this file gives the following results:

[[[00000000000000000855---f6f788e8239794c8dab02ad13d2bb65961ccca7c1aa509623b9c70b027cbfa9a]]]You have now successfully converted the co-occurrence matrix to a PPMI matrix. At this time, each element of the PPMI matrix is a real value greater than or equal to 0. Now we have a matrix of better indices—a better vector of words.

[[[00000000000000000856---31b4514546076a8896d37d56b7a8761e9255a81424cb595f009eba040e6140a3]]]But this PPMI matrix still has a big problem. The problem is that as the vocabulary of the corpus increases, the number of dimensions of each word vector also increases. For example, if the number of vocabularies contained in the corpus reaches 100,000, the number of dimensions of its vectors will also be 100,000. In fact, dealing with 100,000-dimensional vectors is not very realistic.

[[[00000000000000000857---af8ed592116d6585310e2ff5a81f18c7497c1b949840ac94c1bbb7144b81fb31]]]Also, if you look at the contents of this matrix, you can see that many of its elements are 0. This means that most of the elements in the vector are unimportant—that is, each element has a low degree of “importance”. And such vectors are vulnerable to noise and have poor robustness. A common practice for these problems is vector dimensionality reduction.

[[[00000000000000000858---a81869f4bc37ea24e6c8ae40df49ba22ce65af6ffa3a25971003f8315d8d5257]]]dimensionality reduction

[[[00000000000000000859---95bc2f4eb1f9c09bdec485842705c0fb42929f6102f29af4ae91c02de3a48b55]]]Dimensionality reduction literally refers to the technique of reducing the dimension of a vector. However, the point is not to simply reduce it, but to reduce it after leaving as much &quot;important information&quot; as possible. As an intuitive image, as shown in Figure 2-8, look at the data distribution and find the important &quot;axes&quot;.

[[[00000000000000000860---ed98e48ffc85df1b547b4f364a7e231e4aadf6b5077ee4cd32a1d9197592dae2]]]
Figure 2-8 Image of dimensionality reduction: Finding the important axis—the axis of large data spread—to represent 2D data in 1D


[[[00000000000000000861---a7c6c34a77c59add133200db44ccfe52093b8cc6694bb87bf9f2a8fec15509b7]]]In Figure 2-8, a new axis is introduced to represent the data points originally represented by two-dimensional coordinates in a single coordinate axis, considering the spread of the data. Each data point value is then represented by its projected value onto the new axis. The important point here is that by taking the axis that considers the spread of the data, you can capture the essential differences in the data even with just one-dimensional values. Something similar to this can be done with more multidimensional data.

[[[00000000000000000862---20e8d21a10ac05b7a40df463672569050b5bb25213118c531cc93db5d6302d55]]]A matrix (or vector) in which most of the elements in the vector are 0 is called a &quot;sparse matrix&quot; (or &quot;sparse vector&quot;). The point here is to find the important axes from the sparse vector and re-represent them with fewer dimensions. As a result, a &quot;sparse vector&quot; is transformed into a &quot;dense vector&quot; with most non-zero elements. This dense vector is exactly the distributed representation of the word we want.

[[[00000000000000000863---c362d1745cede089699e1b2580e46e87bafd3d0a95388f4563237ac5a7c7894e]]]There are several ways to do dimensionality reduction. Here, we perform dimensionality reduction using Singular Value Decomposition (SVD). SVD decomposes any matrix into a product of three matrices. Written as a formula, it is expressed as:

[[[00000000000000000864---bc1dfea5bf07403d80d9cb883f7a83495c20e2186bf7e1c2c28be1eee3eb66a6]]]SVD decomposes any matrix X into the product of three matrices U, S, and V, as shown in equation (2.7). where U and V are orthogonal matrices whose column vectors are orthogonal to each other. Also, S is a diagonal matrix, which is a matrix of all zeros except the diagonal. At this point, a visual representation of these matrices looks like Figure 2-9.

[[[00000000000000000865---4fba83d448bf5373050ab71475f8821e65ca486d8239f8d00006d014b835c6a5]]]
Figure 2-9 Transforming a matrix with SVD (the &quot;white part&quot; of the matrix represents 0 elements)


[[[00000000000000000866---166451097fb3e57f608a3541fe809c52f59c711e07ae4d90a5b63b482133498a]]]Now, in equation (2.7), U is an orthogonal matrix. And this orthogonal matrix forms the axis (basis) of some space. In our context, we can treat this matrix U as a &quot;word space&quot;. Also, S is a diagonal matrix, and in this diagonal element, &quot;singular values&quot; are arranged in descending order. The singular values can be thought of simply as the importance of the &quot;corresponding axis&quot;. Therefore, as shown in Figure 2-10, it is conceivable to remove unimportant elements.

[[[00000000000000000867---8942425383077b907a3ace619e8557cacaeeb94ca8bba363f93d38f2dace0639]]]
Figure 2-10 Image of dimension reduction by SVD


[[[00000000000000000868---04cf84fc8f0b51b3ec291141e84eb3b7e881ce10bbe5c21b27fcf8dfeb2d4026]]]As shown in Figure 2-10, the small singular values of matrix S are less important, so the original matrix can be approximated by removing the extra column vectors from matrix U. Applying this to the &quot;word PPMI matrix&quot; that we are dealing with, each row of the matrix X stores the word vectors of the corresponding word IDs, and those word vectors have been reduced as a matrix U&#39;. It will be represented as a vector.

[[[00000000000000000869---91cf95bfd5c4b51a6d2f826ff70445fbe35a311c91a5530268f648cfbfed43a9]]]The word co-occurrence matrix is a square matrix, but in Figure 2-10 it is shown as a rectangle for consistency with the previous figure. Also, SVD is only an intuitive, high-level description here. If you want to understand more mathematically, please refer to reference [20].

[[[00000000000000000870---cbd49dd95836db2c72c7efbddcef7104095ff91990ab8aa2c27403b3d8802295]]]Dimensionality Reduction with SVD

[[[00000000000000000871---221d8b06291ce6af1d25d4c762507af25e4cc5501d6d546eb673897daa9f65c2]]]Let&#39;s actually do SVD in Python. This SVD can be executed with the method svd in NumPy&#39;s linalg module. By the way, linalg is an abbreviation for linear algebra. Now, create a co-occurrence matrix, convert it to a PPMI matrix, and apply SVD to it (☞ ch02/count_method_small.py).

[[[00000000000000000872---cc41205060f79640561e1fb2243f1f9cae76bdc6b537b5996f0e64ececccd6b6]]]Now you have SVD running. The variable U above contains the dense vector representation transformed by SVD. Now let&#39;s take a look inside. Here, let&#39;s look at a word vector with a word ID of 0.

[[[00000000000000000873---cd19761f0f95ec328424a3b02e33f278105be589300f6db22b1af0077602f042]]]# co-occurrence matrix

[[[00000000000000000874---7ccf5f5f22286d2593834460ebf690914d668d9616b268309852a18cf6ddc41f]]]# PPMI matrix

[[[00000000000000000875---78c726e869f7c54994d1198e64d84f461eb9c1e117052d7e9bb8bda38948adf7]]]As the above results show, the originally sparse vector W[0] has been transformed into a dense vector U[0] by SVD. And to reduce the dimension of this dense vector, say, if you want to reduce it to a 2-dimensional vector, just take the top two elements.

[[[00000000000000000876---aab7546753804d9ec25ad73dbfe899dc1dceee76896de3feb881f8a80ec13835]]]This concludes the dimensionality reduction. Let&#39;s represent each word as a two-dimensional vector and plot it on a graph. To do so, write:

[[[00000000000000000877---5ea2292c23b81d33e55fd25db0e6114cec641ae682e4b1ff52e57837c3845d4c]]]The function plt.annotate(word, x, y) draws the text word at the point (x, y) on the 2D graph. Now let&#39;s run the above code. The result is shown in Figure 2-11†1.

[[[00000000000000000878---b62b057be7190579124d359f8844244af6e9d3289747fd9e963d422e945c657e]]][†1] The output graph may look different from Figure 2-11 depending on the OS type and Matplotlib version.

[[[00000000000000000879---7dad4d2ac409380ca630d6a976f596714d8f9b32dd24614e836a6cd628d64223]]]
Figure 2-11 SVD is performed on the co-occurrence matrix, and each word is converted into a two-dimensional vector and plotted on a graph (*&quot;i&quot; and &quot;goodbye&quot; overlap).


[[[00000000000000000880---18c0aa25e3251c16cd2be22e39ac15373da3206e2cba6d51eff53919fd0e90e0]]]Looking at this plot, we can see that ``goodbye&#39;&#39; and ``hello&#39;&#39;, and ``you&#39;&#39; and ``i&#39;&#39; are close together. This is relatively close to our intuition. However, since we are using a small corpus, this result is frankly nuanced. Next, let&#39;s do the same experiment using a larger corpus called the PTB dataset. First, a brief description of the PTB dataset.

[[[00000000000000000881---0d3ef0492be9676a02595a7e3ddd1b38d38402e7c1e64da4fdba0d8a266d168b]]]If the size of the matrix is N, the computation of SVD is of the order of . This means that the amount of computation proportional to the cube of N is required. In practice, such calculations are unwieldy, so faster methods such as Truncated SVD [21] are used. Truncated SVD speeds up by truncating small singular values (=Truncated). In the next section, we will use Truncated SVD from the sklearn library as an option.

[[[00000000000000000882---189dbed9afddb333ce75aafb280907926ab8e62f54f36ec53760c841312f4adb]]]PTB data set

[[[00000000000000000883---d0c9d0593cd61a4bb5d9337b15b8a7b7a19d87037a4eb31984a56005676521fa]]]So far we have used very small text data as corpora. Here, we want to use a &quot;real&quot; corpus -- a reasonably sized corpus that&#39;s not too big. It is a corpus called the Penn Treebank (hereafter abbreviated as PTB).

[[[00000000000000000884---7e4ec3b824fef264c14c112a522d5ae1ac06b65da9a3fb0692f4617b263ea698]]]The PTB corpus is often used as a benchmark to measure the quality of proposed methods. This book also uses the PTB corpus to conduct various experiments.

[[[00000000000000000885---b2918bd4f55beed2272f651794f351f90bc725968dfab1f52b5c0147c5cdb039]]]The PTB corpus we use is prepared on the web page of Tomas Mikolov, the inventor of word2vec. This PTB corpus is provided as a text file, and some preprocessing has been applied to the original PTB text. What kind of preprocessing is, for example, rare words<unk> (unk is an abbreviation for &quot;unknown&quot;), and specific numbers are replaced with &quot;N&quot;. We use the text data after such preprocessing as the PTB corpus. For reference, Figure 2-12 shows the contents of the PTB corpus.

[[[00000000000000000886---84d5c694cda85711a3407b66de707990d9bddf3a1c1a02185f9463673e3bf3a6]]]
Figure 2-12 Example of PTB corpus (text file)


[[[00000000000000000887---19e21b5286c9e48cde101b47b88ed0cfac5b800290312777123b4171e300d4d1]]]As shown in Figure 2-12, one sentence is stored per line in the PTB corpus. In this book, we will treat the concatenation of each sentence as &quot;one big time series data&quot;. Also at this time, at the end of each sentence<eos> inserts the special character (eos stands for &quot;end of sentence&quot;).

[[[00000000000000000888---4cae0d5510135f94693d4eb2dadc0b683cf1773c3b28de82050ce1447cff496d]]]In this book, we regard the concatenation of multiple sentences as &quot;one big time-series data&quot; without considering sentence breaks. Of course, it is also possible to work on a sentence-by-sentence basis—for example, to count word frequencies on a sentence-by-sentence basis. However, in this book, in the interest of simplicity, we do not do sentence-by-sentence processing.

[[[00000000000000000889---554ff83c4d904ab72a36148c426ae2f50b94e25adc83c65330274843051da249]]]In this book, we have prepared a dedicated Python code for easy use of the Penn Treebank dataset. This file is located in dataset/ptb.py and is intended to be used from the &quot;chapter directory (ch01, ch02, ...)&quot;. For example, after moving to the ch02 directory, use &quot;python show_ptb.py&quot; in that directory. Here is an example using ptb.py (☞ ch02/show_ptb.py).

[[[00000000000000000890---8b9537ab65a9a8f7e1a190673c76d10af9903903234189a03b051e4b67e3ce00]]]I&#39;ll save the explanation for this code later, but here&#39;s the result of running it:

[[[00000000000000000891---aac63dba6e7c8916037fadfb3513c0b0359f5c4b856aa81c6459d17ebe1377b4]]]The corpus handling is the same as before. corpus stores a list of word ids. id_to_word represents a dictionary that converts word IDs to words, and word_to_id represents a dictionary that converts words to word IDs.

[[[00000000000000000892---4f180343a189ce9e4863948f66659ee35699e0209f9b5dc90fcbc2a06bb4b0c2]]]Load the data with ptb.load_data() as shown in the previous source code. At this time, specify one of &#39;train&#39;, &#39;test&#39;, and &#39;valid&#39; for the argument. This corresponds to either &quot;training/testing/validation&quot; data, respectively. This is the explanation of how to use ptb.

[[[00000000000000000893---897747ba08a6ea9571c0f34a22a06dcec5dc51e9d760ba341f50e65de97f0ddd]]]Evaluation on the PTB dataset

[[[00000000000000000894---6dbba49e0caa9f768d52f6921f8881bb5dc39265236d64585462373b0657d3cc]]]Let&#39;s apply the count-based method to the PTB dataset. Since we are doing SVD on a large matrix here, it is recommended to take advantage of the faster SVD. For that you need to install the sklearn module. Of course you can use a simple SVD (np.linalg.svd()), but it will take a lot of time and memory. Here is the source code (☞ ch02/count_method_big.py).

[[[00000000000000000895---f64a088146bc539b014a93367526cb910e89f7ac5b6bde279e02deb7879647c7]]]In order to do SVD here, we will use sklearn&#39;s randomized_svd() method. This is a truncated SVD that uses random numbers, and by limiting the calculation to those with large singular values, it can perform faster calculations than normal SVD. The rest of the code is almost identical to the previous code for the small corpus. Now let&#39;s run the code above. Then you will get the following result (In the case of Truncated SVD, the result below will be different each time due to the use of random numbers).

[[[00000000000000000896---8bd2fb7127fa63465f5d0926ee0bcccdcc4eb059e2717fc3b16651e84b6db6ac]]]Looking at the results, we can see that personal pronouns such as “i” and “we” are ranked high for the query “you”. This is a common word in terms of grammatical usage. In addition, synonyms such as &quot;month&quot; and &quot;quarter&quot; are obtained when the query is &quot;year&quot;, and &quot;auto&quot; and &quot;vehicle&quot; when the query is &quot;car&quot;. In addition, if you query &quot;toyota&quot;, you can also check that car manufacturers and brands such as &quot;nissan&quot;, &quot;honda&quot;, and &quot;lexus&quot; come. In this way, words that are semantically and grammatically similar are represented by vectors that are close to each other. It seems that this is close to our sense.

[[[00000000000000000897---d3b57d4760dd4e13031b0bc1cbd057235ab7c1c9f2dbfbdcb873c9d0b87b4fbd]]]congratulations! We finally succeeded in encoding the &quot;meaning of a word&quot; into a vector! We used the corpus, counted the words in the context, converted it to a PPMI matrix, and did SVD dimensionality reduction to get a better word vector. This was the distributed representation of the words, with each word represented as a fixed-length vector and then as a dense vector.

[[[00000000000000000898---ff9b8256daff067f280567b76559474fccfc5f40df990cba4b8639959377e055]]]In the experiments in this chapter, we only looked at similar words for some words. However, many other words may also identify such properties. It is expected that using a larger corpus will result in a better distributed representation of words.

[[[00000000000000000899---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000900---b869e2c1cfb831743d06cbe731023ffe8fe170c8eee163b156826ec91153e64b]]]In this chapter, we focused on natural language, and focused on the theme of making computers understand the &quot;meanings of words.&quot; To that end, we have described a thesaurus-based approach, followed by a count-based approach.

[[[00000000000000000901---61d59c97fd0f0057f5517f9202bde3b805a2f7ecd023b03f8fb741eacb64a13b]]]The method using the thesaurus defines the relationships between words one by one by hand. However, such work is very difficult and has limitations in terms of expressiveness (such as being unable to express fine nuances). Count-based methods, on the other hand, automatically extract word meanings from the corpus and represent them as vectors. Specifically, we created a word co-occurrence matrix, converted it to a PPMI matrix, performed dimensionality reduction by SVD to increase robustness, and obtained a distributed representation of each word. And with the distributed representation, we were able to confirm that words that are semantically (and grammatically) similar are located close to each other in the vector space.

[[[00000000000000000902---a9cc5792d57ad1a94a2fc97fc1a4138b9d9ee05bb2b9da1d2a77448c23315941]]]Also, in this chapter, we have implemented some preparatory functions to make it easier to work with corpus text data. Specifically, we implemented a function for measuring similarity between vectors (cos_similarity()) and a function for ranking similar words (most_similar()). These functions will also be used in the following chapters.

[[[00000000000000000903---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000000904---d874ee19d0aa58b4528a1e986e035953f3fc85d2ae890a4b1ed8933ce7791058]]]A thesaurus such as WordNet can be used to perform useful tasks such as retrieving synonyms and measuring similarities between words.

[[[00000000000000000905---65ae0db0a23b097a4415a085044fd792018ad4d3b8848c5fb7e07b5b0c2fa575]]]The method using thesaurus has problems such as the amount of work of the person creating the thesaurus and correspondence to new words.

[[[00000000000000000906---512e3ba8f09e83081c651eee68c4b5254f7ef00a202ad99170f9ab929231bcfd]]]Currently, the mainstream approach is to use a corpus to vectorize words.

[[[00000000000000000907---b7862aff9b7aeaaa7319e7321a2b58de923cda41fd5082473f6715677e0f8089]]]Most recent word vectorization methods are based on the distribution hypothesis that the meaning of a word is formed by surrounding words.

[[[00000000000000000908---d8c796f747496d017ff07cbffac56064ad492903be13a3318cc57def8a00e7ea]]]Count-based methods count and aggregate the frequency of words surrounding each word in the corpus (=co-occurrence matrix)

[[[00000000000000000909---f1a7ca787fe53058bbe24257d09362cb9903c55ac18a0c3385d8269e38e49e0d]]]By converting the co-occurrence matrix to a PPMI matrix and then reducing it, we can transform a large &quot;sparse vector&quot; into a small &quot;dense vector&quot;.

[[[00000000000000000910---91f9197099e10745aedf3f394727e8a88e6d8eeac1fa2ab4cd8124073f0f5e54]]]In a word vector space, words that are semantically close are expected to be closer together.

[[[00000000000000000911---e1b3e6d1018f3a48202006d7c1ead9d579ea3a5c56d5bfae576ac16e118d237b]]]Chapter 5

[[[00000000000000000912---c7ce3b52e44c50571d74de8b755351c3a1fb96b1b260a140cf1baee5b395671d]]]Recurrent Neural Network (RNN)

[[[00000000000000000913---b9bf7d3d48766e48a35fc6dfe55c625f8cab60a3fb83d391001aaf2371f09781]]]In a dark and damp place

[[[00000000000000000914---e9b4700c61a89f9c58bcb6c1979db10cdeea70a4209ff1f6e87683b1eacd833a]]]All I remember is that I was crying.

[[[00000000000000000915---42848968347cd73c49689b1f3ced9b74b844bf7274d51491bca62970704ade6c]]]——Natsume Soseki “I am a cat”

[[[00000000000000000916---1ce77761bcaae2a14e3cbdb3e68b3e59afdbad0c0892d0c08859caa18c0bdb62]]]The neural networks we have seen so far are a type of network called feedforward. A feedforward is a network in which the flow is in one direction. Specifically, the input signal propagates the signal to the next layer (hidden layer), the layer that received the signal propagates it to the next layer, and so on. Signaling is directional only.

[[[00000000000000000917---eca7f816c368585c63d7fc4829a2d16c8b3c98366e68b7726ca69495166bca8f]]]Feedforward networks are simple, easy to understand, and can be applied to many problems. However, this type of network has major problems. The problem is that it doesn&#39;t handle time series data very well. More precisely, a simple feedforward network cannot sufficiently learn the properties (patterns) of time series data. That&#39;s where Recurrent Neural Networks, or RNNs for short, come into play.

[[[00000000000000000918---46db8e0e0edde7bb089c951a148509aba4c590a9b9fa3dfd3469d78b28650dc2]]]In this chapter, we point out a problem with feedforward networks and show how RNNs can solve it beautifully. We will also take the time to thoroughly explain the structure of the RNN and implement its processing in Python.

[[[00000000000000000919---13edcfa94141b3ff86380b77ae02fdea90203f0508db924402d00b3650c389bd]]]Probability and language models

[[[00000000000000000920---9df20a54876c0fdf6bdf26bac9b0ea461cc97e5953cf0ae4c2712b79d0e672b3]]]Here, we will start by reviewing word2vec in the previous chapter as a preparation before starting to talk about RNN. It also describes phenomena related to natural language using &quot;probabilities&quot;, and finally explains &quot;language models&quot; that treat language as probabilities.

[[[00000000000000000921---097f28c601b87923df0920e833c93ce224b9d6415c0284a094c6a2d6b53f26f8]]]Looking at word2vec from a probability perspective

[[[00000000000000000922---b5e79a08895295086fe74f7904e48f0c5b6ea14bce7ad62958ec30e7a9a2a2dc]]]Let&#39;s start by reviewing word2vec&#39;s CBOW model from the previous chapter. Consider a corpus represented by a sequence of words. Then we treat the tth word as the &quot;target&quot; and the words before and after it (t-1th and t+1th) as the &quot;context&quot;.

[[[00000000000000000923---0eaa9b2513d7140888a4e692c954469d838f5b570d7e0ca1b59f8c31525ddb7c]]]In this book, target is the &quot;center word&quot; and context is the &quot;surrounding word&quot; of the target.

[[[00000000000000000924---de76946d116a9ae6f7c14e9f0f8da4e538cded044e7494e1dbc249296e0fc617]]]What the CBOW model then does is infer the target wt from the context wt−1 and wt+1, as shown in Figure 5-1.

[[[00000000000000000925---040f89b2de0d970576addcdcff9321cddf31e81643f93d1733495a26de651953]]]
Figure 5-1 word2vec CBOW model: infer target words from context words


[[[00000000000000000926---7883e85bded22c0102a8e2a76534e80f8cb15036617a34f35ff7b605e5572516]]]Let&#39;s formulate the probability that the target will be wt given wt−1 and wt+1. This can be written as equation (5.1).

[[[00000000000000000927---ca8bdb95845fd9189b2c5f178b4714197385a2a921a727b2546cbe8f7206189b]]]The CBOW model models the posterior probability of equation (5.1). This posterior probability represents the probability that wt will occur given wt−1 and wt+1. Here is the CBOW model when the window size is 1.

[[[00000000000000000928---0b5112119c2c5c932b46518ecf2d9137beb3f6912526b3efff04498143777978]]]By the way, we&#39;ve been thinking about symmetrical windows for context so far. For context, I&#39;d like to limit myself to just the left window. For example, consider the case shown in Figure 5-2.

[[[00000000000000000929---d254678fb776c6301d861a56cf7a735af812d216201360ae27e5dac72bcafa93]]]
Figure 5-2 Targeting Only the Left Window as Context


[[[00000000000000000930---7bf3c1013ab6da81b8c69a1e9769f1260845d9936252edb70ef3fd002de237d6]]]As in Figure 5-2, we will now consider only the two words on the left as context. Then the probability that the CBOW model outputs is given by equation (5.2).

[[[00000000000000000931---e192c0b1034fab60870e96b3d8a6022eabcb6a548f75f9101481bbc2e1f093dd]]]The window size in the context of word2vec is a hyperparameter and can be set to any value. Here, we set the window size asymmetrically with 2 words on the left and 0 words on the right. The reason for this setting is in anticipation of the &quot;language model&quot; that will be explained later.

[[[00000000000000000932---790e8c5553f3e95384cba2b5397f2d67b16d226ca4c12c35d7b9512e47b765e4]]]Now, using the notation of equation (5.2), the loss function handled by the CBOW model can be written as equation (5.3). Incidentally, equation (5.3) is the result derived from the cross-entropy error (see &quot;1.3.1 Loss function&quot; for details).

[[[00000000000000000933---cb990d6f059c0458e6be297fd043d2085d33de4262a4eb4cdde69c483a34978a]]]What we do in training a CBOW model is to find a weight parameter that makes the loss function expressed in (5.3)—more precisely, the sum of the loss functions over the entire corpus—as small as possible. If such a weight parameter is found, the CBOW model will be able to better infer the target from the context.

[[[00000000000000000934---ed3791aedf27369fdd378bed0d839c492f85b56f0c3bc13a4ac2d5706b495b4b]]]In this way, the original purpose of training the CBOW model is to be able to correctly infer the target from the context. Learning to achieve this goal yielded (as a by-product) a &quot;distributed representation of words&quot; that encodes the meaning of words.

[[[00000000000000000935---b74d47d5764eb3912f3918063cfce9bf1fb7ef39224fa7dffc95912a40c8d9ec]]]Then, can the original purpose of the CBOW model, &quot;guessing the target from the context&quot;, be used for something? Can the probability P(wt|wt−2, wt−1) expressed in equation (5.2) be used in practical applications? That&#39;s where the &quot;language model&quot; comes into play.

[[[00000000000000000936---c76726c9a05b71c99512bf96f32f6d62088f6f3d20fc73f8654308dff0aea218]]]language model

[[[00000000000000000937---a2c884564a5679ebb04661e9f91c7754813acc8cfbe50fa86342bd4b0ef152fe]]]A language model gives probabilities to sequences of words. For a word sequence, we evaluate how likely it is—how natural the word sequence is—as a probability. For example, the word sequence &quot;you say goodbye&quot; outputs a high probability (e.g. 0.092), and the word sequence &quot;you say good die&quot; outputs a low probability (e.g. 0.0000000000032). does.

[[[00000000000000000938---a58f13eff4382d34de189340b95476a70d60ed6e3ea7ce6529637f05f9e531f1]]]This language model can be used in various applications. Examples include machine translation and speech recognition. For example, a speech recognition system might generate several sentences as candidates from a person&#39;s speech. In that case, using a language model, candidate sentences can be ranked according to the criteria of ``whether or not they are natural as sentences.&#39;&#39;

[[[00000000000000000939---dad0d9de110d8dac3164c992025f67ced924f72732c3ea17427c09848256ca52]]]Language models can also be used to generate new sentences. This is because the language model can probabilistically evaluate the naturalness of word strings, so words can be “spun” (sampled) according to that probability distribution. Sentence generation using language models will be covered in Chapter 7, Sentence Generation with RNN.

[[[00000000000000000940---c3a329e93b096d667e46fdfcec1dc96a87bea5b5eea1631fe9c2e4ea0d1286a7]]]Now let&#39;s describe the language model using mathematical expressions. Here, consider a sentence consisting of m words. At this time, the probability that the words appear in the order is represented by . This probability is called the joint probability because it is the probability of multiple events occurring at the same time.

[[[00000000000000000941---4df8e1d2700599d7fb306ce99af3b8da7d28166c2d9d5116ceecca2eb8f4448f]]]This joint probability can be decomposed and written using the posterior probability as follows.

[[[00000000000000000942---3c4f4fe383de8af568758204686ba5706ce7bdd96c23502e8d2ca2c3c597c343]]][†1] For mathematical simplicity, we treat P(w1|w0) as P(w1) here.

[[[00000000000000000943---3c7dfd6474ad5b763bf1ac4c9fbea9e3eb2d3d632f8a7005ee393faf99ab4aea]]]The symbol (pi) in equation (5.4) represents the &quot;summation&quot; of multiplying all the elements (the symbol for summation is (sigma), whereas the symbol for the summation is ). As shown in equation (5.4), the joint probability can be expressed as the sum of the posterior probabilities.

[[[00000000000000000944---76aabdb605415960ae9bb7631c9d90c9b84bb914a2e9c48066c2906671fe3cb7]]]The result of equation (5.4) can be derived from the probability multiplication theorem. Here, I would like to take a moment to explain the multiplication theorem and see the process of deriving equation (5.4). Let&#39;s start with the multiplication theorem of probability, which is expressed by the following formula.

[[[00000000000000000945---2d5aa6f5329e74e27206b3457ec1aba3a37a62138ecdeb2249954579dd1e7d94]]]The multiplication theorem expressed by this equation (5.5) is the most important theorem in probability theory. What this theorem means is that the probability P(A, B) of both A and B occurring is the same as the probability P(B) of B occurring and the probability P(A |B)” (this interpretation seems natural).

[[[00000000000000000946---b548aec95a87a06a5ad6157eaf2a94409e6897336077aa71e189fac4078f5de9]]]The probability P(A, B) can also be decomposed as P(A, B)=P(B|A)P(A). In other words, P(A, B)＝P(B|A)P(A) and P(A, B)＝P(A|B)P( There are two representations of B).

[[[00000000000000000947---5849597b6a81788c3f37d06b944928eadf3dec5f0332fdd28aba514e76638442]]]Using this multiplication theorem, we can express the joint probability of m words by the posterior probability. The following is an easy-to-understand representation of the formula transformation performed at this time.

[[[00000000000000000948---21c530e2196a202dd5f6e7743910d3344355cec382a0bbbd204b3566fe2522e6]]]Here, are collectively represented by the symbol A. Then, the formula (5.6) is derived by following the multiplication theorem. Then, for this A(), the same expression transformation is performed again.

[[[00000000000000000949---abd3f70f297f65fe32c5f986009965a308afe16ce2e60a4ec82f21fc4e7d1cbb]]]In this way, the sequence of words is reduced one by one, and each time it is decomposed into posterior probabilities. By repeating the same procedure, we can derive equation (5.4).

[[[00000000000000000950---e15eb25eec557fa4b1202e466b2a6659aa2fc231c9056bd9aad4ccd3ab0c1f79]]]Now, as equation (5.4) shows, the desired joint probability can be expressed by being the sum of the posterior probabilities. It should be noted here that the posterior probability is the probability when all words to the left of the target word are used as context (conditions). This can be represented graphically as shown in Figure 5-3.

[[[00000000000000000951---c4bdb839452371b394d899049f37829bb65a5f8a9034c8e1f8fc748a7bc4829e]]]
Figure 5-3 Posterior probabilities handled by the language model: target the t-th word and all words to the left of the t-th word as context (condition)


[[[00000000000000000952---a87e874c654a7fa3c90ce5be4db878909b8b498b4ab78687e6bc30933eac426d]]]To sum it all up, our goal is to get a probability of . If we can calculate that probability, we can find the joint probability of the language model.

[[[00000000000000000953---180868b49df921b14c4fe202d921b11b8936140810ec6c31fb6f1f7db81ebcb3]]]A model that represents is called a Conditional Language Model. It is also common to refer to a model that represents and call it a &quot;language model&quot;.

[[[00000000000000000954---27480e3218575e87e91d4944ca7582ac183cf903ac61dd91cf3c1756056bba5d]]]CBOW model to language model?

[[[00000000000000000955---149a07f8ee4d2a4ef46ab2dbdb7eb189d524efea73481002191d524b5725f838]]]So how can we (force) apply the word2vec CBOW model to the language model? It can be approximated by constraining the size of the context to some value. This can be expressed as a formula as follows.

[[[00000000000000000956---4f8050962620ee2cdb69f1df367a27d9e09537fb84bdb94764441482b7dde2a8]]]For now, let&#39;s limit the context to the two words on the left. It can then be approximately represented by the CBOW model—by the posterior probability of the CBOW model.

[[[00000000000000000957---e8f0f6c5d485cd6e28a6cdd1dda0f7cac473e3019dc8d9d905c014e7d901445a]]]In the fields of machine learning and statistics, we often hear the term &quot;Markov property&quot; (or &quot;Markov model&quot;, &quot;Markov chain&quot;, etc.). &quot;Markovianity&quot; means that the future state depends only on the present state. Also, when the probability of an event depends only on the N events immediately before it, it is called an &quot;N-order Markov chain&quot;. Since the model shown here depends only on the previous two words to determine the next word, it can be called a &quot;second-order Markov chain&quot;.

[[[00000000000000000958---00e6dea719f8893497eeb8387934f4ccaf1a9a684e326a88e7a7616028735c19]]]Equation (5.8) gives an example with two words as context, but the size of this context can be set to any length (eg, 5 or 10). However, even though it can be set to any length, it must be &quot;fixed&quot; to a certain length. For example, even if you create a CBOW model with the 10 words on the left as the context, the information on the words further to the left of the context will be ignored. An example of where this matters is in Figure 5-4.

[[[00000000000000000959---d82acfa7e532a0a390e02e7b1e7307b5ed633b223e6a0be392261c988d3fbd67]]]
Figure 5-4 What is the word in &quot;?&quot;? : Example of a problem that requires long context


[[[00000000000000000960---f9232f79a0aa3d87ea1ea9cca9fb101dcec05ee73d8aba86e84e8d97055212c5]]]The problem in Figure 5-4 states that &quot;Tom was watching TV in his room and Mary walked into the room.&quot; Given the context, the correct answer would be for Mary to greet &quot;Tom&quot; (or &quot;he&quot;). In order to get the correct answer here, it is necessary to memorize Tom, which appears 18 times before the &quot;?&quot; in the example sentence. If the CBOW model had only 10 contexts, we would not be able to answer this question correctly.

[[[00000000000000000961---a8bb812d186fbc5cd3de07caa97fdfb3dbc54c1f7982f26e3c88508bffcc8891]]]So, would increasing the context size of the CBOW model, say 20 or 30, solve the problem? Indeed, the size of the CBOW model context can be arbitrarily large. However, the problem with the CBOW model is that it ignores word sequences in context.

[[[00000000000000000962---3ebdc53f9450832381321893ca046eb48c96e29dfd3e60a767693332a91444fc]]]CBOW stands for continuous bag-of-words. Bag-of-words means &quot;the words in the bag&quot;, and it means that the &quot;sequence&quot; of words in the bag is ignored.

[[[00000000000000000963---70fd50fbd2aba13f2417ff361e20261396cf77706f66d0834f57b4a3235d3292]]]Let&#39;s take a concrete example to illustrate the problem of ignoring contextual word sequences. For example, if we treat two words as context, the CBOW model puts the &quot;sum&quot; of the two word vectors in the middle layer. If represented graphically, it would look like the left figure in Figure 5-5.

[[[00000000000000000964---772f59d9900b2c5de8831de0f71289ec82ea04954388f413f1aba5417332f0c7]]]
Fig. 5-5 The left figure is a normal CBOW model. The figure on the right shows a model in which the word vectors of each context are &quot;concatenated&quot; in the hidden layer (in this figure, the input layer is described as one-hot vectors).


[[[00000000000000000965---1268e99bd29b7eefc25e0d7bb147d71797ca9be355d27489da735ac8dbf374ea]]]As shown in the left panel of Figure 5-5, the middle layer of the CBOW model ignores the ordering of words in the context because it calculates the sum of word vectors. For example, the contexts (you, say) and (say, you) are treated the same.

[[[00000000000000000966---d5542789579e990ba089c6da02fdbaec84f76122a63e9e2a40e829a8a9099473]]]Ideally, a model that considers the order of words in the context would be desirable. To achieve this, we can &quot;concatenate&quot; the context word vectors in the middle layer, as shown in the right panel of Figure 5-5. In fact, the model proposed in Neural Probabilistic Language Model[28] takes such an approach (see paper [28] for model details). However, if we were to take a concatenated approach, the weight parameter would increase proportionally with the size of the context. Of course, such parameter increases are not welcome.

[[[00000000000000000967---32684dc00da1fb4a6eb31f097cbee9dfc9081e723f925e7dab18f0d32940f4a1]]]So how do we fix the problems we&#39;ve identified here? As you might have guessed, this is where Recurrent Neural Networks, or RNNs for short, come into play. The RNN has a mechanism to remember the context information no matter how long the context is. So with RNNs, you can face even the longest time-series data! Next, I would like to thoroughly enjoy this wonderful world of RNN.

[[[00000000000000000968---4366e5d353bcb102e06ece0a10095739690ebc24e88f91c40801dac09ccfb7e9]]]word2vec is a method devised for the purpose of obtaining distributed representations of words. So it is not (usually) used as a language model. Here, in order to bring out the appeal of RNN, the story was developed by forcibly applying the CBOW model of word2vec to the language model. By the way, word2vec was proposed in 2013, and the language model by RNN, which we will look at later, was proposed in 2010 by a team led by Tomas Mikolov. Although we were able to acquire distributed representations of words with a language model using RNN, word2vec was proposed in order to cope with the increase in the vocabulary and to improve the &quot;quality&quot; of the distributed representation of words. on the background of

[[[00000000000000000969---650a961335b4e9849ab0e2f000a0bcf4c2319bc1020ac41b9b7e20d77d6d95e4]]]What is RNN

[[[00000000000000000970---3a3c18300a911e70ff2eca70968f01eccc72b55cabcee932555d9213bba279f1]]]Recurrent in RNN (Recurrent Neural Network) is a word that comes from Latin. It means &quot;to happen over and over again&quot;. In Japanese, it is translated as &quot;recurring&quot;, &quot;occurring periodically&quot;, and &quot;circulating&quot;. Therefore, the direct translation of RNN is &quot;recurring neural network&quot; or &quot;circulating neural network&quot;. First of all, I would like to think a little about the word &quot;circulate&quot;.

[[[00000000000000000971---9d7da8f5a25b2f948eb5ce55af10d1a3b51a6435ed0f75d5f311ab55a05fc7bb]]]Recurrent Neural Network is translated as &quot;Recursive Neural Network&quot; or &quot;Circular Neural Network&quot; in Japanese (the former is more common). There is also a type of network called a Recursive Neural Network. This is a network mainly for processing tree-structured data, and is different from recurrent neural networks.

[[[00000000000000000972---e68b1e28d83b780139f9235a173e9ffad280529d20df98a56cb5a9bbacd64817]]]circular neural network

[[[00000000000000000973---79452ebb5b1ac1f69f54563c18df87cd98a2a46aacdc66d8cfe97950f77ded38]]]Quick question, what does &quot;circulate&quot; mean? Of course that means &quot;keep repeating&quot;. Things that start at a certain point return to their original location over time, and then repeat. That&#39;s what the word &quot;cycle&quot; means. One thing I would like to point out here is that a &quot;closed path&quot; is necessary for circulation.

[[[00000000000000000974---66342d504f6a788d7349d807a49cdc2e025b74e5293d932854e4800a1a8c9804]]]A “closed path” or a “looping path”—only the existence of such a path allows media (or data) to travel back and forth in the same place repeatedly. And by looping the data, the information will be constantly updated.

[[[00000000000000000975---2f4e7f85fb2af3078717653889594717ba87813e6239fb9cb94ea3fb95fc841b]]]Blood circulates in our body. The blood that is flowing now has continued to flow from yesterday&#39;s blood. And it has a history that has been continuously flowing from a week ago, a month ago, a year ago, and from the time it was given life. As the blood circulates through the body, it is constantly “renewed” from the past to the present.

[[[00000000000000000976---3b687e57101a4fefc0baa8ce87de5f8b3ae2c67c7264195702e07ce3564ede21]]]A feature of RNNs is that they have looping paths (closed paths). This looping path allows data to circulate continuously. And by circulating the data, it is updated to the latest data while remembering the past information.

[[[00000000000000000977---8dfd66252ac95035a653267bf460858cdb7f99f714fc442bf28523a02324d17d]]]Let&#39;s take a closer look at RNNs. Here, the layer used in RNN is called &quot;RNN layer&quot;. This RNN layer can be represented graphically as shown in Figure 5-6.

[[[00000000000000000978---324df3dc15e62b322e234c09670a4d539db78011904e030f4b5d0d9ea3373dcb]]]
Figure 5-6 RNN Layer with Looping Routes


[[[00000000000000000979---184b22c668a0e2e2454b89bc9bea686ce0f1fe3860e2404688f78fea9a765bb7]]]The RNN layer has looping paths, as shown in Figure 5-6. This looping path allows data to cycle through the layers. Note that in Figure 5-6, time is t and xt is the input. This implies that the data is input to the layer as time series data. Then, in a form corresponding to that input, is output.

[[[00000000000000000980---250e8eb37c71952541fd73edc40b0ea8b761db94efd54dc347b311e4eecbf32a]]]Also here, xt is input to the RNN layer at each time, and xt is assumed to be some vector. For example, when dealing with sentences (a sequence of words), one example is the case of inputting the distributed representation of each word (word vector) as xt to the RNN layer.

[[[00000000000000000981---3774f9d192019d2aec725981033229932fdc4a5f61c63d6a526f5b271afcac5f]]]If you look closely at Figure 5-6, you can see that the output has two branches. &quot;Branching&quot; here means copying the same thing and branching. And one of the branched outputs becomes the input to itself.

[[[00000000000000000982---68531f66a30332261e906a06526786112cae8bb677ef456f64be198cbdd0fd8a]]]Now let&#39;s take a closer look at the loop structure in Figure 5-6. Before that, I would like to change the drawing method of the RNN layer as follows.

[[[00000000000000000983---c45bd44bd56baebbc34c02d4aecc9cae5aa5891f278000c26e4d3e34863e64fe]]]
Figure 5-7 Rotate the layer 90 degrees and draw


[[[00000000000000000984---bbf83f599527180a805ae222372ae00773e90a6503d11478c8eee5312b25c6be]]]So far, we have drawn layers assuming that data flows from left to right, as shown in Figure 5-7. However, from now on, due to space limitations, the layers will be drawn on the assumption that the data flows from bottom to top. for drawing).

[[[00000000000000000985---7324670473e28b6b9413ce1f7078ff4ff6ed169b97b6be73c97b3cf2d41c0459]]]unrolling the loop

[[[00000000000000000986---7bb12a6456d8842144dd897ff1cc6c0783e1e3c1af42dc50603fada95dedc427]]]You are now ready to go. Let&#39;s take a closer look at the loop structure of the RNN layer. The loop structure of RNN is a structure that did not exist in conventional neural networks. But by unrolling this loop, we can &quot;transform&quot; it into a familiar neural network. Seeing is believing. Let&#39;s actually unroll the loop (Figure 5-8).

[[[00000000000000000987---ff0e494dfc06152ff13ce15fd6d722fc989773c0477e3910783e14772b797663]]]
Figure 5-8 Unrolling loops in the RNN layer


[[[00000000000000000988---ce85fff7e7a707a0945cf20d5e719f5d42dbabb346c86b3b4f210b9dcfacc532]]]By unrolling the loops in the RNN layer, we were able to transform it into a long neural network extending to the right, as shown in Figure 5-8. This is the same structure as the feedforward neural networks we&#39;ve seen so far (feedforward means data goes in only one direction). However, the multiple RNN layers in Figure 5-8 differ from conventional neural networks in that they are all &quot;the same layer.&quot;

[[[00000000000000000989---66ec64a39a736b4e0b52daa194b56dce4ebc730107f7c24a35729f6824ee1d28]]]Time-series data is arranged in the time direction. Therefore, we use the word &quot;time&quot; to refer to the index of time series data (eg, input data xt at time t). Even in the case of natural language, expressions such as &quot;word at time t&quot; and &quot;RNN layer at time t&quot; are sometimes used while expressions such as &quot;tth word&quot; and &quot;tth RNN layer&quot; are used. .

[[[00000000000000000990---f55093025cc41508a394755d17fc14d16797c856b0adae7c2f8733e9214587a5]]]As can be seen in Figure 5-8, each time RNN layer receives an input to that layer and an output from the previous RNN layer. Then, based on those two pieces of information, the output for that time is calculated. The calculation to be performed at this time is represented by the formula as follows.

[[[00000000000000000991---2a7592140d883a6103a1bf51591826f5c8f3e4065a8e369039bfb2ac819dda1b]]]First, let us explain the symbols in equation (5.9). There are two weights in RNN. One is the weight Wx for converting the input x to the output h, and the other is the weight Wh for converting the previous RNN output to the next time output. It also has b as a bias. Note that here ht−1 and xt are row vectors.

[[[00000000000000000992---32c2de86c0a9fcdc902a080bfa555f0cac95b22dd2141bc09bd7c62995bedd0f]]]In equation (5.9) we perform matrix multiplication and transform their sum by the tanh function (hyperbolic tangent function). The result is the output ht at time t. This ht is output upward toward another layer, and is also output toward the right direction toward the RNN layer (self) of the next time.

[[[00000000000000000993---24fc13577524cba8c68b112c5ec38776085e8fc1c334611034b30eed6d3d9a44]]]By the way, looking at equation (5.9), we can see that the current output (ht) is calculated by the previous output (ht−1). Looking at this from another perspective, it can be interpreted that the RNN has a &quot;state&quot; h, which is updated in the form of equation (5.9). This is why the RNN layer is said to be &quot;a layer with state&quot; or &quot;a layer with memory&quot;.

[[[00000000000000000994---b0492be50f506b75162f85532b03812bbcc0cc8edaf5725c7895bf7310d64918]]]The RNN&#39;s h memorizes the &quot;state&quot; and is updated in the form of equation (5.9) as time advances by one step (one unit). In most literature, the RNN output ht is called the hidden state or hidden state vector. Similarly, in this book, the output ht of RNN is called &quot;hidden state&quot; or &quot;hidden state vector&quot;.

[[[00000000000000000995---09052781d780eacd92db36fd70fc6e15cf94cea49a6cf53bd54c537fc285c226]]]In many documents, the RNN layer after deployment is drawn as shown in the left figure of Figure 5-9.

[[[00000000000000000996---5b3e67894249af8b01446fdbe0b317259dc85bf0f2118803748c6df3b23c7466]]]
Figure 5-9 Comparison of RNN layer drawing methods after deployment


[[[00000000000000000997---363e330d08ed5211deb08303724dfb2fde298ec0136d26d95bec4f6ed9d6ff57]]]In the left figure of Figure 5-9, two arrows are output from the RNN layer, but the arrows are the same data—more precisely, the same data is copied and branched. —— need to be careful. This document will (as before) explicitly show that the output is bifurcated, as shown in Figure 5-9 on the right.

[[[00000000000000000998---cb43dcea4e5228dc324faead779f4c14389f6b957a6f85ac30eda0c5a9194c29]]]When I unfolded the RNN layer, I could think of it as a laterally stretched neural network. Therefore, RNN can be trained in the same procedure as a normal neural network. Graphically, this can be written as in Figure 5-10.

[[[00000000000000000999---ed30776bae51ae239dbd17ca1d54e5c6fc0714452687f4170330fb7e07575aee]]]
Figure 5-10 Backpropagation method for RNN layer after loop unrolling


[[[00000000000000001000---a5103079dd8698ba9b2a12095046681fecdad1536a579508cdd5f0a38648ce78]]]After unrolling the loops, the RNN can use (normal) backpropagation, as shown in Figure 5-10. In other words, by first performing forward propagation and then backpropagating, we can find the desired gradient. The error backpropagation method here is called &quot;backpropagation through time&quot; because it is &quot;the error backpropagation method of the neural network deployed in the time direction&quot;. Or it is called BPTT for short.

[[[00000000000000001001---f8e1cc5a34d3cee3361bc5a13168cfe28b3d11f06afc3069c12447010a6287b0]]]With this BPTT, it seems that RNN can be trained. But before that, there is one problem that must be solved. That&#39;s a problem when learning long time series data. The reason why this is a problem is that as the size of the time series data increases, the computing resources consumed by BPTT also increase. Another problem is that as the time size increases, the gradient during backpropagation becomes unstable.

[[[00000000000000001002---5770f298ded51497b92c017aec59108c07623176a013b961e4dae4460ceeb0cd]]]In order to find the gradient by BPTT, the intermediate data of the RNN layer at each time must be kept in memory (backpropagation of the RNN layer will be explained later). Therefore, as the time series data becomes longer, the memory usage of the computer (not only the amount of computation) also increases.

[[[00000000000000001003---25b735d2364846bc77cc56c905ff62a68b80fee6ce80d74fecc6634a3ca13a85]]]A common practice when dealing with large time-series data is to &quot;break&quot; the network connection at an appropriate length. This is the idea of creating (multiple) small networks by cutting out networks that have become too long along the time axis. Then we run error backpropagation on the truncated little network. This is a technique called Truncated BPTT.

[[[00000000000000001004---03f2754fdd9b8c0e442c7f17b2f251036832d6e42c2d1edf0049434f5338b22c]]]Truncated is a word that means &quot;cut off&quot;. Truncated BPTT is a &quot;truncated&quot; error backpropagation method.

[[[00000000000000001005---356b34b8dcf91a821fc12fb4bb14a807dec39665b4714c8cb38b12dd31fa03c6]]]Truncated BPTT truncates the network, but correctly truncates only the &quot;backpropagation&quot; tether of the network. The important point is that the forward propagation connection remains preserved. In other words, the flow of forward propagation propagates without interruption. On the other hand, the connection of backpropagation is cut at an appropriate length, and learning is performed in the cut network unit.

[[[00000000000000001006---b9502e2bc93c429ccfbcee681c02477d9d596767d885ddb35c07dc878ada2207]]]Let&#39;s take a look at a specific example of Truncated BPTT. For example, let&#39;s say I have time series data with a length of 1,000. In natural language terms, this corresponds to a corpus of 1,000 words. By the way, in the PTB dataset we have been working with so far, we have treated multiple sentences concatenated as one large time series data. Similarly, here, we will treat the concatenation of multiple sentences as a single time-series data.

[[[00000000000000001007---6fb0c44fcda8a42a5b97b98df8ab2de5f0ecc4a974b4b53414337e1d18ca8e25]]]Now, when dealing with time series data of length 1,000, when the RNN layer is deployed, it becomes a network of 1,000 layers horizontally. Of course, it is possible to compute gradients by backpropagation no matter how many layers are arranged side by side. However, if it is too long, it becomes a problem in terms of calculation amount and memory usage. Also, as the layer lengthens, the gradient may gradually decrease, and the gradient will not reach the previous time. Therefore, as shown in Fig. 5-11, we consider cutting off the backpropagation connection of the network that is long in the horizontal direction at an appropriate length.

[[[00000000000000001008---a86d13675ef661d8aec925144ebe86664552945ba0278233b12d07c803421cf1]]]
Figure 5-11 Cut the backprop chain at an appropriate place. Here, we call a series of RNN layers with a backpropagation connection a &quot;block&quot;, and draw the background of the block in gray.


[[[00000000000000001009---22a6c89feb1ea14ae3e97ff70a3cc1072b42769b8e38061d33d8530ca6f1a9fd]]]In Figure 5-11, the backpropagation chain is cut so that the length of the RNN layer can be learned in units of 10. Once you cut the backpropagation connection like this, you no longer need to think about future data. Therefore, we can complete backpropagation for each block—independently of future blocks.

[[[00000000000000001010---ee4e9ab0ae7bfb9be931aafed17d15f9a19287008e2d596241f7b4fd284d805e]]]The point to note here is that it breaks the backpropagation link, but not the forwardpropagation link. Therefore, when training an RNN, we must consider the forward propagation connection. What that means is that the data must be given in order (&quot;sequentially&quot;). I&#39;ll be more specific about what it means to feed data sequentially.

[[[00000000000000001011---624fbf20eb5df1ea486b4f786b463df72a62a135a71f3a46a741be1594bb70ea]]]In the neural networks we have seen so far, data is randomly selected when doing mini-batch training. However, when doing Truncated BPTT in RNN, the data must be given &quot;sequentially&quot;.

[[[00000000000000001012---d43a44af641876d13357237048fe307e33e6ac978ec0f30edf170afa20d62467]]]Now let&#39;s consider training an RNN with truncated BPTT. The first thing we do is give the first block of input data () to the RNN layer. Then the processing we are going to do here will look like Figure 5-12.

[[[00000000000000001013---1fccf7e30f6a79902545d5f79ab0c758a3ccfef36c0ec0f3a6a8c88d8673a1bd]]]
Figure 5-12 Forward and backpropagation of the first block: Since the gradient from earlier times is cut off, backpropagation is completed only within this block.


[[[00000000000000001014---318d52417e80a1ec6293c7aa6937175fefa3610028ee4fce5a5def8d36355754]]]Forward propagation is performed first, followed by back propagation, as shown in Figure 5-12. This will give you the desired gradient. Then, backpropagation is performed on the input data of the next block——x10 to x19——. This can be represented graphically as shown in Figure 5-13.

[[[00000000000000001015---811568394ffcecae2712cece1774057a494a5f51b3929ef34d47ac540f50dfd5]]]
Figure 5-13 Forward and Backpropagation of Second Block


[[[00000000000000001016---21b2f39cb1f9b4c7adf4ad91f76b02d98b9511aa3d97bc699a95602cfe45a3ef]]]Again, just like the first block, forward propagation followed by back propagation. And the important point here is that the computation of this forward propagation requires h9, the last hidden state of the previous block. This allows forward propagation connections to be maintained.

[[[00000000000000001017---6cd00fe9b4ed8c5424550be3d18bfb8b09a8a1a59380f878959df195df5b84fc]]]In the same way, continue learning for the third block. Again, we use the last hidden state (h19) of the second block. In this way, in RNN learning, data is given sequentially, and learning is performed while inheriting the hidden state. From the discussion so far, we can see that the flow of RNN learning is shown in Figure 5-14.

[[[00000000000000001018---672cf12a517777cfc6b4af3e3769083c63b1e49f6b2405907837014c3fbb9f23]]]
Figure 5-14 Data processing order in Truncated BPTT


[[[00000000000000001019---1c1b5d8230eafbfb030f89b44e196cd50c5f5fd66c4864615ac4aaa92b1fc5c6]]]As shown in Figure 5-14, Truncated BPTT trains by giving data sequentially. This allows us to apply backpropagation on a block-by-block basis while preserving forward propagation connections.

[[[00000000000000001020---3c8ba73a8db302e232c3c7ff69fc6b9040d72923123751c439173bdce10465e1]]]Mini-Batch Training for Truncated BPTT

[[[00000000000000001021---4e9e6fe0a666e0b6d0e4160952d2aabfec3089c7cbaa2ce39589032856d9362c]]]The talk about Truncated BPTT so far did not think about the &quot;batch&quot; of mini-batch learning. Strictly speaking, the story so far corresponds to when the number of batches is 1. Since we are doing mini-batch learning, it is necessary to consider batches and give data sequentially as shown in Figure 5-14. To do this, it is necessary to &quot;shift&quot; the starting position for giving data in each batch.

[[[00000000000000001022---c5a345322c7dfce9e021fededa71ab74b81e830e66983176b8b2945fe105c3e5]]]In order to explain the point of &quot;shifting&quot;, as before, let&#39;s take the case of learning with truncated BPTT that cuts the length of time in units of 10 for 1,000 time-series data as an example. will be explained. Then, how can we learn with 2 mini-batches in this case? In that case, as input data for the RNN layer, the first batch (sample data) is given data in order from the beginning. Then, for the second batch, starting at the 500th data point, the data is given in order from there—in other words, the starting position is &quot;shifted&quot; by 500. This can be represented graphically as shown in Figure 5-15.

[[[00000000000000001023---ed54a41e241b40a61fb18ff76d97bc06d437e41dcdadadb6e4e20062b6f512e0]]]
Figure 5-15 When performing mini-batch learning, shift the start position of giving data in each batch (each sample)


[[[00000000000000001024---60eeaa4ffd37ce70ec29288f3638419fd23a3587983d0e8c37115d3975f49829]]]Let the elements of the first batch be , and the elements of the second batch be , as shown in Figure 5-15. Then, this mini-batch data is used as input data for RNN for learning. The data to be given subsequently will be the 10th to 19th data and the 510th to 519th data of the time series data, as it will proceed sequentially. When performing mini-batch learning like this, the starting position of each batch is shifted as an offset and given sequentially. In addition, if you reach the end while giving data sequentially, it is necessary to return to the beginning.

[[[00000000000000001025---d3f8f4e1d2e044e7558f26b9708fee5eaddb4f103129b82d72cf124647911059]]]As we have seen above, the principle of Truncated BPTT is simple, but some caution is required in &quot;how to provide data&quot;. The specific points to note are &quot;to give data sequentially&quot; and &quot;to shift the start position of giving data in each batch&quot;. The story around here is somewhat complicated, so it may not make sense to you at this point. However, I think you can understand it by looking at the actual source code later.

[[[00000000000000001026---1324368d028f9b990160bec90919774df0167f7d87af2145a083efbc71a896d7]]]RNN implementation

[[[00000000000000001027---90994624d4a6478d4e35a57ebddd5c6e3811acb74d11cc255d00dc4b83a5d16c]]]From the story so far, we can see the whole picture of RNN. What we&#39;re going to implement is, after all, a laterally stretched neural network. Considering learning by truncated BPTT, it is enough to create a series of networks of fixed size in the horizontal direction. In practice, this would look like Figure 5-16.

[[[00000000000000001028---875422dd3d46dc79baec92409927f5ac513097531c15db167e20f70e0904f760]]]
Fig. 5-16 Neural network handled by RNN: Horizontal length is fixed


[[[00000000000000001029---563023f32dc975cd4930f3f42607f340ff5cd8a2817e4e198553faa59cad0473]]]As shown in Figure 5-16, our neural network receives time series data of length T, where T is arbitrary. Then, output T hidden states at each time. Here, considering modularity, we will implement the horizontal neural network in Figure 5-16 as &quot;one layer&quot;. This can be represented graphically as shown in Figure 5-17.

[[[00000000000000001030---36e0563581c45ee658d3c770ad205850728fe5fc41ef5d4e46d6f125f221da85]]]
Figure 5-17 Time RNN layer: Treat the layer after loop unrolling as one layer


[[[00000000000000001031---c43a8a11af689d4e18afac594c5eb868b2c2e2fc93716a768a530413326a70b6]]]As shown in Figure 5-17, the horizontal layers can be regarded as a single layer by bundling the vertical inputs and outputs together. In other words, inputting xs bundled can be regarded as a layer that outputs bundled hs. Here, we call the layer that processes one step within the Time RNN layer the &quot;RNN layer&quot;, and the layer that processes T steps collectively the &quot;Time RNN layer&quot;.

[[[00000000000000001032---bc61357b4f755dc1464435ed4799f2a92580798dff76d87a1fea3ace8ee698ce]]]Layers that collectively process time series data, such as the Time RNN, are prefixed with the word &quot;Time&quot;. This is a naming convention specific to this book. We will also implement a Time Affine layer and a Time Embedding layer later, but they also process time series data collectively.

[[[00000000000000001033---6048bd45639a50d8ec26de2358cf334a5708eb887892515978201f75b33cda14]]]In the flow of implementation that will be done from now on, first of all, a class that performs one step of RNN processing is implemented as an RNN class. Then, using the RNN class, the layer that collectively performs the processing of the T step is completed as the TimeRNN class.

[[[00000000000000001034---b2d628192fca8c9df479fb10fe01dd4aa7f37f51b0061b5671c0956070a83f10]]]RNN layer implementation

[[[00000000000000001035---e3fb8af04b56b974de74b32102d601908ec8816a383909632fbb62b84ba60f82]]]Now let&#39;s implement the RNN class that performs the RNN one-step processing. As a review, forward propagation of RNN is expressed by the following equation (5.10) (reprinted).

[[[00000000000000001036---e7e01a94fb67340f50d342f3a1945177bf4186d204f0c25cc204ae0bb9f254e4]]]Here we process the data together as mini-batches. So xt (and ht) store each sample data row-wise. It should be noted that the &quot;shape check&quot; of the matrix is important in the calculation of the matrix. In our calculations, if the batch size is N, the input vector has D dimensions, and the hidden state vector has H dimensions, the shape check can be written as

[[[00000000000000001037---690d8c9b6634d0ef037c8992e25cb4c63ed973824618e3a812d013d6fc1e5ccb]]]
Figure 5-18 Shape check: In matrix multiplication, match the number of elements in corresponding dimensions (bias omitted)


[[[00000000000000001038---ca05406118dcb9f9527b38ed9f259e6f2026d16a9f6bdde80fb27a1ea1ff0cda]]]As shown in Figure 5-18, we can check the correctness of the implementation — or at least the computation — by performing a matrix shape check. Based on the above, I will show the initialization of the RNN class and the forward() method for forward propagation (☞ common/time_layers.py).

[[[00000000000000001039---3541388ea76f1cd48852a39d3b6f654995f61203143a1a06b69acb408ad1fbaa]]]RNN initialization takes two weights and one bias as arguments. Here, the parameters passed as arguments are set as a list in the member variable params. Then we initialize the gradients corresponding to each parameter and store them in grads. Finally, the intermediate data used when calculating backpropagation is cache, which is initialized with None.

[[[00000000000000001040---bdb02e8fabd65cfdb8742af541281f8e1440afd91c778a830a2494aae845c139]]]The forward(x, h_prev) method of forward propagation takes two arguments—the input x from the bottom and the input h_prev from the left. All that remains is to implement it according to formula (5.10). By the way, here, h_prev is the input received from the previous RNN layer, and h_next is the output of the RNN layer at the current time (= input to the layer at the next time).

[[[00000000000000001041---ef82607c15dc380fad537f9ad49535e300695832bf874366bb154032cd4047b6]]]Now let&#39;s move on to implementing backpropagation in the RNN. Before that, I would like to confirm the forward propagation of RNN again with the computational graph in Figure 5-19.

[[[00000000000000001042---868ff554bf5a0c3c4e81c2b8491d59b9b84e6f2d5166a60cd43fd5cdd21942d2]]]
Figure 5-19 Computation graph of RNN layer (MatMul node represents matrix multiplication)


[[[00000000000000001043---bf8537d2dc7d6c954245fb0d8d6ff7bbcb7bc4087972342eac38ae66b0563c28]]]Forward propagation of RNN layers can be represented by the computational graph in Figure 5-19. The calculation here consists of three operations: &quot;MatMul&quot; for matrix multiplication, &quot;+&quot; for addition, and &quot;tanh&quot;. In addition, since the addition of the bias b causes a broadcast, the Repeat node is used to be precise, but for the sake of simplicity, its description is omitted here (see &quot;1.3.4.3 Repeat node&quot;). .

[[[00000000000000001044---f23aa0d8119daef7507f88423ba8aa15943719ebf8f6c8e57347fdde86a49ff6]]]So what does backpropagation look like for the computational graph in Figure 5-19? The answer is simple. Because we&#39;ve already learned about backpropagation for those three operations (see section 1.3 for backpropagation logic). All that remains is to backpropagate each operator in the direction opposite to the forward propagation according to Figure 5-20.

[[[00000000000000001045---4fa71856bd28493174e4dddb74c1cb0d99852252306366e851d9b28e079cedd6]]]
Figure 5-20 Backpropagation through computational graph of RNN layer


[[[00000000000000001046---5bb697c42db0ef4bac969194383fdf5ac0b75d86c4e359d497dde63939147e1d]]]Now for the implementation of backward() in the RNN layer. This can be implemented as follows, referring to Figure 5-20.

[[[00000000000000001047---c323dfc6ae8c5b13c53b4570244f3da60f3a28aeaafa9e34355e449fa9436458]]]The above is the implementation of backpropagation of the RNN layer. Now let&#39;s move on to implementing the Time RNN layer.

[[[00000000000000001048---fd357efdf93411b57bfaf781a5d846586ff50e6e8f2541e41ff600ec8cec6d33]]]Implementation of the Time RNN layer

[[[00000000000000001049---0e9cdfabd6754dbdc80c77ede8b5c5d3c10ee5d527bd47fa6171498174cce9f0]]]A Time RNN layer consists of T RNN layers (where T can be set to any number). Again, this would look like Figure 5-21 graphically.

[[[00000000000000001050---5f5bbf1edbe09cf4c7cf96c1d441396e7336c4c39cb58b12139ef168840da9e2]]]
Figure 5-21 Time RNN layer and RNN layer


[[[00000000000000001051---8f6e0e5c6bd3dd72ebfdf45a13adc8c0b8fdd71591faee31e3723e99016785d6]]]A Time RNN layer is a concatenated network of T RNN layers, as shown in Figure 5-21. This network we implement as a Time RNN layer. And here, we will store the hidden state h of the RNN layer in a member variable. This is used to &quot;take over&quot; the hidden state, as shown in Figure 5-22.

[[[00000000000000001052---2104b7940555df9d658b9e8cd9be22e4f709b43748f32e5c3c44808243f604cc]]]
Figure 5-22 The Time RNN layer holds the hidden state as member variable h. By doing so, the inheritance of hidden state between blocks becomes possible.


[[[00000000000000001053---71d62c24fa5488131b91669bd148afef599d12936cf168eb2549d289e6291c80]]]We will let the Time RNN layer manage the hidden state of the RNN layer, as shown in Figure 5-22. By doing so, those who use Time RNN don&#39;t have to worry about &quot;taking over&quot; the hidden state of the RNN layer. And we allow this feature—whether or not to inherit hidden state—with an argument called stateful.

[[[00000000000000001054---496545255f7cc7f9f78b8b182bb98300ba6d43fef87cee37e8254455dbf032ef]]]Now we present the implementation of the Time RNN layer. First, implement an initializer and two methods (☞ common/time_layers.py).

[[[00000000000000001055---008c038324e645d81d27e0bd13675cd81429c4a46fd0642bc587e8733a0615f2]]]The initializer receives weights, biases, and stateful booleans (True/False) as arguments. For member variables, first there is layers. This is used to keep multiple RNN layers in a list. Also, the member variable h holds the hidden state of the last RNN layer when the forward() method was called. And the member variable dh holds the gradient of the hidden state to the previous block when backward() is called (this dh is explained in Implementing Backpropagation).

[[[00000000000000001056---2fb7118b8ef23b341969f96a68144b3efb8f3abcad575a8cf3eb9a868cd173be]]]For extensibility, the TimeRNN class implements a method to set the hidden state of the Time RNN layer as set_state(h). Also implement a method to reset the hidden state as reset_state().

[[[00000000000000001057---e37df899dbb3430d935b9ec16e076a1a4d53146b275c1ad1eb94a9afdb693014]]]Stateful in the argument above is a word that means &quot;having a state&quot;. In our implementation, we want the Time RNN layer to be &quot;stateful&quot; when stateful is True. &quot;Having state&quot; here means maintaining the hidden state of the Time RNN layer. In other words, no matter how long the time series data is, the forward propagation of the Time RNN layer is propagated without interruption. On the other hand, when stateful is False, we initialize the hidden state of the first RNN layer with a &quot;matrix of zeros&quot; (a matrix with all zeros) each time forward() of the Time RNN layer is called. This mode has no state and is called &quot;stateless&quot;.

[[[00000000000000001058---40c875552b9852f98aaef1ec9ba3ae61ad8140aafec53971815f263684f0b87a]]]When processing long time series data, we need to maintain the hidden state of the RNN. The ability to maintain such hidden state is often described by the word &quot;stateful&quot;. In many deep learning frameworks, the RNN layer argument is called stateful, and you can specify whether to keep the hidden state of the previous time.

[[[00000000000000001059---2d5e436df67433297982405d4e200947d776379ea4ff0537768c6b711aadab73]]]Next is the implementation of forward propagation.

[[[00000000000000001060---fd0e2646ce916da8701b7c88b76f45fca3e107e1ad058702b545825797abd880]]]The forward propagation forward(xs) method receives the input xs from below. This xs is a collection of T time series data. So if the batch size is N and the number of dimensions of the input vector is D, the shape of xs is (N, T, D).

[[[00000000000000001061---e42c5ab603c30f8375be3d345b592e02e2865320009a284d586bfdfa8ba4ff8d]]]The hidden state h of the RNN layer is initialized with an all-zero matrix on the first call (when self.h is None). It also resets h with a zero matrix whenever the member variable stateful is False.

[[[00000000000000001062---0aa2c5328b760d09b5db99c76e09589700f1b1c3f44e39b643dfa7bb88e0eb3a]]]The main implementation first prepares a “container” for output by hs = np.empty((N, T, H), dtype=&#39;f&#39;). Then, create an RNN layer in a for statement that iterates T times and add it to the member variable layers. There the RNN layer computes the hidden state for each time and sets it to the corresponding index (time) in hs.

[[[00000000000000001063---cd32f8faab7ed8d97b4ecc4027b6a792f4f7ca31cb44720052bea29f569174b9]]]When the forward() method of the Time RNN layer is called, the member variable h is set to the hidden state of the last RNN layer. If stateful is True, the member variable h will continue to be used the next time the forward() method is called. On the other hand, if stateful is False, the member variable h is reset with a matrix of zeros.

[[[00000000000000001064---bacb2e8f642c9d73f39797f6e0f5dda71ff769d9b02ab527ff3f53108f3a2098]]]Next is the implementation of backpropagation for the Time RNN layer. This backpropagation can be written as a computational graph as shown in Figure 5-23.

[[[00000000000000001065---7c897df5579434ba45ce2687f4a5eaf5949da2dcf4dba0b7feca4d9b1d33bcdc]]]
Figure 5-23 Backpropagation of the Time RNN layer


[[[00000000000000001066---33e3ecdd920add179bb092cbdc6e684e0c1c530c3ff22fbe0931cb8939b4fbca]]]As shown in Figure 5-23, dhs represents the gradient transmitted from the upstream (output layer), and dxs represents the downstream gradient. Here we do Truncated BPTT, so backpropagation of the previous time for this block is not needed. However, the gradient of the hidden state to the previous time is stored in the member variable dh. This is necessary for seq2seq (sequence-to-sequence), which is covered in Chapter 7 (more on that at that time).

[[[00000000000000001067---0c5af249e58a3c0a4309ba7c2ba53f885080e640bf106ca11b3e5df97471c9aa]]]The above is the overall diagram of the backpropagation of the Time RNN layer. At this time, focusing on the t-th RNN layer, its backpropagation can be written as shown in Figure 5-24.

[[[00000000000000001068---2d57ea18a3f5dd9d3917af3b13394862e52d41c2fc2714faff466c769d502b61]]]
Figure 5-24 Backpropagation for the tth RNN layer


[[[00000000000000001069---8a04f58bb9902708e594b6fbeeacd2486dd630214f9fc48cfc828a1368baccb5]]]The tth RNN layer carries the gradient dht from above and the gradient dhnext from &quot;one future layer&quot;. The caveat here is that the forward propagation of the RNN layer splits the output into two. When branching in forward propagation, each gradient is summed up in backward propagation. So, during backpropagation, the summed gradients ——dht + dhnext——are input to the RNN layer. With the above points in mind, the implementation of backpropagation looks like this:

[[[00000000000000001070---5b314b384735085f728c9ab32f8af8b3aaa430e2b93ed5308d9025e089ed32a3]]]# summed gradient

[[[00000000000000001071---b450f22d909c39a917aa957de85a77a3e989ae56bb2015b9ee1cfe0bddffbcc3]]]Again, we first create a downstream gradient &quot;vessel&quot; (dxs). Then, call the backward() method of the RNN layer in the reverse order of forward propagation, find the gradient dx at each time, and substitute the corresponding index of dxs. Also, for the weight parameter, add the gradient of the weight in each RNN layer, and overwrite the final result in the member variable self.grads using &quot;... (three dots)&quot;.

[[[00000000000000001072---9f8c874b84fb6976dac5fd53ff0ba588ff79b7a4baaa07bd77280fde01625280]]]There are multiple RNN layers within the Time RNN layer. And in those RNN layers we are using the same weights. So the (final) weight gradient for the Time RNN layer is the sum of the weight gradients for each RNN layer.

[[[00000000000000001073---22d5556d4950290d3820f5fcbeea51a6a1aeb710dab05da79fe0db0798bc0e04]]]The above is the description of the implementation of the Time RNN layer.

[[[00000000000000001074---9988d2a6db765b005eae9e1db28d7946239eb52aa245fca66e817b3a24fcd97e]]]Implementation of layers that handle time series data

[[[00000000000000001075---ff4c6bfe6b73bc685354518bd5cc97340444b5701e563581c23f022f512b2233]]]Our goal in this chapter is to implement a &quot;language model&quot; using RNNs. We have so far implemented an RNN layer—and a Time RNN layer that collectively processes time series data. Here, I would like to create some new layers that handle time series data. The language model by RNN is called RNNLM because it is RNN Language Model. Now let&#39;s move on to complete the RNNLM.

[[[00000000000000001076---8151ef1414cfaf9234c601e4244723a35df5b5a6daa9641080e610f17aafdd0d]]]Overview of RNNLM

[[[00000000000000001077---5dbb74fefe789ccab064188e8fb2b7e1d694652247c69d0c7975a4bd969def44]]]First, let&#39;s take a look at the network used in RNNLM. Here, the simplest RNNLM network diagram is shown in Figure 5-25. The left figure of Fig. 5-25 shows the layer structure of RNNLM, and the right figure shows the network expanded on the time axis.

[[[00000000000000001078---c1946ffe81f91be1c23da57808659013f70e00fbe5e09dc8e61786b8f64c44d6]]]
Figure 5-25 RNNLM network diagram (left figure before deployment, right figure after deployment)


[[[00000000000000001079---03543bda3142e8fb449fde3840862049425f2d0f0a1ba7c26c23b909f5e4502f]]]The first layer in Figure 5-25 is the Embedding layer. This layer converts word IDs into distributed representations of words (word vectors). The distributed representation is then input to the RNN layer. The RNN layer outputs the hidden state to the next layer (upward) and at the same time to the RNN layer of the next time (rightward). The hidden state output from the RNN layer is transferred to the Softmax layer via the Affine layer.

[[[00000000000000001080---33d850046b617818b8dff92bdf720acc5ebab5dc1b6c3454925a24e53a2e2727]]]Now, let&#39;s consider only forward propagation for the neural network in Figure 5-25 and try to flow specific data. And let&#39;s see the output result at that time. The sentences to be dealt with here are the familiar &quot;You say goodbye and I say hello.&quot; At this time, the processing performed by RNNLM is shown in Figure 5-26.

[[[00000000000000001081---d928f522dc6080aa672c7abdd79e532d87394556237b4ed0751256faa93bbe3e]]]
Figure 5-26 RNNLM example processing &quot;you say goodbye and I say hello .&quot; as a sample corpus


[[[00000000000000001082---378df44c10430baf5827be743694489149ed7fface890911ef894634dd2dfa9b]]]The input data is an array of word IDs, as shown in Figure 5-26. Let&#39;s focus on the first time. Here, &quot;you&quot; with a word ID of 0 is entered as the first word. Looking at the probability distribution output by the Softmax layer at this time, you can see that &quot;say&quot; is the highest. This correctly predicts that the next word after &quot;you&quot; is &quot;say&quot;. Naturally, such correct predictions are only possible with &quot;good weights&quot; (well-learned weights).

[[[00000000000000001083---5056cdea322527090072df91afc79636da55404506a2b0173ee40a4eb6179f37]]]Next, let&#39;s focus on where we type the second word, &quot;say&quot;. At this time, the output of the Softmax layer is high at two places, &quot;goodbye&quot; and &quot;hello&quot;. Indeed, &quot;you say goodbye&quot; and &quot;you say hello&quot; are both natural sentences (by the way, the correct answer here is &quot;goodbye&quot;). It should be noted here that the RNN layer &quot;remembers&quot; the context of &quot;you say&quot;. To be more precise, the RNN holds the past information of &quot;you say&quot; as a compact hidden state vector. It is the job of the RNN layer to convey such information to the upper Affine layer and then to the next RNN layer.

[[[00000000000000001084---630d667774d95aeafd9b5b586c40707ebb136e9f13cc0a6c11e92eda06f29674]]]In this way, RNNLM &quot;remembers&quot; the words that have been entered so far and uses them to predict the next word that will appear. What makes this possible is the existence of the RNN layer. The RNN layer continuously streams data from the past to the present, allowing past information to be encoded and stored.

[[[00000000000000001085---b9a240c75288e3f50e8550ce19642535db58571bf7c8742ab7bb7da1d2f7d3a5]]]Time layer implementation

[[[00000000000000001086---6ad0a14b4c8578a8a588aa8c064126ea904b188e84ce7e9eae0c2e11e2d737ff]]]So far, we have implemented layers that collectively process time series data as Time RNN layers. Similarly here, I would like to implement a layer that collectively processes time series data with a name such as Time Embedding layer or Time Affine layer. Once we have this Time XX layer, our desired neural network can be implemented as shown in Figure 5-27.

[[[00000000000000001087---21f0cc32acb939f49bf2b560ac65ae065c8532f2f55bdbb33d71d4752b35d6c3]]]
Figure 5-27 Implement the layer that collectively processes time-series data as the Time XX layer


[[[00000000000000001088---993b7e3d3218dcc4ed7bb3e8c8e4e87f9da16cddd3016c6b28c6cd01fd372232]]]We call the layer that collectively processes T pieces of time-series data the &quot;Time XX layer&quot;. If such a layer can be implemented, the layer can be combined like Lego blocks to complete a network that handles time-series data.

[[[00000000000000001089---4c57b9cb871c5a4c7423e985d4bb18b0d16de200f00b35e4a6e72dc12a28ecc9]]]Implementing a Time layer is straightforward. For the Time Affine layer, for example, prepare only T affine layers as shown in Figure 5-28 and process the data for each time individually.

[[[00000000000000001090---e765caf5ee221cf382dd56c4f6c3da4ef1701153e164f2797ef0662d8833e60f]]]
Figure 5-28 The Time Affine layer is implemented as a set of T Affine layers


[[[00000000000000001091---91b16654d30207463ebb15fb837bb65afbb08775cd6b115d1b69538a15e99522]]]Similarly, the Time Embedding layer prepares T embedding layers during forward propagation, and each embedding layer processes data for each time.

[[[00000000000000001092---55d12f35454e11c7810233dce00e76db3576daee8cfe63829cfce7549b73c2e9]]]The Time Affine layer and Time Embedding layer are not particularly difficult, so we omit their explanation. As for the Time Affine layer, we do not simply use T affine layers, but we implement an efficient implementation that processes them collectively as a matrix calculation. If you are interested, please refer to the source code (TimeAffine class in common/time_layers.py). Next is about the time series version of Softmax.

[[[00000000000000001093---91351aba402ec13d65db4f4e4a700c208b3f2b9be928b3d0974abb1ce054845f]]]For Softmax, we also implement the Cross Entropy Error layer, which is the loss error. Here, it is implemented as a Time Softmax with Loss layer with a network configuration as shown in Figure 5-29.

[[[00000000000000001094---d9f2b9c884afc5ce709c32ee67427405a4696a81e30790d25bf9bfef528459ef]]]
Figure 5-29 Overall view of Time Softmax with Loss layer


[[[00000000000000001095---9133c0c84b9f84987ada34723fd34d80c3c2e3bad1a09bfbf5a2733b254b0dce]]]Data such as x0 and x1 in Figure 5-29 represent the &quot;scores&quot; coming from the layers below (scores are values before normalization to probabilities). Also, data such as t0 and t1 represent correct labels. As shown in the figure, T Softmax with Loss layers each calculate the loss. Then add them up and take the average of them as the final loss. In addition, the calculation to be performed at this time is expressed as follows.

[[[00000000000000001096---6d48c4cc812c43fc1800cdc61e4d2d2b9e3ba440f9f4a4ea450e5a0fe9c805aa]]]By the way, the Softmax with Loss layer in this book averaged its loss over mini-batches. Specifically, if we had N data points in a mini-batch, we found the average loss per data point by summing the N losses and dividing by N. Again, by averaging over the time series, the final output is the average loss per piece of data.

[[[00000000000000001097---4b430b0290506c4eaf077858ee6f5c416ba9bc2aa31c3938f0881a08e1ac79d3]]]The above is the explanation about the Time layer. Only a brief overview is provided here. See common/time_layers.py for the actual implementation. Please refer to it if you are interested.

[[[00000000000000001098---1d8de9025291283a5286adf3ffbc6aa5aa4613fd4dbd5971f08e34d399f2efb9]]]RNNLM training and evaluation

[[[00000000000000001099---29f39373b2d8e74daa7fd8aa188e5131e850feb95459ac5c5cf58142519de79f]]]We now have all the layers we need to implement RNNLM. Let&#39;s implement RNNLM and let it actually learn. And I would like to evaluate its performance.

[[[00000000000000001100---11c0d9c545b78bb6b846f469209bb1b09a44c38915d2563c98e197d4cf233fe1]]]RNNLM implementation

[[[00000000000000001101---c41de00feef6221ce73f6ffd6901bcbce4f74db819b1bf30952f5f45210d74ce]]]Here, I would like to implement the network used by RNNLM with the class name SimpleRnnlm. The layer structure of this SimpleRnnlm looks like Figure 5-30.

[[[00000000000000001102---a11daeb2db5075de08cc419f7c8184987231b9d3d7a69285800485ba5a959271]]]
Figure 5-30 Layer configuration of SimpleRnnlm: RNN layer status is managed within the class


[[[00000000000000001103---6e61e7df135efccafd15308678d97bac4a1d7b04b0f940f5a9dc3580e3825c83]]]The SimpleRnnlm class is a neural network with four Time layers, as shown in Figure 5-30. First, here is the initialization code (☞ ch05/simple_rnnlm.py).

[[[00000000000000001104---d7f2704dda40d0528bed1e8e16c66e17b842f50172d7ba8554f23c7229aedc67]]]# Initialize weights

[[[00000000000000001105---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000001106---06f7dbb825277252d3f70b6a892a5aca619776d6c46333a516efdd8f89f3dda7]]]# Collect all weights and gradients in a list

[[[00000000000000001107---87eb6f2744133f5ac18f6bc4afec5f5fe81adbaec90074578c97e1a7d7cb2f0c]]]Here we initialize the parameters (weights and biases) used by each layer and generate the layers we need. Also, the Time RNN layer has stateful set to True, assuming training with truncated BPTT. This allows the Time RNN layer to inherit the hidden state of the previous time.

[[[00000000000000001108---6ee8d35d503ca3530b60d95fd598d40b46aff07440f29dc394f31e5ea9e5daa2]]]In the above initialization code, &quot;Xavier&#39;s initial value&quot; is used in the RNN layer and the Affine layer. By default, Xavier uses a distribution with a standard deviation of , where n is the number of nodes in the previous layer, as shown in Figure 5-31†2. Intuitively, standard deviation can be interpreted as an indicator of data variability.

[[[00000000000000001109---02f2d51fba44db0c26401c76259068a906c3368efe178089de8a2682a9426ed4]]][†2] However, this is a simplified version of the implementation, and the original paper proposes initial weight values that also consider the number of nodes in the next layer.

[[[00000000000000001110---0fb1d66e9fb374339be9c5f3a1c7c9dbbde29e6f589037401354c10a74ab1062]]]
Figure 5-31 Initial value of Xavier: When there are n node connections from the previous layer, use a distribution with a standard deviation of as the initial value


[[[00000000000000001111---1dc60e4e61b1e096d8b6c58257675e55724d6120822d93520d1365a10b39567a]]]Initial values of weights are important in deep learning. I discussed this point in detail in ``6.2 Initial values of weights&#39;&#39; in my previous work, Deep Learning from Scratch. Similarly in RNN the initial weights are very important. By setting a good initial value, the progress of learning and the final accuracy will change greatly. In this document, we will continue to use &quot;Xavier&#39;s initial value&quot; as the initial value of the weight. In addition, in research dealing with language models, there are many cases where a scaled uniform distribution such as 0.01 * np.random.uniform(...) is used.

[[[00000000000000001112---05ad43a66f0c4884095ecf278a180c232d22e2a15f8a5e1e38550729627bb641]]]Next, we show the implementation of the forward(), backward(), and reset_state() methods.

[[[00000000000000001113---e89ac848113bc4b6964408e8a8e025718f8745b3f1058db6b1e70e813aa73428]]]As you can see, this implementation is very easy. Each layer has a proper implementation of forward and backpropagation. So here we just call forward() (or backward()) for each layer in the appropriate order. Also, for convenience, we implement a method to reset the network state as reset_state(). The above is the description of the SimpleRnnlm class.

[[[00000000000000001114---de750cfc2e1f76b38bbd3f61ebafaa3b2634aa5885e34c1d318fef0899d52ed1]]]Language model evaluation

[[[00000000000000001115---d6e52e1983555b3db29b06080518df67e778c07fe97c9946ace600854cbf8408]]]The implementation of SimpleRnnlm is finished. All that is left is to feed the network with data and train it. I&#39;m going to implement the code for training, but before that, I&#39;d like to talk about the &quot;evaluation method&quot; of the language model.

[[[00000000000000001116---c2ce7a1005b703ea52ce5de911cba85a331f7f2dacbec031c6fb8784bb9510a7]]]The language model outputs the probability distribution of words that appear next from given words (information) in the past. At this time, perplexity is often used as an index to evaluate the prediction performance of language models.

[[[00000000000000001117---14d917c74ea8710ee498c1fa685cd779f33081915233cf7c7b01ae941dbcb3ee]]]Simply put, perplexity represents the &quot;reciprocal of probability&quot; (this interpretation is exactly the same when the number of data is one). To illustrate the idea of the inverse of probability, consider the corpus &quot;you say goodbye and I say hello .&quot; At this time, suppose that the word &quot;you&quot; is given to the language model of &quot;model 1&quot; and the probability distribution shown in the left figure of Fig. 5-32 is output. If the next word that appears here is &quot;say&quot;, the probability is 0.8. This is a pretty good prediction. The perplexity at this time can be calculated by taking the reciprocal of that probability.

[[[00000000000000001118---4f4039c1e1abf66d07e8f1e78acae74cf77f0567dd39e0114c47d2278e1f3bd1]]]
Figure 5-32 Example of a model that inputs the word &quot;you&quot; and outputs the probability distribution of the next word


[[[00000000000000001119---ab129b35b4c12553b57032ddc01958409347da06ff946320606c0497cf63e102]]]On the other hand, the model on the right side of Figure 5-32 (&quot;Model 2&quot;) predicts 0.2 for the correct words. This is clearly a bad prediction. The perplexity at this time is

[[[00000000000000001120---78e060587e16828e04d58ff815727c8e1aa1f6bb9627e2722094542f8caa36a9]]]To summarize the story so far, &quot;Model 1&quot; was able to predict with good accuracy, and its perplexity was 1.25. &quot;Model 2&quot;, on the other hand, was a guesswork, with a perplexity of 5.0. This example shows that the smaller the perplexity, the better.

[[[00000000000000001121---46edc7a245183da594a2a666e75743d2574d43e723a69d72c265936275e52a7e]]]So how can we intuitively interpret the values 1.25 and 5.0? This can be interpreted as the &quot;number of branches&quot;. The number of branches is the number of possible next choices—more specifically, the number of possible next word candidates. In the previous example, the &quot;number of branches&quot; that predicted a good model was 1.25, which means that the next candidate that appeared was narrowed down to about 1. On the other hand, the bad model shows that there are still 5 next candidates.

[[[00000000000000001122---58bf47e1fd0787d2196a550865508973e0bbdb8e645f78bfa70b670cfa88c037]]]As the example above shows, perplexity can be used to assess a model&#39;s predictive performance. A good model can predict the correct word with a high probability. Therefore, the perplexity value is small (minimum perplexity is 1.0). A bad model, on the other hand, predicts the correct word with a low probability. Therefore, the perplexity value is large.

[[[00000000000000001123---f25bcdb82f5097771a0ee1d17fa7763bf1c0e735d2678f414e4d8fa088b643e4]]]The story so far is about perplexity when there is only one input data. So what happens when the input data is multiple? In that case, calculate according to the following formula:

[[[00000000000000001124---e85b4d67737a386850863cefa5e520c7210c573baba51994477d705fec81900c]]]Suppose we have N data. Also, tn is the correct label of the one-hot vector, and tnk means the kth value of the nth data. And ynk represents the probability distribution (output of Softmax in neural networks). By the way, this L is the neural network loss, which is exactly the same as (1.8). Perplexity is obtained by calculating eL using this L.

[[[00000000000000001125---8cab404fe1530303af96808c798b0b435aba8db212f761a91f106c9972fa7b91]]]The above equation (5.12) is somewhat complicated, but the intuitive understanding is the &quot;reciprocal of probability&quot; explained when there is only one data, and the understanding of &quot;number of branches&quot; and &quot;number of choices&quot; remains the same. It works. In other words, the smaller the perplexity, the smaller the number of branches and the better the model.

[[[00000000000000001126---ffa40b69a081f879a9db7a8dc44db0f5e900179a50c74fb224841edfb3025d4a]]]In the field of information theory, perplexity is also called &quot;average number of branches&quot;. This is the interpretation that the &quot;number of branches&quot; explained when there is 1 data is averaged for N cases.

[[[00000000000000001127---786b5dd2aad7eb1b05ac46fef6c55c9ec32f29b40e29ac16941c8dfd7b05e2e2]]]RNNLM learning code

[[[00000000000000001128---d24586951b6c6e132c3f8992a6924e81b86658d774582fbaa05674129598fbf6]]]Let&#39;s train RNNLM using the PTB dataset. However, we will only use the first 1,000 words from the PTB dataset (training data). This is because the RNNLM implemented here does not produce good results at all when targeting all training data. This improvement will be done in the next chapter. Here is the implementation for training (☞ ch05/train_custom_loop.py).

[[[00000000000000001129---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000001130---70001ac4979f877d09f05ffad8684919f946c958040c421248f0baaee1fd4111]]]# number of elements in RNN hidden state vector

[[[00000000000000001131---a2061aa7707aeb6e951e0380ea5c7f452b9da3b6cd57d03d74562cd221e2e4d0]]]# Time size to deploy Truncated BPTT

[[[00000000000000001132---31fd08c8d98413aeefe7249d43a40261b0c131f9041a263b9ea2a1dafe82f557]]]# load training data (smaller dataset)

[[[00000000000000001133---0a8fab9fe0eba957e830a351ef5595b0ed9a19c3f9165698d7c2fb70af1142d6]]]# input

[[[00000000000000001134---bc610fa1ba9e830d117e68bb34161a193389ed10ed775e5800ae34ca69ec8f26]]]# output (teacher label)

[[[00000000000000001135---380f0ca840ba87d908be0df2f285aa4d1935fe1b5954c9fe986a1ad64963a499]]]# Variables used during training

[[[00000000000000001136---d2e487f751104844d65f8a00a849ef275b97f3f9f1fd61ddc98846db3788f6c7]]]# generate model

[[[00000000000000001137---6f18e1fd694083233e7c72c5ee1507ab21d2e39ae7405d6b9c438305937df959]]]# ❶ Calculate the reading start position for each sample in the mini-batch

[[[00000000000000001138---b2caf0157a4476502b35583fe6eb2ba1fb52704dbce15298fd4f3efe11df03a2]]]# ❷ Get mini-batch

[[[00000000000000001139---5fabbdf63526c89777116ea6db1fad5d03fdcc74ae885e9cf0013009c5c1180e]]]# find the gradient and update the parameters

[[[00000000000000001140---c5bc5f084e68e843ae0271721bbf6038a1dbdb0579f1ccdfb695b33f5b224f9c]]]# ❸ Evaluate perplexity every epoch

[[[00000000000000001141---c32d037ee64f36c3792635da8995a69cf3fdb5ae4277cae116c3004e3f255669]]]Here is the code for learning. This is basically much the same as training neural networks that we have seen so far. However, in the big picture, it differs from the code you&#39;ve learned so far in two ways. It is the point of &quot;how to give data&quot; and &quot;calculation of perplexity&quot;. Here, I will explain the above code by focusing on those two points.

[[[00000000000000001142---dc4b5b3b97486e57232ca64023fdc1006f2ddd95bb6552de29b6522138ef5faf]]]First, let&#39;s talk about how the data is presented. Here we train with Truncated BPTT. Therefore, the data must be supplied sequentially, and the starting position of the data supply must be staggered in each batch of mini-batches. In ❶ of the source code, the starting position for reading data in each batch is calculated as offsets. Each element of this offsets stores the starting position (“shift”) to read the data.

[[[00000000000000001143---07bbb371e1cf2bd278b1b3530edc607ede1f69d97fda4647305a3f2fb4f460cf]]]Next, read the data sequentially in ❷ of the source code. Here, we first prepare batch_x and batch_t as “containers”. Then, while incrementing the variable time_idx in order (sequentially), get the data at the time_idx location from the corpus. Now add offsets to each batch using the offsets calculated in ❶. Also, if the corpus reading location exceeds the corpus size, the remainder of dividing by the corpus size is used as an index to return to the beginning of the corpus.

[[[00000000000000001144---16d90cad84913fe212ba58f9a0ef8b8292fa87ce34a774f8697eaa7fab9dce87]]]Finally, perplexity is calculated by equation (5.12). This is done in code ❸. Since we want per-epoch perplexity, we average the loss per epoch and use that to find perplexity.

[[[00000000000000001145---1a16e89da28b38bada808160565426b0c97e6b0788bfa856bea6283458a7a173]]]Here is the code description. Now let&#39;s look at the results of our learning. In the code above, the perplexity results for each epoch are stored in perplexity_list, so let&#39;s plot it. The result should look like Figure 5-33.

[[[00000000000000001146---eca1936e4240240d3e3d898e0239b47dc9d1a3d3f48a6d177a85fed31102a8fc]]]
Figure 5-33 Changes in Perplexity


[[[00000000000000001147---b0578820bd15c2eb201e8a1904786fbe2381d95f6dbb8c1585148c0a7a710680]]]Looking at Figure 5-33, we can see that the perplexity decreases smoothly as the learning progresses. The perplexity, which was over 300 at the beginning, approaches 1 (minimum value) towards the end. However, the experiments performed here are for datasets with a small corpus size. In fact, as the size of the corpus grows, the current model simply cannot keep up. In the next chapter, we will point out the problems of the current RNNLM and improve it.

[[[00000000000000001148---9761c5993a80930519c53d20bf3982fb94c8428e4f90504af003307088252f28]]]RNNLM Trainer class

[[[00000000000000001149---09a3ecd9f1e444f07bef0646e2b934f835d7c29089749ff944045a1c34bce482]]]This book provides the RnnlmTrainer class for learning RNNLM. This class hides inside the RNNLM training we just did. Rewriting the previous training code using the RnnlmTrainer class looks like this: Here is an excerpt of the source code (the actual source code is in ch05/train.py).

[[[00000000000000001150---9a025e1ba70bf3ff638a1c4a804a08ac031f6723bd5b093382efb9400ba6f616]]]As above, we first initialize the RnnlmTrainer class by giving it a model and optimizer. After that, training is done by calling the fit() method. At this time, a series of operations are performed internally as in the previous section. If you describe the details,

[[[00000000000000001151---1156f10799ae5114fc9221aefbbab5bfd8919b443a10c9de4ef800e4dd9f27ac]]]Making mini-batch &quot;sequentially&quot;,

[[[00000000000000001152---ea0b038686d039ff493f69398a0110e5605d2f924de1b21b86a890217b3e2387]]]Call forward and backpropagation of the model,

[[[00000000000000001153---b0e2a5a6bab2a57c9a1e3f9e7ab6960d43279de8f8e5698b79adbc6e22835791]]]update the weights in the optimizer,

[[[00000000000000001154---2b0e9456416ab0bfd1f1e4e87b3e7f2ec6f27c8672ce9b2e9013e5455b871582]]]Evaluate Perplexity

[[[00000000000000001155---f84c07ce1a89d3d335c281d3264fab54e51647eaa6b2a9ee01ed7e8b4e589e9e]]]It will be a series of processes.

[[[00000000000000001156---3fbdf6d4e1297c97a4f275ed82255b2edbe446a2f529b1550ed4016dfd3dd33f]]]The RnnlmTrainer class has the same API as the Trainer class described in &quot;1.4.4 Trainer class&quot;. Regular training of neural networks uses the Trainer class, and RNNLM training uses the RnnlmTrainer class.

[[[00000000000000001157---8ba66d043383423c02a1939945d6f998ed307252ad9e98b806bc23a7c6cad649]]]By using the RnnlmTrainer class, we avoid writing the same code each time. For the remainder of this book, we will use the RnnlmTrainer class for training RNNLMs.

[[[00000000000000001158---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001159---ecfd51d98db79b7370e8c499e0f0b48db2c427a588fcbd6b36af0549f4146de4]]]The theme of this chapter was RNN. RNNs provide a continuous flow of data from the past to the present to the future by circulating the data. That gives us the ability to store &quot;hidden states&quot; inside the RNN layer. In this chapter, we spent some time explaining how RNN layers work and actually implemented an RNN layer (and a Time RNN layer).

[[[00000000000000001160---ac0aa4b270c982d8a56e431e27d4787792c86d325b994fcbbc20fc45703f17a8]]]Also, in this chapter, we created a language model using RNN. A language model gives probabilities to sequences of words. In particular, the conditional language model computes the probability of the next word from the list of previous words. By configuring a neural network using RNN here, it is theoretically possible to memorize important information in the RNN&#39;s hidden state, no matter how long the time series data is. However, in actual problems, there are many cases in which learning is not successful. In the next chapter, we&#39;ll point out the problems with RNNs and look at new layers that can replace RNNs -- the LSTM layer and the GRU layer. Those layers are one of the most important layers in time series data processing. In fact, it is often used in cutting-edge research.

[[[00000000000000001161---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000001162---6d009c096036f01fee5c3be104380f50909aeef59ee2743b8024a3a2bce5174f]]]RNNs have looping paths that allow them to store &quot;hidden states&quot; internally

[[[00000000000000001163---b90e88671f9921b6eb50e0ef41cce2a042e0662aa0e449728ab4cbfde5382fd6]]]By unrolling the RNN loop path, it can be interpreted as a network with multiple RNN layers connected, and can be learned by the normal backpropagation method (= BPTT).

[[[00000000000000001164---66381d9da39268da801a81d98c532da0598f32b4d699671c5e4b6add200e3087]]]When learning long time-series data, create a group of data with an appropriate length—this is called a “block”—and perform learning by BPTT in units of blocks (=Truncated BPTT).

[[[00000000000000001165---3fc8bec2b13127de6d29f79140f3dd5d918c01b7ad2fadb1809a918750703abc]]]Truncated BPTT cuts only backpropagation links

[[[00000000000000001166---34ca202309ce0e59630c7880d9555c6d5f6db4673c011d181ca4f473d8e87c5d]]]Truncated BPTT maintains forward propagation connections, so data must be given “sequentially”

[[[00000000000000001167---288a2b3c5322a22735574fccf7f543ca2527e09cebe187dbfaec1daa9ea75958]]]A language model interprets word sequences as probabilities

[[[00000000000000001168---bf13affa9c058940b51880f1a4ae7f01e7d0303ba10c9da73f2b116cb56bda0f]]]A conditional language model using an RNN layer can (theoretically) remember information about words that have appeared so far.

[[[00000000000000001169---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O&#39;Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000001170---d1ebe0826fb25fa5332112e85497b0bd95dd0b5df640c994c96a877086cbda65]]]Deep Learning from scratch ❷

[[[00000000000000001171---78d5a535d0319c8239c884884ab977a37c01a4a5ba97983d65273459c77c5e99]]]Natural language processing

[[[00000000000000001172---fdfd6523655d90694adf21a91d10730e87f6bcb177fc9e48a946684debda0db1]]]July 24, 2018 First edition 1st edition published

[[[00000000000000001173---bc7e0c28434653c1f3ec4c19e9afbf020aa10d04c2d09583a8a27d34cc83bd86]]]
May 8, 2023 First edition 3rd edition published

[[[00000000000000001174---f2f5b6f470310eaf5a90b614a718214b9b06a6adc028924fc6a0d8ba94857806]]]Author Yasutake Saito

[[[00000000000000001175---16f1fe5d091f36817279a9042f8e7f2e6ad26cd5721935ce9cf0857d2ba4fffb]]]Published by Tim O&#39;Reilly

[[[00000000000000001176---e892f0b49ac7c195817b2cb6f935741d2a6d67d4e082b402c808644ac63fb9ef]]]Publisher　　　

[[[00000000000000001177---5120b30e23f0646f3a418cbb6d18df0283e91b171be2b9c3a910b3aff3d0337e]]]O&#39;Reilly Japan Co., Ltd.

[[[00000000000000001178---173f76b1e0e52042ce8770cd7df500066657395900d568d00501179717b0fdf9]]]
　　　　　　12-22 Yotsuyazaka-cho, Shinjuku-ku, Tokyo 160-0002

[[[00000000000000001179---e670ad760848546aa0571bd75410cabcb7eaedab59ad888830f53973b3d582cc]]]
　　　　　　e-mail 

[[[00000000000000001180---f6191178ed3d59d9934e432efc352d4e7d09dc7cdbff312412f9a051ce5b5012]]]production cooperation　　

[[[00000000000000001181---525b80b0d33a191efc087976b152945e86bb5fa5894b81a40506dc24e8ae8f94]]]Top Studio Co., Ltd.

[[[00000000000000001182---dba6f0d126a635bb9165e613e13503eec4ebe84a53f470a1bd360d8a7fbea171]]]System names and product names used in this document are trademarks or registered trademarks of their respective companies. Note that the ™, ®, and © marks may be omitted in the text.

[[[00000000000000001183---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O&#39;Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000001184---d1ebe0826fb25fa5332112e85497b0bd95dd0b5df640c994c96a877086cbda65]]]Deep Learning from scratch ❷

[[[00000000000000001185---78d5a535d0319c8239c884884ab977a37c01a4a5ba97983d65273459c77c5e99]]]Natural language processing

[[[00000000000000001186---f18f6f9783a1c8de9a24cf9c9210b59e4d725b724e6c50464d4714e65306977a]]]Written by Yasuki Saito

[[[00000000000000001187---ca67ef85c9beb2732d0b96ad2113e21ef67f973eb3711ad3b94163c5d7b4a709]]]Chapter 7

[[[00000000000000001188---03483b7fe26acb4c031a01f17c1a5d823fcce750c938eb714909ba0d796172f7]]]Sentence generation by RNN

[[[00000000000000001189---086810dbe9d22354bbab0b38f6fdf6c6d70362e1f6d21be57665e6d1969d44c0]]]There is no such thing as a perfect sentence.

[[[00000000000000001190---fb0c9bc47edee183940949a73efb62acac3a0fcf27b8bb6f61849e30ab772454]]]Let perfect despair not exist.

[[[00000000000000001191---3641f79bec30f037ac70b3292e1c00bdb9826f74341c9e296e5eac843d34cde2]]]——Haruki Murakami “Listen to the Wind”

[[[00000000000000001192---d07e98b5038d2c3f8ede1208facc93fa9c033833c439f3945575d7ea3e6a8e5d]]]In chapters 5 and 6, we looked at RNNs and LSTMs in detail, how they work and how they are implemented. We now understand them at the implementation level. In this chapter, our work so far—RNN and LSTM—will blossom. Here we will implement some fun applications using LSTMs.

[[[00000000000000001193---ff8bdbc5c7f6c9519ee33db087326604d2274504652ff37d7b03f8db6e1b79e8]]]In this chapter, first of all, &quot;sentence generation&quot; is performed using the language model. Specifically, new sentences are created using a language model trained using a corpus. Furthermore, we will see that more natural sentences can be generated by using the improved language model. Through the work here, you will be able to realize (even if it is easy) that you can &quot;let AI write sentences&quot;.

[[[00000000000000001194---93936c040d3f2ed9163367627d294d68ae173dbb4576d35e6fa3332d5b627d71]]]In addition, this chapter deals with a neural network with a new structure called seq2seq. seq2seq is a term that means &quot;(from) sequence to sequence&quot; and converts one time series data to another. In this chapter, we will see how seq2seq can be implemented quite easily by combining two RNNs. This seq2seq can be used in various applications such as machine translation, chatbots, automatic email replies, etc. Understanding this simple yet clever and powerful seq2seq will open up even more possibilities for deep learning!

[[[00000000000000001195---ec8fa18bdaa64c2549b7a46f42ef5291521cbf306d9bcd9df7efb6272ee82831]]]Sentence generation using a language model

[[[00000000000000001196---2cc070aaa747fabf6a71d3c71b5f0c71e544fd6f489522aed2066c12d770617b]]]We&#39;ve covered language models for several chapters so far. As mentioned earlier, language models can be used in many different applications. Typical examples include machine translation, speech recognition, and text generation. Here, let the language model do the sentence generation.

[[[00000000000000001197---560d29e71f71b90856a223d5a0dcd67811f97b82b61608babf764c4431af7b2f]]]Procedure of sentence generation by RNN

[[[00000000000000001198---332b95a1b7b0c45d6600c04d08ab9b3d16bc0411e3db13a2bce8b6fe4b20ece4]]]In the previous chapter, we implemented the language model using the LSTM layer. The language model implemented there had a network configuration as shown in Figure 7-1. By the way, we have implemented layers that collectively process time series data (only for T pieces) as Time LSTM and Time Affine layers.

[[[00000000000000001199---c2a7e7e8f3c9b56ddc4f4351f1ce821746a433c4048b92cfe465802f28f1faa8]]]
Figure 7-1 Language model implemented in the previous chapter: The right figure uses the Time layer that collectively processes time-series data, and the left figure is the layer configuration after deployment.


[[[00000000000000001200---ae29601105c4ae4cdcbfafcc6e0fabe7f38b4ce14ac6bb0f0f634c4f24f6cbcd]]]Now, I will explain the procedure for generating sentences in the language model. As an illustrative example, consider a language model trained on the corpus &quot;you say goodbye and I say hello.&quot; Then, consider the case of giving the word &quot;I&quot; to this trained language model. At this time, the language model outputs the probability distribution shown in Figure 7-2.

[[[00000000000000001201---765a6abd490ef20022b2658e1662145ccef22dacdcc59190f00d183dc94ccb13]]]
Figure 7-2 The language model outputs the probability distribution of the following words


[[[00000000000000001202---84943c08ed37e09663ab42ab9986709cc0280e688f4687fc176601d45d8cba0a]]]The language model outputs the probability distribution of the next word given so far. The example in Figure 7-2 outputs the probability distribution of the word that appears next when the word &quot;I&quot; is given. So how do we generate the next word anew?

[[[00000000000000001203---db40b55035316dfc202948229614f5270033060720002dd8f02dafc65b2ae8ce]]]One possible way is to choose the word with the highest probability. In that case, we just pick the word with the highest probability, so the result is unique. So this is the &quot;deterministic&quot; method. Another option is to select “probabilistically”. By choosing according to the probability distribution, high-probability words are more likely to be selected, and low-probability words are less likely to be selected. At this time, the selected words (words to be sampled) are different each time.

[[[00000000000000001204---ad39b7eed1dc5244dbd97f9adea99698304744cb0d040252b2275061719e88e8]]]Now we want the generated sentences to vary each time. It would be interesting to see variations in the generated sentences. Therefore, we will choose words by the latter method—the method of choosing probabilistically. Going back to our example, suppose the word &quot;say&quot; is (probabilistically) chosen at this point, as shown in Figure 7-3.

[[[00000000000000001205---8b89a267120289602c480af393fb355858b737f2eabb65c46ec186812320a521]]]
Figure 7-3 Sampling one word from probability distribution


[[[00000000000000001206---5b849f4ce3192a11bb8dc0b5bcd60225c6bd6ed0c48874cae9189a36a13581ec]]]Figure 7-3 shows an example of sampling from a probability distribution and the result is &quot;say&quot;. Because &quot;say&quot; has the highest probability in the probability distribution in Figure 7-3, so it has the highest probability of being sampled. However, it should be noted that the choice of &quot;say&quot; here is not inevitable (not &quot;deterministic&quot;), but &quot;probabilistic&quot;. Therefore, words other than &quot;say&quot; will also be sampled according to the probability of occurrence of each word.

[[[00000000000000001207---8aa364e2ff74218df2f7d2c7aac825c9ead774ede3d876a4ec4775fb48c4e46d]]]Deterministic means that the result (of an algorithm) is unique or predictable. For example, choosing the most probable word in the example above would be a &quot;deterministic&quot; algorithm. A &quot;probabilistic&quot; algorithm, on the other hand, determines its outcome stochastically. Therefore, the word chosen changes (or can change) from trial to trial.

[[[00000000000000001208---9930f469c9f56c618dfb2592512a3f3942cf65837637af8012874fdb7753fe0f]]]Next, we sample the second word. This just repeats the steps you just did. In other words, input the word &quot;say&quot; generated earlier into the language model and obtain the probability distribution of the word. Then, sample the next word that appears from that probability distribution. A graphical representation of this work is shown in Figure 7-4.

[[[00000000000000001209---1459c4b1a8a03966ca5145a11d70758c161058258e71f09cd630e1dcbcd1f1d4]]]
Figure 7-4 Repeating Probability Distribution Output and Sampling


[[[00000000000000001210---0724b3890bda44e013a99f607d9b931e115b8c67307cded2cf15138e86979b35]]]Then repeat this process as many times as you want (or<eos> (Repeat until you encounter a sentence terminator such as ). Then you can generate new sentences.

[[[00000000000000001211---1df5adccb14b3afc71a0c113064f81b49092e588fe158211790d4ed5acc60c62]]]It should be noted here that the newly generated sentences above are newly generated sentences that do not exist in the training data. This is because the language model does not memorize the training data, but learns the pattern of word arrangement used there. If the language model is able to correctly learn the appearance patterns of words using the corpus, the newly generated sentences by the language model will be natural sentences for us humans, and we can expect sentences that make sense. increase.

[[[00000000000000001212---d87e1c165286bd7a0c3ace8191deed0837f8a49909cbd7e464512734811e4923]]]Implementation of text generation

[[[00000000000000001213---cee9cfe14ab8597fe6ac36d3225282ed375587e21344046dfe7cd6568f780f27]]]Now let&#39;s implement the sentence generation. Here, based on the Rnnlm class (☞ ch06/rnnlm.py) implemented in the previous chapter, create the RnnlmGen class by inheriting it. Add a method to generate sentences to this class.

[[[00000000000000001214---12e45613927562f42267497f05a2467c4df38be23bb02718c4dc125e8dede9a0]]]Class inheritance means creating a new class by inheriting an existing class. To do class inheritance in Python, write like class New(Base):. This allows you to inherit the Base class and implement a new New class.

[[[00000000000000001215---c7ab040c4edfc96aa5d979343560e2dc47d7aed40619dcd86dddc5a20abd1e31]]]Here is the implementation of the RnnlmGen class (☞ ch07/rnnlm_gen.py).

[[[00000000000000001216---af1673693563d5bed7a7ed2995a3c2b22eb5b916b739423f2bb88f42be08da8b]]]Generate(start_id, skip_ids, sample_size) is what generates sentences in this class. Here, the argument start_id is the first given word ID, and sample_size is the number of words to sample. Also, the argument skip_ids expects a list of word ids (e.g. [12, 20]) and prevents the word ids specified there from being sampled. This is in the PTB dataset<unk> It is used to avoid sampling preprocessed words such as , and N.

[[[00000000000000001217---17bc35293ec8f5bd5b74a6b5dccf2c471c64df133773f9a42f5e06736f96bd36]]]In the PTB dataset, the original sentences were preprocessed, and the rare words were<unk> , the number is replaced by N. Also, in our case, the sentence breaks are<eos> I am using the string

[[[00000000000000001218---9084eafdf327bfbe34268df9ca4b25a18108b7352d2f738a180f7d62e7bd2c2f]]]In the generate() method, we first output the score of each word by model.predict(x) (the score is the value before normalization). Then normalize that score with the Softmax function by p = softmax(score). We now have the desired probability distribution. Then sample the next word from that probability distribution p. In addition, np.random.choice() is used for sampling from the probability distribution. How to use this function was explained in &quot;4.2.6 Negative Sampling Sampling Method&quot;.

[[[00000000000000001219---6c9846f6ce7e52ace6dcc7b54b0dd07fee1c953e842e0fe5e6e1c1f572651e3c]]]The model&#39;s predict() method does mini-batch processing, so the input x must be a 2D array. Therefore, even if you enter only one word ID, consider the batch size to be 1 and format it into a 1 x 1 NumPy array.

[[[00000000000000001220---75c8ffff8f4c959667293ff11fd627504f4b55dc6db19fa395d2c560ae5c9a5e]]]Let&#39;s use this RnnlmGen class to generate sentences. In this case, we let it generate sentences without learning anything—that is, with random initial values for the weight parameters. The code for text generation is shown below (☞ ch07/generate_text.py).

[[[00000000000000001221---da9d00f2a130be433bba21e2011d76d6294ebec4ab08717f282792bad8d783c0]]]# set start and skip characters

[[[00000000000000001222---cbfd6353b869c959a7c70ce26a16ef83c97d9bab9f53fdec8bfed8802999473a]]]# generate text

[[[00000000000000001223---b39c1c3a14a15c2113d94f0a3322fbaaf52f5c9f1f1f931a7725386052ceda82]]]Here, the first word is you and the word ID is set to start_id to generate sentences. Also, [&#39;N&#39;, &#39;<unk> &#39;, &#39;$&#39;]. The generate() method, which generates sentences, returns word IDs as an array. Therefore, it is necessary to convert the array of word IDs into sentences, and here we will do that with a single sentence, txt = &#39; &#39;.join([id_to_word[i] for i in word_ids]). The join() method joins words by writing something like ``&#39;delimiter&#39;.join(list)&#39;&#39;. A concrete example is as follows.

[[[00000000000000001224---31b512f5f13bdef24050cde94c2c4bc5d856b7cb7a4d5e7eae17ec1bf4a4b2ea]]]Now let&#39;s run the code above. The result looks like this:

[[[00000000000000001225---1dc1131bb60ff12fdb2191c419e8c317422225ac23f48d941cd7ced765f830f4]]]As you can see, the output sentence is a list of random words. Of course, the model weights here are random starting values, so the output doesn&#39;t make sense. What about the trained language model? Here, we will use the weights learned in the previous chapter to generate sentences. To do this, use model.load_params(&#39;../ch06/Rnnlm.pkl&#39;) to load the weight parameters learned in the previous chapter before generating sentences. Take a look at the sentences that are generated (results are different each time).

[[[00000000000000001226---0f6924377827aa972d2272b686f347b17e0cc0ccc704d2232556e00e7b961ae5]]]Looking at the above results, there are some grammatically incorrect sentences and sentences that don&#39;t make sense, but there are some sentences that can be read as such. On closer inspection, it produces the correct subject-verb combinations, such as ``you &#39;ll include ...&#39;&#39;, ``there&#39;s no tires ...&#39;&#39;, and ``knight transplants ...&#39;&#39;. They also seem to have some understanding of how to use adjectives and nouns, such as &quot;good problems&quot; and &quot;japanese letter&quot;. Also, the first sentence, &quot;you &#39;ll include one of them a good problems.&quot; is semantically correct.

[[[00000000000000001227---a07f3660bc77ac00fdacc1642c4497c93fca14a8d8ca0eb9058089e16c681929]]]In this way, the sentences generated in the above trial seem to be correct sentences to some extent. However, there are some unnatural sentences, and it seems that there is still room for improvement. There is no such thing as a “perfect sentence”, but we should look for more natural sentences. What should we do? Use a better language model for that!

[[[00000000000000001228---2b9b204dd5c4de92814e482f45d859ee603e6a9d9c52da1e842b6a21550c3371]]]to better writing

[[[00000000000000001229---0b594772bddfbb7be636f3b3369fa225e065136eff3d1c5caf0d11a7c6aa56a6]]]If you have a good language model, you can expect good sentences. In the previous chapter, we improved a simple RNNLM and implemented a &quot;better RNNLM&quot;. So we improved the model from around 136 perplexity to 75! Now, let&#39;s see the power of sentence generation of &quot;Better RNNLM&quot;.

[[[00000000000000001230---1008ef5dc9c3424ba02c8ba1e2b4ee59e1c3b3158560978228e857e084b4668f]]]In the previous chapter, we trained the BetterRnnlm class and saved the weights after training as a file. The experiment here requires that post-training weight file. The learned weight file can also be obtained from the following URL. By placing the weight file in the ch06 directory of the source code of this book, you can run the experimental code ch07/generate_better_text.py here.

[[[00000000000000001231---0454a58ca2065c1b5cab6d7436e6558ca6592b189213d1fdeb4e37929dfc430a]]]In the previous chapter, we implemented a better language model as the BetterRnnlm class. Here, as we did earlier, we will inherit this class and give it a function to generate sentences. Its implementation is exactly the same as the implementation done in the RnnlmGen class earlier, so the explanation is omitted.

[[[00000000000000001232---92df923cff46c50e1e29457fbc0128147c50b881a8bce8cd6e18ecaab6c3519c]]]So let&#39;s let a better language model do the sentence generation. Try giving the first letter you as before. Then the following text is generated (☞ ch07/generate_better_text.py).

[[[00000000000000001233---68d34220108a8260a371c142c989df3dc2c792f9c565c6791adb588beccf003d]]]Looking at this result, it seems to generate more natural sentences than the previous time (although it is somewhat subjective). The first sentence says &quot;you &#39;ve seen two families and the women ...&quot; and uses the correct subject, verb and object. In addition, I have successfully learned how to use and (&quot;two families and their women&quot;). If you read the rest of the text, you might say it&#39;s a decent result overall.

[[[00000000000000001234---b04aa55c00978f5e5e048d4e30a48978637969ba186f4cb345097c50601bce39]]]Although there are still some problems with the sentences generated here—particularly in terms of the semantics of the sentences—our “better language model” is (to some extent) free to generate natural sentences. It seems to be generated in . I&#39;m sure that by further improving this model and using a larger corpus, it will be able to produce more natural sentences.

[[[00000000000000001235---74ef86ff6960cc7a2bbed5451664723863f4967165f52e448f678a7033b7e006]]]Finally, I would like to give our “better language model” the letter “the meaning of life is”. Then have them generate the following words (this is the experiment done in the paper [35]). To do this experiment, we feed our model with the words [&#39;the&#39;, &#39;meaning&#39;, &#39;of&#39;, life&#39;] in sequence and do forward propagation. The result output at this time is not used, but the information of the word string is retained in the LSTM layer. Then, by starting sentence generation with is as the first character, you can generate sentences following &quot;the meaning of life is&quot;.

[[[00000000000000001236---501535fd18a0de8cb75053ed204749659ebc3b04b12a9dd8a93bd8115efd81e4]]]You can run this experiment in ch07/generate_better_text.py. The sentences generated for each execution are different, but here I would like to introduce only one interesting result.

[[[00000000000000001237---7258b65e7aada3d21445f443b642ab4aae5cb27957bcd444334df4e5fb8bc0ff]]]As above, our language model says, &quot;The meaning of life is not a painting in good condition.&quot; &quot;Painting in good condition&quot;... I don&#39;t know what it means, but it may have some deep meaning.

[[[00000000000000001238---13b972cec48095beb3e266b5104471c56be435b69a06fe02e8574c3bdc61b8d2]]]The world is full of time series data. Language data, audio data, and video data are all time-series data. And there are many problems such as converting those time-series data to other time-series data. Examples include machine translation and speech recognition. Other possible tasks are conversational applications such as chatbots, and tasks such as converting source code into machine language, such as compilers.

[[[00000000000000001239---9b90492656a60f256a6da2a0fdfc8cbd8f4ba340d3a4706dcfa64916a3953c86]]]In this way, there are many problems in the world where the input and output are time-series data. From now on we will consider a model that transforms time series data into another time series data. As a method for that, we will look at a method called seq2seq (sequence to sequence) that uses two RNNs.

[[[00000000000000001240---b4579d1e27bd0d04394a3e0f9fbfa1c1617070fc323319676cbdd0cb4b44941b]]]Principle of seq2seq

[[[00000000000000001241---ca8460103994c4c2035cccda0195fbfef979016a5872caf46b929ab35c24e900]]]seq2seq is also called Encoder-Decoder model. As the name suggests, there are two modules in there—Encoder and Decoder. Literally, the Encoder encodes the input data and the Decoder decodes the encoded data.

[[[00000000000000001242---27a899e2e3e9c70c381c572dc5cf05e7f34e23453cb4a21f05118abe8a2facbe]]]Encoding is to transform information based on certain rules. For example, taking character code as an example, converting the letter &quot;A&quot; to &quot;1000001&quot; (binary) is an example of encoding. On the other hand, decoding (decoding) is to return the encoded information to the original information. In terms of character codes, this is equivalent to converting the bit pattern &quot;1000001&quot; to the character &quot;A&quot;.

[[[00000000000000001243---e14594780fa753d11647a2a69471d34c5a2ba25c203cf87354c3b373beab3c3a]]]Now, I will explain the mechanism of seq2seq using a specific example. Here we consider an example of translating from Japanese to English. As an example, let&#39;s take the case of translating the sentence &quot;I am a cat&quot; into &quot;I am a cat&quot;. At this time, seq2seq becomes as shown in Figure 7-5, and time-series data is converted by Encoder and Decoder.

[[[00000000000000001244---a9a981be372d595db26a8bb0c92defe144b4c9fdf63d7c7cb2b99a4327141b15]]]
Figure 7-5 Example of translation by Encoder and Decoder


[[[00000000000000001245---ebb3cfca86c2fff01bc120b77d74e612f1fd6e62446c04237a77c705773f6965]]]As shown in Figure 7-5, the Encoder first encodes the sentence &quot;I am a cat&quot;. Then, the encoded information is passed to the Decoder, and the Decoder generates the desired sentence. At this time, the information encoded by the Encoder compactly summarizes the information necessary for translation. Decoder generates the target sentence based on the compactly summarized information.

[[[00000000000000001246---694d5b0085102f5a150235e342555f8d3260f7f72312e4eb189b9626245ebd5f]]]This is the whole picture of seq2seq. Encoders and Decoders work together to transform time series data into other time series data. And we can use RNN for these Encoders and Decoders. Let&#39;s take a look at the details. We&#39;ll focus on the Encoder process first. Figure 7-6 shows the layer structure of Encoder.

[[[00000000000000001247---4ddca6ab92515416fff8ccfa55ea9b55f4006e31a0c15143de7c15ba911a8af6]]]
Figure 7-6 Layers that make up Encoder


[[[00000000000000001248---74b91d2a390f302e962adfca567ea6985679cecd52275518b33ead37b101adb0]]]As shown in Figure 7-6, the Encoder uses an RNN to transform the time series data into a hidden state vector called h. Here we use LSTM as RNN, but of course it is also possible to use &quot;simple RNN&quot; or GRU. Also, here we consider the case where Japanese sentences are divided into individual words and input.

[[[00000000000000001249---365d1383732c3aa76ea429c5844cdc0fec33e89a3408c4ca76d2eac39dfc6ebb]]]Now, the vector h output by the Encoder in Figure 7-6 is the final hidden state of the LSTM layer. This final hidden state h encodes the information needed to translate the input sentence. And the important point here is that the LSTM hidden state h is a fixed length vector. Encoding boils down to converting an arbitrary-length sentence into a fixed-length vector (Figure 7-7).

[[[00000000000000001250---613902f3971e46b51f75ddbae580a8a99380547ef67ca609506c6a8bb347e738]]]
Figure 7-7 Encoder encodes sentences into fixed-length vectors


[[[00000000000000001251---eecb3ce32701344eabeecd689c384d4adb1643ef6c1764e7e25a9b67e7e3e8d9]]]The Encoder converts the sentences into fixed-length vectors, as shown in Figure 7-7. So how does the Decoder &quot;cook&quot; that encoded vector to produce the desired sentence? We already know the answer. This is because the model that generates sentences that was dealt with in the previous section can be used as is. This can be represented graphically as shown in Figure 7-8.

[[[00000000000000001252---a75ca54381437ae40cfe7b077f49b367284438f32b04427a399fd1a6d4ce42c6]]]
Figure 7-8 Layers that make up the Decoder


[[[00000000000000001253---df058060ae5b9f8fb1cf014658399cb210f80f1b665c5731e1c3331946c9a772]]]As shown in Figure 7-8, the Decoder has exactly the same configuration as the neural network in the previous section. However, there is one difference from the model we saw in the previous section. The difference is that the LSTM layer receives the vector h. By the way, in the language model in the previous section, the LSTM layer did not receive anything (or rather, the hidden state of the LSTM received a &quot;0 vector&quot;). This single, small difference turns a generic language model into a Decoder that can also translate!

[[[00000000000000001254---b34becf2de73e3e5edd5de048d5e0d0bd0b9ba7a7455f876a3bdfa63ee660071]]]In Figure 7-8,<eos> A delimiter (special character) is used. This is a &quot;delimiter&quot; and is used as a signal to tell the Decoder to start generating sentences. Also, if the Decoder is<eos> This is also the end signal, since we want to sample words until we output . in short,<eos> can be used as a delimiter to indicate &quot;start/end&quot; for the Decoder. In other literature, this delimiter is<go> or<start> , or &quot;_ (underscore)&quot; is used.

[[[00000000000000001255---adc2dafadeead1c5762f54aa5d06261f633aa59fb72ea2dff3a10c97b4f6ed31]]]So let&#39;s connect the Decoder and Encoder together to show the layer structure. Then it should look like Figure 7-9.

[[[00000000000000001256---505ccd8968702d918d4e91280afebfe0664a54aa2a0873524b5bf4c31c6486da]]]
Figure 7-9 Overall layer structure of seq2seq


[[[00000000000000001257---530835a003123c91bd49112c1cafdba9c695a5627aa34e57783a35cfbf3781fd]]]A seq2seq consists of two LSTMs—the Encoder LSTM and the Decoder LSTM—as shown in Figure 7-9. At this time, the hidden state of the LSTM layer becomes a &quot;bridge&quot; between the Encoder and Decoder. Forward propagation propagates the encoded information from the Encoder to the Decoder through the hidden states of the LSTM layer. Then, seq2seq backprop propagates the gradient from the Decoder to the Encoder through that &quot;bridge&quot;.

[[[00000000000000001258---708e8bf114369cb5cc93ed520e64a24d3e87ec6f8ccba91b9ae190d8330bc11f]]]A toy problem for time series data transformation

[[[00000000000000001259---d7e89dfc9178c7ba799bdcb6f4c99548a7a37b6a29f189b8607fca5ff719fbdc]]]Now let&#39;s actually implement seq2seq. Before that, let me explain the problem we will be dealing with. Here we deal with &quot;addition&quot; as a problem of time series transformation. Specifically, as shown in Figure 7-10, we give seq2seq a string like &quot;57+5&quot; and learn to correctly answer &quot;62&quot;. By the way, a simple problem created to evaluate such machine learning is called a &quot;toy problem&quot;.

[[[00000000000000001260---7c77983085755475fc491f478fd6672a1e858de84a943fa01630f6598df94139]]]
Figure 7-10 Train seq2seq with addition examples


[[[00000000000000001261---1180aa6a4c1e28543e068d8df4602e7f7ce7801801664bad8164efe8f57a6e80]]]The addition we deal with here is a simple problem for us humans. But seq2seq knows absolutely nothing about addition—or more precisely, the logic of addition. seq2seq learns the letter patterns used in addition &quot;examples&quot;. Can it really learn the rules of addition correctly? That&#39;s the point of this issue.

[[[00000000000000001262---0cc73ed60e27630a2001148e80b3443bb06c86ef72c4337bfe2f06f45118caa5]]]By the way, we have so far divided sentences into units called &quot;words&quot; in word2vec and language models. However, it is not always necessary to break sentences into words. Regarding this problem, I would like to divide the sentence by &quot;character&quot; instead of &quot;word&quot;. Splitting by character means that an input sentence like &quot;57+5&quot; is treated as a list of [&#39;5&#39;, &#39;7&#39;, &#39;+&#39;, &#39;5&#39;].

[[[00000000000000001263---db23f67d66abef5fe3e73d85a5b9208bf249f66b0a38e29582cc80780f5fe41b]]]Variable length time series data

[[[00000000000000001264---d9014de1df4c5efc28b3c599fcec1b43bd1a69082369148d6fdfb49d3bb93296]]]We will treat &quot;addition&quot; as a list of letters (numbers). It is important to note that the number of characters in the addition problem sentences (&quot;57+5&quot;, &quot;628+521&quot;, etc.) and their answers (&quot;62&quot;, &quot;1149&quot;, etc.) varies from problem to problem. For example, &quot;57+5&quot; is 4 characters in total, but &quot;628+521&quot; is 7 characters.

[[[00000000000000001265---8e3aa16a0dc06b93587ca8d2a4118f3ece34887d2768cfbe3fd2938c896b5643]]]As you can see, in this &quot;addition&quot; problem, the size of the data in the time direction differs for each sample. In other words, addition problems deal with &quot;variable length time series data&quot;. Therefore, some kind of ingenuity is required to perform &quot;mini-batch processing&quot; in neural network learning.

[[[00000000000000001266---8f8834e2720b518539b213fc94d2f6c6e92a63c93671949d1740797600d779ac]]]When training in mini-batches, we process multiple samples together. Then (in our implementation) all samples in the batch should have the same data shape.

[[[00000000000000001267---aa029cb0abc18f2b5eea85157414e78fa1557382bdb1a5fc632d0a34b95fa76a]]]The simplest method for mini-batch training on variable-length time series data is to use padding. Padding is a technique that fills the original data with (meaningless) invalid data to make the length of the data uniform. In our example of addition, as shown in Figure 7-11, the length of all input data is made uniform, and invalid characters—here, &quot;space characters&quot;—are inserted in the remaining spaces. .

[[[00000000000000001268---ae89cc9eaf0af666d9ed0bb1a255bbdc705773f39eb0878104750707a6cdb3c8]]]
Figure 7-11 In order to perform mini-batch learning, pad with &quot;blank characters&quot; to align the data sizes of input and output


[[[00000000000000001269---e6727e6e7673add6a29486faf0a824424e9e412ec940eacfa0898a11c253e03b]]]In this problem, we will deal with addition of two numbers from 0 to 999. So if you include the &#39;+&#39; the maximum number of characters in the input will be 7 characters. Also, the result of addition is a maximum of 4 characters (maximum &quot;999 + 999 = 1998&quot;). Therefore, the teacher data is similarly padded to make the length of all the sample data the same. In addition, for this problem, we will prepend the output with an underscore (_) as a delimiter. Therefore, the number of characters in the output data is aligned with 5 characters. Note that this delimiter is used as a signal to tell the Decoder to generate a string.

[[[00000000000000001270---83fa7819ddbf66646868d57990781a54a4d0eb3b1b614264884bbbe1f4126472]]]For Decoder output, teacher labels can include delimiters that signal the end of character output (for example, &quot;_62_&quot; or &quot;_1149_&quot;). However, for the sake of simplicity, we will not provide a delimiter to mark the end of the string generation. In other words, when the Decoder generates a string, it always outputs a fixed number of characters (here, 5 characters including &quot;_&quot;).

[[[00000000000000001271---3097e1daa004b534c580a6680b708c266183ec458876473d325169d1433ad8b5]]]In this way, by aligning the data size using padding, it is possible to handle variable length time series data. However, using padding causes seq2seq to process padding characters that did not originally exist. So, to be precise, we need to add padding-specific processing to seq2seq (when padding is used). For example, when padding is input in the Decoder, it should not be counted in the loss result (this can be addressed by adding a function called &quot;mask&quot; to the Softmax with Loss layer). Also, when the padding is input in the encoder, the LSTM layer will output the input of the previous time as it is. This allows the LSTM layer to encode the input data as if the padding were never present.

[[[00000000000000001272---b86f630ca82a1e98fcb61baa50388e3a434370a33a60159b522e281c5a765a4e]]]The story around here is a little complicated, so it doesn&#39;t matter if you don&#39;t understand it. In this chapter, for the sake of clarity, padding characters (whitespace characters) are treated as normal data without special processing.

[[[00000000000000001273---d01e89499fd19e59770a897e73a04ca05cca95df1b88fcc202a8703ea4f412ed]]]addition dataset

[[[00000000000000001274---24b827dadf2775cfddeb74beef003b3411de34f9e3580ee279b1819720d8107f]]]The training data example for addition explained here is already stored in dataset/addition.txt. This text file has 50,000 addition examples, as shown in Figure 7-12. This training data was created with reference to Keras&#39; seq2seq implementation example [40].

[[[00000000000000001275---6c142d5c0fce474b92c59a683610927fe15fa5c19a52b88f3863d197eda8482b]]]
Figure 7-12 Learning data for &quot;Addition&quot;: Blank characters (spaces) are displayed as gray dots


[[[00000000000000001276---f93b14fa8595908f697508204de4bf011481972f12a9bc4a493d2934cf584fcf]]]This book also provides a dedicated module (dataset/sequence.py) so that the training data (text file) for seq2seq above can be easily handled from Python. This module has two methods: load_data() and get_vocab().

[[[00000000000000001277---b91ad19bb74d2a19ce498407f69845115e703856f62b4a75dbeddcdfec88a9e5]]]load_data(file_name, seed) reads the text file specified by file_name, converts the text to character ids, and returns it split into training and test data. At this time, internally, the random number seed is set to seed, and the data is shuffled and separated into training data and test data. The get_vocab() method also returns a dictionary of character-to-id correspondences (actually two dictionaries: char_to_id and id_to_char). Now let&#39;s see an actual usage example (☞ ch07/show_addition_dataset.py).

[[[00000000000000001278---e460a569d5375000c6506aa4dec5ef951d79d9fcb454dff70758b27f435e7394]]]In this way, you can easily read data for seq2seq by using the sequence module. Here, x_train and t_train store &quot;character IDs&quot;. Also, the correspondence between character IDs and characters can be converted using char_to_id or id_to_char.

[[[00000000000000001279---5cb7dae84fb12d60963b3e8ecee12272b93513c458eb32b1479ba52237d4f184]]]A dataset should be divided into three parts for training, validation, and testing. Train on training data and tune hyperparameters on validation data. And finally, we evaluate the performance of the model on the test data. However, for simplicity&#39;s sake, we&#39;ll split it into just two, one for training and one for testing, and use that to train and evaluate the model.

[[[00000000000000001280---3e1a60c71fa4c86e8e4a5c7142819645246e6b2b6b64f3e129cbb9c61c7836ec]]]seq2seq implementation

[[[00000000000000001281---fe140af6a7184c01519d5f8e05d8e1ee1525201b3f116d619755083af505c237]]]seq2seq is a neural network combining two RNNs. Here, we will first implement the two RNNs as Encoder and Decoder classes, respectively. Then, combine those two classes and proceed with the flow of implementing the Seq2seq class. Let&#39;s start with the Encoder class.

[[[00000000000000001282---ae0ee50e84c1e8ab6adeb2515a889e2c97ad91d3ae1d69ea950b3c621b6ecb9e]]]Encoder class

[[[00000000000000001283---bd568b4b895e02e5dae47d0baf5bc5e29cfe86eec89772db196972f9a4ee76d6]]]The Encoder class takes a string and transforms it into a vector h, as shown in Figure 7-13.

[[[00000000000000001284---5a54dad73bba98d9debfb9915644804da80d1f1751b8946b58410c6e7c80b716]]]
Figure 7-13 Encoder input and output


[[[00000000000000001285---cf698fa781117d425c6fecd9afa471a0282e9f6ad23ae1239a1953bda5682863]]]As explained before, we use RNN to implement Encoder. Here, we will use the LSTM layer and implement it with the layer structure shown in Figure 7-14.

[[[00000000000000001286---81391a577f79f8559217b99dd6a4b29b9a048c76aa683d7dc15478dce5912632]]]
Figure 7-14 Layer structure of Encoder


[[[00000000000000001287---7d2e3925b9a721e5e8f49dd1e0bcb344e9b27cd70f7efe8c243b7621f0528f06]]]The Encoder class consists of Embedding and LSTM layers, as shown in Figure 7-14. In the Embedding layer, we convert characters (character IDs to be precise) into character vectors. Then that character vector is input to the LSTM layer.

[[[00000000000000001288---d4bba6583abf76ba779599c94e0321a3625a7bf9e79490d6475bb6319a2ede9f]]]The LSTM layer outputs hidden states and cells in the right direction (time direction) and only hidden states in the upward direction. Since there are no layers above, the output of the LSTM layer above will be discarded. After processing the last character, the Encoder outputs the hidden state h of the LSTM layer, as shown in Figure 7-14. And this hidden state h will be passed to the Decoder.

[[[00000000000000001289---587ce812714e7e4f101433f20d0d54504c9e5c5f06a182cf93d2cf611b98d9d9]]]In the Encoder, we only pass the hidden state of the LSTM to the Decoder. It is possible to pass LSTM cells to the Decoder, but it is not common to pass LSTM cells to other layers. This is because LSTM cells are designed to be used only by themselves.

[[[00000000000000001290---0d622c2321ccbd552fccd09cde7467a74eef1b44d888caa2f56b2eebb7cc158b]]]By the way, we have implemented layers that collectively process the time direction as Time LSTM layers and Time Embedding layers. Using those Time layers, our Encoder looks like Figure 7-15.

[[[00000000000000001291---705e05af7d22bdae6d14a8f2db6e73161a5e166350496d79398b821aa2592e03]]]
Figure 7-15 Implementing Encoder with Time layer


[[[00000000000000001292---fd51f05f229e8a4e90c737bae395402ccc0841825991da1618c5381e34b6f267]]]So here is the Encoder class: This Encoder class has three methods for initialization, forward propagation, and backward propagation—__init__(), forward(), and backward(). Let&#39;s start with the initialization method (☞ ch07/seq2seq.py).

[[[00000000000000001293---338f370bb923a12faaf381ff3dcf8d798989e3a5fb4e30be3d3d15a8b59e3232]]]The initialization method receives three arguments: vocab_size, wordvec_size, and hidden_size. vocab_size is the vocabulary size, which corresponds to the character type. By the way, this time, the number of vocabulary is 13 characters in total, numbers from 0 to 9, &quot;+&quot;, &quot; &quot; (space character) and &quot;_&quot;. Also, wordvec_size corresponds to the dimensionality of the character vector, and hidden_size corresponds to the dimensionality of the hidden state vector of the LSTM layer.

[[[00000000000000001294---57135d8376f093c990267e87636bf5f557faeb3efd0acb7ab1756bb9c2e9c7b0]]]This initialization method initializes the weight parameters and creates the layers. Finally, aggregate the weight parameters and gradients into the member variables params and grads lists respectively. Also this time, the Time LSTM layer does not maintain state, so we set stateful=False.

[[[00000000000000001295---def28da7b0a3a57e3b95abf57a8e72576d2fc90c425604d301393927ead8f038]]]The language model in chapters 5 and 6 was treated as a problem in which there is only one &quot;long time series data&quot;. There, we set stateful=True in the argument of the Time LSTM layer, thereby processing &quot;long time series data&quot; while maintaining hidden states. On the other hand, this time it is a problem that there are multiple &quot;short time series data&quot;. So we set the hidden state of the LSTM to the reset state (“0 vector”) for each problem.

[[[00000000000000001296---73dc1242357d8889e056fc47f7d8b011981d296768f0eb593118ea851980a2f9]]]Next, the forward() and backward() methods are shown below (☞ ch07/seq2seq.py).

[[[00000000000000001297---46dab1f839f97aa90d51db462eaff5745722f4bc1ec2eb865d5c1f851d0a8d72]]]Encoder&#39;s forward propagation calls the forward() method of the Time Embedding layer and the Time LSTM layer. Then we take only the last time hidden state of the Time LSTM layer and make it the output of the Encoder&#39;s forward() method.

[[[00000000000000001298---0dc185267237d3bbcf601ba1293ee031791135dff13855d41326fa77967652b8]]]Encoder&#39;s backpropagation carries the gradient to the last hidden state of the LSTM layer as dh. This dh is the gradient transmitted from the Decoder. The implementation of backpropagation generates a tensor dhs with 0 elements and sets dh to the appropriate point in dhs. All that&#39;s left now is to call the backward() method of the Time Embedding layer and the Time LSTM layer. The above is the implementation of the Encoder class.

[[[00000000000000001299---3f58597cd3330649e72b9dc6c58defdc523431e177672d3e553aea3be0263ef3]]]Decoder class

[[[00000000000000001300---85bc420ee4c451f85fabc17cec18768d7b329393770ff3b088663be0073f3ea0]]]Then move on to the implementation of the Decoder class. The Decoder class takes the h output by the Encoder class and outputs another desired string, as shown in Figure 7-16.

[[[00000000000000001301---b530510c3d75a6cc7e6fbb94f0aa10852c8881eac26ea2b3cfc243d2ce652f44]]]
Figure 7-16 Encoder and Decoder


[[[00000000000000001302---2197d6b5607472d6f288d71dd8a2c98f7ab09b020e5de81a7055ee4c6053c159]]]As explained in the previous section, Decoder can be realized by RNN. Again, like the Encoder, we use the LSTM layer. At this time, the Decoder layer structure is as shown in Figure 7-17.

[[[00000000000000001303---f7a9b32b68d43f1bf24b45973ee4a34039eb969d6df0295a578949489c16c932]]]
Figure 7-17 Decoder layer configuration (during learning)


[[[00000000000000001304---88c4868ce1ba733ee509d915a86a37d2242e7a0cb0641e74165ce20f0fe809ba]]]Figure 7-17 shows the layer structure during Decoder training. Here, we use the training data _62. At this time, the input data is given as [&#39;_&#39;, &#39;6&#39;, &#39;2&#39;, &#39; &#39;], and the corresponding output is [&#39;6&#39;, &#39;2&#39;, &#39; &#39;, &#39; &#39;].

[[[00000000000000001305---7d9fc71b3b6f1826fa8d60057c1619564813b4e831c05438582af91311e0351e]]]When generating sentences with RNN, the way data is given differs between when learning and when generating. Since the correct answer is known at the time of learning, it is possible to collectively provide data in the time series direction. On the other hand, when inferring--when generating a new string--you give it just one delimiter (&quot;_&quot; in this example) at the beginning. Then, it repeats a series of processes such as sampling one character from the output at that time and using that sampled character as the next input.

[[[00000000000000001306---0be857925b0d292e4eadb0f4f8d02c6a3357d410bf6f2d3c2f8012e0a9a0830a]]]By the way, when generating sentences in Section 7.1, we sampled based on the probability distribution of the Softmax function. Therefore, the generated sentences fluctuated stochastically. The problem this time is &quot;addition&quot;, and I would like to eliminate such probabilistic &quot;fluctuations&quot; and generate an answer &quot;deterministically&quot;. So this time, I&#39;ll try to pick one letter with the highest score. In other words, choose “deterministically” rather than “probabilistically”. Figure 7-18 shows the flow of generating strings in the Decoder.

[[[00000000000000001307---14ff079d448a7ae6beb6d6a1edaf1af8d6dbc4d759f6100b8f76ee3288ba5a7f]]]
Figure 7-18 Decoder character string generation procedure: Select the index (character ID) that takes the maximum value from the output of the Affine layer with the argmax node


[[[00000000000000001308---5a74c25d786b51ab6c1ccb4954a1dbeccde65fa6f90c0a0ca8b999533806df4a]]]There is now a new node called &#39;argmax&#39;, as shown in Figure 7-18. This is the node that picks the index that takes the largest value (letter ID in this example). The configuration in Figure 7-18 is the same as the configuration for sentence generation shown in the previous section. However, this time, instead of using the Softmax layer, we will target the score output by the Affine layer and select the character ID with the maximum value from among them.

[[[00000000000000001309---5014152cb0bfedbeddf96f873dd277f6a41969cd4a8c101723d845b9410fa499]]]A softmax layer normalizes the input vectors. At this time, the value of each element of the vector is transformed, but the magnitude relationship is not changed. Therefore, the Softmax layer can be omitted in the case of Figure 7-18.

[[[00000000000000001310---b06ad031297cf7f3e6e91e34696b74faa908974b6067776657bdeaaec4adb007]]]As explained here, the Decoder treats softmax layers differently during training and during generation. So let&#39;s leave the Softmax with Loss layer to be taken care of by the Seq2seq class that we&#39;ll implement later. Therefore, the Decoder class will be responsible for up to the Time Softmax with Loss layer, as shown in Figure 7-19.

[[[00000000000000001311---b890a380ddd33b8f799c6bd6c31732d2966118d5e07fabd8a9bce5a1d777be09]]]
Figure 7-19 Decoder class configuration


[[[00000000000000001312---bc44da1f4095e3c8417a0e0b972364a8ada003cd9c6dfa50af26b3ab6676d96b]]]The Decoder class consists of three layers: Time Embedding, Time LSTM, and Time Affine, as shown in Figure 7-19. Now for the implementation of the Decoder class. Here, __init__() for initialization, forward() for forward propagation, and backward() for backward propagation are shown together (☞ ch07/seq2seq.py).

[[[00000000000000001313---42f8317738c175c8a9f990f262b46fa9ff4a285e5664e6359ea8142da79f855a]]]Here we will briefly add only about backpropagation. The implementation of backward() takes the gradient dscore from the Softmax with Loss layer above and propagates the gradient to the Time Affine layer, Time LSTM layer, Time Embedding layer in that order. Here, the time gradient of the Time LSTM layer is stored in the member variable dh of the TimeLSTM class (see &quot;6.3 LSTM Implementation&quot; for details). Therefore, extract the gradient dh in the time direction and use it as the output of backward() of the Decoder class.

[[[00000000000000001314---6136af4ff40d1c813a69f3c80e0d230d5977ed3c04d8f7eea74c32e2749fd395]]]As mentioned before, the Decoder class behaves differently during training and during sentence generation. The forward() method above is intended to be used during training. Implement the method to generate sentences in the Decoder class as generate().

[[[00000000000000001315---6115583da0fe5e7a2ba491692b1c88be5a819c3bbca5e0800a5f6ead01b0d7a3]]]This generate() method takes three arguments. It&#39;s the hidden state h received from the Encoder, the first given character ID start_id, and the sample_size of the number of characters to generate. Here, we give each character one by one and repeat the task of selecting the character ID with the maximum value from the score output by the Affine layer. The above is the implementation of the Decoder class.

[[[00000000000000001316---864574087a337fc8645195010783694163f582a77dbf9bf59d5404e09bc744e9]]]In our problem, we set the Encoder&#39;s output h to the Decoder&#39;s Time LSTM layer. At this time, by making the Time LSTM layer stateful, the hidden state is not reset and forward propagation is performed while maintaining the h of the Encoder.

[[[00000000000000001317---b0d5ae7d9408f1f22a1835e3ed50e6af725c765f76565952958cf5a769a668e8]]]Seq2seq class

[[[00000000000000001318---62dbae1998e488ef95e661bc3a618583009489c39626812bee44bf57879f04c1]]]Finally, the implementation of the Seq2seq class. However, all we do here is wire up the Encoder and Decoder classes and use the Time Softmax with Loss layer to compute the loss. So here is the Seq2seq class (☞ ch07/seq2seq.py).

[[[00000000000000001319---769b183bf3b67b5a8a8e6b3460206f3b090be18a93fb537bf30e9ea41ed0a5d0]]]The main processing has already been implemented in the Encoder and Decoder classes. So here we are just piecing them together. The above is the Seq2seq class. Now, I would like to use this Seq2seq class to challenge the &quot;addition&quot; problem.

[[[00000000000000001320---a4a8f896a0cf6c9753e1eb51c7224f6201d8bbeece5a1cfb171b438edec5759f]]]seq2seq evaluation

[[[00000000000000001321---96812f535f4640025cd57aa04ebef574bfb9d19c8a36d1ed4100fe3b21cc5c74]]]Training seq2seq follows the same flow as training a basic neural network. What is basic neural network learning?

[[[00000000000000001322---0cfc8c5d845bab1a984393328ae974296f5ef89188b1285f4a1e31eafcd6998a]]]Pick a mini-batch from the training data,

[[[00000000000000001323---7d638205b31c3aa8fefd5eaf762b247f3e37f9b59d9711e8125121b2aeb332b0]]]Compute the gradient from the minibatch,

[[[00000000000000001324---ab1973a5b37aa4ff54f005a1633355591759ca619ac555cf98c86d3f4c06128d]]]Update parameters using gradient

[[[00000000000000001325---02f8344594ee689510fa180aedc89d31a91f9a4735168c881d667e0142088b77]]]That&#39;s the rule of thumb. Here, we will use the Trainer class explained in &quot;1.4.4 Trainer class&quot; to make the above work. Also, I would like to have seq2seq solve the test data (generate character strings) for each epoch and measure the accuracy rate. Let&#39;s start with the training code for seq2seq (☞ ch07/train_seq2seq.py).

[[[00000000000000001326---2f242dd94bedb7a91e51d5f21495f5dcff95fec3111bf70249f04c32b343c2b2]]]# read the dataset

[[[00000000000000001327---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000001328---2ec92a51a6dd5c8433d82d737685f67a3a65a4b162e3b240d2a210194a01feaa]]]# Generate model/optimizer/trainer

[[[00000000000000001329---3d50ca9f9dd3c405a340502cb1936366b0df03dd727ced96a52eaa779f4ba2ed]]]The code shown here is the same code for basic neural network training. However, here we use the accuracy rate—how many questions were answered correctly—as an evaluation metric. Specifically, each epoch is scored on how many questions in the test data were answered correctly.

[[[00000000000000001330---6a9511c34be5176c489586a8ad2d52d6d48b968e9748682153f826304cad59bc]]]In addition, in order to measure the accuracy rate in the above implementation, the method eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse) in common/util.py is used. This method will give the model a question, generate a string, and determine if it matches the answer. Returns 1 if the model&#39;s answer is correct, and 0 otherwise.

[[[00000000000000001331---cd1ff5d29cabe1fab6d448b7a94a4cb2260cba3c5899ddc28c1c956c397955ea]]]There are 6 arguments for the eval_seq2seq(model, question, correct, id_to_char,verbose, is_reverse) method. First, there is the model that represents the model, the question that is the problem statement (array of character IDs), and the correct answer (array of character IDs). And there are a total of 6 arguments: character ID, id_to_char of the dictionary that converts characters, verbose that specifies whether to display the result, and is_reverse that specifies whether the input sentence is reversed. Setting verbose=True prints the results in the terminal. In this experiment, I decided to display only the first 10 items of the test data. Also, the argument is_reverse will become clear later.

[[[00000000000000001332---bca568dca700f2b7c5917dfdc1124eaf1de4422006a98e4c0136f3a9d4602367]]]Now let&#39;s run the code above. At this time, the following results are displayed on the terminal (console)†1.

[[[00000000000000001333---a769b22cda9048eeffe46d5219f6087c1f7612f37ef9abff5fdba0018353d081]]][†1] In the Windows environment, &quot;O&quot; is displayed instead of &quot;&quot; and &quot;X&quot; is displayed instead of &quot;&quot;.

[[[00000000000000001334---6097248869cf8918ddbdf5752c3e4a8a82c2f6cb1ca4e74b03abadfd6a34a053]]]
Figure 7-20 Example Display of Results in Terminal


[[[00000000000000001335---9f07f21bd8b741ae24fbe9f6e5de3d9e78ed193b8d3d3466504e7269aa6f203c]]]The terminal displays results by epoch, as shown in Figure 7-20. &quot;Q 600 + 257&quot; on each line is the problem sentence, and &quot;T 857&quot; below it is the correct answer. And &quot; 864&quot; is the answer given by our model. If our model gives a correct answer, it will display something like &quot;864&quot;.

[[[00000000000000001336---b837360d2011121a83f944a1a5198abdd821f13412018776824a3c318c7c6dd2]]]Now let&#39;s see how the above results change as the learning progresses. An example is shown in Figure 7-21.

[[[00000000000000001337---261077f29caa36133d7fc162077fcc46f42c29a26828ebc814e3cf8441f3286f]]]
Figure 7-21 Changes in results displayed on the terminal


[[[00000000000000001338---00a8e65402c64ac8ec55a5a94f4ffdef38e1b2fb50b78a2ad85bb7ec880042a3]]]Figure 7-21 shows a selection of some of the results that appear as training progresses. As you can see from this result, seq2seq is having trouble finding the answer at first. However, as you learn more and more, you will gradually get closer to the correct answers, and you will be able to answer some questions correctly. Now let&#39;s plot the accuracy rate per epoch. The result should look like Figure 7-22.

[[[00000000000000001339---663d73e087f255db72ee4162532bc4c36abf2b6fcbc5910be23fc175e2f92a04]]]
Figure 7-22 Changes in Accuracy Rate


[[[00000000000000001340---afbc23ea523011ae88a7117f519f7fde403fdc62a10423e76a7c3a45f6a09ad6]]]As shown in Figure 7-22, we can see that the accuracy rate steadily increases with each epoch. In this experiment, we stopped at 25 epochs, but the accuracy rate at that point was about 10%. Looking at the growth of the graph, the accuracy rate will continue to rise as we continue learning. However, I would like to put this learning to an end and improve seq2seq so that it can learn the same problem (addition problem) better.

[[[00000000000000001341---83370866a5034595783145fba840ce13cf96b0f38c3ba1c9a3f17497373f7c73]]]Improvements to seq2seq

[[[00000000000000001342---2c49693b1aac31a72212cdab9a1d4a8265860b164c52eac7a73eb2784dc1698c]]]Here, I would like to improve the “progress” of learning by improving seq2seq in the previous section. There are several promising techniques for doing so. This section presents two improvement proposals and confirms their effectiveness through experiments.

[[[00000000000000001343---388023514a30df210f1416e0c9f73036eef0ccdd0fd16af2301e6cc20a8af364]]]Reverse input data (Reverse)

[[[00000000000000001344---b223c916a3d882ff1c0f4051e97523bd894931e123f23c793a7feac43c7e4f50]]]The first improvement is a very simple trick. This is to reverse the order of the input data, as shown in Figure 7-23.

[[[00000000000000001345---6011645bc2e8cd46ca573be7212c90030b9eb7d4ebdd303c47e9aea45c54ef54]]]
Figure 7-23 Example of inverting input data


[[[00000000000000001346---3423b4f06a93cd9f8fe72788328e17aae963cf30f81039a78b3e2b1268e821b7]]]This trick of inverting the input data was proposed in reference [41]. It has been reported that in many cases, learning progresses faster and the final accuracy improves when using this. Now let&#39;s do an experiment.

[[[00000000000000001347---250d457e16b84a01f03743ba4316da147233861c9a4f7ed69ffe9ca23c8205fa]]]To invert the input data, add the following code to the training code (ch07/train_seq2seq.py) in the previous section after reading the dataset.

[[[00000000000000001348---2f242dd94bedb7a91e51d5f21495f5dcff95fec3111bf70249f04c32b343c2b2]]]# read the dataset

[[[00000000000000001349---c40d142259f712e959cf277090fc4e01924f0f114e1f8be6ea18c81e9f32b01f]]]As shown here, you can use the notation x_train[:, ::-1] to reverse the order of the array. Let&#39;s see how much the accuracy rate increases by inverting the input data. The result should look like Figure 7-24.

[[[00000000000000001350---53666232afc832f3e130979c390161bc8d2d41ae570600a3d5e4f15ad3330118]]]
Figure 7-24 Changes in the accuracy rate of seq2seq: baseline is the result of the previous section, reverse is the result when the input data is reversed


[[[00000000000000001351---2f77430654df455a9dbb349cb75a9879b92a615f733c1cef72760360db04f343]]]As you can see from Figure 7-24, the learning progress improved just by reversing the input data! The accuracy rate is about 50% at 25 epochs. Again, the only difference from the last time (baseline in the graph) is that the input data is inverted. It&#39;s surprising that that alone makes such a difference. Of course, the effect of flipping the data depends on the problem, but it often leads to good results.

[[[00000000000000001352---99fe02dfdb6e61d0d6645cd3ca11ad1bccad91696293c16a3c833dc119905064]]]So why does simply inverting the input data speed up learning and improve accuracy? Intuitively (I don&#39;t know the theory), I think it&#39;s because the gradient propagation is smoother. For example, when considering the problem of translating ``I am a cat&#39;&#39; into ``I am a cat&#39;&#39;, the word ``I&#39;&#39; is related to the word ``I&#39;&#39;. At this time, the path from &quot;I&quot; to &quot;I&quot; must go through LSTM layers for four words, &quot;ha&quot;, &quot;cat&quot;, &quot;de&quot; and &quot;aru&quot;. Therefore, when considering backpropagation, the gradient transmitted from &quot;I&quot; will be affected by the amount of distance before reaching &quot;I&quot;.

[[[00000000000000001353---e7d1637b58f53f73b9122ccb52a52cf43beda09d006f789da2f781530e7a1000]]]So, what happens if the input sentence is reversed, that is, if we change it to &quot;aru de neko wa wa wa gai&quot;? At this time, &quot;I&quot; and &quot;I&quot; are next to each other, so the gradient is directly transmitted. In this way, at the beginning of the input sentence, the distance to the corresponding converted word becomes closer by inverting it (because there are many cases where this happens), so the gradient is easily transmitted and learning is efficient. It is possible. However, reversing the input data does not change the &quot;average&quot; inter-word distance.

[[[00000000000000001354---f731ce8c819df5e91d079709ec849b94ede818ad49a006e327e54845cd3f4909]]]Peeky

[[[00000000000000001355---6e5d3ba42503c7fdf8e7c483bfaa61e0a1739dbf88514e6e311719f3f463ecc2]]]We then proceed to the second improvement of seq2seq. Before we get to the point, let&#39;s take another look at how seq2seq&#39;s Encoder works. As explained before, the Encoder transforms the input sentence (problem sentence) into a fixed-length vector h. At this time, that h contains all the information necessary for the Decoder. In other words, it becomes the only source of information for the Decoder. However, the current seq2seq uses the vector h only for the first time LSTM layer, as shown in Figure 7-25. Is it possible to make more use of this important information, h?

[[[00000000000000001356---c931aed82c80d19aed7e5e56f497f7c101cee7610e14aeabc7f4937bd1c2bdaf]]]
Figure 7-25 Before improvement: Encoder output h is received only by the first LSTM layer


[[[00000000000000001357---769e262db0a470c3f421e5c7ef0b65bb41c25b21bdb3a52260d1fd80e48575d4]]]Here is the second improvement plan for seq2seq. It feeds the important information-packed output of the Encoder, h, to other layers of the Decoder. For our Decoder, we might have a network configuration like Figure 7-26.

[[[00000000000000001358---73f6213d5cfc7c2b366ecb99eb1b8c3c71439269da7be82c913a0c9dcbd84824]]]
Figure 7-26 After improvement: Encoder output h is given to LSTM layer and Affine layer at all times


[[[00000000000000001359---68f67d030190790fbb96340a03fe8e68e1bee6767067b13e92378650c9d996e4]]]We give the Encoder output h to the Affine and LSTM layers at all times, as shown in Figure 7-26. Comparing Figures 7-26 and 7-25, we can see that the important information h, which was previously exclusive to only one LSTM layer, can now be shared by multiple layers (eight layers in this example). increase. Share important information with many people instead of keeping it private—you can expect better decisions this way.

[[[00000000000000001360---14ca3d53bacff6ebe87c2b214595c3b52407cf1b17162b1aa7db6df036eae514]]]The improvement here is to feed the encoded information to another layer of the Decoder as well. This can be interpreted as letting another layer &quot;peek&quot; into the encoding information. &quot;Peeping&quot; means peek in English, so we call this improved decoder &quot;Peeky Decoder&quot;. Similarly, seq2seq using Peeky Decoder is called &quot;Peeky seq2seq&quot;. Note that (the basis for) this idea is based on [42].

[[[00000000000000001361---ad2b23d8ebc7176c9b6579deda7912743f78fbada098ee74f87ec20c885b3532]]]Now, in Figure 7-26, two vectors are input to the LSTM layer and the Affine layer. This actually means two vectors concatenated. Therefore, the previous figure is a computational graph that is more accurate to write as in Figure 7-27, using a concat node that joins two vectors.

[[[00000000000000001362---ccbe7b5f8b9bf05e89318f95750954dda8b15b9c867931188108406270e4bce3]]]
Figure 7-27 If there are two inputs to the Affine layer (left figure), the input to the Affine layer is the vector that concatenates the two (right figure).


[[[00000000000000001363---f9afa86be7a974a0a438b88b18be7e8b7d5048e4b9fdc70a2fda7f67061c9667]]]So, here is the implementation of the PeekyDecoder class. Only __init__() for initialization and forward() for forward propagation are shown here. Backpropagation backward() and sentence generation generate() are not particularly difficult, so they are omitted (☞ ch07/peeky_seq2seq.py).

[[[00000000000000001364---45378a51bbd967bd33a91e65db8340df42050f1b6d01a30f5dace2b631e8e64a]]]Initialization of PeekyDecoder is almost the same as Decoder in the previous section. The only difference is the shape of the LSTM layer weights and the Affine layer weights. In this implementation, the vector encoded by the Encoder is also input, so the shape of the weight parameter is correspondingly larger.

[[[00000000000000001365---50c16d6a49df44a3dea898b4db3d8cd9782e4c73550046cc96b695e39272dc9a]]]Next is the implementation of forward(). Here, first, h is replicated by np.repeat() for the time series, and it is hs. After that, concatenate the hs with the output of the Embedding layer with np.concatenate() and use this as the input of the LSTM layer. Similarly, in the Affine layer, the input is the concatenation of the outputs of the hs and LSTM layers.

[[[00000000000000001366---fc0c3fef25bef6386299235a9a33ffe88872eda1de9c5046bce630a49b9f3b48]]]For Encoder, there is no change from the previous section. Therefore, we will use the Encoder class from the previous section as it is.

[[[00000000000000001367---54a76fcf327846a3b8097b1604f8d3bfb77c5fd7f1f5c91bd2a0f9d0cc1479d4]]]Finally, implement PeekySeq2seq. However, this is almost the same as the Seq2seq class in the previous section. The only difference is the Decoder layer. While the Seq2seq class in the previous section used the Decoder class, this time we will use the PeekyDecoder. The logic behind is exactly the same. Therefore, the implementation of the PeekySeq2seq class inherits the Seq2seq class from the previous chapter and changes only the initialization part (☞ ch07/peeky_seq2seq.py).

[[[00000000000000001368---71e82e0d140782457349e7114558f1a2d702d692fc711ac64c848b24162de623]]]You are now ready to go. I would like to try the addition problem again using this PeekySeq2seq class. The training code is the code shown in the previous section, just change the Seq2seq class to the PeekySeq2seq class.

[[[00000000000000001369---60379e5cf4c46c7371a87a004f3ec7805f8c5e952fbcc0fc778fb8a9a1dcf940]]]Also, here, we will conduct the experiment after performing the first improvement &quot;Reverse (inverting the input)&quot;. The result should look like Figure 7-28.

[[[00000000000000001370---b537a42129c359a9297719a078985a6801aa602a77b6046dc2d7ed42246d1e10]]]
Figure 7-28 Result of &quot;reverse + peeky&quot; with two improvements in this section


[[[00000000000000001371---edeb3661be67bd58d6e1251cf373539deae22393dc7bebb5f9bf84a21139ac10]]]As shown in Figure 7-28, seq2seq with Peeky performed much better. After 10 epochs, the accuracy rate already exceeds 90%, and eventually the accuracy rate approaches 100%!

[[[00000000000000001372---b91937caea5ccfb013f980c9946e57625177ee6ac8e7b40639085b476e63ed16]]]From the above experimental results, we can see that both Reverse and Peeky are working effectively. With Reverse, which reverses the input sentence, and Peeky, which spreads the Encoder&#39;s information around, these two techniques have given us satisfying results!

[[[00000000000000001373---f6244ff7aa7e3e4ed1a329e3dbdffac4874d1d9c93a0ffd438e308b349fa2b5e]]]However, the experiment here requires caution. Because by using Peeky, our network has extra weight parameters and increased computational complexity. Therefore, the experimental results here must consider the “handicap” of the increased parameters. Also, the accuracy of seq2seq varies greatly with hyperparameter tuning. The results here are encouraging, but in real problems the effect will be different.

[[[00000000000000001374---ffe748ce89911f304595529e1c67823b6a20987012613722c6425db516dfb199]]]This concludes the improvement of seq2seq for the time being. However, the story continues. In fact, the improvements made here can be called “small improvements”. In the next chapter, we will make a “big improvement” to seq2seq. It&#39;s a technique called Attention that can dramatically improve seq2seq!

[[[00000000000000001375---ce559f71db263115f90fc19ce128e149cb008e4d1e74b8c772e93815a5549649]]]Applications using seq2seq

[[[00000000000000001376---3ac163dde86688fa7e6b199689ce18ceedc5a1c2c252445f354b3b9cfbc18494]]]seq2seq transforms &quot;one time series data&quot; into &quot;another time series data&quot;. This paradigm of transforming time series data can be applied to a variety of problems. Specific examples include tasks such as:

[[[00000000000000001377---4bd62020a170572e964de21bcf0559c6091574dcfe5bbc0d1b4328cfa53d5faa]]]Machine translation: converts &quot;sentences in one language&quot; to &quot;sentences in another language&quot;

[[[00000000000000001378---50c89fd54b6a407fc7e8515ac7fd10616e82505fb502c1a351158c5cc59fa17c]]]Auto Summarize: Convert &quot;a long sentence&quot; to &quot;short summarized sentence&quot;

[[[00000000000000001379---d57d0d76d9aeb330b2b62753b378a41f3bb756cfd05980eae1747aeb596da449]]]Q&amp;A: Convert &quot;Question&quot; to &quot;Answer&quot;

[[[00000000000000001380---737a6221d2f9304942d6154978e84eaab60402c0853d7e98d25d795b9c839687]]]Auto-reply email: Convert &quot;Text of received email&quot; to &quot;Reply text&quot;

[[[00000000000000001381---00f61084e96849dc00fefbb97cddbefa7fd76d66f4f0f787ef9e0aa148091e42]]]Thus, seq2seq can be used for problems involving two paired time series data. In addition to natural language, it can also be used with audio and video images. Furthermore, even problems that seem unlikely to be applied to seq2seq may be applicable to seq2seq by preprocessing the input/output data. Here are some applications using seq2seq. I hope that you will feel the possibilities and fun of seq2seq.

[[[00000000000000001382---b9f7c53dd12321d0272c1ebac3b251297646d78108b76638a85d69dd67eba2a7]]]chatbot

[[[00000000000000001383---1011c7392fa625ceac51bde5bf5e2ef52c7530ef8ec07d7a43bab3da8f163dc8]]]A chatbot is a program that allows a person to have a text conversation with a computer. Chatbots are already being used in various services such as Facebook and Twitter.

[[[00000000000000001384---cf355dade243935456d41b75ad2963eb46307c7a7498b61ba412076e86eb4356]]]Of course, you can use seq2seq for this chatbot as well. This is because dialogue consists of &quot;the other party&#39;s statement&quot; and &quot;your own statement,&quot; so it can be understood as a problem of converting &quot;the other party&#39;s statement&quot; into &quot;your own statement.&quot; In other words, if you have dialogue text data, you can train seq2seq on it.

[[[00000000000000001385---c83ae4dceb8a6776e75b35fe6d31ca66a7f3b28704378aacb0e0a407376a5bcb]]]Furthermore, chatbots can be used in practical scenes. Reference [43] conducts experiments with seq2seq-based chatbots for IT help desks. As one of the results, an example of solving problems related to VPN connection has been reported. Here, I will post a part of the conversation exchanged there translated into Japanese.

[[[00000000000000001386---cb9b3b0b32bcf86846c7117574b077eec1fafd572e089aa9cbdbb8203981f035]]]
Figure 7-29 Example of a conversation using a seq2seq-based chatbot (excerpt from Reference [43], translated from English into Japanese)


[[[00000000000000001387---fe3da13d1d7ef4fe7d8fdecb24aea08527075fd6bd8db74c0942fe644551f82d]]]Looking at the conversation in Figure 7-29, we can see that the machine (chatbot) solves the problem beautifully. It led people who had trouble connecting to VPN to a URL link that could solve it. Of course, this is only intended for IT helpdesk issues, so it probably won&#39;t be of general use. However, being able to get answers and hints on a conversation basis is highly practical and seems to have various applications. In fact, (a simplified version of) such a service can already be found on some websites.

[[[00000000000000001388---fcf4d67e42ddbc651622ba55f31a0fc63ed6c80025ba690de6b1ef100c1d56e5]]]Algorithm learning

[[[00000000000000001389---f9dc1a76079b5e9ee3ddbf5d766679d82fe603e6059e752c6b175916300a1502]]]The experiment conducted in this chapter was a simple problem like &quot;addition&quot;. In principle, however, more advanced problems can also be addressed. For example, it is possible to handle code written in Python as shown in Figure 7-30.

[[[00000000000000001390---4b5468f409d4067e5a1b249c5b4c5dfd57e7bb72a8ef5b063da98927f03e3bbb]]]
Figure 7-30 Code example written in Python: Input in the figure is input, Target is output (excerpt from reference [44])


[[[00000000000000001391---caa8215c2a41b64f2712909ad4e97c740bdc486ff32a5e89d0997049452bafe7]]]The source code is also time series data written in characters. Even if the code spans many lines, it can be treated as a single sentence (line breaks can be treated as line breaks). Therefore, the source code can be input to seq2seq as it is, and the target answer can be paired and learned.

[[[00000000000000001392---f60f18afaf1254e1f9b4d850f5d96f6d53e1d44e7f4331083ac4bc357dfa61ce]]]Problems that include for statements and if statements like those shown above won&#39;t be easy to solve. However, even such problems can be handled within the framework of seq2seq. And by devising the structure of seq2seq, we can expect to be able to solve such problems.

[[[00000000000000001393---4f05710df223e4a688923d19c4d13de272173640bd0aaf09a71a2eea7634cc9a]]]In the next chapter, I will introduce a model called NTM (Neural Turing Machine), which is an extension of RNN. There, a computer (a Turing machine) learns the procedure for reading and writing memory and reproduces the algorithm.

[[[00000000000000001394---29690d05becc0f4bcf3937a3c99137136421cc033b69c43e0cf861c6272de519]]]image caption

[[[00000000000000001395---2ce000374f59ff88eb3768ab6aeb27cc22288b8a6b641da140affa024e478d1d]]]We have so far only seen examples dealing with text as an application of seq2seq. However, seq2seq can handle various data other than text, such as images and sounds. Here, we introduce Image Captioning, which converts images into sentences [45][46].

[[[00000000000000001396---16479b0f94609111c2f2c58900b3e1ea83587303ca268f13a85df41e641c64d3]]]Image Caption transforms “images” into “text”. And this too can be solved in the framework of seq2seq, as shown in Figure 7-31.

[[[00000000000000001397---ca899cccd07d07b740d3706c9bf526ed3fa2ed82c6443e9edeb571a438802960]]]
Figure 7-31 Network configuration example of seq2seq for image captioning


[[[00000000000000001398---31cdbdf28d555ce83aefde91d2dd03825fd6ee8d7e0f506fea0b8774a85ce5d2]]]Figure 7-31 is a familiar network configuration. In fact, the only difference is that Encoder has been replaced by CNN (Convolutional Neural Network) instead of LSTM. Decoder uses the same network as before. With just that little change -- replacing LSTM with CNN -- seq2seq can handle images as well.

[[[00000000000000001399---942f3fb162632be56b6366db4ef794916726abbdfd23eefdc45ea622fca37185]]]A brief supplement to the CNN in Figure 7-31. Here CNN encodes the image. The final output of the CNN is then the feature map. Since the feature map is a 3D volume (height, width, channels), we need to do some extra work to make it work with the Decoder&#39;s LSTM. So, we flatten (one-dimensional) the CNN feature map and transform it by a fully connected Affine layer. After that, by passing the converted data to the Decoder, you can continue to generate sentences as before.

[[[00000000000000001400---2a24d637a2a37a0ea7bd6a2f77cc12e7977876885dff9d004f4f591510899fdb]]]For the CNN in Figure 7-31, we use proven networks such as VGG and ResNet, and the weights used there are already trained on another image dataset (such as ImageNet). By doing so, you will get a good encoding and be able to generate good sentences.

[[[00000000000000001401---c83d6b72c4aabd0095b4defe42d9923a931cfa0d12e385a723e2cc71927c89e8]]]Let&#39;s look at some examples of image captioning with seq2seq. Here is an example generated with TensorFlow implementation code called im2txt[47]. The network used here is based on Figure 7-31, but with some improvements.

[[[00000000000000001402---23d9f4ded7beef6922e9315abace471b38a07bccdd2cb34b3c4241555cf377e7]]]
Figure 7-32 Image Caption Example: Converting an Image to Text (Image taken from Reference [47])


[[[00000000000000001403---f123319e5a5abf6521eefc8e2f0f068303ff32fa8c737df292f19ebbb8ae4e07]]]If you look at Figure 7-32, you can see &quot;A person on a beach flying a kite&quot; and &quot;A black and white photo of a train on train track&quot;. ”, which is a very good result. Of course, what makes this possible is the large amount of training data of images and descriptions (and large amounts of image data such as ImageNet). Thanks to the existence of seq2seq, which can efficiently learn the training data, we have obtained excellent results as shown in Figure 7-32.

[[[00000000000000001404---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001405---306f8face31d22bf005c39aaedc4fc2d0eef9f149468b19d780a9a92ed815a40]]]In this chapter, we have been talking about the theme of sentence generation by RNN. In fact, we slightly modified the RNN-based language model that we have seen in the previous chapters and added a function to generate sentences. In the second half of this chapter, we dealt with seq2seq and succeeded in learning a simple addition problem. seq2seq is a concatenated model of Encoder and Decoder, which is a simple structure combining two RNNs. But despite its simplicity, seq2seq has enormous potential and can be used in a wide variety of applications.

[[[00000000000000001406---1f2478c59e9186be4b2a6ff499945afaf82ac5eca71e58b7a8bc958fa3fcc393]]]This chapter also introduced two ideas for improving seq2seq—Reverse and Peeky—and implemented and evaluated them. And we confirmed the effect. In the next chapter, we will further improve seq2seq. This is where attention, one of the most important techniques in deep learning, comes into play. In the next chapter we will describe a mechanism called Attention and implement it to implement the even more powerful seq2seq.

[[[00000000000000001407---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000001408---de13d9593f9574b3530f29a8c88ed42efe44a1b0c28ba6775573c62586c59d0e]]]A language model using RNN can generate new sentences

[[[00000000000000001409---96349b52035b2103942a67e0bd9eb8cb968f06ec394374fb5fb5311d19fd5552]]]When generating sentences, one word (or character) is given and the procedure of sampling from the model output (probability distribution) is repeated.

[[[00000000000000001410---e8c1e18fa5e3e5ac120ae046e5cc42fff30c21775a3fe25ffc1d88c7b60dcf8c]]]By combining two RNNs, time series data can be converted to another time series data (seq2seq)

[[[00000000000000001411---ba310f546608660459acab650a60fb5da77e77044b146c40f096f86f7b83101d]]]In seq2seq, the Encoder encodes the input sentence, the Decoder receives the encoded information, and decodes it to obtain the desired output sentence.

[[[00000000000000001412---24f5039a205d4f9c3afcf787cd80f6a90e733995fe077d2989cb71b54d818d33]]]Reversing the input sentence (Reverse) and providing encoded information to multiple layers of the Decoder (Peeky) are effective in improving the accuracy of seq2seq.

[[[00000000000000001413---331ae66351a8cc697128017c351e04f6e0d3d4b7552739d2737ef4385b28433e]]]seq2seq can be used for various applications such as machine translation, chatbots, image captions, etc.

[[[00000000000000001414---8f61b6bb90e9b1bac938f68867a8ad69b1b90e36b77931f7b26dbdabed771767]]]Chapter 3

[[[00000000000000001415---a06f512493a71e542096172c4ac7af90c0f5234c664ad86b16b9b76ed9471ded]]]It is forbidden to speculate without any basis for judgment.

[[[00000000000000001416---d06375123f8c22fc8e3f2e6efd262bfe11b83af9ad6a6c045ac977369d5eda36]]]—— Conan Doyle, The Adventures of Sherlock Holmes (A Scandal in Bohemia)

[[[00000000000000001417---ae4c9f7fe7b07b84edcf9b64c1db4eedac019a7b3f96040fe63b021218a613ea]]]Continuing from the previous chapter, the theme of this chapter is distributed representation of words. In the previous chapter, we obtained distributed representations of words by the “count-based method”. In this chapter, we will look at ``inference-based techniques&#39;&#39; as a powerful alternative to ``count-based techniques&#39;&#39;.

[[[00000000000000001418---3fae6527d48eb030226b8c67c1d0f56b1cb7a34ab46d8d1da847d442601188c4]]]&quot;Inference-based techniques,&quot; as the name suggests, are techniques that make inferences. Of course, a neural network can be used for that reasoning. And here comes the famous word2vec. In this chapter, we&#39;ll spend some time looking at how word2vec works, and solidify our understanding by implementing it.

[[[00000000000000001419---64dd6a07411382ea277c02e088c373d758c17c06cb41aed308b286f944019589]]]The goal of this chapter is to implement a “simple” word2vec. This simple word2vec sacrifices processing efficiency for clarity. So it can&#39;t handle large datasets, but it can handle small datasets just fine. In the next chapter, we will add some improvements to the simple word2vec in this chapter to complete the “real” word2vec. Now let&#39;s move on to the world of word2vec for inference-based methods!

[[[00000000000000001420---4eae6f314823c041457a5f720b6f054b42310ef07b9e7201571a2892007b77f9]]]Inference-based methods and neural networks

[[[00000000000000001421---0a720a37e76a8e296146b4a6b21b0c8102cfb8cb1bb8da6677af7181f3f815f2]]]There has been a great deal of research into how words are represented by vectors. Looking at the successful methods among them, they can be roughly divided into two. One is the &quot;count-based method&quot; and the other is the &quot;inference-based method&quot;. The approaches to acquiring the meaning of words differ greatly between the two, but both have distributional hypotheses behind them.

[[[00000000000000001422---fb40fd7a1b95e2e0c6501a652276d32bbb4d25e2fe4544c689172b7bebdb8bee]]]Here, we point out the shortcomings of the count-based approach and put into perspective the advantages of its alternative, inference-based approach. Then, to prepare for word2vec, we&#39;ll look at an example of processing &quot;words&quot; with a neural network.

[[[00000000000000001423---5108616a542470b5cfd6879104d8ebc8958faa1df7cd9c9fc2610a8167d8f1ce]]]Problems with count-based methods

[[[00000000000000001424---628101866181626e02b6b4335214670a780505c321d4d77c61c045b04dea0353]]]As we have seen, count-based methods represented words by the frequency of surrounding words. Specifically, by creating a word co-occurrence matrix and applying SVD to that matrix, we obtained a dense vector—a distributed representation of the words. However, there are problems with count-based techniques. That problem arises when dealing with large corpora.

[[[00000000000000001425---a3359304c47bcbae2d91c02e8b7a30f3dbb04c3fe932c05aafe169a469f69703]]]In reality, the number of vocabularies handled by the corpus can be very large. For example, it is said that the English vocabulary easily exceeds 1 million. If the number of vocabulary is 1 million, the count-based method will create a huge matrix of 1 million x 1 million. However, doing SVD on such a huge matrix is impractical.

[[[00000000000000001426---bcec53760ab52d441a3ff26c8a427e1919aa762cfb66ddfd64512b69ce6e980c]]]SVD has O(n3) computational cost for an n×n matrix. O(n3) means that the computation time increases in proportion to the cube of the magnitude of n. At such computational costs, even supercomputers cannot compete. In practice, approximation techniques and the use of sparse matrix properties can improve processing speed. However, even in that case, a lot of computational resources and computing time are required.

[[[00000000000000001427---2b21eccc42e476f8339eb035cb7d634da16cf2be48e42de0ce948be526711c90]]]Count-based methods utilize corpus-wide statistical data (such as co-occurrence matrices and PPMI) to obtain distributed representations of words in a single processing (such as SVD). On the other hand, inference-based methods, such as neural networks, typically train in mini-batches. Training in mini-batches means that the neural network looks at a small (mini-batch) of training samples at a time and iteratively updates the weights. Figure 3-1 shows the difference in frameworks in this learning.

[[[00000000000000001428---0d916a9cc1bb4da74d573ff0677b1ade805c6abeb358a330e647f0321546baeb]]]
Figure 3-1 Comparison of count-based and inference-based techniques


[[[00000000000000001429---2ad0bb25717a2c17dd43ae087f6aa0c1b43e2def07a1db7bef96c38bcd97113a]]]As shown in Figure 3-1, count-based techniques process the training data all at once. Inference-based methods, on the other hand, train incrementally using portions of the training data. What this means is that even if the computational complexity of SVD is enormous and difficult to process in a corpus with a large vocabulary, the neural network can divide the data into small pieces and learn. In addition, neural network learning can be parallel-computed by using multiple machines/multiple GPUs, which speeds up overall learning. Inference-based techniques have advantages in this regard.

[[[00000000000000001430---fb61bfbd92189325a364d1ac86cc4cd4b54bb3a6c3cb68e62e765fd3b51d9177]]]Inference-based methods are also more attractive than count-based methods. I would like to discuss this point again in ``3.5.3 Count-based vs. Inference-based&#39;&#39; after explaining inference-based methods in detail (especially word2vec).

[[[00000000000000001431---e2b4153c782f786575a32871342e1b81e73fcd387cb9d1c17a8948c1714a5050]]]Overview of Inference-Based Techniques

[[[00000000000000001432---d44948373bcdedd1d6c62f880d017d3f91f76c8b03f8b70d077a5c100b39b07d]]]In reasoning-based techniques, &quot;inferring&quot; is the main task. As shown in Figure 3-2, this is the task of guessing what words will appear in &quot;?&quot; given the surrounding words (context).

[[[00000000000000001433---e0711b2e52bf2e31c321fe279d68aa48a62da1cd357379d2702a6de749cd13d8]]]
Figure 3-2 Guessing what words appear in &quot;?&quot; using the words on both sides as context


[[[00000000000000001434---925df2409e418d63f1d264887eda054349dcc071753b90d7a44a4c327b1afc04]]]Solving and learning inference problems such as those in Figure 3-2 are problems dealt with by ``inference-based methods&#39;&#39;. By repeatedly solving such inference problems, it learns word appearance patterns. From the &quot;model&#39;s point of view&quot;, this reasoning problem looks like Figure 3-3.

[[[00000000000000001435---3617dd7e39b3254fc1c9712e5308ae4f0df0774dd8d643a8def34da106a953d0]]]
Figure 3-3 Inference-based approach: given the context as input, the model outputs the probability of occurrence of each word


[[[00000000000000001436---ca53e360aefb4da7a8aca0868df3fb88d6f7f54f66465fc43d641299df83e9a7]]]As shown in Figure 3-3, inference-based techniques introduce some kind of model. We use a neural network for that model. The model takes contextual information as input and outputs the probability of occurrence of each word (that could occur). Within such a framework, the corpus is used to train the model so that it can make correct inferences. And as a result of that learning, we can obtain a distributed representation of words, which is the overall picture of the inference-based method.

[[[00000000000000001437---f940700cc7ff7346a00e9f39db22bcef729783b9843449f4a67adf98f07b96d2]]]Inference-based methods, like count-based methods, are based on distributional assumptions. The distribution hypothesis was that &quot;the meaning of a word is formed by the surrounding words&quot;, but the inference-based method reduced this to the guessing problem above. Thus, in both methods, how to model &quot;word co-occurrence&quot; based on the distribution hypothesis is an important research theme.

[[[00000000000000001438---24e93d97d88422f7394de2213b07990ca6c2a02b62fc11e7138100b86ce4baf2]]]How words are processed in neural networks

[[[00000000000000001439---29312c40ac0b8c7152cb6594d8f9240d97085044e57dbab80c861ef5705d12bd]]]We&#39;re going to use a neural network to process &quot;words&quot;. However, neural networks cannot process words such as “you” and “say” as-is. To process a word in a neural network, we need to convert it to a &quot;fixed-length vector&quot;. One way to do that is to convert words to one-hot representations (or one-hot vectors). A one-hot representation is a vector in which only one element of the vector is 1 and the rest are 0.

[[[00000000000000001440---5e30deeedf115aad02b5250320dbd30aa920b998059b22b2d01754bbf21464d4]]]Let&#39;s take a closer look at one-hot expressions. Here, as in the previous chapter, we will proceed with the story assuming that one sentence &quot;You say goodbye and I say hello.&quot; is treated as a corpus. There are a total of seven vocabularies in this corpus (“you”, “say”, “goodbye”, “and”, “i”, “hello”, “.”). Each word can then be transformed into a one-hot representation as shown in Figure 3-4.

[[[00000000000000001441---16c210a5bf4191d26754f5e757e77c965275e81a3715cea15bc54bc88828abca]]]
Figure 3-4 Words, word IDs, and one-hot representations


[[[00000000000000001442---93018941b2d026bff2a65bbc66b2056eb03c3550a153405e390612199dd090e7]]]A word can be represented by text, word ID, and one-hot representation, respectively, as shown in Figure 3-4. At this time, to convert a word to a one-hot expression, prepare a vector with as many elements as the vocabulary, set the corresponding part of the word ID to 1, and set the rest to 0. Having converted the words to fixed-length vectors in this way, the input layer of our neural network can have a &quot;fixed&quot; number of neurons, as shown in Figure 3-5.

[[[00000000000000001443---b68db785eff23d4c1c547e57f492bf4272c1e2c2bfb92a687fff76a38407bbac]]]
Figure 3-5 Input Layer Neurons: Each neuron corresponds to each word. In the figure, the neuron is drawn in black where it is 1 and white where it is 0.


[[[00000000000000001444---41d6d0d600d096f291135a76206b961b645c99646f0bff9f14fb0803e9832363]]]The input layer is represented by seven neurons, as shown in Figure 3-5. In this case, each of the 7 neurons corresponds to 7 words (the first neuron says &quot;you&quot;, the second neuron says &quot;say&quot;, and so on).

[[[00000000000000001445---1ca7ffa9b58626c73230a8fc0ad90eb5b2565509fa885e44ca64357acbd73381]]]Now things are simple. Because if words can be represented as vectors, the vectors can be processed by various &quot;layers&quot; of a neural network. For example, for one word represented by one-hot representation, when converting with a fully connected layer, it can be written as shown in Figure 3-6.

[[[00000000000000001446---123c7cf7792bfd0c9a1309bc3db4f4b1179d6f695fd69a00e2035a2132cc67b1]]]
Figure 3-6 Transformation by a fully connected layer of a neural network: Each neuron in the input layer corresponds to 7 words (for now, 3 neurons are prepared in the intermediate layer)


[[[00000000000000001447---344986f42031dee2113e9e4ae9a5dcabde0667cdc2949328d3a3b6286b56ea42]]]In a fully connected layer, all nodes are connected by arrows, as shown in Figure 3-6. This arrow has a weight (parameter), and the weighted sum with the input layer neuron is the intermediate layer neuron. Note that we omit biases in the fully connected layers used in this chapter (this is in anticipation of the following discussion of word2vec).

[[[00000000000000001448---83b5761a811021ee23ab260b000d8d7c48df83e7103050516630cf44eadae680]]]A fully connected layer without bias is equivalent to computing a &quot;matrix product&quot;. Many deep learning frameworks allow you to choose not to use biases when generating fully connected layers. In this book, a fully connected layer without bias corresponds to a MatMul layer (MatMul layer was implemented in Chapter 1).

[[[00000000000000001449---7e43e6aa4d43ea27df460968c6911426075db98b313aceb3284a6dad8c124e7b]]]In Figure 3-6, we used arrows to illustrate connections between neurons, but from now on, we will use a diagram like Figure 3-7 to clearly show the weights.

[[[00000000000000001450---5ecedc85aabbd119f08fced365dad2b95628a3acc85302cc09a743e36a2468dc]]]
Figure 3-7 Simplified illustration of a transformation with a fully connected layer: here the weights of the fully connected layer are represented by a matrix W of shape 7x3


[[[00000000000000001451---23d656a79fbac1e4379bd62e5f3cc65fe97dc3a7e6efa559b6aa6889fd17bc1b]]]Let&#39;s take a look at what we&#39;ve talked about so far in the codebase. As a quick note, our fully connected layer transformation can be written in Python as follows:

[[[00000000000000001452---0a8fab9fe0eba957e830a351ef5595b0ed9a19c3f9165698d7c2fb70af1142d6]]]# input

[[[00000000000000001453---f907ea31cb09af8627e6425eed4c350e44475316e2398911eeb8fcab2951e643]]]# weight

[[[00000000000000001454---09839975b60a034efd7b06c39123d8de0c1eef2fb6c135fef8fa265ace86ca10]]]# Intermediate node

[[[00000000000000001455---2fa9f7d2e97102751e43cbad39d40994a03a8f349d2fafd0d5577c84f8bd43ee]]]This code example shows an example of representing a word with a word ID of 0 as a one-hot representation and transforming it using a fully connected layer. As a refresher, we were able to calculate fully connected layers by matrix multiplication. And it can be implemented with NumPy&#39;s np.dot() (bias omitted).

[[[00000000000000001456---4add5fa3af3c32dc68a1665aa1281db465ea9df36e3692f02419d86649e419e6]]]where the number of dimensions (ndim) of the input data (c in the code) is 2. This allows for mini-batch processing, storing each data in the first dimension (the 0th dimension).

[[[00000000000000001457---5ddc72575d512efc402d918ae1595d3e7d8b90857f163b9445343c96e669f58c]]]Now, what I want you to pay attention to in the code above is the product of the matrices of c and W. where c is a one-hot representation, so it is a vector with 1s in the elements corresponding to word IDs and 0s otherwise. So what we&#39;re doing with the matrix product of c and W in the above code is equivalent to &quot;picking out&quot; the weight row vector, as shown in Figure 3-8.

[[[00000000000000001458---4daff495ec887f28c5409314bd175742e7215fe97e9c56eb911db78dde59c934]]]
Figure 3-8 Multiplying context c by weight W extracts a row vector at the appropriate location (the magnitude of each element of the weight is shown in black and white shading)


[[[00000000000000001459---dbfd7f00c078a767bc27814f75038313893c17cb73862592128dc52331838e6c]]]Now, it may seem inefficient to do matrix multiplication just to extract the row vector from the weights. Regarding that point, we plan to improve it in &quot;4.1 Improvement of word2vec ①&quot;. Note that what the above code does can also be done by the MatMul layer (implemented in Chapter 1). This will result in code like this:

[[[00000000000000001460---d3e94598c7b5b3b99ff230d8d2e421f171f047e490bd0d5fde4f2facf21772c3]]]Here, we will import and use the MatMul layer in the common directory. After that, set the weight W to the MatMul layer and perform forward propagation using the forward() method.

[[[00000000000000001461---778c78c73c446549bd130448d86a0bd6595ab69ca01288bbd60a3b8c7f03bbc1]]]simple word2vec

[[[00000000000000001462---4c727bd6d81905c7ca1108184cfe25590bb9d11d1270fc89efa43e0f0e56996a]]]In the previous section, we learned about inference-based techniques and looked at the codebase on how words are processed in neural networks. You are now ready to go. Now it&#39;s time to start implementing word2vec.

[[[00000000000000001463---9426572a23757e710672552d39316bbf24a543cc8b92ed713d602f2e6684f9ab]]]What we&#39;re going to do is incorporate a neural network into the &quot;model&quot; shown in Figure 3-3. Here, we use a model called continuous bag-of-words (hereinafter abbreviated as CBOW) proposed by word2vec for the neural network.

[[[00000000000000001464---afe99bb5a8a77b36d2e8381aefb0cbd1634afe6663e34e3878a9d1e18f8bb04a]]]The term word2vec is originally used to refer to programs and tools. However, the term word2vec has become popular, and depending on the context, it is often used to refer to neural network models. Correctly, there are two neural networks used in word2vec: the CBOW model and the skip-gram model. Here, we will focus on the CBOW model. The differences between these two models are explained in detail in &quot;3.5.2 skip-gram model&quot;.

[[[00000000000000001465---216b043ccb8443e84d2b4b188ec2e679be1801f0910339b9d1acbd39c83138fd]]]Inference processing of the CBOW model

[[[00000000000000001466---5badd35be6c977413797e33ca13148a87aece28527973188cf1eeb6610f131c7]]]A CBOW model is a neural network that aims to infer a target from context (the &#39;target&#39; being the central word and the surrounding words being the &#39;context&#39;). By training this CBOW model to make guesses as accurately as possible, we can obtain a distributed representation of words.

[[[00000000000000001467---9f5d5b7bf3e57b74b120d140fbb4971a8fa58c4a32bbf808d23ba5f1798d7555]]]The input to the CBOW model is context. This context is represented by a list of words like [&#39;you&#39;, &#39;goodbye&#39;]. We convert it to a one-hot representation, so that the CBOW model can handle it. Based on that, the network of the CBOW model can be written as shown in Figure 3-9.

[[[00000000000000001468---9a4eb4daf9257afa572f64e07641fcb69e8b371e89b3f05c69b59de63ac115f9]]]
Figure 3-9 Network structure of CBOW model


[[[00000000000000001469---3a9c60c4c552fb9fff0a95b6a6d44b6bdc822bc939e8e30e1dce32abb839ff7f]]]Figure 3-9 is the network of the CBOW model. It has two input layers, goes through an intermediate layer, and reaches an output layer. Here the transformation from the input layer to the hidden layer is done by the same fully connected layer (weights are Win). And the transformation from the hidden layer to the neurons in the output layer is done by another fully connected layer (with weights Wout).

[[[00000000000000001470---a80bc09cec611363720d73279cf71f1b8333d8daac8f0016fe18984df9817cb1]]]Since we are considering two words as context, there are two input layers. If we deal with N words as context, there will be N input layers.

[[[00000000000000001471---cbbd7ead23f2f1beca1b7cdbf51754a7e418ba6eee0edcc62c563cf2b62e274b]]]Now let&#39;s focus on the middle layer in Figure 3-9. At this time, the neurons in the hidden layer are &quot;averaged&quot; values after conversion by all connections of each input layer. In the above example, if full connectivity transforms the first input layer into h1 and the second input layer into h2, then the middle layer neurons become

[[[00000000000000001472---5accba2e55f7875041ed849d9a881b0a6b4a4c2059771c68c2cb44717109b935]]]Finally, let&#39;s talk about the output layer in Figure 3-9. There are 7 neurons in this output layer, but the important thing here is that they correspond to each word. And the neurons in the output layer are &quot;scores&quot; for each word, the higher the value, the higher the probability of occurrence of the corresponding word. Note that the score is the value before being interpreted as a probability, and the &quot;probability&quot; is obtained by applying the Softmax function to this score.

[[[00000000000000001473---c22cf2797694d51504edf14304cb34ce2f88f3ef80cc6e8a36da98d4f071fd72]]]After passing the score through the softmax layer, the neurons are sometimes called the &quot;output layer&quot;. Here, we call the node that outputs the score the &quot;output layer&quot;.

[[[00000000000000001474---d60fb25ca1e3903fae40be59bdd0a1ac1973f026c0d9905e3d3945c1eec50548]]]As shown in Figure 3-9, the transformation from the input layer to the hidden layer is done by a fully connected layer (with weights Win). At this time, the weight Win of the fully connected layer is a matrix of 7x3 shape, but if we reveal the seeds first, this weight is the identity of the distributed representation of the word. This can be represented graphically as shown in Figure 3-10.

[[[00000000000000001475---ed53f0074574ba73d6bc624486dbd96bc3f99da623b3ff53b14be21641a9583e]]]
Figure 3-10 Each row of weights corresponds to the distributed representation of each word


[[[00000000000000001476---8db04c9ee66b54a9faaa6d1b70fbef406bd90d09885059c5cfe54f422f1f129f]]]Consider that each row of weights Win contains a distributed representation of each word, as shown in Figure 3-10. As it continues to learn, it updates the distributed representation of each word so that it can better guess the words that appear from the context. And surprisingly, the resulting vector also nicely encodes the &quot;meaning of the word&quot;! This is the big picture of word2vec.

[[[00000000000000001477---151a6c32900d558df23d7c0dfd41f5a8ec02a904b557d6ac1f930d2e3b69dc90]]]The key point is to reduce the number of neurons in the hidden layer below those in the input layer. The hidden layer then needs to &quot;compact&quot; the information needed to predict the words, resulting in a dense vector representation. At this time, the information in the middle layer is written in &quot;code&quot; that we humans cannot understand. This is equivalent to the work of &quot;encoding&quot;. On the other hand, the task of obtaining the desired result from the information in the middle layer is called &quot;decoding&quot;. This is the task of restoring the encoded information to a representation that we humans can understand.

[[[00000000000000001478---0834d827c16b5559350902735b22128692a42cad47164f6ef857861b64776b6e]]]By the way, we have so far illustrated the CBOW model from a &quot;neuronal perspective&quot;. Here, I would like to illustrate the CBOW model from a &quot;layer perspective&quot;. The network configuration then looks like Figure 3-11.

[[[00000000000000001479---520195dafff7ea637df7f89e421412b07f938b8d5aea6c237613feb7d45bd3bd]]]
Figure 3-11 Network configuration of CBOW model from layer perspective: Weights (Win, Wout) used in MatMul layer are drawn in each layer


[[[00000000000000001480---75678860ac27929028a9595279ca122d5e027f6d86b1444095b9153cd8a4018d]]]As Figure 3-11 shows, the CBOW model starts with two MatMul layers whose two outputs are added together. Then, by multiplying the added value by 0.5, the &quot;average&quot; is obtained, which is the middle layer neuron. Finally, another MatMul layer is applied to the neurons in the middle layer to output the &quot;score&quot;.

[[[00000000000000001481---2a68866aa0907047315c9e4c7ddad74aa7e00a1a1fd059d919bffa5725e197f0]]]Processing fully connected layers without bias is done by forward propagation of MatMul layers. This layer internally computes the matrix product.

[[[00000000000000001482---20513a079ce66fc935d922a7db5c94d8dcf2d60966eb4844f0526f8180d9735a]]]Let&#39;s implement the inference process of the CBOW model in Python, referring to Figure 3-11. This can be implemented as follows (☞ ch03/cbow_predict.py).

[[[00000000000000001483---c5fc527fa8151122869ae71f01cf0f6953b68edf48e287528eb36fc020c4f8ae]]]# sample context data

[[[00000000000000001484---d7f2704dda40d0528bed1e8e16c66e17b842f50172d7ba8554f23c7229aedc67]]]# Initialize weights

[[[00000000000000001485---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000001486---e9336d2e632f29af850119bc7371a0a76580f1166646d86c7094219e5f0a0fbb]]]# forward propagation

[[[00000000000000001487---4039db4c032fd405a3def093338fbf3eb2baad10854b4aa07830928839702cac]]]Here we first initialize the necessary weights (W_in and W_out). Then, as many MatMul layers as the number of contexts (here, two) are generated to process the input layer, and only one MatMul layer is generated for the output layer. Note that the input layer MatMul layer shares the weight W_in.

[[[00000000000000001488---831cd0fb369a29dacc6784505820b71b98fa9a5a7f00655fca8a10fd1a5b408a]]]After that, call the forward() method of the MatMul layers (in_layer0 and in_layer1) on the input layer side, calculate the intermediate data, and obtain the score of each word by the MatMul layer (out_layer) on the output layer side.

[[[00000000000000001489---11d1da0c1a896739cdc1abc1b0ef88bbf466da839a99285c29e32ff12486a4b7]]]The above is the inference process of the CBOW model. The CBOW model, as we have seen here, is a simple network configuration without an activation function. Aside from having multiple input layers and sharing weights, the rest shouldn&#39;t be difficult. Next, let&#39;s look at training the CBOW model.

[[[00000000000000001490---e5be2af99d7d11ebbf6fde674a3c577798f09adc634bea244b9d35ef9d0057e5]]]Training the CBOW model

[[[00000000000000001491---ec526097e0449ef074ca1470faee1e945f7579e3002f7c85b6074c9dcf83469f]]]The CBOW model described so far has output a score for each word in the output layer. By applying the Softmax function to this score, we can obtain the &quot;probability&quot; (Figure 3-12). This probability represents which word appears in the middle given the context (words before and after).

[[[00000000000000001492---df747903f174485ea4c667be403961f673054668dd15bbca7145fcbfbd9025e2]]]
Figure 3-12 Specific example of a CBOW model (node values shown in black and white shading)


[[[00000000000000001493---334b4de790511ab9385067bd4c1031fcb7b8d5467bdf7c12b47d4cd6b5c6e4fa]]]In the example shown in Figure 3-12, the contexts are &quot;you&quot; and &quot;goodbye&quot;, and the correct label—the word the neural network should predict—is &quot;say&quot;. At this time, if there is a network with &quot;good weights&quot;, it can be expected that the neurons corresponding to the correct answer are high in the neurons representing &quot;probability&quot;.

[[[00000000000000001494---75ec019061d711a3fa5aafe501c5d88557fa74b6ff786e9592a822e66c8cf2a0]]]What we do in training a CBOW model is to adjust the weights so that we can make correct predictions. As a result, on the weights Win—both Win and Wout, to be exact—a vector is learned that captures the pattern of word occurrences. Experiments so far have shown that the distributed representations of words obtained by the CBOW model (and the skip-gram model)—especially those obtained using large-scale corpora such as Wikipedia— There are many cases that match our intuition in terms of grammatical and grammatical points.

[[[00000000000000001495---ded270fad72e21a650452dc1c677b8d56421dc4bbd8f9f7918b720191f6ee770]]]The CBOW model only learns word occurrence patterns in the corpus. Therefore, if the corpus is different, the distributed representation of words obtained by learning will also be different. For example, if you use only &quot;sports&quot; articles as a corpus and only &quot;music&quot; articles, the resulting distributed representations of words will be very different.

[[[00000000000000001496---f96f804abd9f6db5d1c3a6c9f0d583d38d677a58bb980058eb7a8d68a51f8b8f]]]Now let&#39;s think about training the neural network above. However, what happens next is simple. The model we are dealing with here is a neural network that does multi-class classification. So to learn it, we just use Softmax and cross-entropy error. Here, the scores are converted to probabilities with Softmax, the cross-entropy error is obtained from the probabilities and teacher labels, and learning is performed as the loss. This can be represented graphically as shown in Figure 3-13.

[[[00000000000000001497---984fc3c38fcf6bd3b1b512dc31bb8c7a51e727fb29d728208ee739b931b14835]]]
Figure 3-13 Network configuration of the CBOW model during training


[[[00000000000000001498---7c4bd980c2c471edc961153fe8ca7ee453c07532a24fa7ebea7795f925f0d292]]]As shown in Figure 3-13, here we simply add a Softmax layer and a Cross Entropy Error layer to the CBOW model for the inference processing shown in the previous section. You can get a loss on this. The above is the flow of calculation to find the loss of the CBOW model. This is the forward propagation of the neural network.

[[[00000000000000001499---0adb2a5be46f2d2fa7893a559f871229be81bd7a6330929fdcc7d44c3644175e]]]In Figure 3-13, we used a Softmax layer and a Cross Entropy Error layer, but we implemented these two layers as a single layer called the Softmax with Loss layer. So the network we are about to implement can be written exactly as shown in Figure 3-14.

[[[00000000000000001500---c80fee1a7a05f9bbfbc79b6aac61f204f5ded9e59b76333db38cfd3692798602]]]
Figure 3-14 Graph the Softmax layer and the Cross Entropy Error layer together in the Softmax with Loss layer


[[[00000000000000001501---907236504baf3d98239fbe8943fc8b2c669b439e8377cc56143bb92ca9c6bf54]]]word2vec weights and distributed representation

[[[00000000000000001502---0e66289febca69af6d88c246b21abb52660e072d8f659a2a89271e696a864a90]]]As we have seen, the network used by word2vec has two weights. They are the weight of the fully connected layer on the input side (Win) and the weight of the fully connected layer on the output side (Wout). And each row of weights Win on the input side corresponds to the distributed representation of each word. Furthermore, the weight Wout on the output side is also considered to contain a vector that encodes the meaning of the word. However, the weights on the output side store the distributed representation of each word in the column direction (vertical arrangement) as shown in Figure 3-15.

[[[00000000000000001503---26e2e290528c2800954590249202cfb187aa8b5a72c83905ce4358a535bb5ff4]]]
Figure 3-15 Distributed Representation of Each Word Can Be Seen in Both Input and Output Weights


[[[00000000000000001504---824dd9c9702e936e00490d4ff548606f67643d8a39c40ea48e9d11baf18e91a9]]]Then, which weight should be used for the distributed representation of words that will be used in the end? You have three options.

[[[00000000000000001505---d77b2575a69751545d74bf335b1b682b7a9b8a4180a7061f9634c07c4555d8c6]]]Use only input weights

[[[00000000000000001506---1b8a77e7788667044950f4d628f8fcb08025d7654622c0550bb89a95dd98f6b5]]]Use only output weights

[[[00000000000000001507---8acffd83f4091d5ab15f07b78f05e2691df192e918b5e9c42ce9213fdf1d66fc]]]use both weights

[[[00000000000000001508---0edf97b3ae0503cdbb3324757933e772cd7a76fb07caf8050ece8ae8aa4ef8eb]]]Plans A and B are to use only one of the weights. As for the final option C, there are several possible strategies for how to combine the two weights. One way is to simply add the two weights together.

[[[00000000000000001509---8de390e817ceac20639f50538055c5a4b170675c7e838a8e2ac2bc77f0f77bd1]]]When it comes to word2vec (especially the skip-gram model), the most popular option is A, &quot;use only the weights on the input side&quot;. In many studies, only the input weights Win are used as the final distributed representation of the words without using the output weights. We follow suit and use Win as a distributed representation of words.

[[[00000000000000001510---2da56581e032280446e1751bf3362663e500e7934273fcfc560ac7fdb591efd0]]]In reference [38], experiments show the effectiveness of Win in word2vec&#39;s skip-gram model. Another method close to word2vec, called GloVe [27], reported good results by adding the two weights together.

[[[00000000000000001511---6c05b8c23fbb43612f4b104a6ab7b7e4e3d7bc2f0bf1dcbbc71b2255af098e42]]]Preparation of training data

[[[00000000000000001512---472647c7ed32357404e74c3271b59cf84beb88e2d62192026565ae065da2c19f]]]In order to learn word2vec from now on, we will first prepare the training data. Here, as a simple example, we will use the same sentence ``You say goodbye and I say hello.&#39;&#39; as the corpus.

[[[00000000000000001513---fe203322d36ffb8a2c77d73ee31f36e62aa207a84212cabe5b18e75006810d50]]]context and target

[[[00000000000000001514---9b2d314f02a22573b1b3d8d887f65e9a5e6c6a0a0e5455c63498a4e8637345eb]]]The input of the neural network used in word2vec is the &quot;context&quot;. The correct answer label then becomes the central word, which we&#39;ll call the &quot;target&quot;, surrounded by the context. In other words, what we need to do is to increase the probability that the &quot;target&quot; will appear when we input the &quot;context&quot; to the neural network (that&#39;s how it learns).

[[[00000000000000001515---8c6f91ebf5f9650ab7591b309f6caf863dab5ffe05322a44902bbd4cc44fd8db]]]Now let&#39;s consider creating a &quot;context&quot; and a &quot;target&quot; from the corpus, as shown in Figure 3-16.

[[[00000000000000001516---217b82edb15a282ff43dca5bb07cc24a8d3753e7bb04470e6cee93a846ed8a2e]]]
Figure 3-16 Example of creating a context and target from a corpus


[[[00000000000000001517---622449ad2851db972022f19a9f9d1e012978fcd6e132da3ad71f6d48717eed78]]]In Figure 3-16, the target word is extracted from the corpus as the &quot;target&quot; and the surrounding words as the &quot;context&quot;. Do that for all the words in the corpus (but not the words at the ends). The result is the contexts and targets on the right side of Figure 3-16. Each row of this contexts becomes the input of the neural network, and each row of target becomes the correct label (word to be predicted). Note that for each sample data, there are multiple contexts (two in this example) and only one target. Therefore, only the context is pluralized as contexts.

[[[00000000000000001518---78c34045173c3980d1521e95dca8b47c1fe0ebd9d9fd73333033fb633626accd]]]We will now implement a function that creates a context and a target from the corpus. Before that, let&#39;s start by reviewing the previous chapter. The first is from converting the corpus text into word IDs. For this, we use the preprocess() function implemented in Chapter 2.

[[[00000000000000001519---d007ee30a158ba17a51569a89c509242bd7df5d2031ffb0657fce6a0d9a9d355]]]Now, create contexts and targets from this array of word IDs, corpus. Specifically, implement a function that, given a corpus, returns contexts and targets, as shown in Figure 3-17.

[[[00000000000000001520---32c9c09a3d88f011804270e3b4a25903bbcd5ddd0b8bdfb82b4151ed442d138f]]]
Figure 3-17 Example of creating contexts and targets from corpus, which is an array of word IDs (for a context with a window size of 1)


[[[00000000000000001521---97e9312bb31a1a9b67c9671c27fcf7cc40ba36b7d83cec7bcbb09fe62b55ee54]]]contexts is a two-dimensional array, as shown in Figure 3-17. At this time, each context data is stored in the 0th dimension of contexts. Specifically, contexts[0] is the 0th context, contexts[1] is the 1st context, and so on. Similarly, for targets, target[0] is the 0th target, target[1] is the 1st target, and so on.

[[[00000000000000001522---2db7d48c8ffc7c9799410d4de695799ff2f24c5b4755cc1934330484c15d785f]]]Now implement the function that creates this context and target. Here it is named create_contexts_target(corpus, window_size) and implemented as follows (☞ common/util.py).

[[[00000000000000001523---bafd07601c79b7fbfc2ad31e9cc499d6c4948d293b400f613ce34612b18ffd73]]]This function takes two arguments. One is an array of word ids (corpus) and the other is the context window size (window_size). And it returns the context and target respectively as NumPy multidimensional arrays. Now let&#39;s try using this function. Continuing from the previous implementation, we get:

[[[00000000000000001524---37540ffc869c59098e7fe31d85d3c77470fb4a04d348b333abf6c18579b1f55c]]]You have now created a context and a target from your corpus. All that&#39;s left now is to feed this into the CBOW model. However, each element of this context and target is still a word id, so let&#39;s convert this to a one-hot representation next.

[[[00000000000000001525---8ca04a6525d97968c84f3a1e0ceecb8b2db1aff655315ed0f897cf93d683bed9]]]Conversion to one-hot representation

[[[00000000000000001526---13beaf103aed1799ccc577384796931c95b2ecd12b88bd43f3e3f0cb6793e028]]]It then transforms the context and target into a one-hot representation. The conversion to be done at this time is shown in Figure 3-18.

[[[00000000000000001527---f6c9fe37e2a38a421a2811d738e9ca0b844a9eada5cb6a1b5884f6c3571735da]]]
Figure 3-18 Example of transforming &quot;context&quot; and &quot;target&quot; into one-hot representation


[[[00000000000000001528---29c3c815abe2a69cd308b141dac663071ae71365ea4d2d0a159a889893088ad2]]]Convert the context and target from word IDs to one-hot representations, as shown in Figure 3-18. For now, let&#39;s focus on the shape of each multidimensional array. For example, in the example above, the shape of contexts when using word IDs is (6, 2), but when converted to one-hot representation, it becomes (6, 2, 7).

[[[00000000000000001529---2ab3edfd1b39fd441112a2d31db265ff34996c1b236b5a2dc95fab78173b3146]]]To convert to one-hot representation, use the convert_one_hot() function provided by this document. I won&#39;t go into detail about the implementation of this function, but the implementation is in common/util.py and is pretty straightforward. This function gives a &quot;word ID list&quot; and &quot;vocabulary count&quot; as arguments. Now, let&#39;s summarize the data preparation process so far. The result looks like this:

[[[00000000000000001530---9c0ebb84bc94714be01ada0cca27a8ec8fc37dfd00e39ff6663c283077e7f312]]]The training data is now ready. Next, we will proceed to the implementation of the CBOW model, which is the main topic.

[[[00000000000000001531---c9557fa260fd872ff836c81a2254be6aa0a429ae9832eedffc7143382551d0a8]]]CBOW model implementation

[[[00000000000000001532---860dc0912c39151e700addeb97e1b35e55e0a8b34ffe50e88e0e158502791aff]]]Now let&#39;s implement the CBOW model. Again, the neural network we implement here looks like Figure 3-19.

[[[00000000000000001533---8d920ebcb6fb71f0d2f76ba3866027f037e88ffa1d3144516a55844b7ee984e7]]]
Figure 3-19 Network configuration of CBOW model


[[[00000000000000001534---ef469e17c23e24d1886b616454900273f150ea71d6165bbdc5221232a1e39556]]]We will implement the neural network in Figure 3-19 under the name SimpleCBOW (we will implement an improved CBOW class in the next chapter). First, let&#39;s show the initializer of the SimpleCBOW class (☞ ch03/simple_cbow.py).

[[[00000000000000001535---d7f2704dda40d0528bed1e8e16c66e17b842f50172d7ba8554f23c7229aedc67]]]# Initialize weights

[[[00000000000000001536---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000001537---06f7dbb825277252d3f70b6a892a5aca619776d6c46333a516efdd8f89f3dda7]]]# Collect all weights and gradients in a list

[[[00000000000000001538---33a07c0284817444aaa0c98a6c115ec58d5a0921f2a2ee323a7ec261fbb73efa]]]# Set distributed representation of word to member variable

[[[00000000000000001539---7667559abba8c6e806b997e97ed1b6b76d5f29fd74cf2ead2fa666136ec5e4d2]]]Here, we take vocab_size for the number of vocabularies and hidden_size for the number of neurons in the hidden layer as arguments of the initializer. First, let&#39;s talk about weight initialization. Here we generate two weights (W_in and W_out). Each of these two weights is initialized with a small random value. Also, at this time, specify the data type of the NumPy array with astype(&#39;f&#39;). This initializes it with a 32-bit floating point number.

[[[00000000000000001540---d285b6de399ac173571585ba777fc889f7c87d0318daa718cf442dafc4656be6]]]Then create the required layers. First, generate 2 input MatMul layers, 1 output MatMul layer, and 1 Softmax with Loss layer. Here, the MatMul layer that processes the input side context creates as many words as the context uses (two in this example). Then initialize the MatMul layer to use the same weights.

[[[00000000000000001541---0e48714a5761f41958214e8472dddeabe363d9490623adc09394064253b0a3bc]]]Finally, gather the parameters and gradients used in this neural network as arrays in the member variables params and grads, respectively.

[[[00000000000000001542---169deef745075a9a30870eea42407bca57382857e17ed1fb5fec16e17f1975a9]]]Here we are sharing the same weights across multiple layers. Therefore, there will be multiple instances of the same weight in the params array. However, having the same weights in the params array causes optimizers such as Adam and Momentum to behave differently (at least in our implementation). So inside the Trainer class, we do a simple task of de-duplicating parameters when updating parameters. We omit the explanation here. For those interested, see the remove_duplicate(params, grads) function in common/trainer.py.

[[[00000000000000001543---f48aacd0e6be4a7b651bad03ecd23928cd55b85ca372b00292e7aea0c7180e0b]]]Next, implement the forward() method, which is the forward propagation of the neural network. This method takes two arguments, contexts and target, and returns the loss.

[[[00000000000000001544---276e69dfbe2b4fc5f9b1526c50a00e13caeeaa6e7db400c31104e9bbb097255f]]]Here we assume that the argument contexts are 3D NumPy arrays. This results in the shape (6, 2, 7) in the example of Figure 3-18 in the previous section. Its 0th dimension has as many elements as the number of mini-batches, and its 1st dimension has as many elements as the window size of the context. And the second dimension is represented by a one-hot vector. Also, the shape of the target is 2D, for example (6, 7).

[[[00000000000000001545---05515b9156d0b4af65c7c7bed6e4a06ba64875447b0220453801c520639bb691]]]Finally, implement backward() for backpropagation. The computational graph for this backpropagation looks like Figure 3-20.

[[[00000000000000001546---e35abbd0f17b4b121d2d3a2999aaf69d229ba0a199cc2c0f931f2ce806866656]]]
Figure 3-20 Backpropagation of the CBOW model: backpropagation flows are indicated by thick red lines


[[[00000000000000001547---b9fa11243b3fb36bd6deb177249a7b704c2630b62a8c7e36dd0e9079617a8f21]]]Backpropagation in neural networks propagates gradients in the opposite direction to forward propagation. This backpropagation starts at &#39;1&#39; and feeds it into the Softmax with Loss layer. Then, let the output of the backpropagation of the Softmax with Loss layer be ds, and input that ds to the output MatMul layer.

[[[00000000000000001548---69df9853c5cdefde5aaf64790dbdff4e3d19937056128d1088d8ffe326c895a6]]]After that, it becomes backpropagation of the operation of &quot;+&quot; and &quot;×&quot;. Backpropagation of “x” multiplies the gradient by “swapping” the input values from forwardpropagation. Backpropagation of &quot;+&quot; just &quot;passes through&quot; the gradient. Now let&#39;s implement the backpropagation according to Figure 3-20.

[[[00000000000000001549---b75369144ddccd2633cb9afab1822eec6bbb4915b56724b1ceb739eda3ac6b11]]]This completes the implementation of backpropagation. We already have the gradients of each parameter in the member variable grads. So calling the forward() method followed by the backward() method updates the gradients in the grads array. Let&#39;s move on to learning the SimpleCBOW class.

[[[00000000000000001550---21f3aef51cdc2c07751b6d0a362ae1c4dd471dd032f2eee79b0e55da01743b77]]]Implementing training code

[[[00000000000000001551---2a5aa40ea146660132cfb3ecfb3fff9aad80426703b7114dadce5ed0ac89d4a2]]]Training a CBOW model is exactly like training a regular neural network. First, prepare the training data and feed it to the neural network. Then we find the gradient and update the weight parameter step by step. Here, let the Trainer class described in Chapter 1 do the learning process. Here is the source code for learning (☞ ch03/train.py).

[[[00000000000000001552---1ba77384a95f68449863a60d1aac30347b4c0d6830753eb9d5c96e4788b5e34e]]]Several well-known methods such as SGD and AdaGrad are implemented in common/optimizer.py for updating parameters. Here we chose the Adam algorithm. Also, as explained in Chapter 1, the Trainer class will train the neural network. It selects a mini-batch from the training data, gives it to the neural network to find the gradient, gives the gradient to the optimizer and updates the parameters.

[[[00000000000000001553---7af6ca251a77962c70cc5b6753aa468d07ab5384b8831bbc551dcf9f6b43be6f]]]From now on, we will use the Trainer class for neural network training. By using the Trainer class, you can streamline the training code, which tends to be complicated.

[[[00000000000000001554---3d14248ab3ef6157c7d7b039d4226fddbedcd3d51f27169f6b60527231202d3d]]]Now let&#39;s run the code above. The result should look like Figure 3-21.

[[[00000000000000001555---80ce5307d50198f2846e68a6a117f80429e89c23f6e24571cebfd5a861ff1a45]]]
Figure 3-21 Displaying the progress of learning in a graph (horizontal axis is number of times of learning, vertical axis is loss)


[[[00000000000000001556---c552160341fc4133a82adbdfe9414dd101b147f9ad24f71751e3fb3334ffe2c7]]]As shown in Figure 3-21, it can be seen that the loss decreases as the number of learning times increases. You seem to be learning well. Now let&#39;s take a look at the weight parameters after training. Here, we will take out the weights of the MatMul layer on the input side and actually check the contents. The weight of the MatMul layer on the input side is set in the member variable word_vecs. Continuing from the previous code, add the following code.

[[[00000000000000001557---6974e994899a4b813ebfc85ef0671b034b705985083a5ba931202feba612e4e5]]]Here we retrieve the weights under the name word_vecs. Each row of this word_vecs stores a distributed representation of the corresponding word id. Actually running the above code gives the following result:

[[[00000000000000001558---d52761088fa80c4e047fadcc06f5df332e3ceffa13afc1ea3238e9691a735043]]]Finally we can represent words as dense vectors! This is the distributed representation of words. It can be expected that this distributed representation is a vector representation that captures the &quot;meaning of words&quot; well.

[[[00000000000000001559---f2f1b1793b056527b83dc4ce013de4b68f389c01fbe6ca99fbad709998503b92]]]Unfortunately, the small corpus we&#39;re dealing with doesn&#39;t give good results. The reason, of course, is that the size of the corpus is too small. In fact, if you change the corpus to something big and practical, you&#39;ll get good results. However, there is a problem in terms of processing speed in that case. This is because the current implementation of the CBOW model has some problems in terms of processing efficiency. In the next chapter, we will improve the current “simple” CBOW model and implement a “genuine” CBOW model.

[[[00000000000000001560---f379074dda275b8bd239a443b42f5582f88ca21bb8833e766f45c4b130a33af9]]]Supplement about word2vec

[[[00000000000000001561---36e3c3ee91ae14f43a47a8405975dde011f0045a4709ea86f1246ca7bb26a575]]]So far we have taken a closer look at word2vec&#39;s CBOW model. Here, I would like to add some important themes about word2vec that I haven&#39;t been able to talk about so far. First, let&#39;s take another look at the CBOW model from a &quot;probability&quot; perspective.

[[[00000000000000001562---e93687562e178047cc543de5cd8019896833a276337ab5cf4cbc9ba34d2ae6ad]]]CBOW model and probability

[[[00000000000000001563---065a47481b65d61c1e057b8fd5c9cb3f2fe66ac10e2e7d7260dc64a737499021]]]First, let me briefly explain the notation of probability. In this book, probability is expressed as P(・). For example, the probability that event A occurs is written as P(A). Also, the joint probability is expressed as P(A, B). Joint probability means the probability that A and B occur at the same time.

[[[00000000000000001564---6ea358e96311e1739d73047725e31976f860bb3a288f93f1fa77f2b35eac68f4]]]Posterior probabilities are written as P(A|B). This is literally the &quot;after-event probability&quot;. From another point of view, this can be interpreted as &quot;the probability that A will occur given B (information)&quot;.

[[[00000000000000001565---3260b84d9ed0803c3d67542aaaeea366cd42806d2cf188887fdb9659895b095e]]]Let us now describe the CBOW model in terms of probability notation. What the CBOW model did was output the probability of the target word given the context. Here, we will deal with a corpus represented by a string of words. Then consider a context with a window size of 1 for the tth word, as shown in Figure 3-22.

[[[00000000000000001566---45c4ceed3930d1033886aed78073b7b91dfeec38b8dad39b576cfb736274c05d]]]
Figure 3-22 word2vec CBOW model: infer target words from context words


[[[00000000000000001567---5624b6815aeb113497d5bf2733604d757d2c97b1ea427f3b4df51afbc3c317fc]]]Let&#39;s formulate the probability that the target is wt given wt−1 and wt+1 as context. It can be written as Equation (3.1) using the posterior probability.

[[[00000000000000001568---f1b55e588f08bd1afe68c6913a474d8e9bd0d30189bc666175d7b3cd6adfe528]]]Equation (3.1) expresses the probability that wt will occur after wt−1 and wt+1 have occurred. And it can be interpreted as &quot;the probability of wt occurring given wt−1 and wt+1&quot;. In other words, CBOW models equation (3.1).

[[[00000000000000001569---aae624e367e903502cdde1dbe9591bf836c5a7adef85a0aaae164283a5ddc1a5]]]Using equation (3.1) here, the loss function of the CBOW model can also be expressed concisely. To do so, we apply the cross-entropy error (equation (1.7)) described in Chapter 1. Equation (1.7) is where yk is the probability that the k-th corresponding event occurs. And tk is the teacher label, which becomes the element of the one-hot vector. In our problem, ``wt occurs&#39;&#39; is the correct answer, so the corresponding one-hot vector element is 1, otherwise it is 0 (i.e., if something other than wt occurs, they will be 0 for the one-hot label element corresponding to ). Considering this point, we can derive the following formula.

[[[00000000000000001570---05153e4ec868514e237c9fd227d8706bb6c9d13e225767ffa37fd4272cbd3884]]]The loss function of the CBOW model is simply the log minus the probability of equation (3.1). By the way, this is called the negative log likelihood. Equation (3.2) is the loss function for one sample data. Extending this to the entire corpus, the loss function can be written as

[[[00000000000000001571---8970fcdb0b1575c73712a20299e664f173312aae25bc7405f91c15c60cc325a4]]]What we do in training the CBOW model is to make the loss function expressed in Equation (3.3) as small as possible. Then the weight parameter at that time becomes our desired distributed representation of the word. I have only considered the case of window size 1 here, but other window sizes (and general window sizes such as m) can easily be expressed mathematically. .

[[[00000000000000001572---03dbae915a27bd5b217b0d274c50d2cb75e115e5053728531db92b9dcd4bd46f]]]skip-gram model

[[[00000000000000001573---f1c81966de715b89712f4cc13b95007fad7d8f29e8e23b6d92f46979d8241eb7]]]As mentioned before, word2vec proposes two models. One is the CBOW model we&#39;ve seen so far, and the other is a model called skip-gram. A skip-gram is a model in which context and target are reversed in CBOW. To give a concrete example, the problem solved by those two models looks like Figure 3-23.

[[[00000000000000001574---1c6ab6e5e7fcdb9eb9a09cfb4c3176300d8ce8b04a3da88d4d3b1cca3591dc30]]]
Figure 3-23 Problems handled by the CBOW and skip-gram models


[[[00000000000000001575---08e3e138c39fa2f1d694d39671e1e43849da33739c3fb5572b6958dc68587449]]]As shown in Figure 3-23, the CBOW model has multiple contexts and infers the middle word (target) from the multiple contexts. On the other hand, the skip-gram model infers multiple surrounding words (context) from the central word (target). At this time, the network configuration of the skip-gram model will be as shown in Figure 3-24.

[[[00000000000000001576---625986e24796feb6f20047c65df7dfac2ead775e6fc0370cbee3512ffb48c5a8]]]
Figure 3-24 Skip-gram model example


[[[00000000000000001577---5fa3d4257703468a6ba0983b41ec06e770b4f9fd1d321be022309f28e33a7721]]]The skip-gram model has one input layer, as shown in Figure 3-24. And there are as many output layers as there are contexts. Therefore, each output layer calculates its loss separately (e.g. by a Softmax with Loss layer) and sums them up to give the final loss.

[[[00000000000000001578---2e86e8f9500e853975c0459a57832897786e5acafdf1630e945277990734de2b]]]Now let&#39;s use probability notation to express the skip-gram model. Now let&#39;s say the middle word (target) is wt, and we want to infer the context wt−1 and wt+1 from it. Then the skip-gram models the following equation (3.4).

[[[00000000000000001579---935cee001ef45f1fc3af55c02110005f81bfc268e108d360874d51b8ccace52c]]]Equation (3.4) expresses &quot;the probability that wt−1 and wt+1 occur simultaneously when wt is given&quot;. Here the skip-gram model assumes there is no relationship between the words in the context and decomposes them as follows (which correctly assumes &quot;conditional independence&quot;):

[[[00000000000000001580---a68019bed5713b188915c17032b0211fbfc6df463d068ad64f2b5c7980c2d8f5]]]Then applying equation (3.5) to the cross-entropy error leads to the loss function of the skip-gram model.

[[[00000000000000001581---bfabb99b7a272b8035566ae03967761603e6a662433d26e4de1ca08ec56fe102]]]Here, we use the logarithm logxy = logx + logy relationship. As shown in equation (3.6), the loss function of the skip-gram model is the sum of the losses for each context. Equation (3.6) is the skip-gram loss function for one sample data. Extending this to the entire corpus, the loss function of the skip-gram model is given by the following equation (3.7).

[[[00000000000000001582---071c1aef89cb4002d37e9b0aa9eda1a6ff56e16196fe8953c7bc414cf57592b0]]]Comparing equation (3.7) with equation (3.3) for the CBOW model, the difference becomes clear. Since the skip-gram model makes as many guesses as there are contexts, its loss function must sum the losses found in each context. The CBOW model, on the other hand, finds the loss of a single target. The above is the explanation of the skip-gram model.

[[[00000000000000001583---733360076f1d29be46972826e50bbce258a9bcb3021e2ab5b0a2d2b6c013b618]]]So, should you use the CBOW model or the skip-gram model? The answer can be said to be the &quot;skip-gram model&quot;. This is because the skip-gram model often yields better results in terms of the accuracy of the distributed representation of words. In particular, as the corpus grows larger, the skip-gram model tends to yield better results in terms of performance on low-frequency words and analogy problems (see 4.4. .2 How word vectors are evaluated”). In terms of learning speed, the CBOW model is faster than the skip-gram model. This is because in the case of the skip-gram model, the loss is calculated for the number of contexts, which increases the computational cost.

[[[00000000000000001584---8645cff3df3dc5825910bf75b7d97df11eafc15685c010aa33b856554278cbc7]]]A skip-gram model predicts surrounding words from a single word. This seems like a very difficult problem. For example, if we solve the problem in Figure 3-23. At this point, you can easily answer &quot;say&quot; to the CBOW model problem. However, there seem to be various candidates for the problem of the skip-gram model. In that respect, the skip-gram model tackles a “tougher” problem. And it may be that the skip-gram model can obtain a better distributed representation of words by being trained on that tough problem.

[[[00000000000000001585---dee4cd4bb4f696de98f5c779f91c69fff3818f46a337be156a85f632fa5c3fb6]]]As for the implementation of the skip-gram model, if you understand the implementation of the CBOW model, there is nothing particularly difficult. Therefore, I will not explain the implementation of skip-gram here. The implementation of skip-gram is in ch03/simple_skip_gram.py, so please refer to it if you are interested.

[[[00000000000000001586---52173a571bc8eab2177aa3af1ab30e9fc46fba6bec41d4f5db58954852bcb973]]]Count-based vs Inference-based

[[[00000000000000001587---40413cd0d319f2e6cba2ca50347bbff2992b590b7ccdeebd6542416513b113e3]]]So far we have seen count-based methods and inference-based methods (especially word2vec). There was a big difference between the two methods in the learning framework. A count-based method obtained distributed representations of words in a single training session from the entire statistical data of the corpus. Inference-based learning, on the other hand, was performed by looking at a portion of the corpus many times (mini-batch learning). Here, I would like to consider the other points while comparing the two methods.

[[[00000000000000001588---a209135b69d3e52bafbe807a21b2b7c4f960a060590651e6e9fe291f5758fc60]]]First, consider the case where a new word is added to the vocabulary, and the task of updating the word&#39;s distributed representation occurs. At this time, the count-based method must be calculated from scratch. Even if you want to slightly modify the distributed representation of words, you will need to recreate the co-occurrence matrix and perform SVD. Inference-based methods (word2vec), on the other hand, can re-learn parameters. Specifically, by re-learning with the weights learned so far as the initial values, it is possible to efficiently update the distributed representation of words without losing the learning experience. In that respect, the inference-based method (word2vec) has the upper hand.

[[[00000000000000001589---bed6bf48c0add65f67d8bbc5247da905e7056b07d36a634d8e16e0b9663171e5]]]Then, what about the properties and accuracy of the distributed word representations obtained by the two methods? Regarding the nature of distributed representations, we know that count-based methods primarily encode word similarity. On the other hand, word2vec (especially the skip-gram model) is known to capture more complex inter-word patterns in addition to word similarity. This is famous for solving analogy problems such as &quot;king-man + woman = queen&quot; in word2vec (analog problems are explained in the next chapter, section 4.4.2).

[[[00000000000000001590---74737ba26cd314bf04194b85900745a11da923d8bb11c967ebcc41bbea01ad41]]]A common misconception here is that inference-based methods are more accurate than count-based methods. In fact, it has been reported that inference-based and count-based methods are indistinguishable when it comes to quantitative evaluation of word similarity [25].

[[[00000000000000001591---5651ce3130f00d7914c1a8d5e833695cba586db0b8b2e4df74124e34e1bc6244]]]A paper [24] with the title &quot;Don&#39;t count, predict!&quot; was published in 2014. This paper systematically compared count-based and inference-based methods, and reported that the inference-based method consistently outperformed in accuracy. However, another paper [25] later reported that in word similarity tasks, hyperparameters are highly dependent, and there is no clear superiority or inferiority between count-based and inference-based methods.

[[[00000000000000001592---aff5838bb558700c663463f4f149aec89adc61ff8aa483d7409a8188155b25b5]]]An important fact is that inference-based and count-based methods have been found to be related. Specifically, the model using skip-gram and negative sampling (discussed in the next chapter) performs a special matrix decomposition on the co-occurrence matrix of the entire corpus (actually, a slightly modified matrix). [26]. In other words, the two worlds were (under certain conditions) “connected”!

[[[00000000000000001593---1ca6ed72b0a9c1bb44698bf40cc3c7ca6b4c4f4ece2f7e58cb4dd8086a5120c0]]]Furthermore, after word2vec, a method called GloVe [27], which combines inference-based and count-based methods, has been proposed. The idea of the method is to incorporate the information of the statistical data of the entire corpus into the loss function and perform mini-batch learning (see paper [27] for details). By doing so, we succeeded in explicitly merging the two worlds.

[[[00000000000000001594---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001595---d0ce4ef602b355721ed1a006519eaa05dc84eca7bf85f867e1339c6b19491f11]]]word2vec was proposed by Tomas Mikolov in a series of papers ([22] and [23]). Since it was announced, word2vec has received a lot of attention. And its usefulness has been shown in many natural language processing tasks. In the next chapter, I will explain the importance of word2vec—especially the usefulness of word2vec transfer learning—with specific examples.

[[[00000000000000001596---da1722c04f86c9bfc9e46c7b1555030c21c2ce83b4739564a2dcc1d97002752e]]]In this chapter, we described in detail a neural network called the CBOW model of word2vec and implemented it. The CBOW model is basically a two-layer neural network with a very simple configuration. We built a CBOW model using MatMul and Softmax with Loss layers and confirmed that it can be trained on a small corpus. Unfortunately, the current CBOW model has some problems in terms of processing efficiency. But once you understand the CBOW model in this chapter, you are one step closer to the real word2vec. Now let&#39;s move on to improving the CBOW model!

[[[00000000000000001597---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000001598---ef2c3fb1c99c62540d48f17450624d444ebdcddf2e00ebf75579fee2249dc9bb]]]Inference-based methods have the goal of guessing, and as a by-product they can obtain distributed representations of words.

[[[00000000000000001599---2ddfa1a79a7c485fe8b94912aac5e91c53efe5d6f4b9587f4141ba7a8b3d17c6]]]word2vec is an inference-based method, consisting of a simple two-layer neural network

[[[00000000000000001600---a3aeb4cae0171d6f8c0090e0c5087542744d5e0a0b5b5f79968f8db9b431cf88]]]word2vec has skip-gram model and CBOW model

[[[00000000000000001601---3d22cabcf30855570ee847b775a485c2c353e0c72bb6c72454ff9a8effbc7ac1]]]The CBOW model infers a single word (target) from multiple words (context)

[[[00000000000000001602---511ec928bde26d1138aaaf7ce4fa5525998ca4b8ca10d34c3ef83f7bfee319ac]]]The skip-gram model, on the other hand, infers multiple words (context) from a single word (target).

[[[00000000000000001603---0441d4f0349983e9db4bddfd12968d66cf784319b7c7174c8ad80c61f0289034]]]word2vec can re-learn the weights, so it can efficiently update and add the distributed representation of words.

[[[00000000000000001604---ed57a1570552eea464f1f2851658d827c8dc4137c96167d284c699c37d79b1ac]]]Chapter 1

[[[00000000000000001605---7ec644efdfce4761781aabd6bc5bc5e87aea26e1849503cb40e0a9d020e9950e]]]Neural network review

[[[00000000000000001606---abef03578aedc4336e252d7dc4394801ef7399ffd4918a82d92b9e5663d9456d]]]Until we know more than one way

[[[00000000000000001607---661a8cfba46ac5e9ec6ecd30256f96e84166c7cdf45ef48a131ad1753000b85d]]]It doesn&#39;t mean that you understand things.

[[[00000000000000001608---f580b87e7ab3e9d36f2b538598d3ecab611103dc959fad373b6b87e5fc44cc19]]]—— Marvin Minsky (Computer Scientist, Cognitive Scientist)

[[[00000000000000001609---ad305be09c519c0d24d0d4640d5513cf6469f94e96a217b06084672cc50decb9]]]This book is a sequel to the previous work &quot;Deep Learning made from scratch&quot;. Continuing from the previous work, we will further explore the possibilities of deep learning. Of course, as in the previous work, this book also emphasizes &quot;creating from scratch&quot; without using ready-made products such as libraries and frameworks. I would like to explore the fun and depth of technologies related to deep learning through making them.

[[[00000000000000001610---d5af1dd5512bc292a74bfc52ff83c6b39f8776375b6855e24f53525c82bbf089]]]In this chapter, we review neural networks. In other words, the digest version of the previous work corresponds to this chapter. In addition, in this work, we have made some changes to the implementation rules of the previous work (for example, method names and how to hold parameters, etc.), with an emphasis on efficiency. Let&#39;s check that point in this chapter.

[[[00000000000000001611---6a504162932abd445483727ddcd5b199c5cc3d3ad07b5f28cbf671532f01dd13]]]Mathematics and Python review

[[[00000000000000001612---d892ce634a280f9bbaabe1176967a0b2dbbc1b474206ec67fabfba39cef71f10]]]First, start with a review of mathematics. Specifically, we will talk about &quot;vectors&quot; and &quot;matrices&quot; necessary for neural network calculations. I&#39;ll also show some code in Python—especially using NumPy—to help you jump straight into neural network implementation.

[[[00000000000000001613---e8d80c8acdbc0045880d2bf89bf6e1c34a646577bd85a471fd5c222e82668986]]]vectors and matrices

[[[00000000000000001614---c28fef386dfdbb754646bc243d3c2dac9148fa556898e0facc9648e032bb2faf]]]In neural networks, &quot;vectors&quot; and &quot;matrices&quot; (or &quot;tensors&quot;) come up all over the place. Here, we briefly organize these terms and prepare for reading this book.

[[[00000000000000001615---eebdbfae80cb297c9b0670c1c9e8b30acd9d2b928ed4902695a6f24cdb26344c]]]Let&#39;s start with &quot;vector&quot;. A vector is a quantity that has magnitude and direction. A vector can be represented as a collection of numbers in a line, and can be treated as a one-dimensional array in the Python implementation. On the other hand, a &quot;matrix&quot; is a collection of numbers arranged in a two-dimensional (rectangular) shape. An example of vectors and matrices is shown in Figure 1-1.

[[[00000000000000001616---fcb78726ebd17f759573675acfd39998f5728ecac40e5aa9f39cc9f8442bc99d]]]
Figure 1-1 Examples of vectors and matrices


[[[00000000000000001617---e0a33e1782ae56955cf8bd6f7c85b50feb6cf95ad64ab65e6668005d6112d734]]]As shown in Figure 1-1, vectors can be expressed as one-dimensional arrays, and matrices as two-dimensional arrays. In a matrix, the horizontal arrangement is called a row, and the vertical arrangement is called a column. Therefore, the matrix in Figure 1-1 is called a &quot;3-row, 2-column matrix&quot; and is also written as a &quot;3x2 matrix.&quot;

[[[00000000000000001618---26049839fa449fe6a573f3bcec8ef766bf6b25e1219ee08550a04f123a64294e]]]We can also extend vectors and matrices to think of collections of N-dimensional numbers. This is commonly called a tensor.

[[[00000000000000001619---f586a1a5428d07f7fa4b5537c99695d5428d16bcd6b8b13156aff3b8d0d74008]]]A vector is a simple concept, but it should be noted that there are two ways to represent a vector. As shown in Fig. 1-2, one is to represent it as a vertical array (= column vector), and the other is to represent it as a horizontal array (= row vector).

[[[00000000000000001620---023f822134a0e4bff710d4b6aa44fdef50d3ba64405b82814f8aa671d5f9ae3b]]]
Figure 1-2 How to represent vectors


[[[00000000000000001621---1b67c9bcbf0b583c338da07fc3cb259a9710145ba60da1debcb1b7363c95c363]]]In many fields such as mathematics and deep learning, it is common to treat vectors as &quot;column vectors&quot;. However, in consideration of implementation compatibility, this document treats vectors as &quot;row vectors&quot; (in each case, it is clearly stated that they are row vectors). Also, when writing vectors and matrices in mathematical formulas, we use bold symbols such as x and W to distinguish them from single elements (scalars) (variables in the source code are W and written in a font such as x).

[[[00000000000000001622---810e6079f7438e788d19ad4e70e99fe967eacf012abc3d2d7a3ec8bb20a9c7fe]]]When treating a vector as a &quot;row vector&quot; in the Python implementation, it becomes clearer to treat it as a horizontal &quot;matrix&quot;. For example, a vector with N elements is treated as a 1×N matrix. We&#39;ll look at specific examples later.

[[[00000000000000001623---b584b9e272a98c3737e55ab4c5400b4ebefb2ed8e90cf8592f0dc75816579b2c]]]Now let&#39;s use Python&#39;s interactive mode to generate vectors and matrices. Of course, here we use NumPy, the standard library for working with matrices.

[[[00000000000000001624---ad2db941d4ab3e22897dbe774fc121ec01927d6b4c9f4e30d1a197748249a196]]]       # show class name<class 'numpy.ndarray'> &gt;&gt;&gt; 

[[[00000000000000001625---eef443a14997b34b7ddedb352393793f221e4abd8b332e0f1067bc1143d04c84]]]As shown here, vectors and matrices can be created with the np.array() method. This method generates the np.ndarray class, a class for NumPy multidimensional arrays. The np.ndarray class has many useful methods and instance variables.In the example above, we use the instance variables shape and ndim. shape represents the shape of the multidimensional array, and ndim represents the number of dimensions. Looking at the result above, we can see that x is a one-dimensional array, which is a vector with 3 elements. We can also see that W is a two-dimensional array, a 2x3 (2 rows by 3 columns) matrix.

[[[00000000000000001626---061ff32a99f61feb3fb1aa08d74ab2fc6947650a74df1bc2abf218adfc952ef6]]]Matrix Elementwise Operations

[[[00000000000000001627---f817a9543e28020dcfc6a99f095a4ba561204f93bd2c4dd1b53f6e18feb5070d]]]We were able to organize collections of numbers as &quot;vectors&quot; and &quot;matrices&quot;. Now let&#39;s do some simple math with them. First, let&#39;s take a look at &quot;element-by-element operations&quot;. By the way, &quot;per element&quot; is called element-wise in English.

[[[00000000000000001628---9a4baf5730a6599571d0e816a175b813742ad1a2e02eb28df99db93872554311]]]Here, four arithmetic operations such as + and * are performed on multidimensional arrays of NumPy. In this case, the operation is performed on each element in the multidimensional array—each element independently. That&#39;s &quot;elementwise operations&quot; on NumPy arrays.

[[[00000000000000001629---f3d2d683d2c8f601672621d9e9f6249c13bbcad6c0b7c452e165665dd9fc3605]]]broadcast

[[[00000000000000001630---2cc3c17c8ea25569ae4a98727bc5ba8d5231db512f694e8cb5ba7eb495c17b33]]]Multidimensional arrays in NumPy also allow operations on arrays of different shapes. For example, a calculation such as:

[[[00000000000000001631---e8ef0d750a4c662a8fcb234d842dbb48b0cd132d48d13abb1d95908fc16eee41]]]This calculation multiplies the 2x2 matrix A by a scalar value of 10. The scalar value of 10 is then expanded into a 2x2 matrix, followed by element-wise operations, as shown in Figure 1-3. This clever feature is called broadcast.

[[[00000000000000001632---8ef090e3791322b681e4ee8231070fcdcf168605da18e24af51f33e53f8ec338]]]
Figure 1-3 Broadcast example: scalar value &quot;10&quot; treated as a 2x2 matrix


[[[00000000000000001633---9fa27c1eb9f4cc87598e32908a23df34baf8dad48aee5ce64dfff68d469a49a6]]]As another example of broadcasting, consider the following computation.

[[[00000000000000001634---da70fe76f7e284aba2b1319bcccd0d957923155e106f46b8e1c976eb73a83f2f]]]In this calculation, the one-dimensional array b is &quot;intelligently&quot; expanded to have the same shape as the two-dimensional array A, as shown in Figure 1-4.

[[[00000000000000001635---62f89e2a62847261b0d2cd882b34c9812838d624997d8fd8a62aed87c3a632a0]]]
Figure 1-4 Broadcast Example 2


[[[00000000000000001636---288c7e873afe0db4b08c4773d0855bc163ffd2611c7437b81ae579436533312a]]]In this way, NumPy has a function called broadcasting, so you can smartly perform operations on arrays with different shapes.

[[[00000000000000001637---8365bc3796683a595d012fd31bd5ab84eb7b2cbe1dec72e755b935845d8672e4]]]For NumPy&#39;s broadcasting to work effectively, the shape of multidimensional arrays must satisfy a few rules. For detailed broadcasting rules, please refer to [1].

[[[00000000000000001638---0062a75e9438467a1d36bd6abea07ba65bc70f7f79caead34a7218ff77370f04]]]Inner product of vectors and product of matrices

[[[00000000000000001639---4889f24f1f26f80ba3ac10240b864eed585414b39f0f84530e6e859dcd6093c2]]]Next, we will look at the inner product of vectors and the product of matrices. First of all, it is the inner product of vectors, which is expressed in the formula as follows.

[[[00000000000000001640---2a21905f43e5a05954fb631dfc0cf38b1d8b25001f9ccedd80abcd27e88f6fb8]]]・

[[[00000000000000001641---5e60bcbb85113396a9a5a064aa888e6525b4ae16bc375b3d88a78f91d5a07fb3]]]Here we assume that we have two vectors Then the dot product of vectors is the sum of the products of corresponding elements between two vectors, as equation (1.1) shows.

[[[00000000000000001642---71f9e29666c49f1f5b1c64456512b1ac1cdf2461dd0fbac16031b2e6193ed6b7]]]The inner product of vectors intuitively expresses how much two vectors point in the same direction. If the vector length is 1, the dot product of the two vectors is 1 if they point in exactly the same direction. On the other hand, if the two vectors are in opposite directions, it will be -1.

[[[00000000000000001643---573f175d6f9769a705e3146ae9f3cecba6525cd20df2ffa3ba068fce4817ae6f]]]Next is about &quot;matrix multiplication&quot;. Matrix products are calculated according to the steps in Figure 1-5.

[[[00000000000000001644---4453b7221de6352de1264c9a38d433ff70daf427629a5631a51b7b7f197d7ace]]]
Figure 1-5 How to Calculate the Matrix Product


[[[00000000000000001645---61739a8bff083da4036dc66de465d95450badbad517f71316125e49653b64772]]]As shown in Figure 1-5, the matrix product is calculated by the inner product (element-wise product and sum) of the row vector of the left matrix (horizontal direction) and the column vector of the right matrix (vertical direction). increase. The result of the computation is then stored in the corresponding element of the new matrix. For example, the result of row 1 of A and column 1 of B goes to the element in row 1, column 1, the result of row 2 of A and column 1 of B goes to the element of row 2, column 1, and so on. will be

[[[00000000000000001646---e054b649a0137ae15970f18738527a33633905eb5b95cd172b5d22cb0b9083b4]]]Now, let&#39;s implement vector inner product and matrix product in Python. You can use np.dot() for that.

[[[00000000000000001647---174f9320721c7026e162e1a79c994c70955cf30ad2e1d6028cf25731943a7dcd]]]# Inner product of vectors &gt;&gt;&gt; 

[[[00000000000000001648---3f053091e830f12b697fbc564ea6892fc8984d1fa39aed8085b7ac606fda6dc2]]]
32 # Matrix product &gt;&gt;&gt; 

[[[00000000000000001649---155f9609eb261ce5b521f037374ec4b6ee913448ff036a404b709b8414f16463]]]As shown here, np.dot() can be used to both compute vector inner products and matrix products. If the arguments of np.dot(x, y) are both 1-dimensional arrays, it calculates the inner product of vectors, and if the arguments are 2-dimensional arrays, it calculates the matrix product.

[[[00000000000000001650---7551c1cc9c5e3248fb69cf5b4afc19a173f79b3ae50e7a15c5882fbff4ac6940]]]In addition to the np.dot() method you saw here, NumPy provides many other convenience methods for performing matrix calculations. If you can master them, you will be able to smoothly implement neural networks.

[[[00000000000000001651---b102127d6a3dc91f9cfb16b1813e15c066e262f067e9bfa66ff03f1efb17ad0e]]]Practice makes perfect

[[[00000000000000001652---193568f14b40971d17abe46180fd132fe801f0e90772dbe953cb045495fd9fc9]]]To learn NumPy, it is effective to practice by actually moving your hands. For example, &quot;100 numpy exercises&quot; [2] contains 100 NumPy exercises. If you want to gain experience with NumPy, please try it.

[[[00000000000000001653---1b251f3328e2f2364911e0ed0aa224dc460ae10c40ee326d2cc53934f8be2b0a]]]Matrix shape check

[[[00000000000000001654---e7481f3964145ab8377ad095c5aac4a3c2436c90ae732e19cd2fcd9efe916805]]]When working with matrices and vectors, it is important to pay attention to their &quot;shape&quot;. Here, I would like to look again at the &quot;matrix product&quot;, focusing on the shape. We have already explained the procedure for calculating the matrix product, but at this point the &quot;shape check&quot; shown in Figure 1-6 is important.

[[[00000000000000001655---87a3383b24d01e99b5dc65e3dee59c20903e590b5085c95554fb2c79d0a58952]]]
Figure 1-6 Shape check: In matrix multiplication, match the number of elements in corresponding dimensions


[[[00000000000000001656---d452a7babdac87117cfac52536ee6cb58761c0db3bab58eb19e0037f9fe55721]]]Figure 1-6 shows an example where the product of a 3x2 matrix A and a 2x4 matrix B produces a 3x4 matrix C. Then, as the figure shows, the number of elements in corresponding dimensions of matrices A and B must match. The resulting matrix C will then consist of the number of rows of A and the number of columns of B. This is the &quot;shape check&quot; of the matrix.

[[[00000000000000001657---0e75d0ac5493e991190006144bff6cf350929db02789ad70d385f28e42b500ed]]]In calculations such as matrix multiplication, shape checking—focusing on the shape of the matrix and observing its transition—is important. This will allow us to proceed smoothly with the implementation of neural networks.

[[[00000000000000001658---721ae5ed6080be3d34edeffc78521aac8650c96fec3d8f62ba2e8671805b5049]]]Neural network inference

[[[00000000000000001659---cc310b4f942243ba88f62063c6110f721fa9b46b82d2447413dddd7ffe787d7a]]]Let&#39;s start reviewing neural networks. The processing performed by the neural network can be divided into two phases. They are &quot;learning&quot; and &quot;inference&quot;. Here, we will focus only on the &quot;inference&quot; of neural networks. In the next section, we will look at the “learning” of neural networks.

[[[00000000000000001660---2036fd2d13deeeb77bbc7f433517806552bba04afbda42897e918ff7e4c2cb67]]]An overview of neural network inference

[[[00000000000000001661---8b1eb3589cae5cc513d41a3d21d1a41cdfd905f92a6cfce3c45dd9d299beda7e]]]A neural network is simply a &quot;function&quot;. Just like a function is a transformer that puts something in and outputs something, a neural network transforms an input into an output.

[[[00000000000000001662---acdfd95afbe9be773e15fa9af977eba215ed203a4b175126e99d1bdd1a3c635c]]]Here, as an example, consider a function that inputs 2D data and outputs 3D data. To achieve this with a neural network, prepare two neurons in the input layer and three neurons in the output layer. And put some neurons in the hidden layer (hidden layer) as well. Let&#39;s put 4 neurons in the hidden layer here. Then our neural network can be written as shown in Figure 1-7.

[[[00000000000000001663---78823ae910a085f9b7aec9f0681f08a6e4389b7dcdc707be1b30a2730f610987]]]
Figure 1-7 Neural network example


[[[00000000000000001664---57699a8beb3e1e5555f4cefcbb6b82d1d0ecf457b8d7cde0e533d2697544355a]]]In Figure 1-7, neurons are represented by circles, and connections between them are represented by arrows. At this time, there is a weight on the arrow, and the weight is multiplied by the value of the neuron, respectively, and the sum becomes the input to the next neuron (more precisely, the value obtained by applying the activation function to the next neuron input). At this time, a &quot;constant&quot; that is not affected by the values of neurons in the previous layer is also added. This constant is called the bias. Note that the neural network in Figure 1-7 is called a fully connected layer because there are connections (with arrows) between all adjacent neurons.

[[[00000000000000001665---148f161685d3cd010c0454eeca769270f675db2e2344e69981ac48ecae96e98f]]]The network in Figure 1-7 has a total of three layers, but there are actually two layers with weights. In this book, we call such a network a “two-layer neural network”. In some literature, the network in Figure 1-7 is composed of three layers, so it is sometimes called a &quot;three-layer neural network.&quot;

[[[00000000000000001666---aea68f123758c928fac6d8aad3f5e917d74699dc32ba427519890ea7a27b48b1]]]Now, let&#39;s express the calculations performed by the neural network in Figure 1-7 using formulas. Here, the input layer data is represented by (x1, x2), the weights are w11 and w21, and the bias is b1. Then the topmost hidden layer neuron shown in Figure 1-7 is computed as follows:

[[[00000000000000001667---f8ec2740e12469d56e206c3842b1dcc9534be3acecb4351a87e5bc5958789d92]]]Hidden layer neurons are computed by weighted sums, as in equation (1.2). After that, while changing the weight and bias values, the calculation of formula (1.2) is repeated for the number of neurons. Then you can find the values of all neurons in the hidden layer.

[[[00000000000000001668---b742a1003fbab8fcc6233ad999765883effadda517bcb0621e8b57217a65f9c7]]]Weights and biases each have subscripts, but the rules for subscripting—such as what rules set the subscripts to 11, 12, etc.—are irrelevant. The important thing is that it is computed as a &quot;weighted sum&quot;, and that it can be computed together as a matrix product. In fact, the fully connected layer transformation can be written collectively as a product of matrices:

[[[00000000000000001669---acc8d115aeebdfeb45d7eb562c5637a27c7068f452fa68ced40ad8dab6fe1a11]]]Here the hidden layer neurons are grouped as (h1, h2, h3, h4), which can be viewed as a 1x4 matrix (or treated as a &quot;row vector&quot;). Also the input is (x1, x2), which is a 1x2 matrix. And weights correspond to 2x4 matrices and biases to 1x4 matrices. Then equation (1.3) can be simplified and written as

[[[00000000000000001670---8c9155130f7fbd05999d36b3354d420e9533ac84a55ad64067a5fc2100539d6c]]]Here x is the input, h is the hidden layer neuron, W is the weight, and b is the bias. These are all matrices. At this time, if we pay attention to the shape of the matrix in formula (1.4), we can see that the transformation is as follows.

[[[00000000000000001671---67495ed3570eba9344a606639597d782c45d3af3f57013bcd3f6f4f783f51aa1]]]
Figure 1-8 Shape check: check that the numbers of elements in corresponding dimensions match (bias omitted)


[[[00000000000000001672---713a22c6fd7ddecdf3244119c80db16063a18136c302554df314cb8de2829f99]]]Matrix multiplication matches the number of elements in corresponding dimensions, as shown in Figure 1-8. By looking at the shape of the matrix like this, we can confirm that it is the correct transformation.

[[[00000000000000001673---4dfe436f9a6b603123ea6c941749c2d0f90fa110919233df677babbba93c8550]]]Matrix shape checking is important in matrix multiplication. That way you can see if it&#39;s a correct calculation—or at least, if it holds up as a calculation.

[[[00000000000000001674---6dbcfbe50651416a64620cbf7a24fbc962d42df8d9c9bd1006b5adc8957f303a]]]Now we can calculate the transformation by the fully connected layer collectively as a matrix. However, the conversion done here is only for one sample data (input data). In the field of neural networks, inference and learning are performed simultaneously on multiple sample data—this is called a “mini-batch”. To do that, store separate sample data in each row of matrix x. For example, if N sample data are collectively processed as a mini-batch, focusing on the shape of the matrix, the transition is as follows.

[[[00000000000000001675---c255a799f5ca0859c05247c838c6dd52a991eb97bb424254640dd5314afd26e8]]]
Figure 1-9 Shape check: mini-batch version of matrix multiplication (bias omitted)


[[[00000000000000001676---21da93a7360889e41642f95427191c80843f95697a833d72a9a725aacd40b8cb]]]As shown in Figure 1-9, the shape check shows that each batch has been converted correctly. At this time, N sample data are collectively transformed by the fully connected layer, and N neurons are collectively calculated in the hidden layer. Let&#39;s write a mini-batch version of a fully connected layer transformation in Python.

[[[00000000000000001677---95fb00d9a8a4eb3cf3d81f7d1afbf84a34ba065e3b73037d31dd98446bbb01f0]]]   # weight &gt;&gt;&gt; 

[[[00000000000000001678---63cc77999a2c6d1cf39f2684a388f664cfc921568f4dcf87786dfe1ef516bc1e]]]      # Bias &gt;&gt;&gt; 

[[[00000000000000001679---7d6a6b429305d73ecb5e46959c87c887129b72c23705d62bb97191514c68e7fb]]]   # enter &gt;&gt;&gt; 

[[[00000000000000001680---a3b979dc850592132da775ae541acc6789a7b1b8df7182f54d3958171c73abd1]]]In this example, 10 sample data are individually transformed by a fully connected layer. Then the first dimension of x corresponds to each sample data. For example, x[0] is the 0th input data, x[1] is the 1st input data, and so on. Similarly, h[0] is stored in the 0th data hidden layer neuron, h[1] is stored in the 1st data hidden layer neuron, and so on.

[[[00000000000000001681---4386967eb7c20cd8646fc9b18c6e2b9609e9f0f558c4f2bb1f0486726217199b]]]In the code above, broadcasting works in the addition of the bias b1. The shape of b1 is (4,), but it is automatically duplicated so that it has the shape of (10, 4).

[[[00000000000000001682---ee40f559edb95784d688e247d217f707f84594fc6edebf74ee6042a9fb119d9c]]]Now, a transformation with a fully connected layer is a &quot;linear&quot; transformation. The activation function gives this a &quot;non-linear&quot; effect. More precisely, we can increase the expressive power of neural networks by using non-linear activation functions. There are various activation functions, but here we use the sigmoid function expressed by equation (1.5).

[[[00000000000000001683---8c139abce9d9ef7fb56f314c50ed0c4dc165585ffc5cba18f7b27cddbe83b058]]]This sigmoid function is an S-curve function as shown in Figure 1-10.

[[[00000000000000001684---1058f25c4b061996df774012b78310b40b2b8a353c6fc432e9d8fb3acdcf8c5f]]]
Figure 1-10 Graph of Sigmoid Function


[[[00000000000000001685---12fb2bc5f6e310d265dff11e8d2485a5802fff3710be9e7f3fa5d8a96dc8c3ff]]]The sigmoid function takes any real number as input and outputs a real number between 0 and 1. Let&#39;s quickly implement this sigmoid function in Python.

[[[00000000000000001686---a65425ec25a2fca1e63f765ab00691a55895d9c24514d2500168b78cb0a02a54]]]This is a direct implementation of formula (1.5), and should not be particularly difficult. Now, use this sigmoid function to transform the neurons in the hidden layer.

[[[00000000000000001687---8c3388ef62f53a142883d71b51dd627717f97c910e829abd00751a8adc681e15]]]Now we have a non-linear transformation with the sigmoid function. We then transform the output of this activation function, a (which is called the &quot;activation&quot;), by yet another fully connected layer. Here, there are 4 neurons in the hidden layer and 3 neurons in the output layer, so the weight matrix used for the fully connected layer must be set to a 4x3 shape. Now we can get the output layer neurons. The above is the inference of the neural network. Let&#39;s summarize what we&#39;ve learned so far and write it in Python.

[[[00000000000000001688---9e2a0bcf0897b2d1e70efa203fe49d1caa3c51d7527e74bdf63fcd1222cce059]]]Here the shape of x is (10, 2). This means that the 2D data are grouped into 10 mini-batches. And the shape of the final output s will be (10, 3). Again, this means that 10 data were processed together and each data was transformed into 3D data.

[[[00000000000000001689---0da76923ec9593989131ccc995b172f24cce13ed5763998893dec94931dc3fc7]]]Now, the neural network above outputs 3D data. Therefore, by using the values of each dimension, 3-class classification can be performed. In that case, each dimension of the output 3D vector corresponds to a &quot;score&quot; for each class (first neuron for first class, second neuron for second class, etc.). To actually perform the classification, find the largest value in the neuron in the output layer and take the class corresponding to that neuron as the result.

[[[00000000000000001690---911eb724c7eb2f9fa09948c10fa3cc82e5cdf1351620ee63855b075a255776a8]]]A score is a value before it becomes a &quot;probability&quot;. The higher the score value, the higher the probability of the class corresponding to that neuron. As we will see later, if we put the scores into the Softmax function, we get the probabilities.

[[[00000000000000001691---286402839a00ed3e2392d0f91b20fa1fae854defe0cd03de6ab0da7eb74b8ec0]]]The above is the implementation of the inference process of the neural network. Next, we will implement the processing performed here using Python classes as &quot;layers&quot;.

[[[00000000000000001692---76c6e6f3ab312e52042e78c68d9db1543f2168e913258273349533231876c6d4]]]Implementing Classification as Layers and Forward Propagation

[[[00000000000000001693---f972fec7a53193b0b3339a5476a7d822f2e4a6c45a1a38352b38da1e1437547c]]]Now, let&#39;s implement the processing performed by the neural network as &quot;layers&quot;. Here, the transformation by fully connected layer is implemented as Affine layer, and the transformation by sigmoid function is implemented as Sigmoid layer. Since the transformation by the fully connected layer is equivalent to the affine transformation in the field of geometry, we call it the affine layer. Each layer will be implemented as a Python class, and the main transformation will be implemented with the method name forward().

[[[00000000000000001694---17d1e982e8b1c23c8daf44df640c5f9880015ac972e8b2798b263513addf3a26]]]The processing performed by neural network inference is equivalent to forward propagation of neural networks. Forward propagation, as the term suggests, is propagation from the input layer to the output layer. At this time, each layer that makes up the neural network propagates the processing results sequentially from the input to the output direction. Later we will train a neural network, where we propagate data (gradients) in the opposite direction to forward propagation. It&#39;s called backpropagation.

[[[00000000000000001695---264f24ff6d9896efd1c2791ea8b7a562a147360037bfb118a0ed5c340eafb333]]]Various layers appear in the neural network. We implement it as a Python class. By doing such modularization, you can build a network like combining Lego blocks. This document establishes the following &quot;implementation rules&quot; for implementing such layers.

[[[00000000000000001696---c083c3870e070ed2de88f5a8276e6b6009037fa81a9403ef1cbb27884d85dc5e]]]All layers have forward() and backward() as methods

[[[00000000000000001697---9f4c53357584e1a70c1a588e62dd0d8b71afa81737f8ed454760ffecf6f47d26]]]All layers have params and grads as instance variables

[[[00000000000000001698---9ef16169057da2bf59a50f03b79662ee6d5dcae55dc239a56eec98a927992ec0]]]A brief description of this implementation rule. First, the forward() and backward() methods correspond to forward and backward propagation, respectively. In addition, params holds parameters such as weights and biases as a list (since there may be multiple parameters, it is a list). And grads holds the gradients of each parameter as a list corresponding to the parameters in params (more on gradients later). This is the &quot;implementation rule&quot; of this book.

[[[00000000000000001699---3ec21a725f6e7fb1457ca409e4eac96536ff1c89442938c17d24633cac2b50de]]]By following the above &quot;implementation rules&quot;, you can implement with good prospects. The reason for following such rules and their validity will become clear later.

[[[00000000000000001700---3b8e0f65b4886a98344c29bebc46ce99556342f25f64728c284a6ed7fd743e59]]]Since we are only considering the implementation of forward propagation here, we will focus only on the following points of the above &quot;implementation rules&quot;. One is to &quot;implement the forward() method in the layer&quot; and &quot;collect the parameters into the instance variable params&quot;. Implement layers according to these implementation rules. Let&#39;s start by implementing the Sigmoid layer. This can be implemented as follows (☞ ch01/forward_net.py).

[[[00000000000000001701---5a3413524f94a3759946872ea497bb6a1e9f66d0942780c70b5cb8eeb03352a9]]]As above, we implement the sigmoid function as a class and implement the main transformation process as a forward(x) method. Here, the Sigmoid layer has no parameters to learn, so we initialize the instance variable params with an empty list. Next, we show the implementation of the Affine layer, which is a fully connected layer (☞ ch01/forward_net.py).

[[[00000000000000001702---7e3cd55194c0b78dfcc8b2b31296047a5bd24ac7bd6cca3162f671a0d383e9de]]]Affine layers receive weights and biases at initialization. At this time, the parameters of the Affine layer are weight and bias (these two parameters are updated as the neural network learns). So I keep those two as a list in the instance variable params. After that, implement forward propagation processing by forward(x).

[[[00000000000000001703---256f187faa3029557515304ab2179b5ab0a940241adfa640e0706e362e92e27f]]]According to the &quot;implementation rules&quot; of this document, all layers must have the parameters to be learned in the instance variable params. Therefore, all parameters of a neural network can be easily grouped together, which makes it easier to update parameters or save parameters to a file.

[[[00000000000000001704---23a08bc09ce21de3954356e2e8a4777b6ea214d2c7e7e690e407e5b2a90d6eac]]]Now let&#39;s implement the neural network&#39;s inference process using the layers we implemented above. Here, we will implement the layered neural network shown in Figure 1-11.

[[[00000000000000001705---545cb4f11bd2985261ca9017e25d52bcf798d9b7a69e666aae80a29ce8e7f275]]]
Figure 1-11 Layer structure of the neural network to be implemented


[[[00000000000000001706---6df3eb4af696d6269ca4ac5911f13281ce3690b35c1ab9367525341f5162d030]]]As shown in Figure 1-11, here the input x goes through the Affine layer, the Sigmoid layer, and the Affine layer, and the output is the score s. We will implement this neural network as a class named TwoLayerNet and implement the main inference process with a method called predict(x).

[[[00000000000000001707---bd92731cda8fba581dd61ecb173d13c977382e994431b332b0d22b8caac7b068]]]To illustrate neural networks, we have so far used diagrams from the &quot;neuron&#39;s perspective&quot;, as shown in Figure 1-7. Figure 1-11, on the other hand, illustrates the network from a &quot;layer perspective&quot;.

[[[00000000000000001708---5bfc4e0c8fcb3b3a6e2f9db920dee5bcfb1f12a02b11df7b78133e566bd8033f]]]Now, I will show the implementation of TwoLayerNet (☞ ch01/forward_net.py).

[[[00000000000000001709---48112181902a933c61a82baec5265e37f392b6d630e0f2ca2bea54a97c6390d9]]]# Initialize weights and biases

[[[00000000000000001710---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000001711---d7695bddc2f42001523bbdae4adeb9a76b42a6cdcb66c05cf54eb78e770a89cc]]]# Collect all weights in a list

[[[00000000000000001712---cd06a442499bc15c344f19fbb8f496bd4a65051627779c049dc530e446aa0e46]]]The initializer for this class first initializes the weights and creates three layers. It also stores the weight parameters to be learned together in a params list. Here, the training parameters are held in the instance variable params of each layer, so all you have to do is concatenate them. All training parameters now exist in the TwoLayerNet&#39;s params variable. In this way, by gathering parameters into a single list, you can easily perform &quot;update parameters&quot; and &quot;save parameters&quot;.

[[[00000000000000001713---0737cb88b73dcf92243a8969367dc369eddc407a86175a41d547c3c87f088440]]]In Python, lists can be concatenated using the + operator. Here&#39;s one simple example:

[[[00000000000000001714---6329f1d6c12c144d549fb232dedfcef040c0a682a4bd9fa7404974cfc9be162f]]]Addition of lists concatenates them, as shown here. In the TwoLayerNet implementation above, we combined all the training parameters into a single list by adding the params lists of each layer. Now, let&#39;s use the TwoLayerNet class to perform neural network inference.

[[[00000000000000001715---4f2a8967af29a81211577a1a040b32500398fba7a857ef19b6dfae7a7356fcfd]]]Now we have calculated the score (s) for the input data x. By classifying them as layers in this way, neural networks can be easily implemented. Also, at this time, model.params contains a single list of parameters to be learned, making it easier to train the neural network that follows.

[[[00000000000000001716---8b2a016745b9a6d8d52b88da26ed15e8f03f5cc04d2e5dc3d05b71130e063b97]]]Neural network learning

[[[00000000000000001717---90d43bb2bf2887295e6b19ac9db97c73bc15a84be1854efa6a1408cd61e4dd5c]]]A neural network cannot make “good inferences” without learning. Therefore, the general flow is to first perform learning and then use the learned parameters to make inferences. Inference is the task of finding answers to problems such as multi-class classification, as seen in the previous section. Training a neural network, on the other hand, is a task of finding optimal parameters. In this section, we will look at training neural networks.

[[[00000000000000001718---bbf490bd1b57f9cdb0a6edb373d551b63c4a8296a71014081fa232d1b4889be1]]]loss function

[[[00000000000000001719---4c115929c15bcd84d6bce993ae4019c3ae7d56b6e08ee11e0e490c5f354126d1]]]Learning a neural network requires a &quot;metric&quot; to know how well it is learning. Commonly it is called a loss. Loss is a measure of a neural network&#39;s performance at some point in its learning phase. The loss is calculated as a scalar (single numerical value) of how bad it is based on the teacher data -- the correct data given in learning -- and the prediction results of the neural network.

[[[00000000000000001720---638d6bea1e6ae1a426a6ed86d6d540b856553a599e7ba6c6a247466f912850c3]]]Use the loss function to find the loss of a neural network. For neural networks that perform multi-class classification, the Cross Entropy Error is often used as the loss function. At this time, the cross entropy error is obtained from the &quot;probability&quot; and &quot;teacher label&quot; of each class output by the neural network.

[[[00000000000000001721---478ff45f9e85ba7e682bacf7084ba1c7b024e137bfa02201c5183859ac1ff549]]]Now let&#39;s find the loss for the neural networks we&#39;ve been working with so far. Here, a Softmax layer and a Cross Entropy Error layer are newly added to the network up to the previous section (the Softmax layer is the softmax function, and the Cross Entropy Error layer is the layer that obtains the cross entropy error). If you draw the network configuration at this time from a &quot;layer perspective&quot;, it will look like Figure 1-12.

[[[00000000000000001722---1d333b0fbb8fe682ea22651d253a1b50a5bdf69a923897597fea761a287894d7]]]
Figure 1-12 Layer configuration of neural network with loss function applied


[[[00000000000000001723---e874a21ea6ad8c8dc2115a0f035eaeebfb87def47ae4cc9898492486750341d2]]]In Figure 1-12, x is the input data, t is the teacher label, and L is the loss. At this time, the output of the Softmax layer is probability, and the probability and teacher label are input to the Cross Entropy Error layer.

[[[00000000000000001724---ea7b3b4aa5227fe711f4eb5f080d887f4a28124e633164de27951c1f38607907]]]Now let&#39;s talk about the Softmax function and the cross-entropy error. First, the Softmax function is expressed by the following formula.

[[[00000000000000001725---5016113f9eca5870af59358ddd5370dc57fbd86e24ee0258cecbd9fe6f3c7138]]]Assuming that there are a total of n outputs, the calculation formula for the k-th output yk is shown here. This yk is the output of the Softmax function corresponding to the kth class. As shown in equation (1.6), the numerator of the softmax function consists of the exponential function of the score sk and the denominator consists of the sum of the exponential functions of all input signals.

[[[00000000000000001726---614384eebd1c62a91cd7cc434a33f806bd68bc9b0804227588912a4a8f45f59e]]]Each element of the output of the Softmax function is a real number between 0.0 and 1.0. And the sum of all its elements equals 1.0. So the output of softmax can be interpreted as a &quot;probability&quot;. The output of this softmax, the &quot;probability&quot;, is then input into the cross-entropy error. Then the cross-entropy error is expressed by the following formula.

[[[00000000000000001727---01f5677c0a235290f7b4659c4c3e21d3c13e54827bf701180305b4fdafa1eaf8]]]where tk is the teacher label corresponding to the kth class. log represents the logarithm to the base e of Napier&#39;s number (this is correctly written as loge). Teacher labels are expressed as one-hot vectors, such as t = (0, 0, 1).

[[[00000000000000001728---596f6c02374bf2aa65419ca9fe122e314f5ab4f790f35b4f19e4e2f7d430e692]]]A one-hot vector is a vector in which one element is 1 and the others are 0. Here, the 1 element corresponds to the correct class. So equation (1.7) is effectively just computing the natural logarithm (log) of the output corresponding to the element with the correct label of 1.

[[[00000000000000001729---39620e689a837aa3a0e4619caadd0c0120c0fb2cf9cfc90e10f1d1c8fb7170df]]]Also, when considering mini-batch processing, the cross-entropy error is given by Here, assuming that there are N pieces of data, tnk means the value of the kth dimension of the nth piece of data. ynk is the neural network output and tnk is the teacher label.

[[[00000000000000001730---0ba6040d5fdd0e01470bab6866b9cbc7891be02a869c2af270a4c887df815e84]]]Equation (1.8) looks a little complicated, but it is simply extending Equation (1.7), which expresses the loss function for one data, to N data. However, in formula (1.8), the &quot;average loss function&quot; per piece is obtained by dividing by N. Averaging like that always gives a consistent metric, regardless of mini-batch size.

[[[00000000000000001731---63ad63b497ecea3ef411f955dd1896186db2167ffc430a50a50ed684bac0fe81]]]In this book, we will implement the layer that computes the softmax function and the cross-entropy error as a Softmax with Loss layer (combining the two layers simplifies the computation of backpropagation). So our neural network (at training time) has a layered structure like Figure 1-13.

[[[00000000000000001732---717cf5250bc3ed74b70f39e282282aa5c148e963dec94f1a17789135dbbfdb9a]]]
Figure 1-13 Output loss using Softmax with Loss layer


[[[00000000000000001733---312dad364487898c1707f407385d3e75e5813e924a7718bf565f86d56f8a7493]]]We will use the Softmax with Loss layer, as shown in Figure 1-13. We omit the description of its implementation here. The implementation file is in common/layers.py, so if you&#39;re interested, have a look. In addition, the Softmax with Loss layer is explained in detail in &quot;4.2 Loss function&quot; of the previous work &quot;Deep Learning from scratch&quot;.

[[[00000000000000001734---5a43dbc0bc7f3007cb9e21c80013a2f8c2217d50945a98bea7bc8438fe8169a1]]]derivative and gradient

[[[00000000000000001735---677241859a8f4ba44a1e4a0fc50f7e1017f462bb9cfc4d66e71e62c2c2b211cd]]]The goal of neural network training is to find parameters that minimize the loss. What is important here is &quot;differentiation&quot; and &quot;gradient&quot;. Here we briefly discuss derivatives and gradients.

[[[00000000000000001736---b43021233c9ac5185c07fffe0d038717e52be529c0023da4430515268b2427d5]]]Now, let&#39;s say we have a function y = f(x). Then the derivative of y with respect to x is written as What this means is how much the value of y changes when we change the value of x by a small amount—more precisely, when we make that small change as small as possible. It is the &quot;degree of change&quot; that

[[[00000000000000001737---e71228da6a063727c8ed25dd27c0030a7c2c209471431bdf060404c893500dda]]]For example, here we have a function y = x2. Then the derivative of this function can be found analytically, which is And the result of this differentiation represents the degree of change at each x. In fact it corresponds to the &quot;slope&quot; of the function as shown in Figure 1-14.

[[[00000000000000001738---179c47dede2053361ddaeb2b6bc73ca8a78960ddfff19aa025bfab2364187d6d]]]
Figure 1-14 The derivative of y = x2 represents the slope at each x


[[[00000000000000001739---ca2e4da510590144cc7afde19b60567562a0919fb3f6f884895cc7dc0fb3018a]]]In Figure 1-14, the derivative was obtained for one variable x, but the derivative can be obtained for multiple variables (multivariables) in the same way. For example, suppose we have a function L = f(x) where L is a scalar and x is a vector. Then the derivative of L with respect to xi (i-th element of x) can be written as And we can also find the derivative of the other elements of the vector, which can be summarized as follows.

[[[00000000000000001740---e667f468cee1fcbedc304df6fdbeb22c30b03cb2ddedbc195b83669850e76cad]]]In this way, a collection of differentials with respect to each element of a vector is called a gradient.

[[[00000000000000001741---9dfa01da3035d279cbafaeeb125a3b4f03a6d06cb3f1278c53619c59ae334b64]]]You can also think of gradients for matrices in the same way as for vectors. For example, if W is an m×n matrix, the gradient of the function L=g(W) can be summarized as follows.

[[[00000000000000001742---9c05b00b2c6e03cc2716086769097645377d45562512120ff3fa206358ccf787]]]The gradient of L with respect to W can be summarized as a matrix as shown in equation (1.10) (more precisely, we define the gradient of the matrix as above). The important point here is that it has the same shape as W. By using this property that the matrix and its gradient have the same shape, it is easy to update parameters and implement chain rules (chain rules will be explained in detail shortly).

[[[00000000000000001743---482b557a5517f29eaa12b16e7c94ca687765a81782a62a320822b4c954f84ec4]]]The term &quot;gradient&quot; as used in this book is strictly different from &quot;gradient&quot; as used in mathematics. When we say &quot;gradient&quot; in mathematics, we are confined to differentiating with respect to a vector. On the other hand, in the field of deep learning, it is common to define the differentiation of matrices and tensors as well and call it the &quot;gradient&quot;.

[[[00000000000000001744---bd82cdebcfa93a457fb74ae3ebbd8f636dc84bdee4a181dc36b2919cd9239e04]]]chain rule

[[[00000000000000001745---223bf63f997466d9b9b430bd1011da5046449269e9c73ebeb9dee393a110bf2d]]]A neural network during training outputs a loss when given training data. What we want here is the gradient of the loss with respect to each parameter. Once you have that gradient, you can use it to do a parameter update. So how can we find the gradient of a neural network? This is where back-propagation comes into play.

[[[00000000000000001746---151e088ffbcf019b556c630fea3a8bb896a7c886ef6c75286ef4f763925c5073]]]The key to understanding backpropagation is the chain rule. A chain rule is the law of differentiation for a composite function (a composite function is a function composed of multiple functions).

[[[00000000000000001747---caed70c292d60fe409dbe72ee910c19a61bb5845073b344af469ff16c49d8d92]]]Now let&#39;s learn about chain rules. As an example, consider two functions y = f(x) and z = g(y). And suppose the final output z is computed by two functions, expressed as z = g(f(x)). Then the derivative of this composite function—the derivative of z with respect to x—is given by

[[[00000000000000001748---a556d8e6bd33ea8709413e7e20cba02c1a253fcc9552b3f0a91c73fc2be6d107]]]As Equation (1.11) shows, the derivative of z with respect to x is the product of the derivative of y = f(x) and the derivative of z = g(y). This is the chain rule. This chain rule is important because no matter how complex the function we are dealing with—no matter how many functions are linked together—the derivative can be found by differentiating the individual functions. In other words, if we can compute the local derivative of each function, we can obtain the final global derivative by multiplying them.

[[[00000000000000001749---c535e66cfbeb44988fc54ad92c02d34e27fa2d930c6c060e4b4504213a0c2057]]]A neural network can be thought of as a chain of &quot;functions&quot;. Backpropagation efficiently uses chain rules to find gradients for multiple functions (neural networks).

[[[00000000000000001750---f71c94f8c8345630d65d48dad020ef878815b61f35855ce7699e3412e00bc47b]]]calculation graph

[[[00000000000000001751---fb0b7f8a568c7d1e2f5a7f4615efe6da58f2ac4a95ddc08529a16ed43390fd53]]]We will now look at the error backpropagation method, but here we will explain the &quot;computation graph&quot; as a preliminary preparation. A computation graph is a visual representation of a computation. An example of a computational graph is shown in Figure 1-15.

[[[00000000000000001752---450baff90ac1ed9b49f41fd18ff1025cf00c0c3268f24cbe1bd02ab82c95a420]]]
Figure 1-15 Computation graph representing z = x + y


[[[00000000000000001753---f2a0de17fdee07bb8e7de77aeb519e9f965cb0d116d1154cc6b59d67de1c4282]]]A computation graph is illustrated with nodes and arrows, as shown in Figure 1-15. In this case, let&#39;s represent the addition with a &quot;+&quot; node and write the variables x and y above each arrow. Thus, in a computation graph, operations are represented by nodes, and their results flow in order—from left to right in this example. This is the &quot;forward propagation&quot; of the computational graph.

[[[00000000000000001754---3fda79f77b19845dd6af8c88e2bd0d570cce63b014b3e6ff71555d5a55dc262e]]]Calculation graphs allow you to visually understand your calculations. Furthermore, its gradient can also be found intuitively. The key here is that the gradient propagates in the opposite direction to the forward propagation. This backward propagation is called &quot;backpropagation&quot;.

[[[00000000000000001755---63d947ca999c32f9de1aa99d7b9001ca13f72375cef797a4aef8def1ea7d3a42]]]Before discussing backpropagation, I would like to clarify more the big picture in which backpropagation takes place. Here we are dealing with the calculation z = x + y, but we assume that there is &quot;some calculation&quot; before and after that calculation (Figure 1-16). And we assume that the final output is L, which is a scalar (in neural network training, the final output of the computational graph is the loss, which is a scalar).

[[[00000000000000001756---5518fb5245c6f151f667b7307039016386e65da3e8779aa411fc7f9ee8d0e3ce]]]
Figure 1-16 Addition nodes form part of a &quot;complex computation&quot;


[[[00000000000000001757---5ad3fb58683f035434c8b03507981848a2b9e9ecf98e2dba85387f7987068618]]]Our goal is to find the derivative (slope) of L with respect to each variable. Then the backpropagation of the computational graph can be written as in Figure 1-17.

[[[00000000000000001758---ac231962048c90c699a2dc01090f750b19c0dd682e0f2c5018c4192e31a1bdd9]]]
Figure 1-17 Backpropagation of a Computational Graph


[[[00000000000000001759---0e6d1c0801b14b90f22010bc0a8d47742eb593db3f950b70c1e459e23a8062c0]]]Backpropagation is drawn with a thick red arrow, and the &quot;value to propagate&quot; is written below the arrow, as shown in Figure 1-17. In this case, the &quot;propagated values&quot; are the derivatives of the final output L with respect to each variable. In this example, the derivative with respect to z is and the derivatives with respect to x and y are respectively.

[[[00000000000000001760---20043664e0ea60f9f85904ca03c419e066e5532aa3e5dbf0c777410c3dee1a7a]]]And this is where chainrules come into play. According to the chain rule we reviewed earlier, the value of the derivative flowing in backpropagation is calculated by multiplying the derivative flowing from upstream by the local derivative of each operation node (here &quot;upstream&quot; refers to the output side ). So in the example above, and .

[[[00000000000000001761---78dbe222a027671294145e999d9f4a3e9f262c12017020a4d4b44579d33d207d]]]Now, here we are dealing with an operation with the addition node z = x + y. At this time, and respectively (analytically) obtained. Therefore, the summation node multiplies the propagation from upstream by 1 to propagate the gradient downstream, as shown in Figure 1-18. In other words, the slope from the upstream will just flow as it is.

[[[00000000000000001762---3722f060d842f4a15ae84a416d7923ea4a8a67eeb34abe4ec3c6b07edb3b5d01]]]
Figure 1-18 Forward Propagation (Left) and Back Propagation (Right) of a Sum Node


[[[00000000000000001763---80d6c8e5577736395897313df539fedc18106ba5004285e66d4879f59b44f72a]]]The computation graph thus visually represents the computation. And by looking at the flow of gradients by backpropagation, it can be useful to understand the derivation process.

[[[00000000000000001764---5525a4a7840101897a7a61525894a481bd757ca234c72391e8e0a59fef9ad9cf]]]In addition to the &quot;addition node&quot; seen here, various operations can be considered for operation nodes that construct computation graphs. Next, we will introduce some typical operation nodes.

[[[00000000000000001765---47fdb0ce1af5c933eb289b48b539799bb52022135636bb00767d7dbd6fa47a69]]]multiplication node

[[[00000000000000001766---04d43f655fdb6d926aed9c4203d6296ec16e86e6e9c6806b59ee037784701201]]]The multiplication node is the calculation z = x x y. At this time, the derivative can be obtained as Therefore, as shown in Figure 1-19, the backpropagation of the multiplication node multiplies the &quot;gradient transmitted from the upstream&quot; by the &quot;value with the inputs exchanged during forward propagation&quot;.

[[[00000000000000001767---c63c14b47b8b0fd42b7be0ebdd6e88053cebb95d9b58e8f830877987936e6df3]]]
Figure 1-19 Forward Propagation (Left) and Back Propagation (Right) of a Multiplication Node


[[[00000000000000001768---e5de74c4c8e3b5dfb557b8275d61ce3b6a1cd54dbaf93e9063426db9988213a0]]]In addition, in the explanation of addition nodes and multiplication nodes that we have seen so far, the data flowing through the nodes was &quot;one variable&quot;. However, it doesn&#39;t matter if the data flowing is not one variable, but multi-variable - vectors, matrices, tensors. If the data flowing through the addition node (or multiplication node) is a tensor, just compute each element of the tensor independently. That is, in that case, we perform &quot;element-wise operations&quot; independently of the other elements of the tensor.

[[[00000000000000001769---258f169547b3e9660d300ccdc6d5d8a3795a4e15add39e313a40b1edbc7f82fe]]]branch node

[[[00000000000000001770---29f75dedfab8e25d9365abe70c707e498c047b8b69b1ef02964eb1a69abd372f]]]A fork node is a node that branches, as shown in Figure 1-20.

[[[00000000000000001771---13fe8a39a0683f9939f2d1731c2704da1804baa8ead3ba453e73a65ccb6032ec]]]
Figure 1-20 Forward propagation (left figure) and backward propagation (right figure) of a branch node


[[[00000000000000001772---e207ea6b0b208c17a00c4a4be5d3def51a8334c49e54b3f30ea9cf0139c7954d]]]A fork node is not drawn exactly as a node, but simply as a line splitting in two. At this time, the same value is copied and branched. Therefore, a branch node can also be called a &quot;copy node&quot;. The backpropagation is then the &quot;sum&quot; of the gradients from upstream, as shown in Figure 1-20.

[[[00000000000000001773---f8da1be01159a09cbad439984d6032662c15b579becb711a198d3c526b36c906]]]Repeat node

[[[00000000000000001774---c9ca3dd3d561e7bfcca8f6c6e1d0f82a2d178d7faebb70496d369a991ad88b7c]]]The branch node was two branches, but if you generalize this, you can think of N branches (copies). We&#39;ll call it the Repeat node here. Now, let&#39;s write an example of a Repeat node in a computation graph.

[[[00000000000000001775---c3a7525c137d7ba846a436ec00465cec97b3ffe4c997793947433d0f3970c26b]]]
Figure 1-21 Forward Propagation (Top) and Back Propagation (Bottom) of Repeat Node


[[[00000000000000001776---9e017d7fdbc91fb4eeea2f9a4f0ffc53ded1a533be5d72e3b8d3578fc6506ab9]]]As shown in Figure 1-21, here is an example of replicating an array of length D by N times. Since this Repeat node can be viewed as N branch nodes, its backpropagation can be found as the sum of N gradients. A practical implementation example can be written as follows:

[[[00000000000000001777---4958ad87718b3e6533e8afd103bb0c757eb8c8eb2600be00add9a49cc9acdb59]]]               # enter &gt;&gt;&gt; 

[[[00000000000000001778---a40c7f800ae1b973273bf90819499aa619f18f5461e7d08488ae2870d5b4d576]]]              # Temporary Gradient &gt;&gt;&gt; 

[[[00000000000000001779---7780c6357988b23f9fa4197319eadb491a8b7b382ec577afca3a5386c148b3f3]]]Here, the element is duplicated by the np.repeat() method. In the above example, the array x is replicated N times, but by specifying axis, you can specify along which axis to replicate. Also, in backpropagation, we need to find the sum, so we use NumPy&#39;s sum() method. Again, by specifying the argument axis, you can specify along which axis the sum is to be calculated. Furthermore, by specifying keepdims=True in the argument, the number of dimensions of the two-dimensional array is maintained. In the example above, the shape of the result of np.sum() will be (1, D) if keepdims=True and (D,) if keepdims=False.

[[[00000000000000001780---958479962beb050309c86857b24a5e829795c1c70672ea6bcd475f3c4dc42c55]]]NumPy&#39;s broadcast duplicates the elements of an array. This can be represented using a Repeat node.

[[[00000000000000001781---d88e2724e6e0f796cd8a390d6d3fcd1d11b1493e01e6a817b4496b6f0ec8d6a8]]]Sum node

[[[00000000000000001782---d88b96dfabcc468b3363fc68e8bb25dd371a2646e4aa77c585d9df460f95760d]]]The Sum node is a general purpose addition node. Here, we will consider a calculation to find the sum of the N×D array along the 0th axis. Then the forward and backward propagation of the Sum node looks like Figure 1-22.

[[[00000000000000001783---e9f8642f931c69794efe996549567beb56d2caf628a6f6aa3227668048e864ea]]]
Figure 1-22 Forward propagation (upper figure) and back propagation (lower figure) of the Sum node


[[[00000000000000001784---cb64aaf28de891fd60220882bdb413f2f7e40ea27e024c19d329ec4a5c1ec3fc]]]Backpropagation of the Sum node distributes the gradient from upstream to all arrows, as shown in Figure 1-22. This is a natural extension of backpropagation for sum nodes. Now, like the Repeat node, let&#39;s also show an example implementation of the Sum node. It looks like this:

[[[00000000000000001785---70b9f4d35281f9d3422c524c4c50f026b5b62caa565c0bb8055d74827f848226]]]             # enter &gt;&gt;&gt; 

[[[00000000000000001786---269f81908856bef77f189d16461cfb11c5ebe33a9560308c42ef7dc2a2c06491]]]            # Temporary Gradient &gt;&gt;&gt; 

[[[00000000000000001787---dfd79c479c5d5d6ba4b42187fcf2338b715c256d3df00313510bfbb0146dd48b]]]As you can see, the forward propagation of the Sum node can be implemented with the np.sum() method and the back propagation with the np.repeat() method. An interesting point here is that Sum and Repeat nodes are &quot;inversely related&quot; to each other. The inverse relationship means that forward propagation of Sum nodes is equivalent to backpropagation of Repeat nodes, and backpropagation of Sum nodes is equivalent to forward propagation of Repeat nodes.

[[[00000000000000001788---dfe2839fff13e311bff4cf1a504d867050602e0aab550b96a7e14c3821e84c28]]]MatMul node

[[[00000000000000001789---7c4279eb8f7d1258728b6a91ac9c32e34426e506097627dbaf1a15f93bb027c3]]]In this book, we refer to matrix multiplication as a MatMul node (short for &quot;Matrix Multiply&quot;). Backpropagation in the MatMul node is a bit complicated, so I&#39;ll give a general explanation here, followed by an intuitive one.

[[[00000000000000001790---978b2225518588a59f57740b43d9f4b08e51bfb97ef3e2445e09c061c586c471]]]To explain the MatMul node, consider the computation y = xW. Here, the shapes of x, W, and y are 1×D, D×H, and 1×H, respectively (Figure 1-23).

[[[00000000000000001791---072e528854a512df1827f40622911a84b5392fbcd2dcfe4c56c4e23572189fee]]]
Figure 1-23 Forward Propagation of MatMul Nodes: Shape Above Each Variable


[[[00000000000000001792---697d69703ed3dcb95ae44924587738a3e494ddfebace445302b882aa91d20ef1]]]Then the derivative of x with respect to the i-th element is obtained as follows.

[[[00000000000000001793---855e345aab16b2ef51e0b58bf894ff4cffcfa9a99da58930b6302d35950778e6]]]Equation (1.12) expresses the &quot;degree of change&quot; of how much L changes when xi is changed (slightly). Now when we change xi, all the elements of vector y change. And through changing each element of y, we end up changing L. Therefore, there are multiple chain rule paths from xi to L, and the sum is

[[[00000000000000001794---5615a34db291ea62ad1d384a9537290e11eff3f6036993d26f1a8d5430ba4b4e]]]Now for equation (1.12), this can still be simplified. To do so, use the fact that , and substitute it into equation (1.12).

[[[00000000000000001795---1e7125d31bc9d41ae020475b796f02a5a12d6b64d51047658ec4d4d8fc9922cd]]]From equation (1.13), we can see that is determined by the inner product of &quot;vector&quot; and &quot;i-th row vector of W&quot;. From this relationship, the following formula can be derived.

[[[00000000000000001796---05b6b7d41fa1cb9e9cb3cc0558a62d8933f7d1e3bce6df01748ffb76b33b0887]]]is obtained at once by matrix multiplication, as equation (1.14) shows. Here, T in WT indicates that the matrix is transposed. Now, let&#39;s perform a &quot;shape check&quot; on formula (1.14). The result should look like Figure 1-24.

[[[00000000000000001797---d7fd891c2501b42ac89062a5b98b2d4852804147d717b9ff161e5d57237c9684]]]
Figure 1-24 Shape Check for Matrix Product


[[[00000000000000001798---5f20d86706efd231680425af445bf6706c94627762a1cd75ee2a69c19796b2ad]]]From Figure 1-24, we can see that the transition of the shape of the matrix is correct. This confirms that formula (1.14) is a correct calculation. We can also take it backwards—that is, by making sure that consistency holds—to derive the formula (implementation) for backpropagation. To explain how to do this, consider again the calculation of the matrix product y = xW. However, this time, considering mini-batch processing, we assume that x contains N pieces of data. At this time, the shapes of x, W, and y are N×D, D×H, and N×H, respectively, and the backpropagation calculation graph is shown in Figure 1-25.

[[[00000000000000001799---daa21fccd18a65cf35328a35b2f484d8df6a5c21f89543790816e8c78feabc1b]]]
Figure 1-25 Backpropagation of MatMul Node


[[[00000000000000001800---9883295a3d731d2f6e14748a7c01fc81ae8464047439a9249d05f5c698fa372a]]]Now let&#39;s see how it can be calculated. Then the variables (matrices) involved in are the gradient from upstream and W. As to why W is relevant here, it is easier to understand if we consider backpropagation of multiplication. This is because the backpropagation of the multiplication used &quot;values with the inputs exchanged during forward propagation&quot;. In the same way, backpropagation of matrix products also uses &quot;matrices with the inputs exchanged during forward propagation&quot;. After that, we&#39;ll look at the shape of each matrix and construct the matrix product so that its consistency is preserved. This leads to the backpropagation of the matrix product, as shown in Figure 1-26.

[[[00000000000000001801---effe3c753654d4cc0ac09352fef045b63c52ef7f6b6339a6141df994f094955f]]]
Figure 1-26 Deriving the Backpropagation Equation by Checking the Matrix Shape


[[[00000000000000001802---b920d2121d3487756913ba143954b7e5c4e2f8e60806a3d854ec2075c6f10c29]]]By observing the shape of the matrices, we were able to construct the backpropagation formula for the matrix product, as shown in Figure 1-26. You have now led backpropagation for the MatMul node! Now let&#39;s implement MatMul nodes as layers. This can be implemented like this (☞ common/layers.py):

[[[00000000000000001803---7cc8ce39ac6dd850af9d18ba2e9539db6f1ea667ae88008eacb5e8ab2e96af12]]]The MatMul layer holds the parameters to learn in params. And correspondingly, we will store the gradients in grads. Backpropagation finds dx and dW and sets the gradient of the weights to the instance variable grads.

[[[00000000000000001804---94f2ee0a68c04e55475a9dd47180247c2049d7800365ca3101cd2b48cf5ebe21]]]Note that when setting the gradient values, we use a &quot;three-point reader&quot;, such as grads[0][...] = dW. By using this &quot;3-point reader&quot;, the memory location of the NumPy array is fixed, and the elements of the NumPy array are overwritten.

[[[00000000000000001805---029f2bcba2280e580d92da6b74807329cc959d1aa05f0b9f9b36dc5f1b553659]]]Something similar to the &quot;three-point reader&quot; can also be done by &quot;substitution&quot; of grads[0] = dW. On the other hand, the &quot;three-point leader&quot; will &quot;overwrite&quot; the NumPy array. This is the difference between a &quot;shallow copy&quot; and a &quot;deep copy&quot;. An assignment of grads[0] = dW is equivalent to a &quot;shallow copy&quot; and an overwrite of grads[0][...] = dW is equivalent to a &quot;deep copy&quot;.

[[[00000000000000001806---cf6a6da1d71c68a85c81faa09c52fad5a070c82eaa7b84088318beef4fb0b74c]]]The topic of &quot;three-point leader&quot; is getting a little complicated, so I&#39;ll explain with a specific example. Suppose I have two NumPy arrays a and b here.

[[[00000000000000001807---dccc8b7c440cd58ee211ed972ecd10ceecb3f50a938b1106bef70838d365c2e8]]]where a = b and a[...] = b, a is both assigned [4, 5, 6]. However, the location in memory pointed to by a is different. The actual (simplified) memory visualization looks like Figure 1-27.

[[[00000000000000001808---05cb3988d31213e098fdf92e8c4ebf95af0a56a2ebdd37051bda845be527689d]]]
Figure 1-27 Difference between a=b and a[...]=b: &quot;Three-point reader&quot; overwrites data, so the memory location pointed to by the variable does not change


[[[00000000000000001809---2e947b99a06dab5cce27df0488e3174f0b02228a0d98bdabbdba9713c328665f]]]As shown in Figure 1-27, a = b causes a to point to the same memory location as b. This is a &quot;shallow copy&quot; because the actual data (4,5,6) is not duplicated. On the other hand, when a[...] = b, the memory location of a remains fixed and the elements of b are copied to the memory pointed to by a. This is a &quot;deep copy&quot; because the actual data is duplicated.

[[[00000000000000001810---9a2d738e8f7078ef49e6b91187d22fdb09360dc7667a52dfc62e24b5796eeeff]]]From the above, we know that the memory address of the variable can be fixed by using the &quot;three-point reader&quot; (in the example above, the address of a is fixed). In our case, fixing this memory address makes the handling of instance variables grads simpler.

[[[00000000000000001811---0d90c012594bd2059e721c20f589221cbb3093744e21b0eb9eb2fde23baaaa09]]]The grads list stores the gradient for each parameter. At this time, each element of the grads list is generated as a NumPy array only once when the layer is generated. After that, use the &quot;three-point reader&quot; to overwrite the memory location of the NumPy array without moving it. That way, the task of putting the gradients together only needs to be done once in the beginning.

[[[00000000000000001812---c79cb2a344d411217c35a2b14fc58dc78b383bab541b264681454f144254c078]]]The above is the implementation of the MatMul layer. Note that this implementation file is located in common/layers.py.

[[[00000000000000001813---b725acda0b7de2a4fc0ac8e6a98f5cbe8e3c4aa73fbbaef98c53f4f153dead70]]]Gradient derivation and backpropagation implementation

[[[00000000000000001814---f6103a71d30759121e09a6c78baebcf54666d44fe755fc69a53927e6e453dca4]]]Now that we&#39;ve covered the computational graph, let&#39;s implement some practical layers. Here we implement a Sigmoid layer, a fully connected Affine layer and a Softmax with Loss layer.

[[[00000000000000001815---e938077af97a906d3f98cf0f3a95e0e4815629b3f638dab3cc55b22f77a84497]]]Sigmoid layer

[[[00000000000000001816---76789dc48d91bc2b090d52811c0de7af168f00e6002385653095b68f857a5974]]]The sigmoid function is expressed by the formula And the derivative of the sigmoid function is given by

[[[00000000000000001817---6efbf585c36f127dbd5f2a96ce3d5fa0e6ba483b0273345622d334e15020802c]]]From equation (1.15), the computational graph of the Sigmoid layer can be written as shown in Figure 1-28. Here, the gradient () transmitted from the output side layer is multiplied by the derivative of the Sigmoid function () and propagated to the input side layer.

[[[00000000000000001818---0583a77a62dfa4aa4dceb20173e5e0e37f96422d676218842d86bddd20e451c5]]]
Figure 1-28 Computation graph of Sigmoid layer


[[[00000000000000001819---8dad278bb648b8eb9186513d0a0e6b425436c9744b59a031b94100a3fa21b01d]]]Here, we omit the process of deriving the derivative of the sigmoid function. &quot;Appendix A Differentiation of sigmoid and tanh functions&quot; explains the process of finding the derivative of the sigmoid function using computational graphs. Please refer to it if you are interested.

[[[00000000000000001820---e9ab6d16f91f97173a653d5b4e9869d607970ce8c039f67fa7a5f3e09fd9add0]]]Now let&#39;s implement the Sigmoid layer in Python. Referring to Figure 1-28, it can be implemented as follows (☞ common/layers.py).

[[[00000000000000001821---79242f3fb463d313d3c76182524d398e5095587d0be83f690b7a00db11f9a6cd]]]Here, the output of the forward propagation is saved in the out instance variable. Then, in backpropagation, we use the out variables to perform computations.

[[[00000000000000001822---a32f14225e1a8dfca2475aada43820a8515f0b0721aa8cbc7159e34057bd495d]]]Affine layer

[[[00000000000000001823---f77be88894f16ab6b3276e63fa32928e90b34106ceeb618972235f79cf201a66]]]As shown before, forward propagation of Affine layers could be implemented with y = np.dot(x, W) + b. At this time, the bias addition uses NumPy broadcasting. Expressing that point explicitly, the computational graph of the Affine layer can be written as shown in Figure 1-29.

[[[00000000000000001824---497d5c3b898acb756cfb21397777bb69b794317603f297d9cb74eea5b59fa95c]]]
Figure 1-29 Computation graph of the Affine layer


[[[00000000000000001825---97a45249f3b7bc834f2cd043c6fbf098b09d1657c19747e5b5f0d76329f02c64]]]The MatMul node performs the matrix multiplication calculation, as shown in Figure 1-29. Then the bias is replicated in the Repeat node, after which the addition is done (NumPy&#39;s broadcast functionality can be thought of as the Repeat node&#39;s computation being done behind the scenes). Now, here is the Affine layer implementation (☞ common/layers.py).

[[[00000000000000001826---a4255e7066234121da3bce603e2a08f233f0f19b4a0c2e5f97522f74fa5c55c6]]]Following the implementation rules of this document, the instance variables params hold the parameters and grads hold the gradients. The backpropagation implementation can be found by backpropagating the MatMul and Repeat nodes. Backpropagation of the Repeat node can be calculated by np.sum(), but by noting the shape of the matrix at this time, it becomes clear on which axis the sum should be calculated. Finally, set the gradient of the weight parameter to the instance variable grads. The above is the implementation of the Affine layer.

[[[00000000000000001827---e1f24eeef38df5e5428977d6d2a252b585cba4d2b4855712d86a1ad1b240eca5]]]The Affine layer is easier to implement with the MatMul layer we have already implemented. Here, as a review, I implemented it using NumPy methods without using the MatMul layer.

[[[00000000000000001828---88da731ea68caee0ac02ffdfa893306f7c8bb4cc2d91690d3d209999c286c44e]]]Softmax with Loss layer

[[[00000000000000001829---13b6b715773640af2dff95ee3735144c5319ae446579f79fbff222b38cd4fd92]]]We will combine the softmax function and the cross-entropy error to implement a Softmax with Loss layer. At this time, the calculation graph will look like Figure 1-30.

[[[00000000000000001830---c97d3857ded760bd7a413a6fc5c313df30a1fe09f3e1e406f35f03aae5ee475d]]]
Figure 1-30 Computation graph of Softmax with Loss layer


[[[00000000000000001831---ce98529cd89f59ebc334b05dedda82e66596b990020ba459844a6b6ef20ecef1]]]In the computational graph of Figure 1-30, we represent the Softmax function as the Softmax layer and the cross entropy error as the Cross Entropy Error layer. Here, we assume that three-class classification is performed, and that three inputs are received from the previous layer (=the layer closer to the input layer).

[[[00000000000000001832---7f4feb080779efd1e66950d19bd1f68a6246c7289a4107ead885ce17d977e908]]]The softmax layer normalizes the input (a1, a2, a3) and outputs (y1, y2, y3), as shown in Figure 1-30. The Cross Entropy Error layer takes the output of Softmax (y1, y2, y3) and the teacher labels (t1, t2, t3) and outputs the loss L from those data.

[[[00000000000000001833---37ff0ea8376241ccb189de3bbcaae7cf8a68dbfb2989e6f48d717ea5505dabdd]]]Of note in Figure 1-30 is the result of backpropagation. Backpropagation from the softmax layer gives a &quot;clean&quot; result of (y1−t1, y2−t2, y3−t3). (y1, y2, y3) is the output of the Softmax layer and (t1, t2, t3) is the training data, so (y1−t1, y2−t2, y3−t3) is the difference between the output of the Softmax layer and the training label. increase. Backpropagation of the neural network propagates this difference (error) to the previous layer. This is an important property in neural network learning.

[[[00000000000000001834---3a5c81aa146cdb000dd8fb02ac3be7591ef01d742664af63deb94fd02c4f08f6]]]Here, we omit the description of the implementation of the Softmax with Loss layer. Note that the implementation is in common/layers.py. Also, the derivation process of backpropagation of the Softmax with Loss layer is explained in detail in &quot;Appendix A Softmax-with-Loss Layer Computation Graph&quot; of the previous work &quot;Deep Learning from Scratch&quot;. Please refer to it if you are interested.

[[[00000000000000001835---a785c5b2a81609274af4f38ea4cc26cb8a382b72a93a39242abe48686af62122]]]weight update

[[[00000000000000001836---0a2b53e449e672858bbc767c685c01c7dbeb6243ff85b03c6196632489502dbc]]]Once the gradients have been obtained by backpropagation, the gradients are used to update the parameters of the neural network. At this time, the neural network is trained according to the following procedure.

[[[00000000000000001837---3f710f18a16c4c29fbfa61796e9521282dc5cfe0d38a8e458914a655dc29f2c6]]]Step-1 (mini-batch) Randomly select multiple data from the training data

[[[00000000000000001838---82bbf29dd51e0524773dd22ee1b93025b425b20cd0eec3aeab6b10ca8f11919a]]]Step-2 (calculation of the gradient) Find the gradient of the loss function for each weight parameter by the error backpropagation method

[[[00000000000000001839---bda82a5c29263aa579dc956f49f57fe95e36e17318346d6c4d003618c96cf55b]]]Step-3 (Update parameters) Update weight parameters using gradients

[[[00000000000000001840---61bf4bc1f7c0109597428a4dabf702143d89bc49a00fc74c69a83172fc0a2311]]]Step-4 (Repeat) Repeat Step-1, Step-2, and Step-3 as many times as necessary

[[[00000000000000001841---6d9b2831d8489d9f7353b15eaa1af8da587f8120fa401cf05268f0dc9d36f88d]]]The neural network is trained according to the steps above. We first select the data in mini-batches and then obtain the gradients of the weights by backpropagation. This gradient points in the direction of the most increasing loss for the current weight parameters. Therefore, we can lower the loss by updating the parameter in the opposite direction of its gradient. This is Gradient Descent. Then repeat the task as many times as necessary.

[[[00000000000000001842---79f1b0c6ca2cab4ed5c1b8e26b80693bc65b1b62f7a71ed439986283b5d46ed0]]]We update the weights in Step-3 above, and various methods have been proposed for updating the weights. Here, we will implement the simplest method, SGD (Stochastic Gradient Descent). Note that &quot;Stochastic&quot; means using gradients for randomly selected data (mini-batches).

[[[00000000000000001843---888ebc112eb590622166e8087c9bf85ff93cdef9280715c775eed461d303d3bd]]]SGD is a simple method. This updates the (current) weights by a constant distance in the direction of the gradient. Expressed as a formula, it looks like formula (1.16).

[[[00000000000000001844---2c9dc2da1cc2df24388f42ddc90c71d02c03128c181b136cba1b548fde5808e4]]]Let W be the weighting parameter to be updated and the gradient of the loss function with respect to W. represents the learning coefficient, and in practice a value such as 0.01 or 0.001 is predetermined and used.

[[[00000000000000001845---78faf7f169c34fe71fbab40f3306fb0a56781e0fcf316b343d4635e6fa3371ed]]]Now let&#39;s move on to the SGD implementation. In consideration of modularity, I will implement a class that updates parameters in common/optimizer.py (this file also implements SGD, AdaGrad, Adam, etc.).

[[[00000000000000001846---26bbd94e001cfdef91f506d0d6723ed4b203fec4580b0f7112852a99ffd6f3cd]]]And the class that updates the parameters should have a common method called update(params, grads). Here, we assume that the parameters params contain the neural network weights and grads the gradients as a list. And we assume that the same indices in params and grads store the corresponding parameters and gradients respectively. Then SGD can be implemented like this (☞ common/optimizer.py):

[[[00000000000000001847---88ead557ab40360637904e9cede6cebdfb1c574307fe045842aaa679b7fbb3ec]]]The initialization argument lr represents the learning rate. Here we keep this learning factor as an instance variable. Then implement the parameter update process in the method update(params, grads).

[[[00000000000000001848---a4c67d6b3106e3e49c2d3b2d58faae6910909ce07466618f60f2e4ff79509119]]]Using this SGD class, updating the parameters of a neural network can be done as follows (the code below is pseudocode that doesn&#39;t actually work):

[[[00000000000000001849---505105bbc23de1355c43437126b32ad2a1ff722ead0c99d4784f0691efcd9a2b]]]# get minibatch

[[[00000000000000001850---6cbab18e35453d4a1ddc751337aa814dee93a077af37e26d2139bf1c38387875]]]By separating the implementation of the class that performs the optimization in this way, it becomes easier to modularize the functionality. In addition to SGD, this book implements techniques such as Momentum, AdaGrad, and Adam. Their implementations are in common/optimizer.py. We omit the description of these optimization techniques here. For details, please refer to &quot;6.1 Updating parameters&quot; in the previous work &quot;Deep Learning from Scratch&quot;.

[[[00000000000000001851---69a380bdc70588b40dbf4982b996a29454ecb8ced1622928a1fd0eb4b59cd809]]]solve problems with neural networks

[[[00000000000000001852---1e67e5b81b288fb329f2dedd325e729792d1bf4866e4b6523fe2a1d15c386855]]]Everything is ready. We will train a neural network on a simple dataset.

[[[00000000000000001853---1e94321feb9522c27c2d9243b6611b2442960eb6521293052132c6c3f4c8cc51]]]Spiral dataset

[[[00000000000000001854---b37834445e05e3fa762e6f944f0c15cfc08f5e3432dab6b7273e5755ac7c621b]]]This book provides some convenience classes for datasets in the dataset directory. In this section, we will use the file named dataset/spiral.py. This file implements a class that reads &quot;spiral data&quot; and is used as follows (☞ ch01/show_spiral_dataset.py).

[[[00000000000000001855---fa38f5b379e5709dce127a3731eebdeb932f1a86348e03a54fc5bd4f7c006080]]]# settings for importing files in parent directory

[[[00000000000000001856---b5a0fae52e9fadfbcd7279e2981279974a1f5933bd21041403cf2aa052dd46de]]]In the above example, import spiral.py in the dataset directory from the ch01 directory and use it. Therefore, the parent directory is added to the import search path by sys.path.append(&#39;..&#39;) in the above source code.

[[[00000000000000001857---d4b63d21ed2c0a91249a9bdc493cb4caf4e77f4c6c36058dbbc5ec7a0fffe457]]]Then, load the data with spiral.load_data(). where x is the input data and t is the teacher label. Looking at the shapes of x and t, we can see that there are 300 sample data each, x is 2D data and t is 3D data. Note that t is a one-hot vector, labeled 1 for the corresponding class and 0 otherwise. Now let&#39;s plot this data on a graph. The result should look like Figure 1-31.

[[[00000000000000001858---5b7f43475252bcc13824ad2f70169cb970666f4787a6ac9d6cb99fb742946cf9]]]
Figure 1-31 Spiral dataset used for training (×▲● represents three classes)


[[[00000000000000001859---cb79baf2d00509fdf0ff233646d7f8c7ff58efba45f3835b4dcb282d6f01e13e]]]As shown in Figure 1-31, the input is two-dimensional data and there are three classes to classify. Looking at this dataset, we can see that it cannot be separated by a straight line. So it means that we need to learn a non-linear separating line. Can our neural network—a neural network with hidden layers using a nonlinear sigmoid function as its activation function—learn that nonlinear pattern correctly? Let&#39;s experiment now.

[[[00000000000000001860---f89a8d0bd01bf4ab5dabbc42388f2a1f87e7af2dbcbdaf40438b15a7e2cbde2e]]]Since this is a simple experiment, we will not separate the dataset into training, validation, and test data. In a real problem, we separate the dataset into training and test (and validation) data for training and evaluation.

[[[00000000000000001861---812f3b86916e5fdd7cc6f6c51241d8f3609917a16f540aa67a024403ab43a9e6]]]Neural network implementation

[[[00000000000000001862---2cf976ffa16f94e0e3cdde4b8dc6c228308604726be27543dd5921dca6a2765a]]]Now let&#39;s implement the neural network. Here we implement a neural network with one hidden layer. First, here is the import statement and the initializer __init__() (☞ ch01/two_layer_net.py).

[[[00000000000000001863---48112181902a933c61a82baec5265e37f392b6d630e0f2ca2bea54a97c6390d9]]]# Initialize weights and biases

[[[00000000000000001864---7021dcf7079c3447ef0d2e8d46d15dccd4766f8901ea1ef36ddeb4d848ec3fff]]]# create layers

[[[00000000000000001865---06f7dbb825277252d3f70b6a892a5aca619776d6c46333a516efdd8f89f3dda7]]]# Collect all weights and gradients in a list

[[[00000000000000001866---6410cfa28b0df249ee30c098ffb04f5e43c17fc341cc82d43c11851fd2a1f860]]]The initializer receives 3 arguments. input_size is the number of neurons in the input layer, hidden_size is the number of neurons in the hidden layer, and output_size is the number of neurons in the output layer. The core implementation first initializes the biases with a 0 (zero) vector (np.zeros()). And initialize the weights with small random values (0.01 * np.random.randn()). By setting the weight to a small random value, learning can proceed smoothly. It then creates the required layers and collects them in the instance variable&#39;s layers list. Finally, we bundle the parameters and gradients used in this model.

[[[00000000000000001867---19417c6a66673406668b39758d5cb738b13678f22c1d65e25a3937bbe8f2888d]]]The Softmax with Loss layer is handled differently from other layers, so it is not included in the layers list and is stored separately in the instance variable loss_layer.

[[[00000000000000001868---77df1f61ac40d90779e7ced0d87cfec47830b2087229ab06679c66bfd4b7558e]]]Next, implement three methods in TwoLayerNet. Here, we implement the predict() method for inference, the forward() method for forward propagation, and the backward() method for backward propagation (☞ ch01/two_layer_net.py).

[[[00000000000000001869---29b08066748df9147b06caa5669adb74adaa34504ce285998d52f96498e89fce]]]As you can see, the implementation here is cleaner! We&#39;ve already implemented the processing blocks we use in our neural network as &quot;layers&quot;, so all we have to do here is call forward() and backward() on those layers in the appropriate order.

[[[00000000000000001870---c423af039cda5ba96cf03dca58c57a7578fa2c78ff7e6c3f022a6e263a3df9e1]]]source code for learning

[[[00000000000000001871---e325570b366f4cfa2a031faed5d03d7caac011c6b5a26bcedabd585ad60cd161]]]Following is the code that does the learning. Here we load the training data and generate a neural network (model) and optimizer. Then, learn according to the 4-step procedure of learning shown earlier. In the field of machine learning, it is common to refer to a method designed for a problem (neural network, SVM, etc.) as a &quot;model&quot;. Here is the code for training (☞ ch01/train_custom_loop.py).

[[[00000000000000001872---fda9d708687496d464e51316da473677773d5312c2d0702300d0e14f3550e596]]]# ❶ Hyperparameter setting

[[[00000000000000001873---47b5435ab28e656d83569f7a0c059c3579f4ce1cbd6fe76c77270552a3befe6c]]]# ❷ Read data, generate model and optimizer

[[[00000000000000001874---c16f5afe0f59e648c94ad2e89c936911e5c617eae9c522cb6a21333f8c7a9227]]]# Variables used in training

[[[00000000000000001875---de9aaca7d7b6cd2a6a43471547dc1c880cc37af20d0d74f9f083653924a8e20a]]]# ❸ Data shuffle

[[[00000000000000001876---0bd0532976d42850007da85e2f77a9ed879896826b07e96a6b977a2b8421b077]]]# ❹ Calculate gradient and update parameters

[[[00000000000000001877---2197d92edcb7096ce1a6b86f88297334bb5b47f6cb9e7d49ae0ab9ab34dc7f26]]]# ❺ Regular output of learning progress

[[[00000000000000001878---6992f790cde1b4afb3feb62b7273b471b2e5c552db1a7460e4d87f794618bb74]]]First, set the hyperparameters at ❶ in the code. Specifically, set the number of epochs to learn, the batch size, the number of neurons in the hidden layer, and the learning coefficient. Next, at ❷ , data is read and a neural network (model) and optimizer are generated. We have already implemented the two-layer neural network as the TwoLayerNet class and the optimizer as the SGD class. We will use them here.

[[[00000000000000001879---d1e1908bd89f9f8eb3300fb607696c63b9672154d5f68d183584a1395008283c]]]An epoch represents a unit of learning. One epoch is equivalent to &quot;seeing&quot; all of the training data -- one lap around the dataset. Here we train for 300 epochs.

[[[00000000000000001880---a560b2e8b37e9b5515f0d74b827e25d4dbc4ef6d1fc94cbc94b4c675abf4980e]]]For training, we randomly select data in mini-batches. Here, the data is shuffled in epoch units, and the data is extracted in order from the beginning of the shuffled data. To shuffle the data—more precisely, to shuffle the &quot;indexes&quot; of the data—use the np.random.permutation() method. If you give N to the argument, this creates and returns a random sequence from 0 to N−1. A real-life example would look like this:

[[[00000000000000001881---81223c2dc7d1829b34f6a2567457e9a02b381662da4cd4783ffca0a8fdc4c3ae]]]In this way, calling np.random.permutation() will randomly shuffle the index of the data.

[[[00000000000000001882---3a50e5a34e12566ee1eed011aadc0e7b5789aaa587c6925b866be4d2fb37e5cc]]]Next, find the gradient in code ❹ and update the parameters. Finally, in ❺, the results of learning are output periodically. Here we average the loss every 10 iterations and add it to the variable loss_list. The above is the source code for learning.

[[[00000000000000001883---0c2c27254f44e62d6af1ea6f49124de222d93e8113fb8e49e35658f0891d2ad0]]]The code for training a neural network implemented here will be used elsewhere in this book. Therefore, this book provides this code as a Trainer class. Using the Trainer class allows us to keep the details of neural network training inside the Trainer class. Detailed usage will be explained in &quot;1.4.4 Trainer class&quot;.

[[[00000000000000001884---3c82065ebac88105e28d78046b2478835a03a3a0188ddf715f68f3719ef85e67]]]Now let&#39;s run the code above (ch01/train_custom_loop.py). Then you can see that the loss value output to the terminal is steadily decreasing. And when I plot the result, it looks like this:

[[[00000000000000001885---61edd4b5927a4817faafbfcfc877bc7a504f1b3883f3e32520e828671137da07]]]
Figure 1-32 Loss graph: horizontal axis is training iteration (10 times scale value), vertical axis is average loss per 10 training iterations


[[[00000000000000001886---dd16f1e3d2699b36615cc812178d1ed799e6edcccd8c7f675ad4c5201946d306]]]As shown in Figure 1-32, we can see that the loss decreases as the learning progresses. Our neural network seems to be learning in the right direction! Now, let&#39;s visualize how the neural network after training creates a separation region, which we call a decision boundary. The result should look like Figure 1-33.

[[[00000000000000001887---1066187b6fa8d3f4e6445b44b0fd61fae58e7abec23843f6b8945e8b90af733b]]]
Figure 1-33 Decision boundary of the neural network after training (color-coded areas for each class identified by the neural network)


[[[00000000000000001888---cd76861129d882acd80e61d51bd60f0a039dbb0cdeb88840e5a2e2f70acf9b67]]]As shown in Figure 1-33, it can be seen that the neural network after training correctly captures the &quot;vortex&quot; pattern. In other words, it was able to learn a nonlinear separation domain! In this way, neural networks can express complex expressions by having hidden layers. A feature of deep learning is that its expressiveness becomes richer by adding more layers.

[[[00000000000000001889---da65e14b9ef801b9923ed3ff519cec53a48026d86b3e4479edcbcf3233ecc81a]]]Trainer class

[[[00000000000000001890---86fd0fc70e86212414e6d66bebe5ed1bc67ec3a895f3c7d5c39a0c88903db420]]]As I mentioned earlier, there are many opportunities in this book to perform neural network training. There you have to write the training code like you did earlier, but it would be boring to write the same code every time. Therefore, in this book, we provide a class for learning as a Trainer class. The implementation inside is almost the same as the previous source code. Some new functions have been added, but detailed usage will be explained when necessary.

[[[00000000000000001891---c1e7e5ca2b99479be94a6de086a88c02c8c9321c7ce493fa4228081f289df1a8]]]The Trainer class is located in common/trainer.py. The initializer of this class takes a neural network (model) and an optimizer. Specifically, use it like this:

[[[00000000000000001892---172c8fb14ee45c765d0af14f7e03a1d7399196dc2d8c1b8a62313d57b2e0b2b8]]]Then call the fit() method to start training. This fit() method has the arguments shown in Table 1-1.

[[[00000000000000001893---1cd9d6d2c31601c2d2af00dd0a019ff6eb0cd76cf4bf20682e4b238ed4a5bbb2]]]Table 1-1 Arguments for the fit() method of the Trainer class. &quot;(=XX)&quot; in the table represents the default argument

[[[00000000000000001894---7d341bb44cea118093264bfe2372d6f57d3b45fed41f8b5197de111ba1a85a00]]]argument

[[[00000000000000001895---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000001896---e7c31dccd8f58cb2ca6fa30e828f1e69312d05747f8c094e33b3e85b884b3731]]]Input data

[[[00000000000000001897---c89ed92a89817f122bf9d1e9999628868d939db5741aa2ff11461dfd44d1f582]]]teacher label

[[[00000000000000001898---ef7f14bf25e921f49f607b54937df6a421fb13ad7c5e683d1e7865a30c890629]]]number of epochs to train

[[[00000000000000001899---fc8b5fd4cf3d038d172b39f91f7ff907c204e2431a90d3102e14e41e3df0a94b]]]mini batch size

[[[00000000000000001900---887a66b7bd09ca4af95805277e2fe90e31e9f5844a6f67414d325593a0a7d3c0]]]Interval \newline{} to display results (e.g. average loss)

[[[00000000000000001901---1455934d056611d49603db98218e61bb0201bb6ab94e7da7a1d2fd3160c9ba25]]]for example

[[[00000000000000001902---f8edb7955ac8b5c3c9df239e6fa468ef4672cadfd67c2634614d1aac1d06c16f]]]will find the average loss every 20 iterations and output the results to the screen.

[[[00000000000000001903---2fdf96b2792157c732666f1bbb10e3a97fb72d740eeeb1efbe7369e2cf383e04]]]maximum norm of gradient

[[[00000000000000001904---966790a479756e9999df0d9f2ba196ff87e8dc1497de75a19c11d7c8f07ddcbc]]]If the norm of the gradient exceeds this value, reduce the gradient (gradient clipping. See

[[[00000000000000001905---29e113abeaf397281094ebbf426f9dec45fe52548e95a9e9d2e131bda3448fd4]]]Chapter 5 Recurrent Neural Networks (RNN)

[[[00000000000000001906---65ef87d6d3168740f1a6bf0d5762e2291e17fde3a0afd1fbd39f03801fa20330]]]&quot;reference)

[[[00000000000000001907---bf734bdd9007c379b8ae13ba7bd2b38c80a2c92a6ccc4b7055e0061536dc17b4]]]Also, the Trainer class has a method called plot(). This plots the loss recorded by the fit() method—more precisely, the average loss evaluated at eval_interval timings. Now, the code for training using the Trainer class is shown below (☞ ch01/train.py).

[[[00000000000000001908---6f0358ca006c0be892ff1e248f92b0c4bd0da59d0ae29426dbb6d0db6821249f]]]When you run this code, the neural network will be trained as before. By assigning the training code shown earlier to the Trainer class, the code is cleaner. From this point onwards in this book, we will use the Trainer class for learning.

[[[00000000000000001909---7273383eab249b6d24008b164342047de21da0489aeb36abc6c5dfe2d37bea91]]]Calculation speedup

[[[00000000000000001910---566ae41968b2e070f83cff079d0c59bdae8de280a9c3d16a5b46f30ecf3a96cb]]]Neural network training and inference require a lot of computation. Therefore, how to compute neural networks at high speed is an important theme. Here, I will briefly explain &quot;bit precision&quot; and &quot;GPU&quot; that are effective for speeding up neural networks.

[[[00000000000000001911---fec301b1bd4cf6db1719f8a359724981111a4538d7935ec323d84ccc77e3759e]]]In this book, we prioritize simplicity of implementation over speed of computation. However, from the point of view of speeding up calculations, from now on, implementation will be carried out with an awareness of the bit precision of the data. Also, where the calculation takes a lot of time, we have devised it so that it can be executed on the GPU (as an option).

[[[00000000000000001912---4e7daec4e4d15c24a4bd77876e8ba469aadc07f5dfa8851bdb768396c479cd24]]]bit precision

[[[00000000000000001913---0a0ac066d110307a815fdf1cf71f04f597bf45c8530177946de23974be77680d]]]Floating-point numbers in NumPy use a 64-bit data type by default (whether it is 64-bit or not may vary depending on the reader&#39;s environment—OS, Python/NumPy version, etc.). The following code confirms that 64-bit floating point numbers are actually used.

[[[00000000000000001914---2454708a1065d70a8e10f8db8cb73361c7b8e05be98ddaba5adb65ea57b05722]]]You can see the type of the data by the instance variable dtype of the NumPy array. The result above is labeled float64, which represents a 64-bit floating point number.

[[[00000000000000001915---3d29f93c517b4128449aeacce13163257e6b1063db79b6c52a2c849195e026e1]]]As you can see, NumPy uses 64-bit floating point numbers by default. However, it is known that neural networks can be inferred and trained with 32-bit floating-point numbers just fine--with little loss of recognition accuracy. From a memory standpoint, 32-bit is half as good as 64-bit, so 32-bit is always preferable. Also, in neural network calculations, the &quot;bus bandwidth&quot; that transfers data can become a bottleneck. Even then, of course, smaller data types are preferable. And in terms of calculation speed, 32-bit floating point numbers can be calculated faster (the calculation speed of floating point numbers depends on the architecture of the CPU or GPU).

[[[00000000000000001916---ff84d62b3bfab4e4e6d3da889abcd8faec0cc3850c8b92e533245c8806053ba4]]]Therefore, this book prefers to use 32-bit floating-point numbers. To use 32-bit floating point numbers with NumPy, specify the data type as np.float32 or &#39;f&#39;, like this:

[[[00000000000000001917---666b71d84b53827b7e93aee8c2d96cb03d2745ffcb2cf76404c3b40f7655b1d0]]]In addition, it is known that 16-bit floating-point numbers can be processed with almost no degradation in recognition accuracy when limited to neural network inference [6]. However, while NumPy provides 16-bit floating point numbers, general CPUs and GPUs perform calculations in 32-bit. Therefore, even if you convert to 16-bit floating-point numbers, the calculation itself will be performed with 32-bit floating-point numbers, and you will not benefit from the processing speed.

[[[00000000000000001918---53393a678eb2ee52169fb605e25ef8790f1c8c09aadd0ecb977a8dab906065f7]]]However, in some cases, such as storing learned weights (in an external file), 16-bit floats are useful. Specifically, when saving weight data with 16-bit precision, it is possible to save half the capacity of 32-bit data. Therefore, in this book, we will convert the learned weights to 16-bit floats only when storing them.

[[[00000000000000001919---055bea4981a053c63d7c20079ac9d37fce250343dee9bdcfa977b0c28b8add54]]]As deep learning attracts attention, recent GPUs have come to support 16-bit half-precision floating point numbers for both &quot;storage&quot; and &quot;calculation&quot;. Also, Google&#39;s original chip called TPU is devised so that it can perform calculations with 8 bits [7].

[[[00000000000000001920---ff21f33f7069c9bcd1632896af6f869d10804e5b4a79f7e329feba149ef888ce]]]Deep learning computations consist of a large number of multiply-accumulate operations. Many of these heavy multiply-accumulate operations can be computed in parallel, which is what GPUs are better at than CPUs. Therefore, general deep learning frameworks are designed to run on GPUs in addition to CPUs.

[[[00000000000000001921---01f736fd7c26f9cda0edf69ec11f2fed11aac296129ced503883e444a4d21d8c]]]In this book, we use a Python library called CuPy[3] as an option. CuPy is a library for parallel computing with GPUs. To use CuPy, you need a machine with an NVIDIA GPU. You&#39;ll also need to install a general-purpose parallel computing platform for GPUs called CUDA. For detailed installation instructions, please refer to the official CuPy installation document [4].

[[[00000000000000001922---59b49e48b10f036c48eb1509bc2d0a70a1a4cca31688e542c3af8146b7c90466]]]With this CuPy, you can easily perform parallel computation using GPU made by NVIDIA. More importantly, CuPy has a common API with NumPy. A simple usage example would look like this:

[[[00000000000000001923---7d55d20afc21aee2b6c7652df7b54adff96f0471946ff68c55cfe8494e70fab0]]]As shown here, CuPy can be used in basically the same way as NumPy. And behind the scenes, calculations are performed using GPUs. What this means is that if you have code written in NumPy, you can easily turn it into a “GPU version”. Because then all we do is (basically) replace numpy with cupy!

[[[00000000000000001924---73b6cc139fe4b5bca44bbe62dac836a3ae7a663b76ec5c713b39400d709584db]]]As of June 2018, CuPy does not cover all of NumPy&#39;s methods. CuPy is not fully compatible with NumPy, but has many APIs in common.

[[[00000000000000001925---174af7287871d96400725d342c2c4f6bceaee11f715bbbdfa9f5bb14faf5b2b3]]]To reiterate, this document gives priority to easy-to-understand implementation and bases implementation on the CPU. However, for code that takes a lot of computation time, we provide an optional implementation using CuPy. And even when using CuPy, we take care so that readers do not need to be aware of using CuPy.

[[[00000000000000001926---82cad11c50b9e1da953728f3079abbd767d6a36351765bd8f3b427ccdbdb40bb]]]The first appearance of GPU-executable code in the book is in Chapter 4, ch04/train.py. This ch04/train.py starts with the following import statements:

[[[00000000000000001927---0782d07c8326e3ada616c74db4f78ad8b4d68df88b9a02d1a5e777d6520eec20]]]# Remove the following comment out when running on GPU (requires cupy)

[[[00000000000000001928---84fab36ef2a8fc77c1e968ce88b50f8e92ba361099de3423bfd6313736600427]]]Execution of this code takes several hours on the CPU, but takes only tens of minutes with the GPU. And the code provided in this book can be run in GPU mode with just one line modification of the above source code. Specifically, if you uncomment &quot;# config.GPU = True&quot;, CuPy will be used instead of NumPy. By doing so, it is executed on the GPU and training is performed at high speed. If you have a GPU, please use it.

[[[00000000000000001929---58b5cf469db08b52deb5390f8b00caf5fba867d1a7ff1b4409d17dbcb4272cd5]]]The mechanism by which NumPy switches to CuPy is very simple. For those interested, see the import statements in common/config.py, common/np.py, and common/layers.py.

[[[00000000000000001930---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001931---8325a7345b31b4f763faf63cf4518b79daa302a693b7c4bfdc1a40100913bab2]]]In this chapter, we reviewed the basics of neural networks. First of all, we started by reviewing mathematics such as vectors and matrices, and confirmed the basic usage of Python (especially NumPy). And we&#39;ve seen how neural networks work. In particular, I covered some basic parts of the computational graph (such as addition and multiplication nodes) and explained their forward and backward propagation.

[[[00000000000000001932---5090f647641fc0264b241bf834b8d890fc993cad29be0d2e3a6af693a32073ae]]]I also implemented a neural network. For modularity, we implemented the basic parts of neural networks as layers. In the implementation of the layer, the &quot;implementation rules&quot; of this book are to have forward() and backward() as class methods, and params and grads as instance variables. This makes the implementation of neural networks more promising.

[[[00000000000000001933---31c8c6e55cac2e997f22d6f814b6b08f2a5a81c3487f7c2a7dc6f4eb0d109bcd]]]Finally, in this chapter, we trained a neural network with one hidden layer on an artificial “spiral dataset”. And we confirmed that the model can be trained correctly. This concludes our review of neural networks. From now on, we will enter the world of natural language processing with a reliable weapon called a neural network. Let&#39;s move on!

[[[00000000000000001934---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000001935---37b4d374bfcc3ac68503250a15922467f0dad0f8b8f6156a5da58f2e8d67f920]]]A neural network has an input layer, a hidden layer, and an output layer

[[[00000000000000001936---01d2212d0addd7aa9c0b2041a53a6dc2d2d07c412b8f6e3dbd149472f1ae21b6]]]A fully connected layer performs a linear transformation and an activation function performs a nonlinear transformation

[[[00000000000000001937---b7e2dc2115f2fe9aa6a03c42f161d540717f0cba5e9417dbfadb0802ff5eda5b]]]Fully connected layers and mini-batches can be computed together as matrices.

[[[00000000000000001938---8ee6f12df3a19faddb68d5a309a3670f364f70e532d6a398f65e6cedc01dd3ef]]]Backpropagation can be used to efficiently find gradients on neural network losses.

[[[00000000000000001939---7f2c6530f67da26cb6e6c693989651cb3ffe78ec46ce6addd7aeb892b6c9269e]]]The processing performed by neural networks can be visualized by computational graphs, which is useful for understanding forward and backward propagation.

[[[00000000000000001940---a73b0958202ff8f46406aa9f6c7b6fb2a79fe6518deceeed5747b2e9e569dbcb]]]Neural network implementations are easier to assemble by modularizing the building blocks into &quot;layers&quot;

[[[00000000000000001941---68e48188ddb7db77010abf9d370f7c858f452d40002aefd2e8c538af29fe2220]]]Bit precision of data and parallel computation by GPU are important for speeding up neural networks.

[[[00000000000000001942---f1346ad5781c9c4ee349e1ac33c0d22499e89abbeffaf60a56caff66fe5e8c77]]]Foreword

[[[00000000000000001943---e51295a37f29b1a6efa3ae62c773c7a9adef32309dfa77ab272e112e2ec67e4e]]]You can&#39;t understand what you can&#39;t make

[[[00000000000000001944---49cd33eed09738b75e0a875778196df7731930950353e0151b6e776b8a3aff91]]]—— Richard Feynman

[[[00000000000000001945---3deb7801f26323724e60774f6b6bc6a8c80b8b898ba2b76a88ab9066465ca041]]]The world is about to change dramatically with deep learning. Whether it&#39;s speech recognition on smartphones, real-time translation on the web, or forecasting exchange rates, deep learning is no longer possible. Deep learning is making the development of new drugs, diagnosis of patients, and self-driving cars a reality. In addition to this, deep learning is almost always hidden behind advanced technologies. And from now on, the world will continue to move forward with deep learning.

[[[00000000000000001946---41e3183133886866fa9df45792fae4f77661ac1b8863d6f266758602f2574965]]]This book is a sequel to &quot;Deep Learning from Scratch&quot;. Continuing from the previous work, we will deal with technologies related to deep learning. In particular, this work focuses on natural language processing and time-series data processing, and uses deep learning to tackle various problems. And like the previous work, we will thoroughly enjoy the advanced technology related to deep learning, based on the concept of &quot;making from scratch&quot;.

[[[00000000000000001947---d2004be5323098a9f156f4ac1af1be45f6918e3fbfbdcefc3736282f1379c237]]]Concept of this book

[[[00000000000000001948---1198fac90394af96e104673cb346a605ceebc8ff470afb8062f39c58ee132774]]]I believe that the experience of &quot;creating from scratch&quot; is important for deep understanding of deep learning (or any advanced technology). Creating from scratch means starting from a point that you can understand, and completing the target technology without using external off-the-shelf products as much as possible. Through such experiences, you will become familiar with deep learning, not just superficially—that is the goal of this book.

[[[00000000000000001949---bf4818e37b082ecad444e51ef47f2b3f7fdf5adf2b29ca398ccdc7214d8f8919]]]After all, deep understanding of a technology requires the knowledge and skill to create it. In this book, we build deep learning from scratch. We write a lot of code and do a lot of experiments. It&#39;s a time consuming task and can be a headache at times. But that kind of time-consuming work—or rather, that kind of work—has a lot of important essence for a deep understanding of technology. The knowledge you gain in this way will surely be useful whether you use existing libraries, read cutting-edge papers, or create your own original system. And above all, it&#39;s pure fun to understand the mechanisms and principles of deep learning while unraveling them one by one.

[[[00000000000000001950---788317e3692fa2f7d31985bff9256795eaee6581925b8ee37e60aff180f0eba1]]]Enter the world of natural language processing

[[[00000000000000001951---4b36cccc688bda50dcc1f198d11a3ecb20e58a1d26207b541c48fc8251d12658]]]The main theme of this book is natural language processing with deep learning. Simply put, natural language processing is a technology that allows computers to understand the words that we normally speak. Getting computers to understand our language is a very difficult problem, and at the same time an important theme. In fact, this natural language processing technology has greatly changed our lives. Natural language processing technology is used at the core of technologies that have had a major impact on the world, such as web search, machine translation, and voice assistants.

[[[00000000000000001952---409b456a35e03ce15ab85c06d30fe17da8031502b8f2b197dc04fc565ad3f6ec]]]In this way, natural language processing technology is indispensable in our lives, and deep learning occupies an extremely important position in this field as well. In fact, deep learning has greatly improved the performance of conventional natural language processing. For example, it is still fresh in our minds that even Google&#39;s machine translation has made great progress through deep learning-based techniques.

[[[00000000000000001953---03d4d5ed0657ab82ae9301df023f0027f929f4a8b44d6100463d13f5c0f91ddd]]]In this book, you will learn important techniques in deep learning, focusing on natural language processing and time series data processing. Specifically, technologies such as word2vec, RNN, LSTM, GRU, seq2seq, and Attention. In this book, we will explain these technologies in as easy a language as possible, and make sure that you understand them by actually making them. Also, through experiments, I would like you to actually feel those possibilities.

[[[00000000000000001954---4084806d90f4a14a7162f58c3c9f19bd4658f39b14b0721b2719bacae56bce60]]]This book is an adventure book on natural language processing with a focus on deep learning. The book consists of eight chapters in total, but they are structured to be read from the beginning like a series of stories. You encounter a problem, come up with new ways to solve it, and then make improvements. In this way, we will solve various problems related to natural language processing one by one with the weapon of deep learning. And through that adventure, I hope that you will master the important techniques of deep learning at a deep level and experience the fun of it.

[[[00000000000000001955---77401d3750f01304324b3a8212858cad3f3fa641e505e33434a30af3ae97542f]]]who the book is for

[[[00000000000000001956---d86d190839dbc6fd9e1cca3bb1aa050ff125f267d05f84ca16a00bee67accc8d]]]This book is a sequel to the previous work &quot;Deep Learning from scratch&quot;. Therefore, although the knowledge learned in the previous work is assumed, Chapter 1 of this book will review neural networks as a digest version of the previous work. Therefore, we have made consideration so that those who have knowledge of neural networks and Python can read this book without knowledge of the previous work.

[[[00000000000000001957---f388989150efeb38134c73d4a1e6153d794864e8100cec494cc2eb332f308a26]]]In order to gain a deeper understanding of deep learning, this book continues from the previous work by focusing on &quot;creating&quot; and &quot;moving.&quot; Don&#39;t use anything you don&#39;t understand. Use only what you understand—with such a stance, we will explore the world of deep learning and natural language processing.

[[[00000000000000001958---bcd735bb30870773c9f5d798a0ae7059a54de739abd2b3e2ed9c2860708c9ae0]]]Here, in order to clarify &quot;who this book is for&quot;, what this book does and its features are listed below.

[[[00000000000000001959---a8b883ec7816c2e796185fc4aabead0f3949d0a2b651f3c51ddb5702a1c4191a]]]Implement deep learning programs from scratch without relying on external libraries.

[[[00000000000000001960---a2ffb140ab87d49ef1bccc98a43b107032cfe6ef958f3d6e577c24621adc8a53]]]As a sequel to the previous work &quot;Deep Learning from Scratch&quot;, we will focus on deep learning techniques used for natural language processing and time series data processing.

[[[00000000000000001961---b206e7fbbf338752d7537fa19965baaaf550cc15d94c4e663dbcb5981c33f06e]]]Along with working Python source code, it provides a learning environment where readers can experiment.

[[[00000000000000001962---b9ae8a7fe23c94d90bd51228d1f1168e7ce8933741cd470300b1e13c4ef6bfeb]]]Explain in as simple a language as possible and with lots of clear diagrams.

[[[00000000000000001963---e4ec4bc9ac68adf3e98cb9fedb8d865c4fe75b0d6e8e33cfd86970b5b07aee8c]]]We will use formulas, but we will focus more on source code explanations.

[[[00000000000000001964---1faa033fa4b3fedc42192db9f9fc2bea7ece39d8803ff9b07aee3341aa1447b5]]]&quot;Why is the method better?&quot;, &quot;Why does it work?&quot;, &quot;Why is it a problem?&quot;

[[[00000000000000001965---c972a6858d426f01b3d410ddb3f1fc6d80d11d7797c9cb26eeb8eed9ff9827b1]]]Next, I will list the techniques that you will learn in this book.

[[[00000000000000001966---455c3554563ef51253ee868693afe7e8caefbcc6573ccde4ab078f04c87940d3]]]Text processing with Python

[[[00000000000000001967---408dc55f9961e98ad3081464d67b201fb4c4befc601ed2090ceb5886084ad0b9]]]How to express “words” before the advent of deep learning

[[[00000000000000001968---cbedc0d634e09f5cc542bce98a4196252a61dc60d2bba0758927feeb8bde2b3b]]]word2vec to get word vectors (CBOW model and skip-gram model)

[[[00000000000000001969---aaefda84444cd0b35d526fcd9ef615bb881587c2f730dd457114c3520b722073]]]Negative Sampling to accelerate learning of large-scale data

[[[00000000000000001970---e761ec5dde8d4b3951935367a53796e204f9de73e851126bec6e6580388c8885]]]RNN, LSTM, GRU for processing time series data

[[[00000000000000001971---0be2d7c9f303025bc657e4944c2d59904a136cafba441ea33ecff8f09ebbb4de]]]Backpropagation Through Time

[[[00000000000000001972---b8a99d391db4718ecef099b4699470aa38e0a58fb083a619838fa9c747be5dde]]]Neural network for sentence generation

[[[00000000000000001973---4db13c65417a67b4876197ef74e1e7eb03124b8d31b5908e1d1809060a339f29]]]seq2seq to convert time series data to another time series data

[[[00000000000000001974---5bfec32e5c0f410d6cd1a561b92112859512db3376097b222131e9912e687c24]]]Attention to focus on important information

[[[00000000000000001975---4732c22ef02d9f8942fab29f8bf4734f11ca9ae0167b85ff420ff221e1e9aa96]]]In this book, we will carefully explain these technologies in an easy-to-understand manner, and develop the story so that you can master them at the implementation level. In addition, when explaining these technologies, we do not simply enumerate the facts, but develop the story like a series of stories (as a connected story).

[[[00000000000000001976---5253465d968170be719f3cea542fe7be81f7a3caed3f1ed1b1fd2512c6b15ae0]]]Who is this book for?

[[[00000000000000001977---497ff3e6b8c6edecbfd00bf39ff747385c8c47219180a6833eaf209a5d266524]]]I think it&#39;s important to say, &quot;Who is this book for?&quot; Here is a list of what this document does not do.

[[[00000000000000001978---aaea5c825ca990eaedd98d2a00962c8e97e7098b9aa5d8985cd36c991772dd89]]]We do not provide detailed explanations or introductions to the latest research on deep learning.

[[[00000000000000001979---939689d8ecd9732ab3e4cdc3f1331e7d1c7cb0f0058ad2227b4185f11b7c87c7]]]We will not discuss deep learning frameworks such as Caffe, TensorFlow, Chainer, etc.

[[[00000000000000001980---ff1ab7c5f80d3725b677ecb474b4ae0c5ba01759c41c923a3267291234780d36]]]I will not give a detailed theoretical explanation of deep learning.

[[[00000000000000001981---0c6e57f4502ccb5f7a453ef9e52d308cb234f9c0aa6ea5a84e41d3a5d599840c]]]This book is primarily concerned with natural language processing. It does not cover subjects such as images, audio, or reinforcement learning.

[[[00000000000000001982---8f297431fa3b10359c5cadededa41a5258321df19fdc098a5f8f4ce3f11e2804]]]As such, this book does not cover recent research or theoretical details. But once you&#39;ve read this book, you&#39;ll be ready to take the next step to the latest papers and cutting-edge techniques in natural language processing.

[[[00000000000000001983---399fc1a2beb336fcb3c2a8332d70696804f21b23bba15496ddb914c219264e75]]]Implementation environment

[[[00000000000000001984---d9568e6553e65e3e6a876d80640a0a26e53840c7357262ebd6b9e8939dfe8a8a]]]This document provides the source code of Python (3 series). If you use the source code, you can actually run it at your fingertips. By reading the code and thinking about it, and trying out new implementations of your own ideas, you will be able to solidify your knowledge. The source code used in this document can be downloaded from the following GitHub repository.

[[[00000000000000001985---a6ddadb3bf6f19f9256a0bdfb719f2c89b00444458b1ce6ee887fba06ad6b3b0]]]Now, the goal of this book is to implement deep learning from scratch. Therefore, our policy is to avoid using external libraries as much as possible, but we will use the following two libraries as exceptions. One is NumPy and the other is a library called Matplotlib. By using these two libraries, you can efficiently proceed with the implementation of deep learning.

[[[00000000000000001986---0672f0408139bf375465e9c9e42cd4b28ec3094797a81a476ab9505c3e602fdf]]]NumPy is a library for numerical computation. This library provides many convenient methods for advanced mathematical algorithms and array (matrix) manipulation. In the implementation of deep learning in this book, we will proceed efficiently using these convenient methods.

[[[00000000000000001987---ca080412766db43d6857c7de08d13875b6a6177d1abffc04658e381f31a32a1b]]]Matplotlib is a library for graph drawing. Using Matplotlib, you can visualize the results of experiments and visually check the learning process of deep learning. In this book, we will use these libraries to implement deep learning algorithms.

[[[00000000000000001988---9f0fc5f1c864e086d142b65df9e7ad580b50799a4e12a2bac0674af42fff519c]]]Most of the source code in this book is designed so that it can be processed on a typical PC without taking too much time. But some code—especially training large neural networks—can take a lot of time. Therefore, in this book, we also provide code (mechanism) that can be executed on the GPU in order to speed up the time-consuming processing. We will use a library called CuPy for this (CuPy is explained in Chapter 1, Neural Networks Review). If you have a machine that can use NVIDIA&#39;s GPU, you can install CuPy to process some of the code in this book at high speed on the GPU.

[[[00000000000000001989---4414b7e06ebd5e0763725b72098bb8f71d34ab7f9edb7d30aab9e62c2ef0e219]]]This document uses the following programming languages and libraries.

[[[00000000000000001990---d52e1eac81f63d8c293342d29c36363a1999314e1ba7b05bf8285f2c98f0095c]]]Python 3 series

[[[00000000000000001991---facfb8689f171af2fc985281c34f86687bfb96a115ab9ee68213c60d0d5ab2c9]]]CuPy (optional)

[[[00000000000000001992---8e052bba2167a6835076ab81d6c6f0df2787da4a2df8711b27c88b2b2a79fe43]]]on a journey to recreate

[[[00000000000000001993---a07bd10bec0ebdf672e3e7fb02c15dd0ffe648fb9a7af33146055163084a5a35]]]Technology has advanced, and it has become an era where copying can be done easily. It&#39;s a convenient world where you can easily copy photos, videos, source code and libraries. However, no matter how much technology develops and life becomes more convenient, experiences cannot be copied. The experience of creating something with your hands and taking time to think about it cannot be copied. And such things that cannot be copied should have value that does not change over time.

[[[00000000000000001994---46386025211ad33c43581fbcd497d7e4162608cbaaaac3219f65e0d7c5afb2c0]]]This ends the introduction. Let&#39;s resume our journey to create deep learning!

[[[00000000000000001995---476e512d3f22afdc336217483f037bcbe4fbaa5ef4a6c56f14f7f7cfcb4e5454]]]notational rules

[[[00000000000000001996---30e569f8abc53f20a07c0598be9b8623658b8c37b5b37b40a9a69d7e6a6ba030]]]This document follows the typographical conventions described below.

[[[00000000000000001997---eaa3a1768a21aebf4c749f1933cd17b82875c8382de3e31f2e796a64729c15bd]]]bold

[[[00000000000000001998---7627be7c0fffeeba7455369bffe7e56c44fd561f79bcdca0ab4b03e003e61053]]]Represents new terms, emphasis and keyword phrases.

[[[00000000000000001999---0146b007473307a94b91a89c8c2f6849565f6e675e0fe665cf774251e7b94f75]]]Equal width

[[[00000000000000002000---7dbab4d12d7482897a8df2c32d5551ea07f137050f51d4382e74138b8cad3d67]]]Program code, commands, arrays, elements, statements, options, switches, variables, attributes, keys, functions, types, classes, namespaces, methods, modules, properties, parameters, values, objects, events, event handlers, XML tags , HTML tags, macros, file contents, and output from commands. It is also used when referring to that fragment (variables, functions, keywords, etc.) from within the text.

[[[00000000000000002001---576a3c3df75ae76aa331767cd3e6346541668bccbf1fb67114cd74b02d4fd08f]]]monospaced bold

[[[00000000000000002002---a18891419f3bfee392801649c081187ccca66c2216eec5454f689c91bcbaf135]]]Represents commands or text that you enter. It is also used for code highlighting.

[[[00000000000000002003---a8ad620408c5d0124855d9313f70c2d9497e8efa6f4436fd3d2936a6a0d84790]]]monospace italic

[[[00000000000000002004---ba938b96cb30883f2b1e89b6c5b63d61b577bafcb9315f3cc16497f1e304c89c]]]Represents a character string that must be replaced according to the user&#39;s environment, etc.

[[[00000000000000002005---5ed71a16e78e4ac1e4a2859e1ba20eec46a83994189fa2c953a9b066c523b3b2]]]Represents the reference destination of the source code (the GitHub repository of this document).

[[[00000000000000002006---52d33f0a012b4029eb0de3ecfd6655a60cc14548f486039f88d08f1ff193a592]]]Indicates a hint, suggestion, or additional information about something interesting.

[[[00000000000000002007---852388668b84435a5944b94c94f6e4912d1b0a0f468808655f1e125ec1cf75d3]]]Represents cautions or warnings, such as library bugs or common problems.

[[[00000000000000002008---3f4ba0664b5911aa7e85d2a0bed8e833b6c0f07636c0b05d7f28a3c2718ff197]]]opinions and questions

[[[00000000000000002009---25800ed37c8613ea755e1eaeb3f6b2a5c3614cd58d8f632f35faf9ae8cfe5017]]]We have made every effort to verify and confirm the contents of this document, but you may notice errors, inaccuracies, misleading or confusing expressions, and simple typographical errors. If so, please let us know so we can improve it in future editions. Suggestions for future revisions are also welcome. Contact information is as follows:

[[[00000000000000002010---5120b30e23f0646f3a418cbb6d18df0283e91b171be2b9c3a910b3aff3d0337e]]]O&#39;Reilly Japan Co., Ltd.

[[[00000000000000002011---ec8d2613e28e64bb50245bc847105532b2de636be27692fcc83ae4d8ad1114b0]]]E-mail japan@oreilly.co.jp

[[[00000000000000002012---27022ad01a7c677d2c3b78e89be1b554229f1fdbcb01807b95d083bd5405e0bb]]]The web page for this document can be accessed at the following address:

[[[00000000000000002013---6d85d6b71120616c6f56bb6fbbf29047f56f4d864ead0270f78cc99311f2d81e]]]For additional information about O&#39;Reilly, please visit the O&#39;Reilly website at:

[[[00000000000000002014---a6ea4185ec562859a24bf754fb86337fad51ed222ecba568f153de6c2a332da1]]]https://www.oreilly.com/ (English)

[[[00000000000000002015---d3076c9ecabd5c8dfe5fe91d3fb0ad49efaf30da2df7e295d7c897ee2c9da150]]]in conclusion

[[[00000000000000002016---a1b0c0b42490f0669c3ad50ac32281d56933fa32008decbe166f6e5e2a32976e]]]To climb a steep hill, it is important to walk slowly at first.

[[[00000000000000002017---9bfcb58d14d34021f2eda5574aaad2d234752d964f9a51d3739016af1c236202]]]—— Shakespeare

[[[00000000000000002018---85fec37ad6e3dbfe977bfa99b29dad8d6cdcba10e4b7bb028ccad2a86d7f5dd8]]]So far, we have walked slowly through the world of deep learning with the theme of natural language processing. I&#39;ve actually implemented a lot of code for natural language processing and learned a number of important techniques through a lot of experimentation. I hope that the readers have learned something through such an experience. And, as an author, there is no greater joy than if you can feel the fun and depth in it.

[[[00000000000000002019---db174d1f0d09a6a874d985cc47c31a9e2e0673778d25a48045520c1bed7613e0]]]Currently, the field of deep learning is progressing at an accelerating pace. Thousands of papers are published almost daily, and new ideas are constantly being proposed. Unfortunately, it would already be impossible to read them all. No one can accurately and clearly predict what direction deep learning will take in the future.

[[[00000000000000002020---2d2863f20a7d02191173ebd132b374216d2b0f2e8f0b884cb36bc3b12a471caa]]]However, on the other hand, important technologies for deep learning have (to some extent) been solidified. I believe that the techniques related to deep learning learned in this book will continue to be important in the future. I hope that by using the knowledge you have learned in this book as a foothold, you will be able to steadily advance into the ever-expanding world of deep learning.

[[[00000000000000002021---0367782cc07bbc2ccad8a9a351ffaadae7f4dc68c8d24ef596b7fa576b2ffc54]]]This is the end of the book. Thank you for reading this far. Historically, we are the generation that is witnessing the process of deep learning permeating the world and changing the world. The author happened to be born in such an era, and just happened to write a book on such a theme. However, I am glad that I was able to interact with you through this book through such a coincidental connection. thank you very much.

[[[00000000000000002022---32ab2a02a0d4b95b0a2ededc4dfdc60d2b0051008128f92fbc152e0e8facf0b8]]]Acknowledgments

[[[00000000000000002023---b9a79cd109620c8d4326a14c8773185a6fc6ffbf259e83bf2043b0b81b331de1]]]This book exists because of the great pioneers who have pushed deep learning, artificial intelligence, and even the natural sciences. First of all, I would like to thank such people. I would also like to thank the people around me for their support and cooperation. They have directly and indirectly encouraged and supported me. thank you very much.

[[[00000000000000002024---b630b440c1c8bb694fc568aba690cd654cf951af74c9d70e7e1db09dc545f965]]]In writing this book, I adopted a proofreading method called &quot;public review&quot; as a new attempt. For this public review, we have published the manuscript of this book on the web for anyone to view and comment on. As a result, we received more than 1,500 useful comments during the review period of only about one month. We would like to thank everyone who participated in the review. It is undoubtedly the help of such reviewers that made this book even better. I&#39;m really thankful to you. Of course, any deficiencies or errors in this document are the responsibility of the author, not the reviewers.

[[[00000000000000002025---7356711be66ff189ecfc11b18ab789c3fbcbe62ee9409848239f60184fc10aa1]]]Finally, thanks to people all over the world for this book to exist. Many people whose names I don&#39;t know influence me every day. Without one of them, I don&#39;t think there would be a book like this (at least in its current state). The same can be said for the nature around me. Rivers and trees, soil and sky, there is my daily life. I would like to express my heartfelt gratitude to the nameless people and the nameless nature.

[[[00000000000000002026---6dcd71e49fcb0e6742beed9713a299dfdb7046dbd576f53c740305f2abef2c45]]]June 1, 2018

[[[00000000000000002027---b035edd8672762cb270a81ac48bf45f2b02c7a631a1fe74d3a58fed6b8a8c4bd]]]Yasutake Saitoh

[[[00000000000000002028---e196c210c5aca71f09475a94ca1750789fda03bb1b9bf85bd1fe22554073460b]]]peer review

[[[00000000000000002029---8567d277c6ee9e10d2885f462aa30bd639cea6354ca8bb9219f9fa279796cf78]]]Akihiro Yorita Teruyuki Tamagawa Akinari Yoshinaga Atsushi Takayama Taiki Sato

[[[00000000000000002030---0b997692a7b64c877ed39bcd174cee30e74a0dc96bba1fb2e0151253fdb2a04e]]]General Kobayashi Hidetomo Takaya Daishi Ezaki Hyodo Oki Shun Suzuki　

[[[00000000000000002031---d313753790b8d150df5e560d46d4f5f4e4b52b52ee317d8867d6876519ab7975]]]Hideaki Fujioka Hideaki Tajima Hidetaka Harada Hideyuki Kawamura Hiroki Tanioka

[[[00000000000000002032---6a10999f808409b5895d3382f713ebe981f99de89d4294f887fb454ea22b0672]]]Hiroshi Arai Kai Fuse Hiroyuki Fuchigami Hisao Setoguchi Isamu Fujita　

[[[00000000000000002033---0bb42b9f14ba225c6c2017723a5defff9a6e043773267ffb0999252546e2e38f]]]Takanori Ishikawa Kageyuki Kamikubo Hiroyuki Kobe Takashi Kanazawa Kazuhiro Nagata

[[[00000000000000002034---d7a4b6b609f92f5ccc034f3c6e0821420210be86d509a990471d952aa61aa739]]]Kazunori Ishizu Kazunori Koga Tomoya Sasaki Keisuke Ohtaki Kenta Yamauchi

[[[00000000000000002035---e1cf0a7c4798f3c879501ae280ed4f41113b9239601465b0e63aa8b0a0bf54ea]]]Manabu Ozawa Manabu Terada Masahiko Hasegawa Masaki Yamamoto Masanori Fujii

[[[00000000000000002036---8df739b338a72f523d8e5108210778b24beec5d5220d5e6cfbd79613d5b564c6]]]Minoru Mizutani Yasuyuki Miura Muneyuki Noguchi Ryosuke Naito Naoki Yonezawa

[[[00000000000000002037---f9a4a5c717c0889de86e96e84467afe0a336ddeb3fc780e5a1a422c35be70fac]]]Naoji Sato Toshiaki Noguchi Nobuhiro Ito Osamu Yamashita Ryusuke Sato

[[[00000000000000002038---215c445d2f980cdbdbfbf16c8262aae9bbe6267dca5d2f9f3564916e41ace663]]]On Sato Kiyoshi Sato Masamine Shintani Shigeru Kobayashi Yutaka Shimizu　

[[[00000000000000002039---15e54f18981cd383626058ea8b7e3f1dca7b6d40d097dc47250dd6f1e6166aac]]]Shinsuke Nagata Shinsuke Sugita Shinya Wada Hirotomo Shiraki Sho Nakamura　

[[[00000000000000002040---7d6a405376398d3e75969799d6e8bfeefe806ccba6919891d75b539c858837a9]]]Shota Senda Shuhei Fujiwara Eita Suzuki Koshi Abe Takeo Miyasaka

[[[00000000000000002041---73f73f373accb7f0064ee8fd3e42809f515a198d8dc7ca787498afde9a5abefd]]]Takeshi Wakasugi Takuya Suzuki Takuya Shinmura Satoshi Tanaka Tianxing Tsai 　

[[[00000000000000002042---9d8e01b2eec9d0fa1dcc08422aa4a43f8796057df770956fd25797267628485a]]]Tetsuro Ishikawa Toru Takajo Toshiyuki Morihara Tsubasa Ohno Taiho Doi

[[[00000000000000002043---4d46ea1f5622337aa4c3a8cba93a599b7b5dd08721d26428c56edfeff1b5b093]]]Taishi Nishida Yasutomo Takano Yoichiro Hasebe Yoshihiro Ogiwara Yudai Hayashi　

[[[00000000000000002044---55dcfc5df640e378acacee2c851cd2edb1be102ff93fdf5db7faf624f720aafe]]]Yuichi Masumiya Yuji Koike Yuka Tobinaga Yuuki Yoi Yusuke Fujimoto

[[[00000000000000002045---247319fac19d41c5462e1a0481266945f7dfa0306c2129490c56118a1ff4f4a5]]]Yusuke Furukawa Yusuke Nishiguchi Yuto Yoshinari Yu Mizuhara　　　　　　 　　

[[[00000000000000002046---0340a746452cd90aec6e427193c5a698a46544d21489ad0cff0e15dc2588cf16]]]TAICHI KAWABATA AIMY —Yamagata Artificial Intelligence Community—

[[[00000000000000002047---da2dadcd3ff9c5bc455a1fb290f57841ab906e42d94613ffa9e978a8770e3c9c]]]production

[[[00000000000000002048---316b62b31be496c93aa368122b16d3102451dd5ff29861bf5c5389a630d0e9fb]]]Takeshi Mutoh

[[[00000000000000002049---fad57992d6fd1dd6c738c1cb96193c1656aebb97f28bd57db816e4ff5273569f]]]Moe Masuko

[[[00000000000000002050---11f9049ddab593adbbeccbad6d070f959cb4d60f41351acce2a9ead4281028b8]]]edit

[[[00000000000000002051---2ed590e436542778edbed46bcadd8fe8162eaa0de4a6abe060a194f3fc863e0e]]]Naoki Miyakawa

[[[00000000000000002052---c882f036e3715fc8ab51e4cfa1cad2f96b626cc3ecc42252e2f4c4b1ebeb920d]]]Mio Iwasa

[[[00000000000000002053---7e30891d89bcfa73947df871d7de8cfcd15bf2b2cfd998167599e638f6e1288a]]]Sara Koyanagi

[[[00000000000000002054---a66fc8ffc5427c249ed59925da9020f42c09148dc5dbe0cbeeb81fd6d305193e]]]Appendix B

[[[00000000000000002055---1fe1494fd5ecb4c82608b5bac5a187f0226729595394323345a8521aeaad2bf1]]]Run WordNet

[[[00000000000000002056---d91e92f12b600cc3627fe204ddef1d3bb5dfd35a6fc74292279643613d928de5]]]In this appendix, we will actually run WordNet. Let&#39;s see what kind of &quot;knowledge&quot; exists in WordNet by actually running it from Python. In addition, the implementation that will be done from now on will be limited to a simple experiment for the purpose of getting a feel for the atmosphere of the thesaurus.

[[[00000000000000002057---7039f9c8bbee331fc020b719a13f14130b3a244881a6b383f5628808e394c25b]]]Here, we will only give a brief introduction to WordNet and NLTK. A detailed explanation of NLTK can be found in &quot;Introduction to Natural Language Processing&quot; [14]. If you are interested, please refer to it.

[[[00000000000000002058---33c414e8a2a84cab3c8228d029af0b8f58a5d56e0b179061d1e0bc95d8237a29]]]Install NLTK

[[[00000000000000002059---a64c3469a8af23e6cd500d1fc0f718d1364393530818dfc32bb503e909399101]]]To use WordNet from Python, use a library called NLTK (Natural Language Toolkit). NLTK is a Python library for natural language processing and contains many useful functions related to natural language processing. Examples include part-of-speech tagging, parsing, information extraction and semantic analysis.

[[[00000000000000002060---6d6d237327e86e5cdda74595df34e004fd4f25ed02a2435a8da0131176dca3a4]]]Now let&#39;s install NLTK. There are several installation methods, but here we will explain the installation using pip (for other installation methods, please proceed as appropriate according to the reader&#39;s environment).

[[[00000000000000002061---17d0312b8e8c3a42f345be23ca02c542aefe4e16d1723c3a5abeef4404905b49]]]To install NLTK, enter the following single line from Terminal:

[[[00000000000000002062---a087a468a7a44a571185b17ddf62f8cbc182cba6cb7af700073777251eea7201]]]This completes the NLTK installation (it will take some time). After installation, import NLTK to make sure it was installed correctly.

[[[00000000000000002063---442fbbbb06fd35c0b2f3ff7deab8706f5e494c126c68c2e77a7819a74c5b4c3f]]]Here I started the Python interpreter and imported NLTK. If NLTK is installed correctly, you should not see any errors as above.

[[[00000000000000002064---e0975adbb03ff2b3fe268760adce10678b7a4535d6656f9b5a69fbd1d5423cb8]]]Get Synonyms on WordNet

[[[00000000000000002065---671986d524fed955312ca62daa4426e872eca9724c85592a400d59786b18b234]]]Now let&#39;s see WordNet in action. To do that, import the wordnet module from nltk.corpus.

[[[00000000000000002066---053aebcc2549aafa827b187083de5f9fd782969673de3bfd2f0cc66c3f24a6ba]]]You are now ready to go. Let&#39;s try to find synonyms for the word &quot;car&quot;. First, let&#39;s see how many different meanings exist for the word &quot;car&quot;. To do so, use the method wordnet.synsets().

[[[00000000000000002067---beabd1a560101c2baa861de833f1d26673fe9309571295ced35a2c7d5526c0b2]]]WordNet organizes each word into a group of synonyms called synsets. To get the synonym group for &quot;car&quot;, just call the method wordnet.synset(), with one caveat. The word &quot;car&quot; (like many words) has multiple meanings. Specifically, in addition to the meaning of &quot;automobile&quot;, there are also meanings such as &quot;(railway) vehicle&quot; and &quot;gondola&quot;. Therefore, when retrieving synonyms, it is necessary to specify which meaning (out of multiple meanings) it corresponds to.

[[[00000000000000002068---e34e2704745df8c448a66d3b929b76f6804191a636f49c4c57ca9d90722aa18c]]]Now let&#39;s get the synonyms of &quot;car&quot; in WordNet.

[[[00000000000000002069---a834d76bed4fdc3168e3dc0695c82df447c1258d6408da00b58f8398854d6d25]]]Here, a list of 5 elements was printed. What this means is that the word &quot;car&quot; has five different meanings -- five different synonym groups, to be exact.

[[[00000000000000002070---753d3173b218a119b3982ebba58a7dd280ff59cce3554a1a445b16b47177d8f6]]]It is also important to note that the elements in the list above display the &quot;headword&quot; of car. A &quot;headword&quot; used in WordNet is specified by three elements separated by dots, as shown in Figure B-1. For example, the headword &quot;car.n.01&quot; indicates that it is the meaning (group) of &quot;the first of the nouns car.&quot;

[[[00000000000000002071---e82a7b8e093af5ade6d3c555c96aadf861ee4e5410a16ccc5bd8c5fa53b3c56e]]]
Figure B-1 How to read &quot;headwords&quot; in WordNet: n is the first letter of noun


[[[00000000000000002072---628e697c1ef02077c9f4a0e7edc0481b3eb7ee49e9372e518bb1d9f7c48598ce]]]Many words have multiple meanings. WordNet uses &quot;headwords&quot; to specify a word with a specific meaning from among multiple meanings. Therefore, when specifying a word name as an argument in a WordNet method, instead of specifying &quot;car&quot;, use an entry word such as &quot;car.n.01&quot; or &quot;car.n.02&quot;. There are many cases where

[[[00000000000000002073---d6fde12392e380d9465744dc79037d889d18e836ec79e728d87a987504b5933e]]]Now, let&#39;s check the meaning of the synonyms specified in the headword &quot;car.n.01&quot;. It uses the wordnet.synset() method to get the synonym group for &quot;car.n.01&quot;. Then it calls the definition() method on that synonym group.

[[[00000000000000002074---49f3701fab6c7a55c93cf082f13c128bd7692295b4852ee1b30600460fd7a82f]]]  # synonym group &gt;&gt;&gt; 

[[[00000000000000002075---f628d388b80a5a506c37a22bfcfe265953e200277fa296a321667ede1e40893a]]]The literal translation of the above result is &quot;a vehicle with four wheels, usually propelled by an internal combustion engine.&quot; This is the meaning of the synonym group &quot;car.n.01&quot;. Note that the definition() method used here is primarily used by humans (not computers) to understand the word.

[[[00000000000000002076---676d127aad03560ed1cd503e8652960a0c0d57df0934130038f72647f226160d]]]Now, let&#39;s actually see what words exist in the synonym group for the headword &quot;car.n.01&quot;. To do so, use the method lemma_names().

[[[00000000000000002077---1861f1b1547ae9fa5f55b84fff6d563ce5283270d0092cc7fce5c4ad647e3a7c]]]By using the method lemma_names() like this, you can get the word names that exist in the synonym group. Looking at the results above, the word ``car&#39;&#39;---to be precise, the headword ``car.n.01&#39;&#39;---has four synonyms: auto, automobile, machine, and motorcar. You can see that

[[[00000000000000002078---8725b40bf2fe5404df167a9bdd392a0f824551b67307b32ad97e53a805e4edb6]]]WordNet and word networks

[[[00000000000000002079---f8ee7d81214be2c0645dc8659d11910a4d790b7ee31f6d2b94de0a8b2e522159]]]Next, let&#39;s use the word network of &quot;car&quot; to see the semantic upper and lower relationships with other words. To do so, use the hypernym_paths() method. Hypernym is a word mainly used in linguistics, and has the meaning of &quot;hypernym&quot;.

[[[00000000000000002080---39cbce0a24d2f2d7b0ac6c4597e7aaf1de199bc1206489786767206c92732997]]]Looking at the results above, we can see that the word &quot;car&quot; starts with the word &quot;entity&quot; and follows the path &quot;entity→physical_entity→object→...→ motor_vehicle→car&quot; (here, &quot;heading The notation of &quot;word&quot; is omitted). Looking at each word specifically, above &quot;car&quot; is the word &quot;motor vehicle,&quot; and above that is the word &quot;self-propelled vehicle.&quot; Come. And if you go further up, you&#39;ll come across abstract words like &quot;object&quot; and &quot;entity.&quot; Thus, in the word network that makes up WordNet, each word is arranged in such a way that the words are more abstract at the top and more specific at the bottom.

[[[00000000000000002081---0192c660139b7122ad2475840ed09dc5de26a333f1f0b0feb2ef46b326932c68]]]In the example above, car.hypernym_paths() returns a list. Elements of that list contain specific route information. The reason we return a list is because there are multiple paths between words. In the above example, there are multiple paths from the starting word &quot;entity&quot; to the ending point &quot;car&quot; (some words may have only one path).

[[[00000000000000002082---49760de5c3acc93da099d229f47e97fbd8d1d74aba4e4ccde77a797ed28aab2d]]]WordNet Semantic Similarity

[[[00000000000000002083---f8da9086482dcea028611d975ca028e3821197630b7447191be137219ae49b98]]]As we have seen, in WordNet many words are grouped by synonyms (synonyms). Also, a semantic network is built between words. Knowledge of such connections between words can be used in a variety of problems. Here, let&#39;s look at an example of calculating similarity between words as an example.

[[[00000000000000002084---3aa262d1e752f1ed5b5fc1e391c90a970b025029558633f60510caddf4742b02]]]To find the degree of similarity between words, use the method path_similarity(). This method returns the degree of similarity between words, which is a real number between 0 and 1 (the higher the number, the more similar). Now let&#39;s actually find the degree of similarity between words. Here, we will calculate the degree of similarity of the three words &quot;novel&quot;, &quot;dog&quot;, and &quot;motorcycle&quot; to the word &quot;car&quot;.

[[[00000000000000002085---0acb6e3162691c9a373609081560c70889aca0e9bc0a590192f16750bd5f89ed]]]Looking at the results obtained here, the word &quot;motorcycle&quot; has the highest similarity to the word &quot;car&quot;, followed by &quot;dog&quot;, and the least similar word is &quot;novel&quot;. &quot;Thats how its going to be. Also, looking at the similarity values, the similarity between &quot;car&quot; and &quot;motorcycle&quot; is high, many times higher than the other two words. Looking at these results, it seems that it is certainly close to our sense.

[[[00000000000000002086---a22604cf21d23f6353ed632822434403f55d1722d64d08c14f3cabfc39b72405]]]By the way, the method path_similarity() used in the above example, behind the scenes, the similarity between words is calculated based on the common path of the word network as shown in Figure B-2.

[[[00000000000000002087---406979a2b8df9dfa0905f9edbaa0b4b0f120b16ae3bd652c5b3d48d8870eac8c]]]
Figure B-2 Calculating the semantic proximity of words based on the common path of the word network (dashed lines indicate multiple words in the middle)


[[[00000000000000002088---7837231e9fec5302467399b04ad22210e4bdada35f8a6209ad2a0416e63fc9c3]]]Figure B-2 shows an excerpt of WordNet&#39;s word network—with middle words omitted. From this figure, we can see that &quot;car&quot; and &quot;motorcycle&quot; have many routes in common. In fact, the route to the word &quot;motor vehicle&quot; second from the bottom is the same. On the other hand, if you compare &quot;car&quot; and &quot;dog&quot;, the path branches at the word &quot;object&quot;. Moreover, if you compare &quot;car&quot; and &quot;novel&quot;, they already branch at the top word &quot;entity&quot;. The method path_similarity() uses this information to calculate the degree of similarity between words, and the result (in this example) is close to our perception.

[[[00000000000000002089---4ae0e22cedcb9a11057bbfc594b0338ab2b1966da3817ac0c5185ba9a5013900]]]In this way, word networks can be used to calculate the similarity between two words. Being able to find the similarity between words means that the semantic closeness between words can be measured. Such work cannot be performed correctly unless you understand the &quot;meaning of words.&quot; In that respect, the thesaurus can be interpreted as (indirectly) giving the computer &quot;word meanings&quot;.

[[[00000000000000002090---ff9a3b276bb76bcb33a442d238354e159c9e97e1a9539f34731d3c548ba499b8]]]In addition to the path_similarity() method, WordNet provides several methods for measuring similarity (such as Leacock-Chodorow similarity and Wu-Palmer similarity). Interested readers can refer to the WordNet web documentation [18].

[[[00000000000000002091---3a0824baa1752fb7403fd9483c25c1f2909dbae9da80b9699f1c258f4cc88ea5]]]● Author introduction

[[[00000000000000002092---299ac4470df01946755c3411ec8c9fd68793965c598cb2725a530e9847394c99]]]Kouki Saito

[[[00000000000000002093---62e872ae3e8cd22d9ee84b7fdedee936ba68a4ed199ac73d39db7388487cce34]]]Born 1984 in Tsushima, Nagasaki Prefecture. Graduated from the Faculty of Engineering, Tokyo Institute of Technology. Currently engaged in research and development related to artificial intelligence at a company. He is the author of &quot;Deep Learning from Scratch&quot;, and translated books include &quot;Practical Python 3&quot;, &quot;Theory and Implementation of Computer Systems&quot;, and &quot;Practical Machine Learning Systems&quot; (O&#39;Reilly Japan).

[[[00000000000000002094---edc3efb149a25b9613ce36ac49d50fa08fbda9e7b7120a27b83d2c9a69b24a3f]]]References

[[[00000000000000002095---0665f6e569e243e6a63654c868a921ff44df12225b9cfe50eada72af69f21993]]]Python-related

[[[00000000000000002096---5e7ca4f6145cb35e0f43539e77296eae6f2e858be002040f6c94af36a109392d]]]Basics of deep learning

[[[00000000000000002097---15a1c0fdff5b013bf50b5ac4a3456ea381e8fbbfc6863a02113b6b55323db0b4]]] Yasutake Saito: &quot;Deep Learning from Scratch: Theory and Implementation of Deep Learning Using Python&quot; (O&#39;Reilly Japan)


[[[00000000000000002098---c3c6c9178b3babd6748bf874993c10802700cd515b32d4d2370ec956387f802b]]]Natural language processing by deep learning

[[[00000000000000002099---60f11fcfc4b2ad5dda1d386f3c6f1045b263c63f911974c24289cc34434210e8]]] Yuta Tsuboi, Yuya Unno, Jun Suzuki: &quot;Natural Language Processing by Deep Learning (Machine Learning Professional Series)&quot; (Kodansha)


[[[00000000000000002100---1e9de21307907c5309bffffaec8cd0ad9f03c0a6f3ac0fce48645a9c0d286f5f]]]Natural language processing before the advent of deep learning

[[[00000000000000002101---7464c20ff12095f6b51032c798454532c4636254e7008dd222583027e82cff46]]] Steven Bird, Ewane Klein, Edward Loper: &quot;Introduction to Natural Language Processing&quot; translated by Masato Hagiwara, Takahiro Nakayama, Takaaki Mizuno (O&#39;Reilly Japan)


[[[00000000000000002102---2931a70b642bdbbf76348cbbbff6f8eb87ea4a7ddb50d227e7738f2ed69c1d21]]] Jeffrey EF Friedl: &quot;Detailed Regular Expressions 3rd Edition&quot; Longtail Co., Ltd. / Translated by Takahiro Nagao (O&#39;Reilly Japan)


[[[00000000000000002103---59fa732b4ebf8ebd55fcc35a9f4429da4a525df7fd30e3ae54a748d7766caabb]]] Christopher D. Manning, Hinrich Schütze: Basics of Statistical Natural Language Processing, translated by Tsuneaki Kato, Genichiro Kikui, Yoshihiko Hayashi, Tatsunori Mori (Kyoritsu Shuppan)


[[[00000000000000002104---608410914882ca56290c1a484711c6e2e1117176522545352e275cf81d4c5e37]]]Word vectors by count-based method

[[[00000000000000002105---23928596afc7fbad7ed5c6b85b02f25f71b68237bf83897a49ea223a25326062]]]word2vec related

[[[00000000000000002106---71c064e33cf88542e0873d7f3c634108e2a1d7e5a44274b6b2f21fa2650d7f06]]]RNN-related

[[[00000000000000002107---813668965e24c7ef4dca7189a12288be9b4454f54c06756aea89253afac6b1c4]]]Language model by RNN

[[[00000000000000002108---8c1ad5c5b94ebffc40da363279151d432a8def228067eb0b99bee477abb39467]]]seq2seq related

[[[00000000000000002109---532a585057bf94d38ce61d281452e449c03d867ea509aea01e97a99a08e5c8e2]]]Attention-related

[[[00000000000000002110---686b0e2789ba12960a96657b39acab5f56a58cb1ba42268db1e31b05a34b958f]]]RNN with external memory

[[[00000000000000002111---1c730b625c1e38795f23a2ad626ab6abb0cf204565b688e7387052eb57ca49c1]]]Chapter 8

[[[00000000000000002112---f27eccbdcdee1d9df600993347f30d5cba7223d7c67639ea8e050eba14c6fed4]]]attention is everything

[[[00000000000000002113---4cf3f74a5bb27f73cd5e04a6b434c9c3b38938c289a30a373304f9837113df7a]]]——Paper title of Bazwani et al.[52]

[[[00000000000000002114---b117c52521c1a36505024ac7eeaf8a65b97e831c85e75d73363c7ab522596123]]]In the previous chapter, we generated sentences using RNN. And by concatenating two RNNs, we were able to transform the time series data into another time series data. We call it seq2seq and we&#39;ve managed to get it to solve simple problems like addition. We have also made some improvements to seq2seq and confirmed that simple addition can be solved almost perfectly.

[[[00000000000000002115---dd62028dedfa59218448815eebf9e39fccfb829e52a25418734d10b67435a0c3]]]In this chapter, we will further explore the possibilities of seq2seq—and the possibilities of RNNs. And here comes a powerful and beautiful piece of technology called Attention. This attention technology is undoubtedly one of the most important techniques in the field of deep learning in recent years. Our goal in this chapter is to understand how Attention works at the code level. And I would like to apply it to a real problem and experience its wonderful effect. Let&#39;s move on to the final chapter!

[[[00000000000000002116---e035bb9efb1cb391439421360002361241cabeff2493a82915467e63ca246b4e]]]How Attention works

[[[00000000000000002117---a411c518d9dfe072b785074ea1a1a5444c29df313588a66f0e67ecbf1e9b50ef]]]As we saw in the previous chapter, seq2seq is a very powerful framework with many possible applications. Here, we introduce the idea of an attention mechanism (hereinafter abbreviated as Attention) that makes seq2seq even more powerful. With this attention mechanism, seq2seq can be made to pay attention only to necessary information, just like us humans. Furthermore, by using this Attention, it is possible to solve the problems that seq2seq had so far.

[[[00000000000000002118---919a656b6e971d1470bbe5288628f5bbd65a86de53988af4db9dced2ab4d27ab]]]This section starts by pointing out the problems with the current seq2seq. After that, while explaining the mechanism of Attention, I would like to implement it in parallel. Let&#39;s look again at what seq2seq does.

[[[00000000000000002119---d6c9c0a876f7cd610af3ba5d8e0a341edb81b8bfc73e8e7b288c236793005936]]]I improved seq2seq in the previous chapter, but it was rather a &quot;small improvement&quot;. The technology called Attention, which I will explain below, is a “major improvement” that solves the fundamental problems of seq2seq.

[[[00000000000000002120---e90ab4c65bf25a2b0cdca3536685f540de5ce7414b189c055d6c3b08f908b236]]]Problems with seq2seq

[[[00000000000000002121---ff9532d8a6088aa8dd70e227e2acbe804c965f985f696190e2cc210fd8d8dc75]]]In seq2seq, Encoder encodes time series data. Then pass the encoded information to the Decoder. At this time, the Encoder&#39;s output was a &quot;fixed-length vector&quot;. In fact, there is a big problem hidden in this &quot;fixed length&quot;. This is because fixed-length vectors must always be converted to vectors of the same length, regardless of the length of the input sentence—no matter how long it is. Using the translation example from the previous chapter, whatever text is input must be packed into a fixed-length vector, as shown in Figure 8-1.

[[[00000000000000002122---eaaa5bcfa9dd2a80a20a6cf320dd81082bf77bd863f90e1427b28018f81db3d6]]]
Figure 8-1 Encoder Pushes Into Fixed-Length Vector Regardless of Input Sentence Length


[[[00000000000000002123---1f7f0195c090f5a575345bbbf3a30a33baf2323f05cde1903334d10fe474e730]]]The current Encoder converts any long sentence into a fixed-length vector. Like a wardrobe full of clothes, the Encoder forces it into a fixed-length vector. But that soon hits its limits. After all, the necessary information overflows the vector, just like clothes overflowing from a closet.

[[[00000000000000002124---6f435793df1a5f03acb733d3dcbffcc8f742ac6c4e22a2e79d1155b60a84b1b2]]]Now let&#39;s work on improving seq2seq. First, improve the Encoder, then improve the Decoder.

[[[00000000000000002125---acc20379e79422b895a929b9bf1a52f44325b05b49888f12832fd785dfd33a23]]]Encoder improvements

[[[00000000000000002126---1171ad90ee5b46d14c9b909e5992996cd1143c62e8f99237c5d596f59329d6f8]]]So far we have only passed the last hidden state of the LSTM layer to the Decoder. However, the Encoder&#39;s output should vary in length depending on the length of the input text. That is the improvement point of Encoder. Specifically, we use all the hidden state vectors of the LSTM layer at each time, as shown in Figure 8-2.

[[[00000000000000002127---9049106a90763f43a1250abd824299f98826ee02a8d0bace585da0b972067238]]]
Figure 8-2 Use all the hidden states of the LSTM layer at each time (each word) of the Encoder (denoted here by hs)


[[[00000000000000002128---d58c9878364fdac5a586f4e5e0d2ae896432acaa09435524198459bf1bfe4f83]]]As shown in Figure 8-2, if we use all the hidden state vectors at each time (each word), we can obtain as many vectors as there are input word sequences. In the example in Figure 8-2, five words are input and the Encoder outputs five vectors. This will free Encoder from the constraint of &quot;one fixed length vector&quot;.

[[[00000000000000002129---25548a51e59e91bb7aa5a1ced97e989277967ab96b5c52d826cac8c8a6193e21]]]In many deep learning frameworks, when initializing the RNN layer (or LSTM layer, GRU layer, etc.), &quot;return the hidden state vector at all times&quot; or &quot;return only the last hidden state vector&quot; You can choose either For example, in the case of Keras, when the RNN layer is initialized, it is set by giving True or False to the return_sequences argument.

[[[00000000000000002130---d78bdafeba2af67aaa53137e609dbd7b2a8c64ddf115cf29cd40973b715a99aa]]]Now, what I would like to focus on in Figure 8-2 is the &quot;content&quot; of the hidden state of the LSTM layer. At this time, what kind of information is packed in the hidden state of the LSTM layer at each time? One thing that can be said is that the hidden state at each time contains a lot of information about the word that was input immediately before. In the example of Figure 8-2, for example, when the word &quot;cat&quot; is input, the output (hidden state) of the LSTM layer is most affected by the input word &quot;cat&quot; at that time. . Therefore, the hidden state vector is considered to be a vector containing many &quot;cat&quot; components. Considering this, the matrix hs output by the Encoder is as shown in Figure 8-3. It seems that it can be regarded as a set of vectors corresponding to each word.

[[[00000000000000002131---be60bdd3c48227da4e17f57628bd3511d1241ed336bf8d096f1e2c2f711d0a0a]]]
Figure 8-3 Encoder output hs has as many vectors as there are words, and each vector contains a lot of information corresponding to each word


[[[00000000000000002132---3f154bb0451d112a7c56e05d94a51d579adc539388cecaf65f975379afc70228]]]Since the Encoder processes from left to right, the &quot;cat&quot; vector above would contain information for exactly three words: &quot;wa&quot;, &quot;wa&quot;, and &quot;cat&quot;. Considering the overall balance here, you might want to include information about the &quot;surroundings&quot; of the word &quot;cat&quot; in a well-balanced manner. In such cases, a bidirectional RNN (or bidirectional LSTM) that processes time series data bidirectionally is effective. Bidirectional RNN will be explained later. Here we will still use a unidirectional LSTM.

[[[00000000000000002133---968df86c61bb8771a65f7b151eddf4aa6cbb9ae18c940ff454ba5c0ef5b36e00]]]The above is the improvement of Encoder. Our improvement here is simply extracting the Encoder&#39;s hidden state for every time. But with that small modification, the Encoder can now encode information proportional to the length of the input sentence. So how should the Decoder cook the output of that Encoder? Next, let&#39;s move on to improving the Decoder. Since there is a lot to talk about improving the Decoder, we will divide it into three sections.

[[[00000000000000002134---d583cb0451fa47d6085b1b7eff2681c1dac5de33c8eab26ced46f56e40b6a1aa]]]Improvement of Decoder ①

[[[00000000000000002135---e5bbe4a273c430c3db37c4c08690fe748f4af53cdfbdad42012129a5a219cde4]]]Encoder collectively outputs the hidden state vector of the LSTM layer corresponding to each word as hs. This hs is then passed to the Decoder for time series transformation (Figure 8-4).

[[[00000000000000002136---53257b0146970201c13e1fb49fb861021cc07a546842223308035ba5708f5210]]]
Figure 8-4 Relationship between Encoder and Decoder


[[[00000000000000002137---34ecb1c3c708258a0062d62843beabcf38cc605710d557b088f716c10d079d15]]]By the way, in the simplest seq2seq shown in the previous chapter, we passed only the last hidden state vector of the Encoder to the Decoder. More precisely, we substituted the &quot;last&quot; hidden state of the Encoder&#39;s LSTM layer with the &quot;first&quot; hidden state of the Decoder&#39;s LSTM layer. When actually represented graphically, the Decoder layer configuration can be written as shown in Figure 8-5.

[[[00000000000000002138---a4528aebc0518754edbebdfdae9c72d651537b073cdd1a28f05e60c3a16d5a55]]]
Figure 8-5 Decoder layer configuration in the previous chapter (during learning)


[[[00000000000000002139---a6a109c9399cc30708d6ba4e8f3a78af90fd0cc69a5e9f7f069117f5bb40f708]]]The Decoder in the previous chapter used only the last hidden state in the Encoder&#39;s LSTM layer, as shown in Figure 8-5. Using the symbol hs will extract just the last line and pass it to the Decoder. Now let&#39;s move on to improving our Decoder so that we can take advantage of all this hs.

[[[00000000000000002140---74b4bb5dc11c914298d900a048df264901099769bcc74cb028c56efd9687c4fe]]]So what do we do in our heads when we translate? For example, how about translating the sentence &quot;I am a cat&quot; into English? I&#39;m sure you&#39;re using knowledge such as &quot;I = I&quot; and &quot;Cat = cat&quot;. In other words, it can be thought that we focus on &quot;a certain word (or a group of words)&quot; and convert that word at any time. So can we reproduce something similar with our seq2seq? More precisely, is it possible to make seq2seq learn the correspondence between &quot;which words are related in the input and output&quot;?

[[[00000000000000002141---aaad706a1206fd0bb27e2975d139f89a22326ae3ed7cdc01e003e909137d7d66]]]In the history of machine translation, many studies have been conducted using knowledge of word correspondences such as &quot;cat&quot;. Information representing such word (or phrase) correspondence is called alignment. Until now, alignments have mainly been made manually. However, a technology called Attention, which will be explained below, has succeeded in automatically incorporating alignment ideas into seq2seq. Here too, the trend is moving from &quot;manual work&quot; to &quot;automation by machines.&quot;

[[[00000000000000002142---8c3cadbb2aa6ce0de133b9530ce712ebd07992fd446f91bb61b1aff7bf774748]]]Our goal is to select the information of the &quot;source word&quot; that has a corresponding relationship with the &quot;translation destination word&quot;, and to use that information to translate. In other words, we aim to focus attention only on the necessary information and perform time series transformation from that information. This mechanism is called Attention, and this is the main theme of this chapter.

[[[00000000000000002143---c672b2e1a8d207e27556f684315007399d25b2cc200fb731f34eb2a2c056debf]]]Before going into the details of Attention here, I would like to show the overall framework first. Figure 8-6 shows the layer structure of the network we want to create.

[[[00000000000000002144---4e4105829d23c6c6b58be06524e05c8ca316b6995d14b06f001be680e9dba5f9]]]
Figure 8-6 Decoder layer configuration after improvement


[[[00000000000000002145---ab333f1923d8af6ab333b2edc9fa51cd3c4498b4d208580ad3e7d480f3931662]]]As shown in Figure 8-6, consider adding a new layer that does some computation. This &quot;something&quot; takes the hidden state of the LSTM layer and the hs from the Encoder at each instant. From there, only the necessary information is selected and output to the Affine layer. Note that we still pass the Encoder&#39;s last hidden state vector to the Decoder&#39;s first LSTM layer.

[[[00000000000000002146---b1474abd5858a7451e1dc5926f919a5460b361ba659dd2a4c5e81aaf118b3fd8]]]Now, what we want to do with the network in Figure 8-6 is word alignment extraction. Specifically, this means selecting from hs the vector of words that correspond to the words input to the Decoder at each time. For example, when the Decoder in Figure 8-6 outputs &quot;I&quot;, I want it to select the vector corresponding to &quot;I&quot; in hs. In other words, we want to realize such an operation of selecting by &quot;some kind of calculation&quot;. But here comes the problem. The problem is that the operation of choosing -- the operation of choosing (some) of things -- is not differentiable.

[[[00000000000000002147---94d8db7114262a835a1bfcfb8474d9c568d4ef08c6c336e0dc9f8593756ebbf6]]]Training neural networks is (generally) done by backpropagation. Therefore, building a neural network with differentiable operations allows learning within the framework of backpropagation. But you can&#39;t (basically) use backpropagation without using differentiable operations.

[[[00000000000000002148---ddebe931fa6131f2ff0e90349467ffb6b8299b8b170b69e3868f29a66b241390]]]Is it possible to replace the operation &quot;choose&quot; with a differentiable operation? The idea to solve this is actually pretty simple (but like the Columbus egg, it&#39;s hard to come up with first). The idea is to &quot;pick all&quot; instead of &quot;pick one&quot;. At this time, as shown in Figure 8-7, we will separately calculate the &quot;weight&quot; that represents the degree of importance (contribution) of each word.

[[[00000000000000002149---f93f7ca5bb2413f59fc9e3f20763fde829da39d8f181254aa89dc82964fd24eb]]]
Figure 8-7 For each word, find a &quot;weight&quot; that indicates how important it is (how to find it is described below)


[[[00000000000000002150---d44973f6f8b598fa4546bc98799c9a0af855f64f5df6e31327f5d9025d4c55a6]]]As shown in Figure 8-7, we use a &quot;weight&quot; (denoted by the symbol a) to represent the importance of each word. At this time, each element of a is a scalar (single element) between 0.0 and 1.0, just like a probability distribution, and its sum is 1. Then, by calculating the weighted sum of the weight a representing the importance of each word and the vector hs of each word, we obtain the target vector. Figure 8-8 shows this series of calculations.

[[[00000000000000002151---b4053a1793820760b16c1d79cce21f3532c5b8a260b88025ba578aa451363e71]]]
Figure 8-8 Calculate the &quot;context vector&quot; by calculating the weighted sum


[[[00000000000000002152---26322a77f7bfc19ede8b9e04597eba979f6d7ced45285d2ed2f558fa27a50485]]]Compute the weighted sum of the word vectors, as shown in Figure 8-8. We&#39;ll call the result the &quot;context vector&quot; and use the symbol c. By the way, if you look closely at Figure 8-8, the weight corresponding to &quot;I&quot; is 0.8. What this means is that the context vector c contains many components of the &quot;I&quot; vector. In other words, it can be said that this weighted sum replaces the operation of &quot;selecting&quot; the &quot;I&quot; vector. If the weight corresponding to &quot;I&quot; is 1 and the others are 0, this is equivalent to &quot;choosing&quot; the &quot;I&quot; vector.

[[[00000000000000002153---9b986cae8bfb3401ca52525834bc3112d9b07b424354ba3615f1d38374074cfc]]]The context vector c contains the information needed to do the conversion (translation) of the current time. More precisely, it learns from the data to do so.

[[[00000000000000002154---433463a00f5fba85da7ec764d67c1703a92841629613860d44047664a037d360]]]Now let&#39;s take a look at the codebase we&#39;ve talked about so far. Here, we will show an implementation that appropriately creates the hs output by the Encoder and the weight a of each word, and calculates the weighted sum. Focusing on the shape of the multidimensional array, it looks like this:

[[[00000000000000002155---6eaa7735a268f9922ccd2d244da752e9f290b0dee76c5ddd849c01d47670bd06]]]Here, the length of the time series is T = 5, the number of elements of the hidden state vector is H = 4, and the process of calculating the weighted sum is shown. First, let&#39;s focus on the code above where ar = a.reshape(5, 1).repeat(4, axis=1). This code converts a to ar as shown in Figure 8-9.

[[[00000000000000002156---3b83ec1d24ea4c2377203111409f67839ca0908233165b1a56a26e6de48a9dcb]]]
Figure 8-9 Generate ar from a with reshape() and repeat() methods (its shape is shown to the right of the variable)


[[[00000000000000002157---93748d9895c78c5c89ead2916c1007697e9d13c7a081a3167107451a38b7fa71]]]As shown in Figure 8-9, what we want to do here is copy a of shape (5,) to create an array of (5, 4). Therefore, the shape of a is (5,), and is transformed to the shape of (5, 1) by a.reshape(5, 1). Then, by repeating the first axis of this formatted array four times, an array with shape (5, 4) is generated.

[[[00000000000000002158---f568e628debf6505bf5dcb3da548160b368576cc6945220c5345106285f657f5]]]The repeat() method copies the elements of a multidimensional array to create a new multidimensional array. This can be used like x.repeat(rep, axis) when you have a NumPy multidimensional array x. Here, the argument rep specifies the number of times to repeat the copy, and axis specifies the axis (dimension) to repeat. For example, x.repeat(3, axis=1) when the shape of x is (X, Y, Z) will copy x in the 1-axis direction (1-dimensional direction) and change the shape to (X, 3*Y, Z) multidimensional array is generated.

[[[00000000000000002159---caa935a52f1a043cc38a33396f62b88c36f2ad0bcacad98b9ab6e46f089fe10c]]]It is also possible to use NumPy&#39;s broadcast instead of using the repeat() method here. In that case, ar = a.reshape(5, 1) and compute hs * ar. At this time, ar is automatically expanded to match the shape of hs, as shown in Figure 8-10.

[[[00000000000000002160---b8193ba825eea1d172dde6c95d7fad6a6a81559efc8a140eb9cfb8bf39d516fd]]]
Figure 8-10 NumPy Broadcast


[[[00000000000000002161---e726d958dd449fd8efe58983a47a8d8ded51cf56e56312ceb3db3c30fedf26ad]]]For implementation efficiency, we should not use the repeat() method here, but instead use NumPy&#39;s broadcast. However, in that case, let&#39;s be careful that the elements of the multidimensional array are copied (repeated) behind our eyes. According to &quot;1.3.4.3 Repeat node&quot;, this corresponds to the Repeat node in the computation graph. Therefore, when obtaining backpropagation, it is necessary to backpropagate the Repeat node.

[[[00000000000000002162---00e9395929d2c8152afbaf177ad1fe1c577d2a105b572272fc93a65e9b90fd43]]]After calculating the element-wise product as shown in Figure 8-10, the sum is calculated by c = np.sum(hs*ar, axis=0). Here, there is an argument &quot;axis&quot;, and you can specify which axis (dimension) direction the sum is to be taken. Again, focusing on the shape of the array makes it clearer how to use the axis. For example, if x has shape (X, Y, Z), then np.sum(x, axis=1) will give the output (sum) of shape (X, Z). The important point here is that the sum is calculated so that the first axis &quot;disappears&quot;. In the above example, the shape of hs*ar is (5, 4), and by &quot;erasing&quot; the 0th axis, a matrix (vector) of shape (4,) is obtained.

[[[00000000000000002163---cefc78c4c8a49f331804c9486e7ecafd5b58334c5f528c46d74e309b4f9be49c]]]The simplest and most efficient way to calculate the weighted sum is to use &quot;matrix multiplication&quot;. In the example above, a single line of np.dot(a, hs) would give the desired result. However, it will be processing for one data (sample). Unfortunately, extending it to batch processing is not trivial. If you want to do that, you&#39;ll need to do a &quot;tensor product&quot; calculation, which gets a little more complicated (in which case you&#39;ll use the np.tensordot() and np.einsum() methods). Here, instead of using matrix multiplication, we will implement the weighted sum using the repeat() and sum() methods, giving priority to ease of understanding.

[[[00000000000000002164---956567134161a7faf95b4119891bd03a58e479fe71734fb5b0651a733a67b36a]]]Next, we will implement a batched version of the weighted sum. This can be implemented as follows (here we choose to randomly generate hs and a):

[[[00000000000000002165---bec315cf5bb521f8a4ae9217d7a926cbdc1b3232ebcc7f0b6327dd4b6155a2f0]]]# ar = a.reshape(N, T, 1) # if using broadcast

[[[00000000000000002166---6c6acd11fef37e91c214dc00132199872cf376071ce078b034103d2c2620c59e]]]Batching here is almost identical to the previous implementation. If you pay attention to the shape of the array, you&#39;ll quickly know which dimension (axis) to specify for repeat() and sum(). Then, as a summary so far, I will express the calculation of the weighted sum as a &quot;computation graph&quot;.

[[[00000000000000002167---976b942a690c31e78e122d6a6e3164f2f1a941982d0ce660546896fdc12e6533]]]
Figure 8-11 Weighted Sum Computation Graph


[[[00000000000000002168---1349d45b645842d1499865eece173baebebd6a0968dd30082caf615b5687b9c8]]]As shown in Figure 8-11, we use a Repeat node to copy a. After that, the &quot;×&quot; node calculates the product for each element, and the Sum node calculates the sum. Now let&#39;s think about backpropagation of this computational graph. However, you already have all the knowledge you need. Backpropagation of Repeat node and Sum node was explained in Chapter 1. In a nutshell, &quot;backpropagation of Repeat is Sum&quot; and &quot;backpropagation of Sum is Repeat&quot;. If you pay attention to the shape of the tensor, you can easily tell which axis to sum and which to repeat.

[[[00000000000000002169---db05c9f96bc68486b80c8512410eadcd351b700b7e5990b5c833d1317039ea1e]]]Let&#39;s implement the computation graph in Figure 8-11 as layers. Let&#39;s call it the Weight Sum layer, and its implementation is shown below (☞ ch08/attention_layer.py).

[[[00000000000000002170---e912e0746cd6e8d7c8dd97265e5cfa2e7be3c0322fe96df77f234929f0d840d2]]]# backpropagation of sum

[[[00000000000000002171---52b3721b115b322766010ae21135b778e9585edb11a37e85ea24fc5f675de2b2]]]# Backpropagation of repeat

[[[00000000000000002172---53b82bab130d8f47e8e3b84086e955ce6e6b74e9a06a9735bd9bb60ed93aa012]]]Here&#39;s an implementation of the Weight Sum layer that finds the context vector. This layer has no parameters to learn, so we follow the implementation rules of this document and set self.params = []. After that, it shouldn&#39;t be too difficult. Let&#39;s move on.

[[[00000000000000002173---4c7ba56b2aff8d3e0bdca7e94d89bcbc74a6d0f24c27685b3bccbab09c285c0e]]]Improvement of Decoder②

[[[00000000000000002174---040a82fbf2f1fe8f4da2a979f0059c03f1c00fd0d8c7ac3bed4776a5cb475a20]]]If we have a weight a representing the importance of each word, we can get a &quot;context vector&quot; by weighted sum. Then, how should this a be obtained? Of course, we do not do things that are specified manually. Arrange them so that they can automatically learn from the data.

[[[00000000000000002175---f686c5f8506240091c57128aff01b4b0447b6e4dd2d5a7c93e09eaf74f8821fa]]]Now let&#39;s see how to find the weight a for each word. To explain this, Figure 8-12 shows the process until the LSTM layer outputs the hidden state vector at the first step (time) of the decoder.

[[[00000000000000002176---8c638cb815fc67c444b9f942d11c2134c4661a2d645f97aee32fab1d7b28dbe5]]]
Figure 8-12 Hidden State Vector of Decoder&#39;s First LSTM Layer


[[[00000000000000002177---2e4f44bdcb57d1b104befe06c90618fdc5afdbe710e2721ce294776d04b18220]]]In Figure 8-12, the hidden state vector of the Decoder&#39;s LSTM layer is denoted by h. Our goal is to quantify how &quot;similar&quot; this h is to each word vector in hs. There are several ways to do this, but I&#39;d like to use the simplest method, the &quot;inner product&quot; of vectors. By the way, the dot product is expressed for vectors with

[[[00000000000000002178---2a21905f43e5a05954fb631dfc0cf38b1d8b25001f9ccedd80abcd27e88f6fb8]]]・

[[[00000000000000002179---265a4c790c097e3618e0314e512a94281baff4de73f9cbcdf457f62732a01821]]]The inner product is given by (8.1), and its intuitive meaning is how much two vectors point in the same direction. Therefore, using the inner product as the &quot;similarity&quot; between two vectors is a natural choice.

[[[00000000000000002180---247cff872dd2d04ed28e0ba73272209b34ff4934cf0bd1499ba1a870a33e0c83]]]There are several other ways to calculate vector similarity besides dot products. Besides dot products, we also see cases where we use small neural networks to output scores. Reference [49] proposes several methods for outputting scores.

[[[00000000000000002181---479d4fff7782140b8586423776b836758d46a14a1f0fa54c6441c47ccf2c1411]]]Now, let&#39;s use the inner product to represent the processing up to calculating the degree of similarity between vectors (Fig. 8-13).

[[[00000000000000002182---548e65f5484f0aa7d6c755582252b914d920d7b59c3ac749dd2a3d6e6da08986]]]
Figure 8-13 Calculate the similarity between each row of hs and h by the inner product (the inner product is illustrated by the dot node)


[[[00000000000000002183---eebf63698ec81019140251ce67004d0e230f7333420c85307bcf3338ce2862c0]]]As shown in Figure 8-13, we calculate the similarity between h and hs with each word vector by vector inner product. And let&#39;s denote the result by s. Note that this s is the value before normalization and is sometimes called the &quot;score&quot;. We then apply the customary Softmax function to normalize s (Figure 8-14).

[[[00000000000000002184---9e4f7231c5e4bc460f33d038d6b6989efafb9a8787a48e95e541a0bf0f7da282]]]
Figure 8-14 Normalization with Softmax


[[[00000000000000002185---540784cf4d0b69e998899eb03fb3ddc0cf8ef1f5c96c029fd0b128eb118add59]]]By using the Softmax function, each element of the output a is a value between 0.0 and 1.0, and the sum is 1. With the above, we were able to obtain a, which represents the weight of each word. Let&#39;s take a look at the codebase to see what we&#39;ve done so far.

[[[00000000000000002186---d04cdcdc32fe44744d77b734018c6a64ab2e691fa54983ad1ac8c369dcbf69e6]]]# hr = h.reshape(N, 1, H) # for broadcast

[[[00000000000000002187---79858f36609dabdba788ee80ef18a55499203b94b66079596dae7ec1d9a93c9b]]]Here is the code for mini-batch processing. Again, the reshape() and repeat() methods generate a properly shaped hr, as explained earlier. Note that repeat() is not necessary when using NumPy broadcasting. Figure 8-15 shows the calculation graph for this case.

[[[00000000000000002188---f9de618904a41b98aadc958c197a719bd62a750349bb681884252bd34375ddec]]]
Figure 8-15 Computation Graph for Weighting Each Word


[[[00000000000000002189---b083d5c1603d53c4ddf57f6258f6b491c93303bd52c3aba2f0c93685635568d6]]]As shown in Figure 8-15, our computational graph consists of a Repeat node, an element-wise product &#39;x&#39; node, a Sum node, and a Softmax layer. We will implement the processing represented by this computational graph as an AttentionWeight class (☞ ch08/attention_layer.py).

[[[00000000000000002190---3a80f909123220e1ae26ea9d04028e2976d4356a2a4337b67e12306238242540]]]In this implementation, as in the previous implementation of the Weight Sum layer, Repeat and Sum operations appear. For backpropagation, if you pay attention to those two operations, nothing else is difficult. Now let&#39;s move on to the final step in improving our Decoder.

[[[00000000000000002191---23a350e8e377c5b2b6408b82b9f23cf2ea7c7f9a0d87359bdbcf37a2178a81a0]]]Improvement of Decoder ③

[[[00000000000000002192---7514631e9287cbb802515d71a5d5bd0bdf8a73e839e5c2cd5889204367c890ac]]]So far, I&#39;ve divided the Decoder improvements into two sections. In Section 8.1.4, we implemented the Attention Weight layer, and in Section 8.1.3, we implemented the Weight Sum layer. Now let&#39;s combine those two layers. The result should look like Figure 8-16.

[[[00000000000000002193---4e57570080347c105a73c5c44c8c0a1f35eca732e58c5a472ec9aaf108295f0e]]]
Figure 8-16 Calculation graph for calculating the context vector


[[[00000000000000002194---943c382bc8f5ccdbbf10c140d7dde581ddb2db5d5a153bd105e479ae06833c16]]]Figure 8-16 shows the entire computational graph for obtaining the context vector c. We implemented the calculation in two layers, the Weight Sum layer and the Attention Weight layer. Again, in our computation, the attention weight layer pays attention to the vector hs of each word output by the Encoder and finds the weight a for each word. The Weight Sum layer then takes the weighted sum of a and hs and outputs it as the context vector c. We call the layer that performs this series of calculations the Attention layer (Figure 8-17).

[[[00000000000000002195---8f1364b0209480b06095f912af25f73b21b2336017bbdba74442caa1619bc58c]]]
Figure 8-17 Summarize the computation graph on the left as an Attention layer


[[[00000000000000002196---32a1502db67e31f87f35c45e2a6ed70ae20ff30dcac1ed652c976fa41499d4ea]]]The above is the core of the technology called Attention. We pay attention to the important elements of the information hs passed by the Encoder, calculate the context vector from it, and propagate it up (in our case, the Affine layer awaits above). Here is the implementation of the Attention layer (☞ ch08/attention_layer.py).

[[[00000000000000002197---e03ff2a6e7731f355c7532b9774e80279788c90406f8b1c2dfca1bb3b174f71b]]]Here we just do forward and backward propagation through two layers—the Weight Sum layer and the Attention Weight layer. At this time, we set a member variable called attention_weight so that we can refer to the weight of each word later. This completes the implementation of the Attention layer. This Attention layer we insert between the LSTM layer and the Affine layer. Graphically, this looks like Figure 8-18.

[[[00000000000000002198---744c89a2ec3889be33f73f3df1a0237b2dd603cd9bcea9886c81c7fd30580ea2]]]
Figure 8-18 Decoder layer configuration with Attention layer


[[[00000000000000002199---9b7f2ad02ee4e32aeeba970522057a83019d89b112190cc5e37cbc42bfb325dd]]]The Attention layer at each time is input with hs which is the output of Encoder, as shown in Figure 8-18. Also here, we input the hidden state vector of the LSTM layer to the Affine layer. This is a natural extension considering the improvement from the Decoder in the previous chapter. This is because, as shown in Figure 8-19, we will &quot;add&quot; Attention information to the Decoder in the previous chapter.

[[[00000000000000002200---e2b387dc78d5931ae04bed93467ce09c6273f0c28ad596bc613cbb1b13f9fe41]]]
Figure 8-19 Comparison between the Decoder in the previous chapter (left figure) and the Decoder with Attention (right figure): Picking up and drawing the part from the LSTM layer to the Affine layer


[[[00000000000000002201---efd6a204604bdde96efe40c01370a40c1bbcb4e034aaec25c4d0c92396bba2f7]]]As shown in Figure 8-19, we will &quot;add&quot; the information of the context vector by the Attention layer to the Decoder in the previous chapter. Therefore, we will give the Affine layer the hidden state vector of the LSTM layer as before, and additionally input the context vector of the Attention layer.

[[[00000000000000002202---37488cdbabf3f5a6a025dc68dbdcacc48603da70d278334101ffbb7335da6ed7]]]In Figure 8-19, two vectors, the &quot;context vector&quot; and the &quot;hidden state vector&quot;, are input to the Affine layer. As mentioned before, this means concatenating the two vectors, and the concatenated vector is input to the Affine layer.

[[[00000000000000002203---e4f06b4e40f3c94a1cc36948bf4242bfbcd3d73a96826d732170524412308e3f]]]Finally, we will collectively implement multiple attention layers spread in the time series direction in Figure 8-18 as a time attention layer. This can be represented graphically as shown in Figure 8-20.

[[[00000000000000002204---262996b383ca187deeddca67cf6cbfde94b68102fc9b00117b1d8959fc95ddea]]]
Figure 8-20 Implementing multiple Attention layers collectively as a Time Attention layer


[[[00000000000000002205---ff064819ac1e2d775e29b9cbc764ad7489cc72cb6c6aea1c691799ac98ba9bd3]]]As shown in Figure 8-20, the Time Attention Layer simply aggregates multiple Attention Layers. Now, here is the implementation of the Time Attention layer (☞ ch08/attention_layer.py).

[[[00000000000000002206---4ca6af7e810b0ddd376a858eb22db9e96bd329cd788fbd12b2b7716d1e123501]]]Here, we create as many Attention layers as we need (T in the code) and forward and backward propagate each. We also have a list of weights for each word in each attention layer as attention_weights.

[[[00000000000000002207---fc9c726ebfdcfc1490608b328f1b175093988a453e30d2d8bf20c73370fe08d5]]]This is the end of the explanation of the mechanism called Attention and its implementation. Next, use this Attention to implement seq2seq. And I would like to challenge a realistic problem and confirm the effect of attention.

[[[00000000000000002208---74a87dda96007b7653a0750088db1de01f440a439375861e4086357d866ad125]]]Implementation of seq2seq with Attention

[[[00000000000000002209---8db09777404da19a57094a930f56a0317e27683ab044aca6715d63931c22b0e9]]]The implementation of the Attention Layer (and Time Attention Layer) was finished in the previous section. Here, we use that layer to implement &quot;seq2seq with Attention&quot;. Actually, we will implement the same three classes -- Encoder, Decoder, and seq2seq -- as in the previous chapter, but the three classes will be implemented as AttentionEncoder, AttentionDecoder, and AttentionSeq2seq classes, respectively.

[[[00000000000000002210---d399ee91a52ead735489c11763498272827b0f1b08b30411886458122ebcc6cf]]]Encoder implementation

[[[00000000000000002211---ab8924a0138ff1ec498e2a326177c51656ff28e41af14ee92e5d23de4a634e02]]]Start by implementing the AttentionEncoder class. This class is almost the same as the Encoder class implemented in the previous chapter. The forward() method of the Encoder class in the previous chapter only returned the last hidden state vector of the LSTM layer. In contrast, this time it returns all hidden states. That&#39;s the only difference. Therefore, here we inherit the Encoder from the previous chapter and implement it. So here is the implementation of the AttentionEncoder class (☞ ch08/attention_seq2seq.py).

[[[00000000000000002212---1e313be94491f0f5d764281912ae8c56f58d7a4d68b5eb74228f258070b86148]]]Decoder implementation

[[[00000000000000002213---1c5475366d53c47b9546c144daf03987f4ef026962e4f7f221f8baeba276424c]]]Next is the implementation of Decoder using Attention layer. The layer structure of Decoder using Attention is shown in Figure 8-21.

[[[00000000000000002214---68ad7103a9154736867b2a5bacfeaf426c367504d2fd63e83baf6521e89eba96]]]
Figure 8-21 Decoder layer structure


[[[00000000000000002215---778187e04ce09fa2ebcfd9c0e6d1712208960ef9ca5a5dfe4eaaab0543c7f539]]]As shown in Figure 8-21, we will implement the Decoder up to the Softmax layer (more precisely, the Time Softmax with Loss layer) as in the previous chapter. Also, as in the previous chapter, in addition to the forward() method for forward propagation and the backward() method for backward propagation, we will also implement the generate() method for generating a new word string (or character string). Here we only show the implementation of the Attention Decoder layer&#39;s initializer and forward() method (☞ ch08/attention_seq2seq.py).

[[[00000000000000002216---66828788f2990571961d0344994c6ade143564e02911890837732176cd1358bd]]]# see source code

[[[00000000000000002217---66828788f2990571961d0344994c6ade143564e02911890837732176cd1358bd]]]# see source code

[[[00000000000000002218---18aee463ec629ccf380e82b8216dcbe37832d6b79bb6b0675116c4ba79b8ac72]]]The implementation here is not much different from the Decoder class in the previous chapter, except for the new Time Attention layer. Note that the forward() method combines the output of the Time Attention layer and the output of the LSTM layer. The code above uses the np.concatenate() method for that concatenation.

[[[00000000000000002219---84cb955494eafe5c25f2d149f85b7eb69d754accf7a498e4eec591ca76866ca6]]]The implementation of the backward() and generate() methods of the AttentionDecoder class is omitted here. Finally, use the AttentionEncoder and AttentionDecoder classes to implement the AttentionSeq2seq class.

[[[00000000000000002220---3e1a60c71fa4c86e8e4a5c7142819645246e6b2b6b64f3e129cbb9c61c7836ec]]]seq2seq implementation

[[[00000000000000002221---4420a3a8c71f1de4d12ad7d1508e19f0f2203c5f3ab9d13d0cba7a8fab18df72]]]The implementation of the AttentionSeq2seq class is almost the same as the seq2seq implemented in the previous chapter. The only difference is that the Encoder uses the AttentionEncoder class and the Decoder uses the AttentionDecoder class. Therefore, the AttentionSeq2seq class can be implemented simply by inheriting the Seq2seq class from the previous chapter and changing the initializer (☞ ch08/attention_seq2seq.py).

[[[00000000000000002222---2070b5306d584a01d1491f368bddc8661bdd3b22f2a3fbead929ad2ce3761770]]]This completes the implementation of seq2seq with Attention.

[[[00000000000000002223---1386c0338abec115cebfad4d4ed463bc91021a1f8fe9b673ba57556ca1ac0cb3]]]Attention rating

[[[00000000000000002224---6070e1913f3b32f7efa93cd468b12488acacc1bc0ddcc22badc4cb3f6cbe9100]]]Now, let&#39;s try a practical problem using the AttentionSeq2seq class implemented in the previous section. Here, I would really like to tackle a problem like translation and see the effect of Attention. Unfortunately, it&#39;s hard to find datasets of reasonable size for translation. Therefore, in this book, I would like to tackle the problem of changing the &quot;date format&quot;--the size of the data is small and rather artificial--and confirm the effectiveness of seq2seq with Attention.

[[[00000000000000002225---5117efaffbd168404cb76a35f9d4c9d38bee31777f9f512b4942e30eb79790a9]]]Among the datasets for translation, the translation dataset called &quot;WMT&quot; is famous. This dataset contains paired training data for English and French (or English and German). The WMT dataset has been used as a benchmark in many studies and is often used to evaluate the performance of seq2seq. However, its size is large (more than 20GB) and it is not available casually.

[[[00000000000000002226---5cb390443c5031dbf480f6f873130cca5403a752c2c0a09a7633e5ef2796dc82]]]Date format conversion problem

[[[00000000000000002227---62c01da41ae8c8ce8d47041612b7241aa422246ca0cd754b3e64ccfb8fcd27dc]]]The problem we tackle here is &quot;date format conversion&quot;. This is a task whose goal is to convert various date representations used in the English-speaking world into a standard format. For example, as shown in Figure 8-22, convert human-written date data such as &quot;september 27, 1994&quot; to a standard format such as &quot;1994-09-27&quot;.

[[[00000000000000002228---c2d0d92ed1f696283a5b7d593ab9cea1abab5442a1ef751860dcadd2e3f43466]]]
Figure 8-22 Example of Date Format Conversion


[[[00000000000000002229---2f77adef0571fc2da595d42e1b610d77916ad275c41cd86c8ad90e9b1caa8d38]]]There are two reasons for adopting the &quot;date format conversion problem&quot; here. For one thing, this problem is not as easy as it seems. This is because the input date data has various variations, and the conversion rules are rather complicated. If we were to write out all the conversion rules by hand, it would be a very tedious and laborious task.

[[[00000000000000002230---016ff1f23a8989cef2c96db300a0baca12b985bb80b50ef508e3eb9e1778b706]]]Another reason for adopting this problem is that there is an easy-to-understand correspondence between the input (question) and the output (answer) of the problem. Specifically, there is a corresponding relationship between year, month, and day. So you can check if your attention is correctly paying attention to each element.

[[[00000000000000002231---d3618218767cf1e7cf59ee2e61ae707294d1165363cb88760e58b519b7088496]]]The data for the date conversion we are working with here is already prepared in dataset/date.txt. This text file contains only 50,000 training data for date conversion, as shown in Figure 8-23.

[[[00000000000000002232---97cf942929b0a06688341e9686f5eefe5fa3f09b6e6829dea909e020c165a051]]]
Figure 8-23 Training data for date format conversion: Blank characters (spaces) are displayed as gray dots


[[[00000000000000002233---fea7891e81ab0ac90fec8667744f4b65f3e35cb3c9a95bbf647336f6cba49da4]]]The date data set prepared in this book is padded with blank characters to make the length of the input sentences uniform. Also, &quot;_ (underscore)&quot; is placed as a delimiter between input and output. Note that this problem does not use a delimiter to signal the end of the output, as the number of characters in the output is constant.

[[[00000000000000002234---e5edb45517384a1d1816d4dd5a90dba592787635de281d2dc8b60a08857090ad]]]As explained in the previous chapter, this book provides a module for easily handling the above training data for seq2seq from Python. That module is in dataset/sequence.py.

[[[00000000000000002235---84b49a9d5269283e2fb9ef13db23d704261af47a4b5df5d62b92635722ef027a]]]Learning seq2seq with Attention

[[[00000000000000002236---9a0ad054f28e573eb8432628f8bd47d0f5c97b67f33425f29fc3a86d64119f37]]]Let&#39;s train AttentionSeq2seq on the data set for date conversion. The training code is shown below (☞ ch08/train.py).

[[[00000000000000002237---05c3b68912d36a95e530e768bb3c99298575fcfd017e4b2f5fabaf04b0aaf74c]]]# load data

[[[00000000000000002238---120bda65ffaec0e99db7dd975c418131a57add46307e40f7a66e8f07c2f578c2]]]# reverse the input sentence

[[[00000000000000002239---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000002240---73f226da1e729927c2909601b89112254ae971df247e2f6857eb7922b6489a21]]]The code shown here is almost the same as the learning code for &quot;Addition&quot; in the previous chapter. The difference is that we load date data as training data and use AttentionSeq2seq for the model. We also use the technique of reversing the input sentence (Reverse) here. After that, while learning, we measure the accuracy rate using test data for each epoch. At this time, for the first 10 questions, the results are output to the terminal, including the question text and the correct answer.

[[[00000000000000002241---2fe6d11de42a618e5dcdb4fa9300900156db3da5694f35a2fa7418ffa5cc746b]]]Now let&#39;s run the code above. Then, as the training progresses, the results shown in Figure 8-24 are displayed.

[[[00000000000000002242---7ddb51cd5113f3f2ab7676c287eaa810c59d8f6c23711c17890bb3781cc4a15f]]]
Figure 8-24 Changes in Results Displayed on the Terminal


[[[00000000000000002243---480d4510bbdd9938789c6148179d6ab039204d7b7c9a0b982629aca678b7b59b]]]As you can see in Figure 8-24, seq2seq with Attention gets smarter as it learns. In fact, you&#39;ll be able to find the correct answer to most problems in no time. At this time, if you plot the accuracy rate of the test data (acc_list in the code above), it will look like Figure 8-25.

[[[00000000000000002244---79ec51f01fe94c6d45fad5346c598ade25cc40652c1611e847d81df19a0d8842]]]
Figure 8-25 Changes in accuracy rate


[[[00000000000000002245---73efae9944982aacdb779577d82876179c8f8d4935da0fbb30e974a75d6bdea5]]]As shown in Figure 8-25, the accuracy rate increases rapidly from the 1st epoch, and by the 2nd epoch, almost all questions are already correct. This seems like a very good result. Now let&#39;s compare this result with the model in the previous chapter. The result should look like Figure 8-26.

[[[00000000000000002246---7875a84c4165162d450245e5934267c4b91cb5dbec0f94009aeb6db3fc9ee445]]]
Figure 8-26 Comparison with other models: &quot;baseline&quot; in the graph is the simple seq2seq from the previous chapter, and &quot;peeky&quot; is an improved seq2seq with &quot;peek&quot; (input sentences are reversed in all models)


[[[00000000000000002247---630bca5d7a19fa6ac23d2e420c68174f19ac7964ef5653d2439ac6d8507be18c]]]Looking at the results in Figure 8-26, we can see that the simple seq2seq (baseline of the graph) is completely useless. Even after 10 epochs, it is still incorrect for most problems. On the other hand, Peeky, which uses &quot;peeking&quot;, shows good results. This model increases the accuracy rate from the 3rd epoch and reaches 100% accuracy in the 4th epoch. But again, Attention is slightly superior in terms of learning speed.

[[[00000000000000002248---cc687c43272427f90534e3f496f30946fa74faa37d92e2a84df3bc31642bc086]]]In this experiment, both Attention and Peeky gave similar results in terms of final accuracy. However, in reality, as time-series data become longer and more complex, attention will become advantageous not only in terms of speed of learning, but also in terms of accuracy.

[[[00000000000000002249---e60918e80569794e659a3a18b455bfac5c414f720791576daa7442eef7c9672f]]]Attention visualization

[[[00000000000000002250---dd5ac32447c0a164ef02376aaafa0a63b400f451ee1a4787ef953157a6036b1f]]]Next, I would like to visualize Attention. This is an attempt to actually see what elements Attention pays attention to when performing time series transformations. This is because the attention layer holds the weight of attention at each time as a member variable, so it is easy to visualize it.

[[[00000000000000002251---4e88a6e2748929198b29e1665fd016ec0cc5cf9cc245507c731fdcb6c4b5e743]]]In our implementation, the attention weights for each time are held in the member variable attention_weights in the Time Attention layer. Using this, you can draw the correspondence between each word in the input sentence and the output sentence as a two-dimensional map. Here, I would like to visualize the attention weight when date conversion is performed for the learned AttentionSeq2seq. I won&#39;t post the code here, just the result is shown in Figure 8-27 (☞ ch08/visualize_attention.py).

[[[00000000000000002252---7f885089e46a2566589142766c559fb5c0112c1ef77b460dabf514c0107bf9db]]]
Figure 8-27 Visualize the attention weight when time series transformation is performed using the trained model. The horizontal axis is the input sentence to the model, and the vertical axis is the output sentence of the model. For each element of the map, the closer it is to white, the higher the value (closer to 1.0).


[[[00000000000000002253---bac4d88bdd720ab6a4ff42bdd3b620c6f2852d690b60c6a4c82e9b633f3d6f38]]]Figure 8-27 is the result of visualizing the attention weight when seq2seq performs time series transformation. For example, when seq2seq outputs the first &#39;1&#39;, we know that the &#39;1&#39; in the input sentence is our attention. The important point here is the correspondence between year, month, and day. Looking at the above result in detail, &quot;1983&quot; and &quot;26&quot; on the vertical axis (output) correspond to &quot;1983&quot; and &quot;26&quot; on the horizontal axis (input). Furthermore, it is surprising that the input sentence &quot;AUGUST&quot; corresponds to &quot;08&quot; representing the month. seq2seq learned from the data alone that &quot;August&quot; corresponds to &quot;August&quot;!

[[[00000000000000002254---0cce95ff67e2a5b3cbb9613bc2f9dd5a3d96dd4bfb765f44250f763c5c599ce9]]]Now, some other examples are shown in Figure 8-28. Again, you can clearly see the correspondence between year, month, and day.

[[[00000000000000002255---7d747169f989dab28625416cd0a801aec1e7f259e092674cfa013bb01af5d064]]]
Figure 8-28 Attention Weight Example


[[[00000000000000002256---c465c082a54753d5ed470e115714e7602333f81295d599dc1b4244d464fb5e25]]]By using Attention in this way, seq2seq was able to pay attention to the necessary information, just like we humans do. From a different point of view, it can be said that &quot;Attention has made it possible for us humans to understand the processing performed by the model.&quot;

[[[00000000000000002257---25e079042974e7e10ed77821dc66ed7a1af895c1db26bcb41ca23e4f9943dc3d]]]Humans cannot understand what kind of processing a neural network is doing internally -- what kind of logic is processing it. On the other hand, Attention gives the model &quot;structure and meaning that humans can understand&quot;. In the above example, we were able to see the relationship between words by using Attention. By doing so, we can judge whether the processing logic of the model follows that of a human.

[[[00000000000000002258---37d87c84e2b96b07d153c0023f35cf18de0aa1604593d75afd19fdcab307a637]]]This concludes the evaluation of Attention. Through the experiments here, you must have realized the wonderful effects of Attention. That&#39;s the end of the main Attention story, but there&#39;s still a lot more to say about Attention. From the next section onwards, I would like to introduce more advanced techniques, focusing on Attention.

[[[00000000000000002259---54ab6646eebe899b3f47367880b9827c759bdfa323ae1c2a6053f419f452d9a7]]]Remaining subject of Attention

[[[00000000000000002260---c4cf25544586c570a4530d6f5ac199a850def9621631633cfb73201201040548]]]So far we&#39;ve seen Attention—or, more precisely, seq2seq with Attention. Here, I would like to introduce some topics related to attention that have not been covered so far.

[[[00000000000000002261---e31d7ae4885adf5334a8f96da3ac6244e65b0b9436d017fd31942cf937164191]]]Bidirectional RNN

[[[00000000000000002262---e5efa39543a1c2fd2ac08ba14ab47ee783c6efc7551c440001509c852042ad7d]]]Here we focus on the seq2seq Encoder. As a quick review, the Encoder up to the previous section is expressed as shown in Figure 8-29.

[[[00000000000000002263---46a5a2c42a7c93f5205c4c26c1637612516c81ef0366dcf48809f272055ab8de]]]
Figure 8-29 Output hs by LSTM layer


[[[00000000000000002264---0b93092553c41cb87a283d762b90331ae7827cc0e4e02bd1dec4f371877b7401]]]The hidden state vector at each time in LSTM is summarized as hs, as shown in Figure 8-29. Here each line of hs output by the Encoder contains many of its corresponding word components.

[[[00000000000000002265---bd969f9979d10e1cd05efaf9e9700084013c7bd34c0d9d1495c6ff4f93a71a1a]]]Now, notice that we read the sentence from left to right. So, for example, the vector corresponding to the word &quot;cat&quot; in Figure 8-29 encodes the information for the three words &quot;wa&quot;, &quot;wa&quot;, and &quot;cat&quot;. Considering the overall balance here, you would want to include information about the &quot;surroundings&quot; of the word &quot;cat&quot; in a well-balanced manner.

[[[00000000000000002266---9c9e35143bc9b9bfb9c3564585501358dda44b0c0b4e81c189a59b5c1ee2d49d]]]In our translation problem, we are given all the time series data (sentences to be translated). Therefore, text can be read (processed) not only from left to right, but also from right to left.

[[[00000000000000002267---d6698ec14bb540982087835c95ae4186a7746bff7155c3401f9dd4a483419356]]]Therefore, it is conceivable to process LSTM from both directions. This is a technique called Bidirectional LSTM (Bidirectional RNN). A bidirectional LSTM can be represented graphically as shown in Figure 8-30.

[[[00000000000000002268---399a7cc85bc57b644740f7016d496a9812bef4e170d19afe6940b0c1fa66c02c]]]
Figure 8-30 Example of encoding with a bidirectional LSTM (here the LSTM layers are simplified)


[[[00000000000000002269---fc90d19be51e9d663555d3677a24cf2f75b90bac849f46e6d9f7b8581e49176c]]]As shown in Figure 8-30, a bidirectional LSTM adds an LSTM layer that processes in the opposite direction in addition to the previous LSTM layers. Then, at each time, we concatenate the hidden states of the two LSTM layers to form the final hidden state vector (other than &quot;concatenation&quot;, we can think of &quot;sum&quot;, &quot;average&quot;, etc.).

[[[00000000000000002270---aa8f7e3e790204c7df7fae63a3a345d14b132fb396b9bd9d1bfc5f81243976a2]]]By processing bidirectionally in this way, the hidden state vector corresponding to each word can aggregate information from both the left and right directions. It encodes balanced information.

[[[00000000000000002271---c25280b464c5c5cef9b88d1da2dfe693fe20a563b89c31a966e34e63e7201756]]]A bidirectional LSTM is easy to implement. (One way to implement it) is to use two LSTM layers (in our case the Time LSTM layer) and adjust the sequence of words given to each layer. Specifically, we give one LSTM layer the same input sentence as before. This is a generic LSTM layer that processes the input sentence &quot;left to right&quot;. On the other hand, another LSTM layer is given the words of the input sentence as a sequence from right to left. For example, if the original sentence was &quot;ABCD&quot;, change the order of &quot;DCBA&quot;. By giving this permuted input sentence, another LSTM layer will process the input sentence from &quot;right to left&quot;. Then just concatenate the outputs of the two LSTM layers and you have a bi-directional LSTM layer.

[[[00000000000000002272---88ccc3ae20c6bf0532243866e66e0857c08a4870a74e8d90e79257bd1a195cf3]]]In this chapter, we used a unidirectional LSTM for the encoder for the sake of clarity. However, it is of course possible to use the bidirectional LSTM described here as an Encoder. If you are interested, please try to implement seq2seq with attention using bidirectional LSTM. Note that the bidirectional LSTM implementation is in the TimeBiLSTM class in common/time_layers.py. Please refer to it if you are interested.

[[[00000000000000002273---45fa84d339ab814d173302ff52542e58a32dc185a21485870d93d4b684b0d286]]]How to use the Attention layer

[[[00000000000000002274---638768a96eb0d1b4e9c9311a21f283942f1f5360201f53d305272d2a5954dccd]]]Next, I would like to think about how to use the Attention layer. As a refresher, so far we have used the Attention layer with the layer structure shown in Figure 8-31.

[[[00000000000000002275---9cb61599d58a4c34aa34cc00e1a4aa064893d08eb85d6272bc2b85e9655627f6]]]
Figure 8-31 Layer structure of seq2seq with Attention used up to the previous section


[[[00000000000000002276---bcd8c4913d18bbc05f63229652fc37bc21fea9fa0479dc308219b693246d8618]]]We have inserted an Attention layer between the LSTM layer and the Affine layer, as shown in Figure 8-31. However, the places where the Attention layer is used are not necessarily as shown in Figure 8-31. In fact, there are several other candidates for models that use Attention. For example, Reference [48] uses Attention in the configuration shown in Figure 8-32.

[[[00000000000000002277---ff34423b4c17ee4cbe6c630d27c37bb829d0da6928617b378e14fa6fafc3f938]]]
Figure 8-32 Another usage example of the Attention layer (here, a simplified network configuration is illustrated with reference to [48])


[[[00000000000000002278---3260c7141ef5d89ff27a94e25cb6c810983e0d7c6721c0f016b4a31d7181c10a]]]In Figure 8-32, the output of the Attention layer -- the context vector -- is connected to the input to the next LSTM layer. Such a configuration allows the LSTM layer to make use of the context vector information. On the other hand, the model we implemented used the context vector for the Affine layer.

[[[00000000000000002279---6404d453f0e42f35ae207418ac9a3fa750843d88fa51351220c8813faac789d6]]]So what effect does the difference in attention layer location have on the final accuracy? The answer is that you won&#39;t know until you try. The reality is that we can only verify using actual data. However, in both cases the context vector is a construct that makes good use of the two models above. So you may not see a big difference in accuracy between the two models.

[[[00000000000000002280---724483fa8a4f31131c231f3a442c2868578f9dc11e31a3de636e4a529d5bbb28]]]From an implementation point of view, the former configuration — inserting an Attention layer between the LSTM layer and the Affine layer — is easier. This is because in the former configuration, the data flow in the Decoder is one direction from the bottom to the top, so it is easy to modularize the Attention layer. In fact, we could have easily modularized it as a Time Attention layer.

[[[00000000000000002281---df6eeb449f98ccfdef5a108d9bee530280634f6d754d5041fb7117f4e326ad57]]]seq2seq deepening and skip connections

[[[00000000000000002282---d17b9e8818d6b1c36ea2ead1e32604941e1ecc72ab78ea85ac88cb1b7a70ee62]]]In real-world applications such as translation, the problem to be solved becomes more complicated. In that case, seq2seq with Attention should have higher expressiveness. The first thing to consider is to layer the RNN layer (LSTM layer) deeply. By making the layers deeper, we can create models that are highly expressive. This is the same story for seq2seq with Attention. So, what happens if we deepen seq2seq with Attention? One example is shown here in Figure 8-33.

[[[00000000000000002283---e15a79913242a4ff8153700efae32c825cc914c2c5b271b483096b1441534604]]]
Figure 8-33 seq2seq with Attention Using 3 LSTM Layers


[[[00000000000000002284---69111a02c28ca3e20be7a750eb9e06be51aafd652d7ea00bf9cbddb7ca1af5d0]]]The model in Figure 8-33 uses three LSTM layers in the Encoder and Decoder. As in this example, Encoder and Decoder generally use the same number of LSTM layers. Also, there are many variations on how to use the Attention layer. Here we input the hidden state of the Decoder&#39;s LSTM layer to the Attention layer. Then, the context vector, which is the output of the Attention layer, is propagated to multiple layers of the Decoder (LSTM layer and Affine layer).

[[[00000000000000002285---18a06ee31b0c9b42117abe141285532239bc69a6e5ac546459b0bb844cbb0d52]]]The model shown here in Figure 8-33 is just one example. In addition to this example, various variations are possible, such as using multiple Attention layers, inputting the output of Attention to the LSTM layer of the next time, etc. Also, as explained in the previous chapter, it is important not to drop the generalization performance when deepening the layers. In that case, techniques such as Dropout and weight sharing are effective.

[[[00000000000000002286---16431d0fc8d2524f3a979b01f43297809a14b41a2ac376b73eceed1cf88b5034]]]Also, one of the important techniques used when deepening layers is the skip connection (also called &quot;residual connection&quot; or &quot;shortcut&quot;). This is a simple technique of &quot;connecting lines&quot; across layers, as shown in Figure 8-34.

[[[00000000000000002287---c911e7da09206eb128071f41425ce3029e9ee80946fd883f4de3c926eb92ac04]]]
Figure 8-34 Example of skip connection in LSTM layer


[[[00000000000000002288---da08591d0552985d924203346cc5f907c8c0893422a58c6a2b918cd499c9bfac]]]A skip connection is a &quot;cross-layer connection&quot;, as shown in Figure 8-34. In this case, the two outputs are &quot;summed&quot; at the junction of the skip connection. This addition (more precisely, addition for each element) is an important point. This is because the addition just &quot;flows&quot; the gradient during backpropagation, so the gradient in the skip connection flows unaffected to the previous layer. As a result, even if the layer is deepened, the gradient will propagate without vanishing (or exploding), and good learning can be expected.

[[[00000000000000002289---fc4fb3f843c32d48b75d9b31008c9bd50b00b6b89686f2ea99457849dd9395ef]]]Backpropagation in RNN layers causes gradients to vanish or explode in the time direction. Vanishing gradients can be addressed by &quot;gated RNNs&quot; such as LSTM and GRU, and gradient explosions can be addressed by &quot;gradient clipping&quot;. On the other hand, the skip connection described here is effective for vanishing gradients in the RNN depth direction.

[[[00000000000000002290---13958fa633171738f53701911f0a7fb864038301d32dba87c53ff73a3da0799c]]]Application of Attention

[[[00000000000000002291---97792d41f30c7bcf75bf1a2ab4904004a7dc443099f1b7add3a0fe35e1fcaaf3]]]So far we have only applied Attention to seq2seq. But the idea of Attention itself is generic and has a lot more potential. As a matter of fact, in recent deep learning research, attention has appeared in various situations as an important technique. In this section, we introduce three cutting-edge research examples using attention, with the aim of getting people to feel the importance and potential of attention.

[[[00000000000000002292---1d9109eb2f997768d74a2700983446d2366593f6fc99a3938d53a4365c36701c]]]Looking at the history of machine translation, we can see that the main methods have changed over time. The trend has changed from &quot;rule-based translation&quot; to &quot;example-based translation&quot; and then to &quot;statistical-based translation&quot;. And now, in place of these conventional technologies, neural translation (Neural Machine Translation) is attracting a lot of attention.

[[[00000000000000002293---e55e5b04fe576e05d7842da2e690e84a7843daf773c54585046c3d840f395f31]]]The term neural translation has been used in contrast to statistical translation. However, recently, it is used as a general term for machine translation using seq2seq.

[[[00000000000000002294---9dacde758f83748657fd34c2e526849e6281f3a7b9968eea74d9f6d70af54bfc]]]Google Translate has been using neural translation since 2016 as an actual service. This machine translation system is called GNMT (Google Neural Machine Translation), and the details of the technology are described in reference [50]. Here, I would like to take a look at the GNMT architecture, focusing on its layer structure. Figure 8-35 shows the layer structure of GNMT.

[[[00000000000000002295---8eb4cdeacd9c92d8b977bd815b054d22be1ee1948ea16adffffc0ebb46793f1c]]]
Figure 8-35 Layer structure of GNMT (The figure is extracted from Reference [50])


[[[00000000000000002296---be7cf501f07b78814e89af2838eda9b400757ece531d911ad14395a4ceb42c50]]]GNMT consists of Encoder, Decoder and Attention, like seq2seq with Attention implemented in this chapter. However, unlike our simple model, we see many refinements to improve translation accuracy. For example, we see multiple LSTM layers, bi-directional LSTM (only the first layer of Encoder), and skip connections. In addition, in order to perform training at high speed, distributed learning is performed on multiple GPUs.

[[[00000000000000002297---94cfa9bd6618fcbd195c413ccddbcfade40a657f4c9f7ac2af78b617ec088389]]]In addition to the above architectural innovations, GNMT has various innovations such as handling low-frequency words and quantization for speeding up inference. Utilizing such techniques, GNMT has achieved very good results. The actual reported results are shown in Figure 8-36.

[[[00000000000000002298---52ec00e2c886e7779757618b79400d8e6fa5c29676d86e7737f42950f60c62aa]]]
Figure 8-36 Accuracy evaluation of GNMT: The vertical axis is the quality of translation, evaluated by humans on a scale of 0 to 6 (the figure is extracted from Reference [51]).


[[[00000000000000002299---e280d78b4a6a11c0935b2b5b4ffd928d0e455c3063d83bdf8319506bb4f7b213]]]As shown in Figure 8-36, GNMT succeeded in improving translation accuracy compared to conventional methods—“phrase-based machine translation,” which is one type of statistical-based machine translation. And the accuracy is approaching the accuracy of &quot;human (translation)&quot;. Thus, GNMT has shown excellent results, greatly demonstrating the practicality and potential of neural translation. However, as you can see by using Google Translate, there are still unnatural translations and mistakes that people would never make. Machine translation research will continue to advance in the future. In fact, GNMT was just the beginning, and there is still active research being done around neural translation.

[[[00000000000000002300---4a20771bb3f6000864581ae8213e41acdc4d211c829879cc3db4fcd47f89c217]]]Achieving GNMT requires a large amount of data and a large amount of computational resources. According to the reference [50], they use a large amount of training data and use nearly 100 GPUs (for one model) for 6 days of training. In addition, ensemble learning that learns eight models in parallel and reinforcement learning are also devised to further improve accuracy. Such a thing cannot be achieved by one person. But we already have the core knowledge of the technology behind it!

[[[00000000000000002301---229006c2ca8b699d622277f028faf6c4aca04d3e9d6e28a9166a2c1a291bb31a]]]We&#39;ve used RNNs (LSTMs) everywhere so far. Beginning with language models, sentence generation, seq2seq, and seq2seq with attention, RNN has always appeared as a component. And with this RNN, variable length time series data can be handled well and (often) with good results. However, RNNs also have drawbacks. One of its drawbacks is parallel processing.

[[[00000000000000002302---358700ddcda0d06b096e3e599af1d839030def82ce905729933c49b41ee007f8]]]RNN performs calculations sequentially using the results calculated at the previous time. Therefore, RNN calculations cannot (basically) be calculated in parallel in the time direction. This point is a big bottleneck when it is assumed that deep learning training is performed in a parallel computing environment using GPUs. This is where the motivation to avoid RNNs arises.

[[[00000000000000002303---99d5a5b54f01512cfae8d74f67d3c1efe09f06c0546809ba5a783bca5b91c721]]]Against this background, research to remove RNNs (or research on RNNs that can be parallel-computed) is actively being conducted. One of the most famous among them is the model called Transformer[52]. This is the method proposed in the paper titled &quot;Attention is all you need&quot;. As the title suggests, it uses Attention instead of RNN to handle things. Here&#39;s a quick look at this Transformer.

[[[00000000000000002304---cfa70ff3d5ea16a248cf99c9d3d303726d3c5ea3891ab376beae57ab9249deec]]]Besides Transformer, there have been several other studies trying to get rid of RNNs. One of the results is research [54] that uses convolution layers instead of RNNs. I won&#39;t go into the details of that work here, but basically, we construct seq2seq using convolutional layers instead of RNNs. This allows parallelization of computations.

[[[00000000000000002305---d18b163d1a4fe463708025f204821539b42940e92443b59febca0ce86b8b56ee]]]Transformer consists of Attention, but the important point is that it uses a technique called Self-Attention. Self-Attention literally translates to &quot;attention to oneself&quot;. In other words, this is attention for one time-series data, and it is to see how each element is related to other elements in one time-series data. Using our Time Attention layer, Self-Attention can be written as in Figure 8-37.

[[[00000000000000002306---1628365148640745593535b7bd1d35d466d9148349d1ef9d00a42ca726cfde7a]]]
Figure 8-37 Normal Attention on the left, Self-Attention on the right


[[[00000000000000002307---15f13006b6dd6f9bf2b4fb1a7a6cca0834ebe350699cf5ba46b16053077172dc]]]So far, we have sought the correspondence between two time-series data such as &quot;translation&quot; by attention. At this time, two different time-series data are input to the two inputs to the Time Attention layer, as shown in the left diagram of Figure 8-37. In Self-Attention, on the other hand, one time-series data is input to two input lines, as shown in the right figure of Figure 8-37. By doing so, the correspondence between each element in one time series data can be obtained.

[[[00000000000000002308---94c99098549916b884ae37989c182df55580cd2b15ab95a84cebc5dc761329e9]]]Now that Self-Attention has been explained, let&#39;s take a look at Transformer&#39;s layer structure. Your Transformer configuration should look like Figure 8-38.

[[[00000000000000002309---06631fde23638b3c9d2a18989b787dfc6a85c48709f70cf2b210bb90c1bb886f]]]
Figure 8-38 Transformer layer configuration (illustrating a simplified model with reference to [52])


[[[00000000000000002310---14f3bae2e9abe3f6dc8e357c111dfb5327663603b05bea6ab9e74b0009eb3b44]]]Attention is used in Transformer instead of RNN. In fact, looking at Figure 8-38, we can see that both the Encoder and Decoder use Self-Attention. The Feed Forward layer in Figure 8-38 represents a feedforward network (a network that processes independently in the time direction). Specifically, a fully-connected neural network with one hidden layer and ReLU as the activation function is used. Also, &quot;Nx&quot; in the figure means that the elements surrounded by a gray background are stacked N times.

[[[00000000000000002311---4a5206395d336d6204b38522ceed4ac6feebe70310906c6bad94ee455344d971]]]Figure 8-38 shows a simplified Transformer. In practice, skip connections and layer normalization [8] are used in addition to the architecture shown here. In addition, we can see ingenuity such as using multiple attentions (in parallel) and encoding the position information of time series data called Positional Encoding.

[[[00000000000000002312---e3ca672d713fc966526302c1794083e98d6666e9d7fb0e73cebb79fd38eb1a0e]]]By using this Transformer, you can reduce the amount of computation and benefit more from parallel computation by GPU. As a result, Transformer has succeeded in significantly reducing the learning time compared to GNMT. Furthermore, in terms of translation accuracy, as shown in Figure 8-39, we have improved accuracy.

[[[00000000000000002313---cd88b178bdc29c38b4796fd34eeca345f90f3c90f38a5523c8af98b16b9633c4]]]
Figure 8-39 Results of evaluating the accuracy of English and French translations using WMT benchmark translation data. The vertical axis is the BLEU score, which is an index of translation accuracy, and the higher the number, the better (the figure is taken from Reference [53]).


[[[00000000000000002314---e757c5613a998751818911c2da49d693cf8a137588daa92f8e3663d4dd4906c2]]]Figure 8-39 compares the three techniques. The results show that ``seq2seq using convolutional layers&#39;&#39; (denoted as ConvS2S in the figure) has higher accuracy than GNMT, and Transformer even surpasses it. In this way, Attention is a promising technology not only in terms of computational complexity, but also in terms of accuracy.

[[[00000000000000002315---79570c124c3c5daefde47fcebb20809c3ae5ef1ec6777ac850290e84f1755038]]]We have been using Attention in combination with RNNs. However, as the research here suggests, Attention can also be used as a replacement module for RNNs. This may lead to more opportunities to use Attention.

[[[00000000000000002316---40cfacaa95e03b875f3a984f64c31b4072ea6a53ac7b57f08b1cd6ada6276363]]]We humans often use &quot;paper&quot; and &quot;pen&quot; when solving complicated problems. From a different point of view, we can interpret this as an extension of our abilities through the external &quot;memory devices&quot; of paper and pen. In the same way, we can give neural networks additional power by making them use &quot;external memory&quot;. The topic covered here is &quot;extension with external memory&quot;.

[[[00000000000000002317---b6e1f67aa326263c3614540006da1cc0eabc67edbc1406b28430ee82c605ddb2]]]RNNs and LSTMs were able to store time series data by using internal states. However, its internal state is of fixed length, limiting the amount of information that can be packed into it. Therefore, we can consider an approach such as placing a storage device (memory) outside the RNN and recording the necessary information there as appropriate.

[[[00000000000000002318---3fec4d1c757a5e4eefd9a09189071e2c23d4ad85aeed286c1104870b42055665]]]Now, in seq2seq with Attention, the Encoder encodes the input sentence. The encoded information was then used by the Decoder via Attention. What I would like to focus on here is (again) the existence of Attention. With this attention, Encoder and Decoder realize something like &quot;memory operation&quot; in computer. In other words, it can be interpreted that the Encoder writes the necessary information to memory, and the Decoder reads the necessary information from the information in that memory.

[[[00000000000000002319---4135dd4afe94432adf6e7adf97a255c9800f0d83a532c66c1de5a897f9fe1579]]]Thinking like that, we can see that the memory operations of a computer can be reproduced by a neural network. A quick idea is to place a memory function for storing information outside the RNN and use Attention to read and write the necessary information from that memory. In fact, several such studies have been conducted. One of the most famous studies is NTM (Neural Turing Machine) [55].

[[[00000000000000002320---51fc2d8003bf85a7f7b1037ea9c2354af91eb6d12b37ceb1acf01824a0767594]]]NTM is research done by the team at DeepMind. It was later refined into a method called DNC (Differentiable Neural Computers) [56], the technical paper of which was published in the scientific journal Nature. DNC can be thought of as a more powerful version of NTM&#39;s memory operations, but the essence of the technology is the same.

[[[00000000000000002321---10b227cb99bd8ebadf6e39e5d2b1e5b9b5a9bc5b51a04e421cb6f2f4b258440e]]]Before explaining the contents of NTM, I would like to first look at the outline of NTM. The attractive picture in Figure 8-40 is perfect for that purpose. This is a visual representation of the processing performed by NTM and nicely summarizes the essence of NTM (exactly the diagram used in the DNC commentary article [57], which developed NTM).

[[[00000000000000002322---ed60aad119f7ba683969ef8a7e6188816703e05e0abcfbde8de687fa1e43c065]]]
Figure 8-40 Visual diagram of NTM (The figure is extracted from Reference [57])


[[[00000000000000002323---b7486572e4d8dee0d15181b8bdd8301fbff6788bfa9a7b7f47e8cffa4eb08c88]]]Now turn your attention to Figure 8-40. What I want to focus on here is the module called &quot;controller&quot; in the middle of the diagram. This is a module that processes information, and specifically, it is assumed to use a neural network (or RNN). Looking at the figure, you can see that the controller is continuously receiving data of &quot;0&quot; and &quot;1&quot;, processing it and outputting new data.

[[[00000000000000002324---6e4b1dc42be0c3e5f9b827fd7b5d67118b1972497481b8923af68fa896c05592]]]What is important here is the existence of &quot;large paper (= memory)&quot; outside this controller. This memory gives the controller the power of a computer (or Turing machine). Specifically, that ability is the ability to write necessary information, erase unnecessary information, and read necessary information on a &quot;large piece of paper&quot;. By the way, the &quot;big paper&quot; in Figure 8-40 is roll type, so each node can read and write data where it is needed. This means that you can move it wherever you want.

[[[00000000000000002325---b2612fe59896453e96588b498f3fc33fb36ee79d044c8f784e55590a5456c0ad]]]In this way, NTM processes time series data while reading from and writing to external memory. And the interesting thing about NTM is that it builds such memory operations into &quot;differentiable&quot; computations. Therefore, it becomes possible to learn the procedure of memory operation from the data.

[[[00000000000000002326---22d729074026f377366ff75ba333c245d80314e83d24fcb610af33d27be62723]]]Computers are run by programs written by humans. NTM, on the other hand, learns the program from the data. This means that you can learn the &quot;algorithm itself (the logic)&quot; from the &quot;inputs and outputs of the algorithm&quot;.

[[[00000000000000002327---278c4f67240a11d099c6d435534447de48e52531d89fcf15c7c72713bd592009]]]NTM reads and writes to external memory like a computer does. In this case, the NTM layer configuration can be simplified as shown in Figure 8-41.

[[[00000000000000002328---3bf8be70a2a80c1e7474a4b685193fbe6d213c7b1d4e5ed6f705e58d3188022c]]]
Figure 8-41 Layer structure of NTM: New Write Head layer and Read Head layer appear, and they write and read memory


[[[00000000000000002329---50028e3052144d0b92f87995a0fcb70b6a6dc6cc2ef97554c2be0e06e4be56fe]]]Figure 8-41 is a simplified NTM layer structure. Here, the LSTM layer acts as the &quot;controller&quot; and performs the main processing of NTM. Then, at each time, the Write Head layer receives the hidden state of the LSTM layer and writes the necessary information to memory. In addition, the Read Head layer reads important information from memory and passes it to the next LSTM layer.

[[[00000000000000002330---8793909dcbe218aa6b0d4113f31651e3434a7f62afc9771b6e34258ac88de0f5]]]So how do the Write Head and Read Head layers in Figure 8-41 perform memory operations? Of course, Attention is used here as well.

[[[00000000000000002331---018aa0e601c7159827101d2efd88c64e13a40162d0463220709d991c5355c8d5]]]Again, when reading (or writing) data that exists at a certain address in memory, we need to &quot;select&quot; the data. This selection operation itself cannot be differentiated. Therefore, we use Attention to select data at all addresses, and use a &quot;weight&quot; that represents the degree of contribution to each data. By doing so, we can replace the operation of &quot;choosing&quot; with a differentiable computation.

[[[00000000000000002332---12ba08e5615790b87d401701aaef374b54843721bbe3e8cf7dd5940e8b68c7d3]]]The memory operations performed by NTM utilize two attentions to mimic computer memory operations. They are &quot;content-based attention&quot; and &quot;location-based attention&quot;. Content-based attention is the same as the attention we have seen so far, and it is used to find similar vectors from memory for a given vector (query vector).

[[[00000000000000002333---a8c566ba7838408485b5f9d4f98a47f8ac1ac46f8b2662c7850a5d874c5d971c]]]On the other hand, position-based attention is used to move (shift) the memory position (= weight for each memory position) focused on the previous time. We won&#39;t go into the details of this technique, but it is achieved by a one-dimensional convolution operation. This ability to shift memory locations makes it easier to reproduce the computer-specific movement of reading while advancing memory locations one by one.

[[[00000000000000002334---444615488e5ae520b50b8470a40d9d1ce36e266d38251dc3ae906f9b21b1e37d]]]NTM&#39;s memory operations are somewhat more complicated. In addition to the operations described above, it also includes processing to sharpen the weight of attention and processing to add the weight of attention at the previous time.

[[[00000000000000002335---2bff2d70ef8fd53495fd72c8daa6b2f0b734e326029d013e4cc4fc62383f6298]]]In this way, NTM gains great power by freely using external memory. In fact, NTM has achieved amazing results even for complex problems that seq2seq could not solve. Specifically, NTM successfully solved problems such as memorizing long time series and sorting (arranging numbers in descending order).

[[[00000000000000002336---a763ff1a423caef51ecf414045518edc25d613de2bbf758243352a8978000f67]]]Thus, NTM gains the ability to learn algorithms by using external memory. And attention is used as an important technique there. Expansion by external memory and attention——These will be used in various places as an increasingly important technique in the future.

[[[00000000000000002337---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000002338---7d0d1058f00076b2bb9510d3e32ab8f2d097768d0f42bf19ccfa1f35b045f7e2]]]In this chapter, we learned how Attention works and implemented an Attention layer. Then, we implemented seq2seq using Attention and confirmed the wonderful effect of Attention through a simple experiment. In addition, we visualized the attention weight (probability) during model inference. The results show that attention-equipped models pay attention to necessary information in the same way that we humans do.

[[[00000000000000002339---378fb8fd67d7fcf1b110eed1c06128f3ff6160545f1649e88225de4c2d698314]]]In this chapter, we also introduced trends in cutting-edge research centering on attention. Looking at the example, we can see that Attention has further expanded the possibilities of deep learning. As such, attention is a versatile technique with many possibilities. In the field of deep learning, attention itself will continue to attract a lot of attention!

[[[00000000000000002340---19850c73ec605a12530fa271fcc972171b998181be314b2c4c58825fcf2e154b]]]What we learned in this chapter

[[[00000000000000002341---0c56ad08728b6aa16e5fda07dfb1a43c5335d22097513eb6c5a5c2d97007ca6a]]]In tasks such as translation and speech recognition, where one time-series data is transformed into another time-series data, there are often correspondences between the time-series data.

[[[00000000000000002342---f084e8624a04ccdae7c64724cea029da099a02e3b0257e50136a5374a9d3296e]]]Attention learns the correspondence between two time-series data from the data

[[[00000000000000002343---2cf06242aa1e2a5326396d4e8b4bf8085f151772badaaa83ce069d28418a4e21]]]Attention calculates the similarity between vectors using the inner product of vectors (as one method), and the weighted sum vector using the similarity is the output of Attention.

[[[00000000000000002344---12b52f5699e28fca97b97f0ad8381fbadf460bd692f16ebcf8930e19996e5fa6]]]Since the operations used in Attention are differentiable, they can be learned by error backpropagation.

[[[00000000000000002345---5644a142be0aeb89b763da6a4a2a659a35faf42cc48bf92e594b31746d04fb46]]]By visualizing the weight (probability) calculated by Attention, you can see the correspondence between input and output.

[[[00000000000000002346---4e58cef98ada30fe34278498bc93471b3edb677dba48290ab7e1e69893055102]]]In the research example of extending neural networks with external memory, attention is used for reading and writing memory