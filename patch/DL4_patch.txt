

[[[00000000000000000000---ed57a1570552eea464f1f2851658d827c8dc4137c96167d284c699c37d79b1ac]]]Chapter 1

[[[00000000000000000001---5adf4d595df0226b1c13d3eeb9bf65336786066afa3c177ca06d377d8105d9a0]]]bandit problem

[[[00000000000000000002---54b18d80797dea67db5f5d33eee07979dc1c8e1bb785bb21b8973f651a43390c]]]We humans can learn without being taught by a teacher or coach. For example, toddlers naturally learn to grasp objects, walk, and run through trial and error (with some support from their parents, of course). Reinforcement learning involves spontaneous learning of better solutions through interactions with the environment, without the need for a teacher. Such abilities are important qualities of human beings—and living creatures. Learning the field of reinforcement learning also leads to learning about more versatile intelligence.

[[[00000000000000000003---ba29b9991140ea6f9a90bb64c385f715d52d7bc0e5057dfa77fe21ac3756c554]]]Now we will learn about reinforcement learning. In this chapter, we will first look at the position of reinforcement learning in machine learning. After briefly describing the features of reinforcement learning, we will immediately tackle specific problems. This chapter deals with the most basic “bandit problem” in reinforcement learning. Solving this problem will make you realize how difficult and interesting reinforcement learning is.

[[[00000000000000000004---01215c02fd2be915ac874b9d55d63317af1ca281f7d3f17600f89838127d2b5f]]]Machine learning classification and reinforcement learning

[[[00000000000000000005---86fac9c10f0756f316e29e77d3d917497bd138265861489a1f6115960bac4c10]]]Machine learning, as the name suggests, is a method of giving data to a machine and making it learn. You feed a machine or computer some data and let it find some rules or patterns for itself. Instead of humans writing 'rules' as programs, computers themselves learn 'rules' based on data.

[[[00000000000000000006---87e72b53f7d74638c44881fa715c23efbe3281be45fa617418e473b30609720b]]]The methods used in machine learning can be divided according to the structure of the problem. The typical categories are “supervised learning”, “unsupervised learning” and “reinforcement learning”. Here is a brief description of each of those three frameworks. Let's start with supervised learning.

[[[00000000000000000007---03e3b863cd527fb10bd7a923440e031eca81d5e22adcea88dc4e12b27bb146cb]]]supervised learning

[[[00000000000000000008---df407f7c7058820bcbaaceb6b6abac03e6563b18752d6aa02d1f9d46eebc526e]]]The most orthodox method in machine learning is supervised learning. In supervised learning, we are given input and output pair data. For example, in the case of the problem of recognizing handwritten digits, 'image of handwritten digits' and 'correct label' are given as pair data. Using such data, computers learn how to transform input into output (Figure 1-1).

[[[00000000000000000009---111eed819f5f457d82ec9ef9ca5baccde5012fa29114d81c2b163550e2dc5086]]]
Figure 1-1 Example of supervised learning (when an image of a handwritten digit '8' is input, learning is performed so that the probability of determining that it is the correct label '8' increases)


[[[00000000000000000010---435b225a3906c04f6694721b8c5626929cd18aa8dd567db6da97a144bd2175d6]]]A feature of supervised learning is the presence of 'correct labels'. The input data is given labels that are correct answers by the 'teacher'. Teachers are often us humans. In the example in Figure 1-1, a human assigns a correct label to each image.

[[[00000000000000000011---9f854bd894c643ca2c60f226741af2acd88d1e65059470f95bb5d257f5529845]]]unsupervised learning

[[[00000000000000000012---48ac05b8b15917817a6d2aba18b5ffcf050cbdc7efe879b122481d55dbbce44d]]]In unsupervised learning there is no 'teacher'. There is no 'correct label' given by the teacher, only the data. The main goal of unsupervised learning is to find structures and patterns in data. Examples include grouping (clustering), feature extraction, and dimensionality reduction. Figure 1-2 shows an example of visualizing features obtained from data by reducing them to two dimensions using an algorithm called t-SNE[1].

[[[00000000000000000013---75cfd98ee12e288003fbedcd1487575c2299a972942e0339312638ba1eca7e93]]]
Figure 1-2 Example of unsupervised learning (dimensionality reduction by t-SNE)


[[[00000000000000000014---234efe951cf230f8c1a9a1d2ff7d7d868deb31e1fa7da02156d8742993fcd27d]]]Unsupervised learning does not require correct labels. Therefore, even big data (huge data) can be prepared relatively easily. On the other hand, in supervised learning, correct labels often need to be assigned one by one manually. The work of attaching correct labels by hand is called 'labeling work' or 'annotation'.

[[[00000000000000000015---8059fd044ee2c440f102d7c75186eb37e7fccf0e1e363a3e00bbed643cefc456]]]Labeling work takes a lot of time. For example, in the case of ImageNet, a huge dataset for image recognition (more than 14 million images), labeling work alone would take about 20 years.

[[[00000000000000000016---5b1c228b2f243e083fe6d2f7ee715773ffc03ee14bdbd44579c0da1f9085bfc8]]]reinforcement learning

[[[00000000000000000017---effe9022df8ad100ac828e050f64f1ba57e0ce73a522aebbdc12319a599cdf12]]]Next, I will explain the outline of reinforcement learning, which is the theme of this book. Reinforcement learning is a different problem setting (framework) from conventional supervised learning and unsupervised learning. In reinforcement learning, the agent and environment interact as shown in Figure 1-3.

[[[00000000000000000018---6212935283a86c15f3aadc34aac81b2c2eacbed9edbc6f799be5febdc132533d]]]
Figure 1-3 Reinforcement learning framework


[[[00000000000000000019---1c47222d495869e8e4504b0b5d1c41c32e57e7dadbb911b632a69c9edbbc11fb]]]An agent is a subject that acts. As an example, it's helpful to imagine something that behaves like a robot. An agent is placed in some environment, observes the 'state' of the environment, and performs 'action' based on it. As a result, the state of the environment changes, and the agent receives a 'reward' from the environment and observes the 'new state' at the same time. The goal of reinforcement learning is to acquire a behavioral pattern that maximizes the sum of rewards obtained by the agent.

[[[00000000000000000020---ea8d80541937ce90680e170308257018cd431e1696f4670ec36b281b779b43e4]]]As an example of reinforcement learning, consider the problem of a robot walking. A robot is placed in a real space (or on a simulator) and learns how to walk in it. The robot's goal is to learn how to walk efficiently forward. In this problem, the robot takes the action of moving its limbs. And the surrounding environment (state) changes depending on the action. The reward obtained is the distance traveled (Figure 1-4).

[[[00000000000000000021---998bd2821ae64dbc9fd6ec0d184d46512b4d1406b496b43392d1fd3455a15730]]]
Figure 1-4 Mutual interaction in robot walking problem


[[[00000000000000000022---7dba65d33570807dfdc8f3e814455185fa5eebe820b056941b70028c3602058d]]]In the robot gait problem, the robot does not directly learn how to walk well (eg moving its limbs at certain angles makes it easier to walk). It is very difficult to prepare a 'teacher' to teach you how to use such correct limbs. What a robot can do is to perform some action, gain feedback (reward), accumulate experience, and learn from that experience. Good behavior is learned from the experience of losing balance when you move your hand to one angle and not losing balance when you move it to another angle. In other words, you collect data by trial and error yourself, and learn good manners of behavior from the collected data.

[[[00000000000000000023---ab0f3fa082529a075fe1b7d187a3752754ad14f2fc6d43a57df76006c9130189]]]In reinforcement learning, we receive a “reward” as feedback from the environment. This reward is different in nature from the correct label in supervised learning. Rewards are feedback for actions, and it is not possible to judge from the rewards whether the actions actually taken are the best actions. On the other hand, in supervised learning, correct labels are provided. In the context of reinforcement learning, this means having a teacher suggesting the best course of action for any given course of action.

[[[00000000000000000024---21efffff51891be0388041207af82b0a1562b8037fb62f982d5a8b2dc7764fdd]]]These are the major categories in the field of machine learning. The characteristics of each field can be summarized as follows by focusing on the data used.

[[[00000000000000000025---72257bad69c80fc0dfffbe7169e96cab3426766a65cb953d052218c50e8b5eea]]]Supervised learning: Learn how to convert input to output using paired data of input and output (correct label)

[[[00000000000000000026---6e04a7141167c0fae2830fccb5a6a5bd14b707d124069f7a30565d0df0d778f4]]]Unsupervised learning: learning the structure hidden in data using data without correct labels

[[[00000000000000000027---9f156a264b474076fdbc8951244ae443552532b8d94e8783c5190f5132064845]]]Reinforcement learning: Learn how agents interact with the environment and use the data they collect to get high rewards

[[[00000000000000000028---01097fbc2cdf38be50e503a0033f91be1e32d65a89a61cf4894d87f184d69aa5]]]In this way, reinforcement learning has a big difference from other divisions. Learning through interaction with the environment, learning through trial and error, this is reinforcement learning.

[[[00000000000000000029---5adf4d595df0226b1c13d3eeb9bf65336786066afa3c177ca06d377d8105d9a0]]]bandit problem

[[[00000000000000000030---e00a1abb43612d10f9a7e7f3f1d95e2ce96500674baa213e67d12299c6c3742f]]]Next, we will look at specific problems in reinforcement learning. Here, I will explain one of the simplest problems in reinforcement learning, the bandit problem. The bandit problem is a simple problem, but it has the essential characteristics of reinforcement learning problems. By solving the bandit problem, the characteristics of reinforcement learning will become clearer.

[[[00000000000000000031---4be8daec9d1286708b42ba3a85049afee41b80d14afb2b069d36ff8b5c577f30]]]What is the bandit problem?

[[[00000000000000000032---5b245e93f11adbe6f15279cfdf086a0a67cfca5c69dcd07f653fe53cc653fe49]]]Now let's talk about the bandit problem. First of all, the word 'bandit' is used as another name for 'slot machine'. As you know, the slot machine has a lever, and when you pull the lever, the pattern of the slot machine changes all at once. And the number of coins you get will change depending on how the marks are arranged (in the case of 'losing', you will get '0' coins).

[[[00000000000000000033---63f04e29265fe725c2454dcb7e9b9b2a592264581f3dec5a6deaaf888554d1b9]]]Bandit is a word that means 'bandit' or 'bandit'. Although it did not originally mean 'slot machine', gamblers later sarcastically referred to slot machines as 'one-armed bandit'. The reason for the 'one-armed thief' is that slot machines in casinos have one lever, and when you insert a coin and pull the lever, the coin is (in most cases) stolen.

[[[00000000000000000034---b0a452ab1426bb359ef8e14490bf32154614aca4470e077f6b94ad063b314fd8]]]The bandit problem is properly called the Multi-armed Bandit Problem. A multi-armed slot machine is one that has more than one arm (lever). This 'multi-arm' setting is the same problem setting as when there are multiple single-lever slot machines as shown in Figure 1-5. In this book, we will proceed with the story while imagining the latter case--when there are multiple slot machines with one lever.

[[[00000000000000000035---04c2a51249f77dbfd1407164cb6314dd83076936ed5cf33e529029a5521de8d1]]]
Figure 1-5 Multi-arm bandit is a problem with multiple slot machines


[[[00000000000000000036---440a962769896d551252f0412fd8fdb7d275f56a5144e9f26b7727fb366c18d7]]]In the bandit problem, each slot machine has different characteristics. Different characteristics mean that there are machines that win a lot and machines that lose a lot. In such a situation, the player will play the slot machine a fixed number of times (say 1000 times). At first, the player has no information about the slot machines—such as which machines are more likely to win. It is necessary to actually play and look for a machine that looks good while looking at the results. Then, the goal is to get as many coins as possible in a set number of times.

[[[00000000000000000037---de4d6a6d6dc43c113ac7d89b811602f4ecef92f0dba2b4b943d196ff031931d2]]]We will now look at the algorithm for solving the bandit problem, but before that, let's explain the bandit problem I just described using the terminology of reinforcement learning. First of all, the slot machine that appears in the bandit problem is called the environment in the framework of reinforcement learning. A slot machine player is called an agent. These two characters -- the environment and the agent -- 'interact' with each other, which is the framework of reinforcement learning.

[[[00000000000000000038---c467995f1532998974748c2a2cf1bee53b1c53789d6842ab7a8dfb468a9f36ff]]]So what kind of 'interaction' is there between the environment and the agent? In the bandit problem, players choose one of several slot machines to play. This is the action that the agent takes. And as a result the player receives coins from the slot machine. This coin is your reward. Figure 1-6 shows the above relationships graphically.

[[[00000000000000000039---3c2ddf413f88633cb9904afe48c4ba4bcc5f86c693b4fc70eb28639eb7def4f3]]]
Figure 1-6 Bandit Problem Framework


[[[00000000000000000040---b3ab3c7b7cbfb9897c26e34ed40b2c4f721c6d36472de8ba86436eceaf26696f]]]As shown in Figure 1-6, agents perform actions on the environment and receive rewards as a result. This interaction between the agent and the environment is the framework of reinforcement learning, and the bandit problem is included in it.

[[[00000000000000000041---15832dc629b7466f7b328fb654a5a029c14823616860138d4bc33f7aeff28726]]]In common reinforcement learning problems, the environment has a 'state' inside it. When the agent performs some action, the state of the environment changes, and the agent performs appropriate actions according to the state. In the bandit problem, players play against the same bank of slot machines each time. So you don't have to think about the state, because the state of the environment doesn't change. The problem of changing state is dealt with in Chapter 2 Markov Decision Process.

[[[00000000000000000042---10c9d07100935caa0feb3580c14c81de3f71763e1a3391a6a3305b771b9a82ec]]]Now let's consider an algorithm to solve the bandit problem. Our goal is to collect as many coins as possible. For that purpose, of course, it is required to choose a “good slot machine”. So what is a 'good slot machine'? First of all, let's think about the 'goodness' of slot machines.

[[[00000000000000000043---ac9d2a4d88cda9ea74aa7c2030a6557adaa4576a5233993b30cd29a1eee84f0e]]]What is a good slot machine

[[[00000000000000000044---14368074975a07645f6acc3961d7d32bdc05c632b24662afa8d90ac41691aa41]]]When thinking about slot machines, the first important thing is that slot machines have randomness. Randomness means that the number of coins (reward) obtained by playing the slot machine changes every time. We use 'probabilities' to combat this randomness. Quantitatively describe how much randomness there is using probability.

[[[00000000000000000045---33d5c255964fd9bc6263321d45af0c24ae28bdd19ac3dbaff5a6e84ff128551d]]]Let's take a concrete example and think about it. As an example, consider the case where there are two slot machines with rewards and probabilities as shown in Figure 1-7.

[[[00000000000000000046---294c60d99c0c69311fb2fd2d9e05f92784a54e856ccee50d584cbbc22943d134]]]
Figure 1-7 Slot Machine Probability Distribution Table


[[[00000000000000000047---cac44d05c6e06f5b793abae9ab18ca24ed9c20bbc0fb97de5a7d8014881dc75e]]]The table in Figure 1-7 is called a probability distribution table. Usually such a probability distribution table is unknown to the player. Here, let us assume that we know the probability distribution table shown in Figure 1-7. Which slot machine should the agent choose then?

[[[00000000000000000048---709860193f2377cdd1cedf6d0eb43d620e516fe1158148b3cf295135dad52da8]]]The table in Figure 1-7 is a discrete probability distribution table. “Discrete” means that random variables take “discrete” values such as 0, 1, 5, and 10. On the other hand, if a random variable takes continuous values (for example, all real numbers), it is called a continuous random variable.

[[[00000000000000000049---c612d8e9b1f1070f1b581f7b58031f0af04a8166b76ffe91c33fef1f019cad27]]]So which of the two machines in Figure 1-7 is the better machine? What criteria should be used to judge the superiority or inferiority of slot machines? As mentioned earlier, the results of playing slot machines are probabilistic. However, when you play a lot—in theory, infinite times—the number of coins you get averaged is fixed at “one value.” That is the Expectation Value. Using this expected value as an index, the one with the larger value is judged to be a better slot machine.

[[[00000000000000000050---faf54d0ec38761135ec76a920a6fe1c2eef02dd0f50fdf0dd3c36ac740703ed1]]]Given a discrete probability distribution table, its expected value can be calculated by multiplying the 'value (the number of coins you get)' and the 'probability' and summing them up. If you actually calculate the expected value using the probability distribution table in Figure 1-7, it will be as follows.

[[[00000000000000000051---42af68db7a9088aa48f9d76fb6a79e5497c98a37fcb1d0ba8c1a7f5f11943821]]]The expected value of each of the two slot machines can be calculated as above. This expected value is the average number of coins you get when you play two slot machines. Comparing the two expected values, we see that slot machine a is larger. In other words, if the expected value is used as an index, slot machine a is a better machine. So if you have two machines, a and b, and you are going to play 1000 times, then always choosing slot machine a is a good strategy.

[[[00000000000000000052---e78fe8cb097dc909b6b7e082733f5993e942c9b443e907266a0c115953cf2a7a]]]The thing to remember here is that probabilistic events like playing slot machines can be evaluated using 'expected value'. In other words, we need to use “expected value” as a metric to avoid being misled by randomness.

[[[00000000000000000053---eaf23c3a62e01a9cea73f35bae347a9b427963e4d7974bc65a4ccb3c41d37c7a]]]In the bandit problem, the expected value of the reward is sometimes called a special name, value. In particular, the 'expected value of rewards obtained for actions' is called action value. Based on that point, the expected value of the number of coins obtained when playing slot machine a can also be called 'value of slot machine a' or 'action value of a'.

[[[00000000000000000054---c4052e722bef86e1572f8315f9ce50d888eaa2afa5221f3fcf5c7c3ff8ba7a0d]]]use a formula

[[[00000000000000000055---ce7c6d2fd40072b14aef234f11e321d8bba794b8fce35be2a8e2cb829b6f0157]]]Here, in order to familiarize yourself with mathematical notation (symbols), we will review the contents explained so far using formulas. First, let's use the initials of Reward to represent rewards. In the bandit problem, the number of coins returned by the slot machine is the key. In the previous example, one of the values is taken, and the 'easiness of taking' each value is determined as a probability. A variable whose value is determined stochastically in this way is called a random variable.

[[[00000000000000000056---e75ff0a2701f2620f8d3e575d6f102f671433206d5009acdc09c7ed10690d1fc]]]In bandit problems (and reinforcement learning problems), agents perform actions once, twice, and so on in succession. Therefore, if we explicitly express the reward obtained in the round, we will write .

[[[00000000000000000057---cb4fcb0a44ba1e4bf0dbfc764560604ee024a07261d67347863e57fee249dee8]]]Next, the action performed by the agent is represented by taking the first letter of Action. This time, if we represent the actions of choosing slot machines a and b respectively, the variable takes either value.

[[[00000000000000000058---4efa71ab009e1df355736db9d9106abf474cae6cad4fbe7d9b3db09e8e34fff6]]]Next is the 'expected value' defined for the random variable. The expected value is represented by the symbol from the initial of Expectation. For example, the expected value of the reward is written as Also, the expected value of the reward when choosing the action is written as. Here, it represents 'conditional probability (conditional expected value)'. Write the condition on the right side of . For example, the expected value of the reward for choosing the action , or simply write .

[[[00000000000000000059---1ead01535bc576f69559251d01707d982f307ef36aca2334406b1d5922292a6e]]]The expected value of the reward is called action value. In the field of reinforcement learning, it is customary to use the symbol Y for action value (which is considered an acronym for Quality). here

[[[00000000000000000060---46d082c0557dfbee4afc03cc740edbf5dd72917ab203d4a9e0c7463a6f9ac796]]], the value for action is represented by the symbol .

[[[00000000000000000061---dc9dd34d33e50566ee34a4634b2dadd5dd7163c1c56e15f98dbaa308e6991870]]]Behavioral value is sometimes expressed as , and sometimes expressed using capital letters. The lower case represents 'true action value'. On the other hand, capital letters indicate estimated values. As will be explained in more detail later, the agent cannot know the true action value, so it must 'estimate' it. In that case, we denote the estimated action value as .

[[[00000000000000000062---494dafba88a9eb5e6e06050846649948e0f48d6635fa9be40efbe67b312bb05b]]]The above is confirmation of symbols for formulas. Next, we will consider algorithms for solving the bandit problem.

[[[00000000000000000063---5ecec13e77ad230c0d582b923f3571c6154fcb9e5a458c4efa446ce960447c15]]]bandit algorithm

[[[00000000000000000064---76f8dae1a9a4fb0e21a7a3e60af011491bb31cff183f49f619ccd8eeb9aee7b4]]]In the bandit problem, the player cannot know the 'value (=expected value of reward)' of the slot machine. In that situation, you are required to choose the slot machine with the maximum value. In this case, the player must actually play the slot machine and extrapolate from the results how good (or bad) his choice was. The story up to this point can be organized as follows.

[[[00000000000000000065---64db1fb722eee408a6693268276ca442f6ff23360a05ba8ba1fbbaf2883cb5df]]]If the value (expected reward) of each slot machine is known, the player can choose the best slot machine.

[[[00000000000000000066---36e1d6919db04a762d293b79c14c1edb2255cf34a6359223c63247ba7b69dea5]]]But the value of each slot machine is unknown to the player

[[[00000000000000000067---92fc9e150be247566427bfb983901b165912e8aa4fa433dbd5336885e50d3e57]]]Players are therefore required to estimate (as accurately as possible) the value of each slot machine.

[[[00000000000000000068---bc839e38735109862ab2e9d6320f66218d2c0ff176f9d77305bde8ca4e0b4b9e]]]A measure that satisfies the above points and increases the total number of coins obtained is required. Next, let's consider how to estimate the value of a slot machine by looking at a concrete example.

[[[00000000000000000069---375a9689a3b75c4611e9a3437335fd44fc1232ee5f2cb5f0dd3a9051fb886d2d]]]In the case of supervised learning, a correct label is provided for a given problem. So even if (the model being trained) makes a wrong prediction, it will still be given a correct label. To use the terminology of reinforcement learning, it means that if you do something wrong, you can get a correct answer that the correct behavior was like this. On the other hand, in the case of reinforcement learning, the player himself acts and only the result (reward) for that action is obtained. For example, let's say you play a slot machine and get 1 coin as a result. In that case, the value of 1 is just one clue. As for what the correct answer is and what the best course of action is, it is up to the player to estimate and judge by themselves, using the reward as a clue.

[[[00000000000000000070---0b4b9955133819b116315d4ad2a607540f94f160ff4a70d33128f6d610e97ab5]]]how to estimate value

[[[00000000000000000071---170bb54fae9c8aa20c4b2bcd478aeb1d9e178a4b19b56de6448bc4eddf2a30b1]]]Suppose we have two slot machines, a and b, each played once with the following results:

[[[00000000000000000072---38d8a26150a38027f7ddf09163b8dfb96783332eb17bf346f62df6a12615e3f9]]]
Figure 1-8 Result of playing two slot machines once each


[[[00000000000000000073---e817c3868e4c69c9f0391cdbbc3058c9fa510bf3bb94da7f12f82aed49d90cf0]]]Player plays slot machine a and gets 0 coins, as shown in Figure 1-8. Then play b and get 1 coin. Here we consider the ``average actual rewards obtained'' as an estimate of the value of the slot machine. Since we have only played once this time, we are forced to assume that slot machine a has a value of 0 and slot machine b has a value of 1.

[[[00000000000000000074---568428672a029020ca845744e27c2c41dfd62ee0db748db926dc4742a2a57b28]]]Of course, a one-time play wouldn't give you a very reliable estimate of the slot machine's value. However, the more you play, the better your estimate of slot machine value. Here, we consider the case of playing as follows following the results in Figure 1-8.

[[[00000000000000000075---163a9c51d3ced2f175f72b98ea814cc4354046cb2c5fba47e861268d73eca7af]]]
Figure 1-9 Result of playing more times


[[[00000000000000000076---852185574be6c99bb093ffaf6a1a8ff75bf0263675cac6f766fc5ee88e4a759f]]]Players played slot machines a and b three times each, as shown in Figure 1-9. For slot machine a, we got 0, 1 and 5 coins. From this result, if we denote the estimated value of slot machine a by , we can calculate:

[[[00000000000000000077---7f5780ddf5b7fe231f614c139f564d951b2b44fbd82c805b1d578fcaa638cfaa]]]From the calculation above, we got an average of 2 coins per turn. This average is an estimate of the value of slot machine a. On the other hand, I played slot machine b 3 times and got 1, 0, 0 coins. From this, the estimated value of slot machine b is

[[[00000000000000000078---3a4b1ed5575da250740cab0f349dfb99fa930e792e9228f8d6f46efc1050e2b3]]]becomes. From this result, it can be estimated that slot machine a seems to be better. Of course, our estimates at this point also don't match the 'true value' of slot machines. However, it is considered a more reliable estimate than playing slot machines a and b only once each.

[[[00000000000000000079---9d377dc1d197c682861559b4eb7e7b7bc5d49229708b63b2c054921b506fcd33]]]The rewards obtained by actually playing slot machines are 'samples' generated from a certain probability distribution. Therefore, the average value of rewards actually obtained can be called the sample mean. The sample mean approaches the true value (expected reward) as the number of samples increases. According to a principle of probability theory called the 'Law of Large Numbers', if you sample an infinite number of times, the sample mean will match the true value.

[[[00000000000000000080---00d57e700c9d20601c1491c6f6ff858a512ade86a5e6e998de3c109a00f7d8a3]]]Next, let's write code to estimate the value of a slot machine.

[[[00000000000000000081---779c0a2b39b61f83d59fff0288a602232d3b730bb7edc7790b99e437fcd4dc94]]]Implementation to find the average value

[[[00000000000000000082---9fb056222ac4e9634ec379504434e95c0118c7a298f2e7c1a3efe99b12a154b6]]]For now, let's focus on just one slot machine and consider playing that slot machine a number of times. In other words, consider the case where only one action is performed, and estimate the action value. Let the reward actually obtained at this time be represented by , and the estimated value of the action value at the time of the action is represented by . Then can be written as

[[[00000000000000000083---48c36645465f68a9bf2b7989db7de105d439212522aa0aab6568e497dec39167]]]As above, the estimated action value for the round is obtained as the sample mean of the individual rewards. Now, let's implement the sample mean calculation according to formula (1.1). Let's assume that we get the reward 10 times in a row, and get an estimate each time we get the reward. Here is the code:

[[[00000000000000000084---6430b7f05014e82450baca288bdf147ae668808dbdaf4044741e014347c9f9cd]]]# fix seed

[[[00000000000000000085---5f794b1786c284821b4b7fffebf027f020e42a04f9198beb172cd6fda24e590e]]]# from 1 to 10

[[[00000000000000000086---18e2da345dbad4d2da83a0d0ecf2ade376dd53ed965d8057ed1a8e880d896b02]]]# dummy reward

[[[00000000000000000087---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000088---9bff933cb938d9a2b4c0326b5d3a6d633e91d280062abec258c9c5e00ab91723]]]Here, one real random number (random number between 0.0 and 1.0) is generated as a dummy reward. In the code, we add the earned rewards to the rewards list. Then we can implement the calculation of formula (1.1) as it is. Of course, this code correctly finds the sample mean. However, there are improvements to this implementation method.

[[[00000000000000000089---1771ad37387f2581faf6a1e7dc2967246444f016798b097b18af712bb7499b10]]]The problem with the above implementation method is that as the number of times played (n) increases, the elements of rewards increase. In addition, the amount of computation required to calculate the sum of n values—sum(rewards) in the code—is similarly increasing. Therefore, as the number of times you play increases, the amount of memory and computation increases.

[[[00000000000000000090---36896fcec0ab1adac1d1cb3afe7aae5625a205247fb1784fce0eb5c5d38258b1]]]Now let's discuss how to better implement sample means. As a preparation for that, we will focus on the estimated value of the action value at the time of the second round. This is represented by the following formula:

[[[00000000000000000091---5425186eee8b5a73153d962be26c4f186c7289457cafb358bb2a7a2f9990f150]]]Multiplying both sides of this equation gives:

[[[00000000000000000092---f3966fa52c2e7b47aa1042a6298afa96989820e07260d932f9421b4771c8433e]]]From equation (1.2), can be expressed as

[[[00000000000000000093---af307b123d54e62e0d6c3662aea89f0fd868a9b268d9737f6dbbc5f079127759]]]The point here is to substitute formula (1.2) in formula (1.3). This leads to the relationship between , as in Eq. (1.4). Finally, from equation (1.4), we can find that we need three values of , , and . The important point is that all rewards () obtained so far can be calculated without having to use them every time.

[[[00000000000000000094---413597f95332678315f82ccc8d05f0da56143485e5b295cdd7709938dd04577e]]]Next, we transform equation (1.4) further.

[[[00000000000000000095---46bddefbfd94bc07f47a21adc00b38d8fe44ce6962eac2677e71e983a253c3c3]]]What should be noted in formula (1.5) is that it has the form From this, it can be seen that is based on . Then, the reference () is updated by the second term on the right side ————. The positional relationship here is as shown in Figure 1-10.

[[[00000000000000000096---961ce9e2ebc4f69001d7818a292b229a1e90206e732a6026f91d3730d9462a13]]]
Positional relationship of Figure 1-10


[[[00000000000000000097---524dd4a591ffbd9132ebc46921ea4b45c6357dd0a87aa16884c0f71f1b447619]]]The amount to be updated is the value multiplied by the length, as shown in Figure 1-10. When is updated to , it advances in the direction of , but how far it advances is adjusted by . This acts as a 'learning rate' as it regulates the amount of updates.

[[[00000000000000000098---55a7379932e8c4bf2e6d03a2bca85ccc6bda73744202c11a1b1cbc9b0f0ef6ae]]]decreases as the number of trials increases. Therefore, the amount updated decreases as the number of trials increases. For example, in the case of . At this time, it will be updated to . As another extreme example, the case becomes . In this case, is not updated at all.

[[[00000000000000000099---a1c3e577b6af1adbec533e06f07479d8c8042c89165e83dd407208a4187fe593]]]Based on the above, let's implement an implementation that calculates the sample mean. Assuming the problem setup is the same as before, we can implement it as follows:

[[[00000000000000000100---de99f2d62fdb2fb15d77320a80bb8d93b7f7687d2da6539e14e0493e9a773bc4]]]The above code Q = Q + (reward - Q) / n implements equation (1.5). The point to note here is that the same variable Q is used for all variables corresponding to (1.5). Figure 1-11 makes it clear why a single variable is fine.

[[[00000000000000000101---63075a2b329205d2a46f5be1fd923bbd1605dcc1da2657aef046c2ffe19f274d]]]
Fig. 1-11 Actual Q on the right and left sides of =


[[[00000000000000000102---a9a7166e9e72a569587efc37bc2ab5c6b4add9ed0bb717c3452aa86a49c3601f]]]Q in the code to the right of = refers to the previous estimate, as shown in Figure 1-11. The result calculated on the right side is newly assigned to Q on the left side. Therefore, we can update the old estimate to the new estimate using only one variable Q. Also, such Q updates can be written using the += operator. In that case, write:

[[[00000000000000000103---3c25d9e3679b566e602318e767fda81aa0d5228d5d417b8ab50eb23b0da16e84]]]The overriding code Q = Q + … can be written as Q += … as above. The above is an efficient implementation for finding the sample mean. This can also be called an 'incremental implementation', since it can be found in order, incrementing one by one (incremental means 'sequentially').

[[[00000000000000000104---f44ce87f56deb69025134e22eb23382f0582e8c8e4ff9359885e0c0e6bb8d871]]]player's strategy

[[[00000000000000000105---2cfd2acd23bef6219b41a9410db19edc50f853fc48d00fcf532df3651c8b472d]]]Next, let's consider what kind of strategy the agent (player) should take. In other words, how can you find the slot machines with the greatest value? A player's first strategy is to choose the best slot machine based on the results of actual play. Specifically, it is possible to choose the slot machine with the highest value among the estimates of the value of each slot machine (the average value of the actual rewards obtained). This can be described as 'greedy behavior'.

[[[00000000000000000106---4ffa5124b73f050aa413e7bd88d792c6fff9f8362b2e7fe171879d11fc71ce27]]]The word greedy translates to 'greedy' in Japanese. What it means is that 'choose the best move from only the information at hand without thinking about the future'. The greedy behavior in the bandit problem also amounts to picking the best machine based solely on the experience you've played so far—an estimate of the value of each slot machine.

[[[00000000000000000107---7be276df291acfebe59fb1496cd07a6080cf2def26cfc07a165c386183bfdecb]]]Greedy behavior sounds good, but there are problems. For example, consider playing slot machines a and b once, as shown in Figure 1-8. Now suppose slot machine a has an estimated value of 0 and slot machine b has an estimated value of 1. Then, if you act greedy, only b will be chosen from then on. However, a may actually be a better slot machine.

[[[00000000000000000108---db3f086a141c96622e7738c78dd5c099604ee237d8f0a1af2e28ac24ea7c5978]]]Such problems are caused by the 'uncertainty' in the estimates of slot machine values. If you rely entirely on the 'uncertainty' estimates, you may miss the best course of action. Players are required to reduce the 'uncertainty', i.e. increase the reliability of the estimates. Considering the above points, players are required to:

[[[00000000000000000109---43bbbb9d239dd437b5c1b5aed32ac7f8c835fcf747176636807bfadcef20d0b0]]]Playing a slot machine that you think is the best based on what you have actually played (=greedy behavior)

[[[00000000000000000110---1647d6e333e3903779821880d3416ae937f72213d540e6a7dd47a6e3e4756e96]]]Trying out different slot machines in order to get an accurate estimate of their value

[[[00000000000000000111---79591fe65d2da7476eb57e6a36e60fc79993bd5570b8ad5e49e3ad1cf5937608]]]The first point above is to choose the best course of action based on past experience. This is a greedy behavior, alternatively called Exploitation. As explained earlier, for greedy actions, the best action is chosen based on previous experience. But it's also possible that you might miss a better choice. So we have to try the second non-greedy behavior. This is called exploration. Exploration allows us to better estimate the value of each slot machine.

[[[00000000000000000112---f32cfb0b682b9a7f34fc522da1c0b436a78e8f7b1888a08c315e112e0fbdd9f7]]]Exploitation and exploration are in a trade-off relationship. The trade-off here is that you can't choose between Exploitation and Exploration—one must be sacrificed.

[[[00000000000000000113---183cdc588fa7bef59207941841d51ff00b743c19bd1896c2a9fe1dc7793a5357]]]In the bandit problem, if you want to get a good result with only one next trial, you should take 'exploitation'. However, if you want to get good results from a long-term perspective, you need to “explore”. Because the more you 'explore', the more likely you are to find a better slot machine. If you can find a better machine, then that machine will give you better results in the long run from then on.

[[[00000000000000000114---81440d6b60a645f74a24f280eee813456a4e01789546a8b66ccc8de2e176b779]]]Reinforcement learning algorithms ultimately come down to how to balance 'exploitation and exploration'. Various algorithms have been proposed to find ways to achieve this balance. There are a wide variety of algorithms, from simple to complex, but the most basic and applicable algorithm is the ε-greedy method (epsilon-greedy method).

[[[00000000000000000115---147d77bd4e43b02a9ed968197f73f698ae47bc6ab783b7639a84b84c9a68de1f]]]The -greedy method is a simple algorithm. This is a method of performing 'explore' with a probability of - for example, with a probability of (10%) - and performing 'exploitation' otherwise. In 'exploration', various actions are tried evenly by choosing actions at random. Doing so makes the estimate of the value of each action more reliable. Then, it performs a greedy action ('conjugation') with a probability of .

[[[00000000000000000116---ae654b03b0ea7323ddee338b6d1f1010a08c5aa1295316c088e4ec579c650c79]]]The -greedy method can 'use' the experience gained to date. And yet, by (occasionally) experimenting with non-greedy behaviors, we 'explore' other behaviors that are better. This -greedy method can be expected to solve the bandit problem efficiently.

[[[00000000000000000117---9b5649996fe7790e1b728513d059eb04ec39b79f093893734e4d5d33c7a58336]]]The above is an explanation of the bandit problem and its representative algorithm, the -greedy method. Next, we will implement what we have explained so far in Python.

[[[00000000000000000118---71a3c0e529736017bc890ebab358cd4161860d72cdcbb6f62caa3347af213c34]]]Implementation of the Bandit Algorithm

[[[00000000000000000119---4eaf01625dca5001f526ee9bced81657db687975400cf31e62c606f36d98de7b]]]Now let's implement an algorithm to solve the bandit problem. Let's keep things simple and consider the case where the number of coins returned by a slot machine is limited to either 0 or 1. In other words, if you play a slot machine, you will be rewarded with either a win (1) or a loss (0). And each slot machine has a set probability of winning (probability of returning one coin). For example, if your win rate is set to a value of 0.6, you have a 60% chance of getting 1 coin and a 40% chance of getting 0 (losing). In this case, the value of the slot machine (expected value of coins returned by the slot machine) is 0.6. In other words, the winning percentage is directly the value of the slot machine.

[[[00000000000000000120---0b2817b462b51c673306ee5bf9e10db5934eebebab0dd7f89151918bf163c207]]]Let's say you have 10 slot machines set up with different odds of winning. Players do not know the odds set for those slot machines. Therefore, you should look for slot machines with a high winning rate based on real-world experience.

[[[00000000000000000121---3297642d934dc77ab13f00d3985fe1cd4dbcbac1c4f831f3ce0020ca080a58c0]]]slot machine implementation

[[[00000000000000000122---7772417c96e60579661f0ab42149b73d0cdf547c2c0f92beeb6482dda8d0a281]]]Now, let's implement 10 slot machines (but the odds of winning are set by random numbers). Here, we will implement the slot machine as a Bandit class. Implement 10 slot machines in this Bandit class. Here is the code:

[[[00000000000000000123---30b6d7846de23f6aa55896fc65e3796c340e9d2f84f1ab57fb2b047d0761ab7b]]]# win rate for each machine

[[[00000000000000000124---4123aeddf6c3156ce8ac563fa1785c4cbe98802aa4e3fd42d604e471da0b2b03]]]First, let the argument for initialization be arms. This arms means 'the number of arms', which corresponds to 'the number of slot machines' in this problem. The number of slot machines is set to 10 by default. Also, in the __init__ method, we randomly set the win rate for each machine.

[[[00000000000000000125---38663f44de82186c60d4c344fd55a27dd80a34c1a4a34221c88fbb56ebe39ec7]]]np.random.rand() generates one random number between 0.0 and 1.0. At this time, random numbers are generated from a uniform distribution. A uniform distribution means that random numbers are generated uniformly without bias—evenly spread between 0.0 and less than 1.0— is not). Also, np.random.rand(10) will generate 10 random numbers between 0.0 and 1.0. This will randomly set the odds of winning for each of the 10 slot machines between 0 and 1.

[[[00000000000000000126---7078a63369786d71e50fff6d0bbfb3423bbaf788d07d34033edf5759fad9779c]]]Next is the method play(self, arm). The argument arm of this method specifies what arm (slot machine) to play. The code inside generates a single random number between 0.0 and less than 1.0 using np.random.rand() after getting the winning percentage of the armth machine. This random number is compared with the winning rate of the slot machine specified by arm, and if the winning rate is greater than the random number, 1 is returned as a reward, otherwise 0 is returned.

[[[00000000000000000127---245a9eae5679e80cb56691b281eab81db1732ccbae0b69d292f0b73921c502b8]]]If you can implement this Bandit class, you can actually play the slot machine as follows.

[[[00000000000000000128---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000129---245a8388849ba9f8e1732233beab6cf35ef5feb81945eee9c6e982f1322b023e]]]Here, we are printing the number of coins obtained by playing the 0th slot machine 3 times in a row (this output will change for each run). The above is the implementation of the slot machine. Next, we move on to the implementation of the agent (player).

[[[00000000000000000130---afcb2912c0ea2b49d2e8ba962d5f34aacddf7f9d8f3d9bd5c2289a99c53054a0]]]Agent implementation

[[[00000000000000000131---9257dbfd605704073e5bfc38d2f720e9c15add35505ae74743647ef41c2bb320]]]In the previous section, we learned an efficient implementation — an incremental implementation — of finding the sample mean. As a review, let's focus only on the 0th slot machine and estimate the value of that slot machine. The code looks like this:

[[[00000000000000000132---4105122e5e4815681a3fa76e7f498b13ec05baf37f0d6113866d2050cb46e472]]]# play the 0th slot machine

[[[00000000000000000133---8d8e1ccc23c4c7b7b52484f2c894d19b6b266fbad6c116d0fc631e04c5170f0d]]]The code above will play the 0th slot machine 10 times in a row, updating the slot machine value estimate each time you get a reward. This is a review of the previous section. Next, find an estimate of the value of each of the 10 slot machines. This can be implemented like this:

[[[00000000000000000134---23cd62272a975e67acac41edabdec4b3aaa0f569db2f93c92afdc9a7626b45d3]]]# random behavior

[[[00000000000000000135---cddd8ed069f2c913db44bc9abeb1b9d145b90ea6c57b91fbf1ee715edc41ea31]]]Here, we prepare new one-dimensional arrays with 10 elements called Qs and ns (every element of Qs and ns is initialized to 0 with np.zeros()). Each element of Qs stores an estimate of the value of the corresponding slot machine. And each element of ns will store the number of times the corresponding slot machine has been played. This allows us to estimate the value of each slot machine.

[[[00000000000000000136---4b30651b605ba91c7c7d0b44d04b6a7b568e8d03d6004d23471ec1870db4da8c]]]Based on the above knowledge, we will implement a class called Agent. This Agent class implements an algorithm that chooses an action by the -greedy method. Here is the code:

[[[00000000000000000137---03ceeae01ee7e6e7e45b2a1dcdb6fe5c4b629d5378711f3c87779bfe912d5b90]]]The initialization argument epsilon is the random behavior probability for the -greedy method. For example, with epsilon=0.1, there is a 10% chance of random behavior. The initialization argument action_size is the number of actions the agent can choose from. For the bandit problem, it corresponds to the number of slot machines.

[[[00000000000000000138---d8253a486aba24837e0e69ceb36002104c668f76454a7456a9b5f148dba528e2]]]In the code above, it is the update method that estimates the value of the slot machine. The code is almost identical to the code shown earlier. Finally, implement the get_action method that selects an action by the -greedy method. The code inside picks a random action with a probability of self.epsilon (e.g. 0.1), otherwise picks the action with the highest estimated value. By the way, np.argmax(self.Qs) gets the index of the largest element in the array self.Qs. The above is the implementation of the Agent class.

[[[00000000000000000139---337a9571613add2d6cd9823cd64ee9b9d32b97e2f80714d7e60d42ad9eefacdc]]]try to move

[[[00000000000000000140---293a58d50efe41cc4147059d7e750d342c602c8f30784e4a5c1229a25e17f87a]]]Now let's move it using the Bandit class and the Agent class. Here, let's perform 1000 actions and see how much reward you can get. Here is the code:

[[[00000000000000000141---17bafcebcd70f46cecaed5ee44cd30786f6a45aa81a533692677241c95a8e3d2]]]# import matplotlib

[[[00000000000000000142---4bebf5e1c7e3214745e27624664342fb8b39b2be5c222fd9ba7070966962f99e]]]# ①Choose an action

[[[00000000000000000143---65066248ef554c7a8e235470f0504ebdf888214517bc8f150e44764e97baa21d]]]# ② Get rewards by actually playing

[[[00000000000000000144---5eb877ec67ccfd7f4ddba96245a71e603c00f3dc1c7693c597e17bd48fd5d7e0]]]# ③ Learn from actions and rewards

[[[00000000000000000145---145af4fc57f83d8c86a57c5e287148d360cbafcc7acbd6a1650fb2ef9341f472]]]# Draw a graph (1)

[[[00000000000000000146---a143f8425c43ddff9775be7be77989dbe9d51ab12c15a09d2f6126f2ecd302fa]]]# Draw a graph (2)

[[[00000000000000000147---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000148---b4dbd62c946996db8f167deaade21fa73151ae555fb92e635b9f5ecf62fae8aa]]]First, let's focus on the inside of the for statement. There, the agent and the environment 'talk' to each other. First, the agent chooses an action in (1). Then, in (2), the environment is actually made to perform the 'action selected in (1)' and a reward is obtained. Finally, in ③, let the agent learn about the relationship between action and reward. Do this 1000 times in a row and add the sum of rewards obtained up to each step to a list called total_rewards. Then add the win rate (proportion of wins) so far to a list called rates.

[[[00000000000000000149---f1b3108c0a944f3aa1843fc59b9c3d7c23efe4d3ebf159bd127162134afb88ba]]]After running the code above, the final total reward is 859 (this result will change each time you run it). This means that out of 1000 plays, 859 of them were 'hit'. For reference, the transition of total_rewards is represented by the following graph.

[[[00000000000000000150---5e3d4b7a127d7945ac581a155fa1e0a3e6acab678dd93d903f5ba373d5f9e0e5]]]
Figure 1-12 Graph of Number of Steps and Sum of Rewards


[[[00000000000000000151---fb6c18b3c1e8d9375f4a9f817c76db1199bf35a535f614f41e9bec60dfc5ef56]]]As shown in Figure 1-12, we can see that the sum of rewards steadily increases with each step. However, the characteristics of the increase (for example, how the reward increases as the number of steps increases) are difficult to read from this graph. So let's look at the second graph. Here, the vertical axis is the winning percentage (percentage of winnings) (Figure 1-13).

[[[00000000000000000152---0de60cdbae027aa47c9ba33612c61f179b782f6d688070566e2f33b82c837c1f]]]
Figure 1-13 Graph of number of steps and winning rate


[[[00000000000000000153---45f18380925728e3827cb4fa24873f576e10dbe7eb77661d47b33d1498d4d2f1]]]Looking at Figure 1-13, the value at 100 steps is approximately 0.4. This means that the win rate at 100 steps is about 40%. As you take steps from there, you'll see your win rate improve. The win rate rises quickly in the beginning and slowly improves after 500 steps. In the end, it has reached a win rate of over 0.8. The -greedy method we implemented seems to learn correctly.

[[[00000000000000000154---1c83ece85959fffcb4b86dbb6ddf04711cecd350886cd18ce19314afbbdb9a88]]]Average Properties of Algorithms

[[[00000000000000000155---8b50c4ebf78329e632a98712c69503d402d418708d424ab7264a879f07f71d18]]]The code we just implemented changes the result each time it is run. For example, the graph in Figure 1-13 changes shape significantly each time it runs. As a test run of the same code 10 times and plotting the results in a single graph, you get Figure 1-14.

[[[00000000000000000156---0365e61d83eab56bc70cabc7a313c4a14c6f7cdebe50a7b03fda707dc3f3f926]]]
Figure 1-14 Drawing the results of 10 times together


[[[00000000000000000157---d88e5a351ab26ff802f8b9b385e5fa6fc35a55b5489fdd85f493486b79b44d3e]]]As shown in Figure 1-14, different results are obtained for each experiment. The reason the results are different each time you run the code is because of the randomness of the code. Specifically, it is the part that sets the winning percentage of the 10 slot machines. There, the odds of winning are set randomly. Also, the agent chooses actions randomly in the -greedy method. Because of such randomness, results will vary from experiment to experiment.

[[[00000000000000000158---b0649a0f4e29ee5b17b6d52a0241736aca44b96386f93e85d5f33453736f2776]]]The randomness here comes from changing the 'seed' at the code level. If you keep the seed fixed--like np.random.seed(0)--you'll get the same result every time.

[[[00000000000000000159---063c8859badb3e5d386527ddc3ab1435234ac6ebc2680b299d4af4cb058289c6]]]When comparing reinforcement learning algorithms, reporting the results of a single experiment does not make much sense due to the (often) random nature. Instead, it is more useful to evaluate the 'average goodness' of algorithms. One way to do this is to run the same experiment many times and average the results. That way you can see the average goodness of the algorithm.

[[[00000000000000000160---a2de4cece7d4291acbc390fff26e5257516918c47890839bdde472b0c6191255]]]Here we average the results of 200 previous experiments of playing the slot machine 1000 times. In your code, write:

[[[00000000000000000161---c4bca5712d725d26c158ba724a77708f37d3529e35692bd924084ddeebed1ec1]]]# an array of shape (200, 1000)

[[[00000000000000000162---039dc6d3cc177e7e0ada14c2a6911a1a7b4cf8735d9c8e77e8f79e0b529824e2]]]# 200 experiments

[[[00000000000000000163---8cac291dd5a5dc80d2251477347646304a040ab617dad24ee637c46d0fba3c1a]]]# ① Record reward results

[[[00000000000000000164---bc41b6bdc00c5813b408c8003d3d3cdf7fcb361ee8a04dc39f5cc54801def28e]]]# ② Calculate the average at each step

[[[00000000000000000165---551fbf6def7d02d0e96307ddaee805a7a29d2bfc387c6d3f186b9ce52912d633]]]# draw the graph

[[[00000000000000000166---ccb2c9efc8a95684499dc686b70cd9bf323e689f0b4c40b5a4aee792c1b5f826]]]Do the same experiment 200 times as above and store the results obtained in each experiment in all_rates. Specifically, at point ① in the code, rates (an array with 1000 elements) is stored in the corresponding location of all_rates. Then, by specifying axis=0 in ② in the code, the average at each step can be calculated. This calculation will become clearer if you look at Figure 1-15.

[[[00000000000000000167---c43b637b4d4bc16fa14e5e01ee94fc3ed05c0e30824a0dc4b6217a8408914e00]]]
Figure 1-15 Calculate the average at each step


[[[00000000000000000168---f33796c1c520b9670379dfb723e68a6a834bc5262bc158ebf58c6301a571d51f]]]After running the code above, we get the following graph.

[[[00000000000000000169---3d0f13b02b390e11826b909fdc1e6a3aab0385e553ee6d8739c56dd96f469a8f]]]
Figure 1-16 Graph of number of steps and winning rate (graph that averages the results of 200 trials)


[[[00000000000000000170---09408920bb414941988e24393ba3db5824171958721ec75bb6b32527aac8e908]]]Figure 1-16 is the average result of 200 experiments. It is important to use these averaged results when evaluating algorithms. By looking at Figure 1-16, the features of this algorithm (the -greedy method) are more stable. You start with a win rate of about 0.5, and your win rate increases rapidly with each step. It hits a plateau around step 600, and eventually ends up with a win rate of about 0.83.

[[[00000000000000000171---bb05ab87bbd25605eee1c321e409b3c3ead5ae165f5c137136e110e7cf2be7c3]]]By the way, the result above is the result of '' in the -greedy method. Changing this value will change the result. Now let's change some values of . Here we try three values of 0.01, 0.1 and 0.3. The result looks like this:

[[[00000000000000000172---9699c07d50da616dddea852655c1a0f843d609dc5f3a3f628ce0ade91b9269aa]]]
Figure 1-17 Results of changing the -greedy method


[[[00000000000000000173---82f8908c7c3225ea5b8300e3849a1f6e8eca53e534d8eb6fda2cb5dcdbcc5498]]]As shown in Figure 1-17, the winning percentage obtained varies depending on the value of . Looking at the results, in the case of , the winning rate rises immediately, but 'growth' stops when the number of steps exceeds 400. This is because we are searching 30% of the time, so there is a limit to how often we pick the best machine. In other words, you may have explored too much.

[[[00000000000000000174---0d47b6e5769a47fa23823730020eb977f82650eeea1461a3bb0a9f857cac2577]]]Next, in the case of (1% probability of searching). Here, rewards increase slowly, but the speed is the slowest result of the three. This could be because the search rate was too small and the chances of finding the best machine were small.

[[[00000000000000000175---b7f065d4e6217c7470bfe150188e0252905681a3798a4506488c920b2fb9abb6]]]Finally. As you can see, it's the best result of the three. From this, it can be said that the case of ——10% chance of exploration—has the best balance of “exploitation and exploration” among the three. In this way, you can adjust the balance between 'exploitation and exploration' depending on the value of . Of course, what's best depends on the problem. For example, 100 steps gives the best result out of the three we tried. By trying several, as we did here, you will find the optimal value. This completes the implementation of the -greedy method.

[[[00000000000000000176---57be8ca3931f897f465baab79d1b3edc4c2ea0d377e35b08df49331999b4adc4]]]Transient problem

[[[00000000000000000177---b01f66d95927b67cbf030b90c9781e75b98eb5d0b6e2f007d3eeb177a70c8d8a]]]The bandit problems we have worked on so far fall into a problem set called Stationary Problem. A stationary problem is a problem in which the probability distribution of the reward is stationary (does not change). In our bandit problem, the set win percentage (slot machine value) for the slot machine remained fixed. In other words, the nature of the slot machine is set once and does not change for how long the agent plays (for example, 1000 plays). We can see that the bandit problem explained in the previous section is a stationary problem by looking at the following code of the Bandit class.

[[[00000000000000000178---fc12a20c1b7012cd1884f4a3a08eb43f33b826a19973baac036b580187ecf611]]]# Once set, it doesn't change

[[[00000000000000000179---e9dacd4be2f1c2055a58bf724c99c8ea10ac200dbfbe74814f8c7e2a629c4030]]]As above, self.rates does not change after being initialized. This setting makes the Bandit class a stationary problem. The next problem I want to consider is that this self.rates will change slightly as you play. For example, let's look at the following code.

[[[00000000000000000180---3d3da920f24be13289145087a03481444728aabd806def75e999fbbb18cf69c2]]]# add noise

[[[00000000000000000181---dd95a8874acbf63481d61d4c5e2df5992888a86c430579d7b030373bf61e856b]]]Here we implement the NonStatBandit class. It's actually a single line of code added from the Bandit class. The added code adds a small random number to self.rates each time you play. By the way, np.random.randn() generates random numbers from a normal distribution with mean 0 and standard deviation 1. Every time you play with this, the value (win rate) of the slot machine will fluctuate. Such a problem setting is called a non-stationary problem. Next, let's consider how to solve nonstationary problems.

[[[00000000000000000182---e3d341ba301ec7035b3590cefcf10c557bc71c33512cf243d36a61abe4c63ae5]]]to solve transient problems

[[[00000000000000000183---595f9df3b6ddc5144b6a11de895b1aef2325e791953b7ce242c94229860df294]]]Start by reviewing. In the previous section we calculated sample means to estimate the value of slot machines. Expressing it mathematically, if we express the reward actually obtained by , the sample mean is expressed by the following formula.

[[[00000000000000000184---d7685d4d574c9c9e9ad2a640dbb10e6a19440974b8a1bcc2fed6d098324a9ee7]]]As in the above formula, the sample mean is determined by the mean of the rewards obtained. The point I want to pay attention to here is that all rewards are multiplied. You can think of this as a 'weight' for each reward. In that case, it can be expressed as shown in Figure 1-18.

[[[00000000000000000185---2e872da057604eea1b74e16e465caf12848e58a27b81e949e57fa311beb1fb9a]]]
Figure 1-18 Weight for Each Reward (for Sample Mean)


[[[00000000000000000186---778d8d813f5838aa8f67751e2594586d08e6291d53bbfdf92af04ecc8abf0376]]]All rewards have the same weight, as shown in Figure 1-18. This means that all rewards—whether new or long-earned—are treated equally. Of course, these weights are inappropriate for nonstationary problems. Because the environment (slot machine) changes with time in non-stationary problems, the importance of past data (rewards) should decrease with time. Conversely, the weight of the newly obtained reward should be increased.

[[[00000000000000000187---2b939b384bf043c12f930254904b3075c8c0b718f8d7964c985366e764c559dd]]]As explained in the previous section, the sample mean can be obtained efficiently. Again, it can be obtained by the following update formula.

[[[00000000000000000188---bada1cdd42559feb1741d8a9484bfca073000e8183e5a7379e6d671f696ce3b4]]]We can incrementally update the action value estimate as in equation (1.5). Now, let's get to the point. Change the step size in equation (1.5) to a fixed value of The formula is as follows.

[[[00000000000000000189---7aaefbe0451d95cadc6a2bd082cc533697984f43982533c7c0de46862403b54f]]]What will be the weight for each reward if we update the action value estimate according to equation (1.6)? Figure 1-19 shows the answer first.

[[[00000000000000000190---2b8ff5646c16254a2735d73f490877238ac068d1b27e053a8f4e1eb829d3e658]]]
Figure 1-19 Weight for each reward (updated by fixed value)


[[[00000000000000000191---697a0162072b91ac83f4b54c163e9a9fe09886d6d13256efc9ecbca7ff8dbb11]]]When updating with a fixed value, as shown in Figure 1-19, the older the reward obtained in the past, the smaller the weight of the reward. And it's not just the weights that get smaller, they decrease exponentially. An exponential decrease means, for example, that the weight of is multiplied by the weight of , and the weight of is multiplied by the weight of , and so on. The weights shown in Figure 1-19 can handle non-stationary problems. This is because we can attach more weight to newly obtained rewards.

[[[00000000000000000192---bb5f661560d50e809fbb4a91860a02ad39d8494f5723c202a64ba18cfbcf11d1]]]Why, then, is the exponential weight shown in Figure 1-19 created when updating with a fixed value as in Equation (1.6)? Here's a formula to explain why. First, expand the formula as follows:

[[[00000000000000000193---5bf42f17c940d9c381556f6d04865d13fe3b3fbc217133cfbaef5c0bfbcef956]]]As shown in formula (1.7), I modified the formula to clarify the relationship with Here, if we subtract one from equation (1.7)—substituting for——we get the following equation.

[[[00000000000000000194---62433420f2b6d290048ee5d54c6c0a1d42c15136c39f6bb12684ef50a162029d]]]Equation (1.8) expresses the relationship between Now let's substitute formula (1.8) into formula (1.7).

[[[00000000000000000195---e5f9358ab93e82d7f141e9365151b926b1c28ef14fb7279fad006f2100f48cb0]]]Looking at equation (1.9), the weights of are . Next, let's apply the procedure we have done so far to formula (1.9).

[[[00000000000000000196---4fe93c7325c918d420001088904addaf5c70b3396482bcc2dfdf517568b7f161]]]You can repeat the same operations below. If you repeat the operation done here several times, you get the following formula:

[[[00000000000000000197---fad7b7e6053d5dfe329e75f3aff3fb030e43f11b00daaef29884e11f7dbf0746]]]A weight is expressed for each reward, as shown in equation (1.10). Specifically, you can see that it is decreasing exponentially as follows.

[[[00000000000000000198---3ea16ffde2dab25f25f7abd8363e7f46d31d1f42ee724a6c0fe2e45ee8968967]]]is the weight of

[[[00000000000000000199---3ea16ffde2dab25f25f7abd8363e7f46d31d1f42ee724a6c0fe2e45ee8968967]]]is the weight of

[[[00000000000000000200---3ea16ffde2dab25f25f7abd8363e7f46d31d1f42ee724a6c0fe2e45ee8968967]]]is the weight of

[[[00000000000000000201---3ea16ffde2dab25f25f7abd8363e7f46d31d1f42ee724a6c0fe2e45ee8968967]]]is the weight of

[[[00000000000000000202---83dca2f440a3bdc0beb176f7056600ad90fa1962e9e24d3cc822f6445170c1d0]]]The calculation of formula (1.10) is also called Exponential Moving Average or Exponential Weighted Moving Average, because it is exponentially decreasing as above.

[[[00000000000000000203---fb53c045bde6d7dbe145c8a849e792f5d85f2be61a17462ca28362bbcc4d11a6]]]Note that in equation (1.10) the value of is used to find . is the initial action value, which is the value we set. So the value of will be affected by the initial value we set. In other words, the values we set will cause a bias in the learning results. The sample mean, on the other hand, does not introduce such a bias. More precisely, in the sample mean case, the user-supplied initial value 'disappears' after the first reward.

[[[00000000000000000204---5486e865870b52e89b6d4c7d798432da7493acc9c34a5bcfbf8ee279bca9f1f3]]]solve transient problems

[[[00000000000000000205---516f8bb047e60518724fc0d884df63f474f36693673c6c44842e36e94d2ac842]]]Let's actually solve the non-stationary bandit problem. All we have to do is update the estimate with a fixed value. Here we implement an agent that updates the estimated value with a fixed value alpha as the AlphaAgent class as follows.

[[[00000000000000000206---825e5d02eb3079f90b6302f3318493816a34dd9c877048f4026a80628dd5bacb]]]# updated with alpha

[[[00000000000000000207---ee7ae22e7b6c42b0d603f1766514cab7e547dfecb45ff9d9f49bd858f9b19599]]]The code is fairly simple. Just update the estimate with a step size of self.alpha. Now let's use this AlphaAgent class to play the NonStatBandit class, which is a nonstationary problem. The result should look like Figure 1-20 below.

[[[00000000000000000208---70922f277a4517aee2e727dffd3641af1a77cd76f3eed46ef80fe43874041c69]]]
Figure 1-20 Comparison of Sample Mean and Fixed Value Updates


[[[00000000000000000209---563ca3e86eeabec13c470c1719abf7e2fc93a8876121c8e55488375557304de5]]]The alpha const update in the graph above is the result of updating with a fixed value of alpha. The parameter is alpha=0.8. For comparison, the sample average is also plotted when the estimated value is updated with the sample average. Comparing the two shows that updating with a fixed value gives better results over time. On the other hand, the sample mean behaves well at first, but as time passes, there is a gap with the agent updating with a fixed value. Sample averaging does not respond well to changes over time. From this, we can see that the method of updating with fixed values is suitable for this transient problem.

[[[00000000000000000210---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000211---0c669ca4eab3ed5818f0bf883ff4e2ec8b610008b8125add8743c3bdbb656f46]]]This chapter first provided an overview of reinforcement learning. Reinforcement learning is a branch of machine learning, but there are distinct differences between it and supervised and unsupervised learning. It is the interaction between the environment and the agent. Agents receive rewards for their own actions, and aim to learn behavioral patterns that maximize the sum of rewards.

[[[00000000000000000212---d470e45ee49b2a471c0e85fcbaee90c0269fc72d92de43814219eb9e53739299]]]This chapter tackled the bandit problem. Algorithms for solving the bandit problem can be used for the problem of choosing the best option from multiple options. This chapter uses slot machines as an example, but it can be used for many other problems. For example, the bandit algorithm can be used for problems such as choosing the web design that drives sales, or choosing the most effective medicine.

[[[00000000000000000213---3d76841faf25da62b3d94e87692ebe11fd424f5580a7f0e01d2e2c6bb64c4349]]]Balancing 'exploitation and exploration' is important in bandit problems (and reinforcement learning). In this chapter, we learned the -greedy method as an algorithm to realize it. The -greedy method “uses” the experience gained so far and (occasionally) “explores” other better behaviors by trying non-greedy behaviors. By this method, we were able to solve the bandit problem efficiently. In addition to the -greedy method, various bandit algorithms have been proposed. Typical methods include the UCB (Upper Confidence Bound) algorithm [2] and the gradient bandit algorithm [3].

[[[00000000000000000214---312be26dbf46d951cae8a83d1287e7d904ac74f317169c7e6af9cde56d0604b7]]]You also learned about 'average' in this chapter. There were two averages mentioned in this chapter. One is the 'sample mean' with uniform weights. And the other is 'exponential moving average' that weights more newly obtained data. Estimates of action value can be calculated using 'sample mean' or 'exponential moving average'. Which method to use depends on the nature of the problem. Use the sample mean for stationary problems and the exponential moving average for non-stationary problems. Also, these two averages can be calculated incrementally as follows:

[[[00000000000000000215---6f7e17371cf756e90f790bdb173f312d2bf02f55ed094378ab0b41f8a5ba1069]]]As above, the sample mean and the exponential moving average update at a fixed value.

[[[00000000000000000216---f1346ad5781c9c4ee349e1ac33c0d22499e89abbeffaf60a56caff66fe5e8c77]]]Foreword

[[[00000000000000000217---edaf59d7743531fa92232c963580e135f70a36158f1ca8f13c1b69ddf82cd32b]]]Some time has passed since deep learning came into the world. Deep learning is now being used in a variety of places with great results. There are too many success stories to date. Among them, a number of surprising results have been achieved in a field called deep reinforcement learning. For example:

[[[00000000000000000218---50aae2083157b62441dd84f0bd065a43edeaa65e0168d1d182e5d55f4ba70ec7]]]play video games better than others

[[[00000000000000000219---09c48202e19bf78aaf50a52be2bf6b44f9b5ad1c95d9c90b5cbbc7158b478aa9]]]beat the Go world champion

[[[00000000000000000220---fdb38d335246445a9cec67ddd8d32f5be9b26bdb1fd81c6a1bf946ebbd697d87]]]beat the world champion in esports

[[[00000000000000000221---96405828c3c9c6e7cc34572107401aa88770081755b5ddc7cb483ca288701352]]]Such success stories are the first to come from the combination of reinforcement learning and deep learning. More recently, we've seen success stories in the real world (and not just in virtual worlds like games). For example, great achievements have been reported in the industrial world, such as the operation of robots and the design of semiconductor chips.

[[[00000000000000000222---26168129f636a4cb824b6895089f34d5af74ac50a91934c2c55c811e2d574bf5]]]Reinforcement learning is also seen by many researchers as a key technology to achieve “general artificial intelligence,” or at least to get closer to it. AGI is an intelligence that can handle various problems flexibly like us humans, and aims to solve various problems with one system. Reinforcement learning and deep learning will be important technologies in realizing artificial general intelligence.

[[[00000000000000000223---85e041214b904f0866e7cb1ddf4c3ba628cb4ac047cc304e35a03b91994b3b8c]]]In this way, reinforcement learning and deep learning have great potential. This book is the fourth in the series 'Deep Learning from Scratch'. The theme is reinforcement learning. Starting from the basics of reinforcement learning, you will learn all the way through to cutting-edge technology that uses deep learning. When reading this book, if you have basic knowledge of Python and mathematics, you can start from this book without needing knowledge of the previous work.

[[[00000000000000000224---d2004be5323098a9f156f4ac1af1be45f6918e3fbfbdcefc3736282f1379c237]]]Concept of this book

[[[00000000000000000225---d3c8444de3afa053e52cdf7677b8138c14c859a67140d8481187983cdc8bcb17]]]The field where reinforcement learning and deep learning are combined is called “deep reinforcement learning”. Deep reinforcement learning is a very hot field. These days, new algorithms and applications are announced almost daily. In this way, the field of deep reinforcement learning is developing at an accelerating pace. On the other hand, the ideas and techniques underlying reinforcement learning have not changed since ancient times. Even state-of-the-art algorithms are based on old ideas. Therefore, in order to understand modern deep reinforcement learning, it is a shortcut to learn the basics of reinforcement learning that have been cultivated over time.

[[[00000000000000000226---a854d6ef3a33728eace8deec477da23d70b787bd25e43c314a3826ab6adfc997]]]For example, there is a well-known deep reinforcement learning algorithm called DQN (short for Deep Q-Network) (DQN became famous after successfully playing the video game 'Atari 2600'). This DQN is based on an old theory of reinforcement learning called Q-learning. The theory of Q-learning is just a few lines of math. However, memorizing the formulas does not bring us closer to the essence of reinforcement learning (and just memorizing them is not fun). Starting with the basics of reinforcement learning, this book presents key ideas and techniques in a 'connected' story. By accumulating knowledge in this way and connecting it organically, we get closer to the essence of reinforcement learning. In the process, you should be able to naturally understand the formulas of Q-learning.

[[[00000000000000000227---25c4743ccbfc6acd073f8c078045d55018285f9ab18f8c41e6413e7153bc0420]]]The feature of this book is that it is 'made from scratch' as the title says. We create reinforcement learning algorithms from scratch without relying on external libraries that act as black boxes. The code should be as simple as possible and implemented in such a way that the key ideas in reinforcement learning are highlighted. In addition, the problems tackled in reinforcement learning are structured so that various problems can be touched while raising the level step by step. By doing so, I think you will be able to experience the difficulty and fun of reinforcement learning.

[[[00000000000000000228---a9e50c9151f41508a7991caf3f897981051f4dc87c9861430937cb69af10af89]]]overall flow

[[[00000000000000000229---b42254430fa5c7105b2d00a53d3dcab7412c5ae0504beab40b416f2400909396]]]This book is divided into two parts. In the first half of this book (Chapters 1-6), you will learn the basics of reinforcement learning. Deep learning will not appear there, and you will learn important ideas and techniques that have been cultivated in reinforcement learning one by one. In the second half (Chapters 7-10), we will learn how to apply deep learning to reinforcement learning problems. And we will unravel the state-of-the-art deep reinforcement learning algorithms.

[[[00000000000000000230---bf675b11d7c1a882ec903bdf41d2bc7ba84c79816f9c30ed2a5504f30a46d1b2]]]Deep learning appears in Chapter 7 of this book. It will take some time to get to that chapter. However, if you understand the basics of reinforcement learning, applying a tool called deep learning is not that difficult. Conversely, if you omit the basics of reinforcement learning and just move on to deep reinforcement learning, you will only have a superficial understanding.

[[[00000000000000000231---25d0a1c87bd59fd1d54000fe21d0629737b83e90d62304311ed65e03d13971cd]]]I will briefly explain the flow of each chapter. First, in Chapter 1, we will solve the bandit problem, a problem in which the best one is sequentially searched from multiple candidates. The bandit problem is the simplest problem in reinforcement learning and is perfect for beginners. Next, in Chapter 2, general reinforcement learning problems are defined in a framework called 'Markov decision process'. In Chapter 3, we derive the 'Bellman equation', which is the key to finding the optimal answer in the Markov decision process. You will then learn various methods for solving the Bellman equation: dynamic programming (Chapter 4), Monte Carlo (Chapter 5), and TD (Chapter 6). Now that you have a better understanding of the basics of reinforcement learning, it will be easier to move on.

[[[00000000000000000232---ce048fed8ea0f420b0ee5a79368187ba701e9fdbd6ccc5b79cd4b8cfbf202299]]]In Chapter 7, we will learn about deep learning. Then learn how to apply deep learning to reinforcement learning algorithms. Chapter 8 implements DQN. We will also look at a technique that extends DQN. In Chapter 9, we will look at an algorithm called 'policy gradient method' as a different approach from DQN (policy gradient method is a general term for algorithms, and in fact, algorithms called REINFORCE and Actor-Critic are implemented. increase). The final ten chapters provide an overview of modern deep reinforcement learning algorithms, specifically A3C, DDPG, TRPO, Rainbow, and others. We also introduce case studies of deep reinforcement learning and discuss the possibilities and problems of deep reinforcement learning.

[[[00000000000000000233---0b2fc3b0c68e4b1e3dc1259a7ff3aec346cfd967d69e45cd6f856f5379615779]]]Required software

[[[00000000000000000234---ace444718be9030bba56299690d4337b9b590b44b1491aa6f5a52a4d580926d7]]]The Python versions and external libraries used in this book are:

[[[00000000000000000235---d52e1eac81f63d8c293342d29c36363a1999314e1ba7b05bf8285f2c98f0095c]]]Python 3 series

[[[00000000000000000236---5db77bcea0275e12a708a3e737b1b99188a6f3e2119f4fde303632e5a58c9ac9]]]DeZero (or PyTorch)

[[[00000000000000000237---a39235326eeec4967a8a1054c86f9960bb3482bce473386dee3e7a52132efa8a]]]In this book, we use DeZero as a framework for deep learning. DeZero is a framework created in the third installment of the 'Deep Learning from Scratch' series. DeZero is a simple framework, so you will quickly understand how to use it. Chapter 7 explains how to use DeZero. Also, although DeZero is used in the explanations in this book, you can read this book assuming other frameworks (PyTorch, TensorFlow, etc.).

[[[00000000000000000238---daedca8554ea20f12375086baf670173153558c6217c19bbabd8b75fe99a5de2]]]DeZero and PyTorch have much in common with their APIs. So porting DeZero's code to the PyTorch version of the code is easy. The GitHub repository for this book (see below) also provides the code for the PyTorch version.

[[[00000000000000000239---14eede86655386ad01cbbe4ebeacf64c4b72cdec2651decd44560b2bf2927d3a]]]OpenAI Gym is a simulator environment for reinforcement learning. Chapter 8 explains how to install and use it.

[[[00000000000000000240---d12a19b19659b3ff9208f54a97d99f893c8828a1ffb345747152286c5ce22b79]]]file organization

[[[00000000000000000241---233ae953a99efdfbc807581cdab7f2f8245d0c8ce1f92f1b5cb1bfc1e6817a73]]]The code used in this book is available from the following GitHub repositories:

[[[00000000000000000242---b6815672e3d60149d427f1980237a46b1a9962e18d0e86579b5d36f54e05e64c]]]The folder structure of this repository is shown in Table 1.

[[[00000000000000000243---6eab75ce0e56024bfc09cd15f853694096d7d388eb1b9f1ba8001e07fda237cf]]]Table 1: Folder structure

[[[00000000000000000244---a073ec543ffd1f48dfad06dd3586661b74bac7346e5d7dc97b425b6357fda716]]]Folder name

[[[00000000000000000245---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000000246---dc3120c0eb97dce1a15b440b1a6b47cb73f4225da45a4e4fe28bcd3df1a1d1de]]]Source code used in Chapter 1

[[[00000000000000000247---fe532d24d7fd2648ff579ebca1dc1c0a80547c03e599cfa41bf8a18ee73ba46c]]]Source code used in Chapter 9

[[[00000000000000000248---dbb7374b51fca6c49caa9c91fbe56b6fecc374a2457168b4940115d899d39718]]]Commonly used source code

[[[00000000000000000249---f0f1566a1b1a3390f3375084c117ab9700ca21cecd2878e02ac273f2ff51c423]]]Source code ported to PyTorch

[[[00000000000000000250---396717a9735d8a19e1852f83a349d487b4e9b0e057e09071c6757565790ea216]]]Folders ch01 to ch09 contain files used in each chapter of this book. To run those files, run the Python command as follows (you can run the Python command from any directory):

[[[00000000000000000251---f97f28c283d9d8f3188f53fa986cbbfa8687b9ef914d06e4391ba4b137357b83]]]things that change, things that don't change

[[[00000000000000000252---6ffde6b2ff947202624c35f8c7937d3b7d54a7beedb8eb189e8210c70bfea109]]]AI is currently developing at a tremendous pace. Technologies and applications related to AI are changing rapidly, and new booms are appearing one after another. However, many of them are destined for obsolescence. On the other hand, there are things that do not change, things that are handed down. What you learn in this book is 'things that don't change'. The basic principles of reinforcement learning, the Markov decision process, the Bellman equation, Q-learning, neural networks... I believe they will continue to be important in the future. Through this book, I hope that you will learn the basics of the field of reinforcement learning firmly and enjoy the beauty of 'things that do not change'.

[[[00000000000000000253---476e512d3f22afdc336217483f037bcbe4fbaa5ef4a6c56f14f7f7cfcb4e5454]]]Notational rules

[[[00000000000000000254---30e569f8abc53f20a07c0598be9b8623658b8c37b5b37b40a9a69d7e6a6ba030]]]This document follows the typographical conventions described below.

[[[00000000000000000255---eaa3a1768a21aebf4c749f1933cd17b82875c8382de3e31f2e796a64729c15bd]]]bold

[[[00000000000000000256---7627be7c0fffeeba7455369bffe7e56c44fd561f79bcdca0ab4b03e003e61053]]]Represents new terms, emphasis and keyword phrases.

[[[00000000000000000257---0146b007473307a94b91a89c8c2f6849565f6e675e0fe665cf774251e7b94f75]]]Equal width

[[[00000000000000000258---7dbab4d12d7482897a8df2c32d5551ea07f137050f51d4382e74138b8cad3d67]]]Program code, commands, arrays, elements, statements, options, switches, variables, attributes, keys, functions, types, classes, namespaces, methods, modules, properties, parameters, values, objects, events, event handlers, XML tags , HTML tags, macros, file contents, and output from commands. It is also used when referring to that fragment (variables, functions, keywords, etc.) from within the text.

[[[00000000000000000259---576a3c3df75ae76aa331767cd3e6346541668bccbf1fb67114cd74b02d4fd08f]]]monospaced bold

[[[00000000000000000260---62dd72324e8d20df5c8e1595f1ca9e41b545051c421effdec598651c0ddcf0da]]]Represents commands or text that you enter. It is also used for code highlighting.

[[[00000000000000000261---a8ad620408c5d0124855d9313f70c2d9497e8efa6f4436fd3d2936a6a0d84790]]]monospace italic

[[[00000000000000000262---8cad4f36ef7f5e8b9faa3430e931adcfdf8019438c5932cd91e96f68af13c31a]]]Represents a character string that must be replaced according to the user's environment.

[[[00000000000000000263---52d33f0a012b4029eb0de3ecfd6655a60cc14548f486039f88d08f1ff193a592]]]Indicates a hint, suggestion, or additional information about something interesting.

[[[00000000000000000264---852388668b84435a5944b94c94f6e4912d1b0a0f468808655f1e125ec1cf75d3]]]Represents cautions or warnings, such as library bugs or common problems.

[[[00000000000000000265---3f4ba0664b5911aa7e85d2a0bed8e833b6c0f07636c0b05d7f28a3c2718ff197]]]opinions and questions

[[[00000000000000000266---25800ed37c8613ea755e1eaeb3f6b2a5c3614cd58d8f632f35faf9ae8cfe5017]]]We have made every effort to verify and confirm the contents of this document, but you may notice errors, inaccuracies, misleading or confusing expressions, and simple typographical errors. If so, please let us know so we can improve it in future editions. Suggestions for future revisions are also welcome. Contact information is as follows:

[[[00000000000000000267---5120b30e23f0646f3a418cbb6d18df0283e91b171be2b9c3a910b3aff3d0337e]]]O'Reilly Japan Co., Ltd.

[[[00000000000000000268---ec8d2613e28e64bb50245bc847105532b2de636be27692fcc83ae4d8ad1114b0]]]E-mail japan@oreilly.co.jp

[[[00000000000000000269---27022ad01a7c677d2c3b78e89be1b554229f1fdbcb01807b95d083bd5405e0bb]]]The web page for this document can be accessed at the following address:

[[[00000000000000000270---c7fc1eac054daea71feb57312103dfc70aa95e8c774c6e5e2cc1baa8f0273cc5]]]For additional information about O'Reilly, please visit the O'Reilly website at:

[[[00000000000000000271---a6ea4185ec562859a24bf754fb86337fad51ed222ecba568f153de6c2a332da1]]]https://www.oreilly.com/ (English)

[[[00000000000000000272---da14299534ba28d101cb9da012463647af73411d71f4a60c134d52f8f1fa9369]]]Appendix C

[[[00000000000000000273---be74e3ec7e479b5f3dafc737b1a99a3f991ea980b5f37f549d8c850ba102be7f]]]Understanding Double DQN

[[[00000000000000000274---d74e4299a5c778520d1948d58fcc7c0322394ba2a4d82bd3db83984158fcdd12]]]In '8.4.1 Double DQN', I pointed out that there is a problem with the TD target of DQN (there is room for improvement). Specifically, it is a problem that the calculation of '', which is the TD target, is 'overestimated'. Here I will explain what it means to be 'overrated' and how Double DQN can improve it (this explanation is based on the blog [51]).

[[[00000000000000000275---0b3825dbcca26293e25688a2372b8fa458dd3df75dad2a4b7171b2b380289d87]]]What is overestimation

[[[00000000000000000276---3c86e3fae1c18465722bbb530edc4586761048ea097e6ea3d7526afc976b3bdf]]]Suppose there are 4 tasks with possible actions that can be taken here, and all the values of the Q function in the state are the same. That is. In this case the following formula holds:

[[[00000000000000000277---69eef402f7a1c6133118e4471b9024389a8426386af8b2e03b238c2ecb797a7d]]]As above, the value of the Q function is all 0, so the calculation of the operator in the expected value is also 0.

[[[00000000000000000278---9c5ab8ddfb26d978625af197ea4852d22de28900e44810d285bcf76a15c375dd]]]Next, consider the case of using the Q function being estimated. Here, we denote the Q function we are estimating by , and assume that its values contain random numbers generated from a normal distribution as errors. in this case,

[[[00000000000000000279---913fd5135dc49cd0b86dddead461c7cd15036c6f89a748c80db692163592e0e1]]]is established. In other words, it is evaluated higher than the true value (0). This is what is meant by overestimation.

[[[00000000000000000280---8c9d79c8a30f6fc076c6cd8c3b8fd749cf388072fdd1a9d609a4023f2ba7ab30]]]Now let's see this phenomenon of overestimation in real code.

[[[00000000000000000281---f8fc2bd14cb319d67cc2fa970d2de90b3911739a1dd98111c868cb045afd8688]]]
Figure C-1 Data distribution of Q.max() (red line is true expected value, light blue line is expected value obtained using Q)


[[[00000000000000000282---81b07a454445c855a90ec5355e3c4912bb523fc57e7ca877a92a3f1c833e110b]]]The Q function has a true value of 0. Since this is an estimated value, random numbers generated from a normal distribution (mean 0, standard deviation 1) are added to Q as noise. Here, Q.max() selects the maximum value 1000 times and plots the distribution as a histogram. As shown in Figure C-1, we can see that it deviates to the right from the true value (0)—that it is overestimated.

[[[00000000000000000283---c0e1fd133ebd651f5e28b16b68595178dd287ff75aa61a92c94135d16449f991]]]Solution for overestimation

[[[00000000000000000284---96b48508916cccd5d81574f46587c5e3418c10278b145c2a130962d8c0489c54]]]Next, I will explain 'Double DQN' that prevents overestimation. As the word Double suggests, we do this by using two Q functions. First, let me show you the code:

[[[00000000000000000285---7186c219cb00b42fcd13f232a92de9acb6a2df0ea65b4931d79dcdddd485b894]]]# another Q function

[[[00000000000000000286---afc4240c40a94c57ab5ff2c9841ac09b88d0817ddb3446af24be8bb1b1d90ae3]]]
Figure C-2 Data distribution for Double DQN (red line is true expected value, light blue line is expected value obtained using Q and Q_prime)


[[[00000000000000000287---2020ff842de3e5454bd31b0b1d3b74f52da2911e880c1445ccdc47b372048bcb]]]Looking at Figure C-2, the histogram is centered around 0. The red and light blue lines in Figure C-2 overlap, indicating that overestimation has been eliminated. The code differs from the previous one in that it uses two Q functions. Both Q functions are estimates and contain errors. However, the errors are independent of each other. Here, two Q functions are used for calculating the max of the Q function. in particular,

[[[00000000000000000288---67669ddb9e953c415f2a1530c3031c2a25eea266db81963d518b14201f5a894e]]]Use Q when choosing the max action,

[[[00000000000000000289---577df4dc0bb6bc3708d17908b8927fa18341fbab8b0082135dc48b350d0626b1]]]Get the value for that chosen action from another Q function, Q_prime

[[[00000000000000000290---7b451048980ffdd768b755d629831ceac264a6eb4369407d494671d874627878]]]That's the flow. In that way, overestimation can be prevented by performing 'action selection' and 'value acquisition' with different Q functions. The above is an explanation of the overestimation problem in DQN and how to solve it with Double DQN.

[[[00000000000000000291---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O'Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000000292---cf76f02501ccb0f5061f30864bd3ed4d6065bcf38520d121593a39a47b53d245]]]Deep Learning from scratch ❹

[[[00000000000000000293---7c05b957c25fe05f05721a109656bcae8205d1146d50e1049584e909e746010d]]]Reinforcement learning

[[[00000000000000000294---4172a4add4eccc9fc9864066b28190a9d86de67f02006a351f84325c8817850c]]]April 4, 2022 First edition 1st printing

[[[00000000000000000295---f2f5b6f470310eaf5a90b614a718214b9b06a6adc028924fc6a0d8ba94857806]]]Author Yasutake Saito

[[[00000000000000000296---16f1fe5d091f36817279a9042f8e7f2e6ad26cd5721935ce9cf0857d2ba4fffb]]]Published by Tim O'Reilly

[[[00000000000000000297---e892f0b49ac7c195817b2cb6f935741d2a6d67d4e082b402c808644ac63fb9ef]]]Publisher　　　

[[[00000000000000000298---5120b30e23f0646f3a418cbb6d18df0283e91b171be2b9c3a910b3aff3d0337e]]]O'Reilly Japan Co., Ltd.

[[[00000000000000000299---173f76b1e0e52042ce8770cd7df500066657395900d568d00501179717b0fdf9]]]
　　　　　　12-22 Yotsuyazaka-cho, Shinjuku-ku, Tokyo 160-0002

[[[00000000000000000300---e670ad760848546aa0571bd75410cabcb7eaedab59ad888830f53973b3d582cc]]]
　　　　　　e-mail 

[[[00000000000000000301---f6191178ed3d59d9934e432efc352d4e7d09dc7cdbff312412f9a051ce5b5012]]]production cooperation　　

[[[00000000000000000302---525b80b0d33a191efc087976b152945e86bb5fa5894b81a40506dc24e8ae8f94]]]Top Studio Co., Ltd.

[[[00000000000000000303---edc3efb149a25b9613ce36ac49d50fa08fbda9e7b7120a27b83d2c9a69b24a3f]]]References

[[[00000000000000000304---3d14f5512f5bbab2f00b3ac21c7fd3c0aac7887c5e96dcca0409bb781ca3e1ac]]]Chapter 1 The Bandit Problem

[[[00000000000000000305---c52e64ee6e11e419ba7b29940efe54643d674208d21681201cbe91bc3d9c0d8b]]]Chapter 2 Markov Decision Process

[[[00000000000000000306---0f98846c73eb57f54fc3690303df00a3b217b63f624f11e2fcf78aa20877f89c]]] Csaba Szepesvari 'Rapid Reinforcement Learning - Basic Theory and Algorithms' (Kyoritsu Shuppan)


[[[00000000000000000307---4873c5ca8f8e181c824b5ddf5087b7c256d7cb3c0e527441ea27dd795f71c479]]]Chapter 4 Dynamic Programming

[[[00000000000000000308---f55fa46fe00a1b942322d8674b15b9e79f258a6422bfdb31d33f1effc233e9b6]]]Chapter 7 Neural Networks and Q-learning

[[[00000000000000000309---8d6930428cda78e8d63c87e73c288869d5963ba340c83db04c207506994cdb8b]]]Chapter 8 DQNs

[[[00000000000000000310---e12ea515f034b39fcf64f47baba88e3cd18708112fd3526bcad919b8d96ccc13]]]Chapter 9 Policy Gradient Method

[[[00000000000000000311---055fb59ee76e4b3ec0324f895fb462e5dd15d668c1a1a4e99288b48467542bab]]]Chapter 10 Further

[[[00000000000000000312---5a702dfe63cc8cce9e85a68d88cac62404a45e1d837e81edd10a18d38d73e540]]]appendix

[[[00000000000000000313---8518c8d366ef389cdc1a4e916b4b423b643d3eb01ab505a11219fb1376e01a30]]] horomary 'Evolutionary history of DQN ②Double-DQN, Dueling-network, Noisy-network'

[[[00000000000000000314---8f61b6bb90e9b1bac938f68867a8ad69b1b90e36b77931f7b26dbdabed771767]]]Chapter 3

[[[00000000000000000315---071f95745c0af1b5993a2c06687bf87302e52facb0b2eab8789ed73f6f881182]]]Bellman equation

[[[00000000000000000316---217db1881e1fc7ad7f4a5d12dc1631dd8d4ee52f44dbf977e818e72ce15b4191]]]In the previous chapter, we tackled the problem of 'a two-square grid world'. In that problem, we assumed that the environment is deterministic and that the agent also behaves deterministically. Therefore, the backup diagram extends in a straight line as shown on the left side of Figure 3-1.

[[[00000000000000000317---a36da6ef0a18dd57d20f8427219a666f2e737a26cf3b5dbf8c7c1e7e503ec70c]]]
Figure 3-1 Deterministic backup diagram (left) and probabilistic backup diagram (right)


[[[00000000000000000318---053f850e986c71e4516809e2a33277a53fa500d13d947ddec3e004690f29f5c7]]]As shown in the left diagram of Fig. 3-1, if the backup diagram extends along a single straight line, the state value function could be obtained by the method in the previous chapter (calculation using formulas). However, MDP may also behave probabilistically. In that case, the backup diagram will spread out as shown on the right side of Figure 3-1. Unfortunately, such a state-value function cannot be obtained by hand calculation in the previous chapter.

[[[00000000000000000319---acaad55a4ac970bfc14b66927fb6d3a9a392f75b94c901b208383d683ef332e6]]]In this chapter, we aim to find the state value function even in the situation shown in the right figure of Figure 3-1. The key to this is the Bellman Equation. The Bellman equation is the most important equation in MDP and provides an important basis for many reinforcement learning algorithms.

[[[00000000000000000320---245e59d011fa797ad4de3d3ae2673dec23210a1aac0d85fb2a16d0aedd19aa23]]]Derivation of the Bellman equation

[[[00000000000000000321---ba0e896d903a280de24ec8315bba53f5c90d05a90ca88cbf867b5667ae104fa3]]]Here we derive the Bellman equation. First of all, as a preliminary preparation, we will review probability and expected value while looking at a simple example. We then proceed to the derivation of the Bellman equation. If you are already confident about probability and expected value, you can skip '3.1.1 Probability and expected value (preparation for Bellman equation)' and proceed to '3.1.2 Derivation of Bellman equation'.

[[[00000000000000000322---ddb8dd61ceaa9c5ae10d9f688e3e3f7721e05b0b29828a0859d5e87a3fcc4d19]]]Probability and expected value (preliminary preparation for the Bellman equation)

[[[00000000000000000323---cb7b58c80772b2e45e6fe950af7aad39f6ed9d2be34ac8692544387ce4b7ebea]]]Here, we will use dice as an example. The dice consider an ideal hexahedron in which the odds of each outcome are equal. In mathematics, if we represent the number of dice as a random variable, is an integer from 1 to 6. And the probability is here

[[[00000000000000000324---07f16090ec515b1895d10ec9b1f6d479b83598d56ea3c452cc512a3f4221c6f2]]], to represent the probability of the outcome of the dice. Now, let's calculate the expected value of the number that comes out when rolling the dice. This can be calculated as

[[[00000000000000000325---3fe734e63f3841a9034fb15cf6544dedad87ed3b0d02aebd8a1a2bc671f9d829]]]As above, multiply each 'value of eye' and 'probability' and sum them up for all cases. By the way, when written using , the expected value is expressed by the following formula:

[[[00000000000000000326---5769d48ef45b2df855113289b5523e86bdafe08b012e1122dabd2406b95b3b23]]]Also, if you draw the dice trial this time with a backup diagram, it will be as follows.

[[[00000000000000000327---0a4a94d5c8ce1400db7375ccdaa309298e5dddff7631de399f4dd0d42113a3e0]]]
Figure 3-2 Dice backup diagram


[[[00000000000000000328---a8f213eb672c96c00929b5ef1ebbfd31261131a1abc8a2ea6cb0bf688098cd28]]]As shown in Figure 3-2, draw a diagram in which the dice's number (potential number) extends downward.

[[[00000000000000000329---9d8a9a7be764a55e0a42e01ef29ef2894eaea278b0ab1810797f6ce79dc70726]]]Next, consider the problem shown in Figure 3-3.

[[[00000000000000000330---d285f9fb24b77cc2e711a0833002c6b6b37e1c25ed088733325878b3083f5c18]]]
Figure 3-3 Problem of tossing dice and coins in order


[[[00000000000000000331---7956219cb6c5dfebe08eb517dc20886ac03a4665fe932f7caf52cf65089dde38]]]The problem this time is to try tossing the dice first, followed by a coin toss. If you roll the first dice and get an even number, you will be given a coin with a good heads—a coin with a probability of 0.8 heads. of coins—— shall be given. Next, if you toss the given coin and get heads, you will receive the dice roll as a reward. If it is tails, the reward will be 0. for example

[[[00000000000000000332---d99c399cb6a1b230f30285f7c73cee2828464f9b580cc11226a708e298f1b086]]]If the die roll is 4 and the coin (coin that tends to come up heads) is headed, the reward is 4

[[[00000000000000000333---73946ed584589bcf824fc51a550895570828c11ca43ae1ce92ce87f4ed091b2e]]]If the die roll is 5 and the coin (regular coin) is tails, the reward is 0

[[[00000000000000000334---60582c0d825863d0b36ab731b649c364c3f28b8ff88324e802f3024969941de9]]]It looks like.

[[[00000000000000000335---172d455816ef0b643593a5429efaaae77f6fc516489e970d88b19938503585a2]]]Now, let's find the 'expected value of reward' for this problem. First, let's draw a backup diagram, and it will look like this:

[[[00000000000000000336---d64b457de497b0e322eff244393ff8aa21c325d929e6103ca38d639ce50f20da]]]
Figure 3-4 Back-up diagram for a dice and coin toss problem


[[[00000000000000000337---74daa0619fa35bc643ace12698865ce479d92f06abec3e86e875eaa32a71aa6f]]]From Figure 3-4, we can see, for example, that the probability of rolling a 1 on a dice is , followed by the probability of a coin turning heads. And then the reward will be. in short

[[[00000000000000000338---77684980b7dc8eecb1531aac4d9e113bff9ad9f14c29d61aa6d24cc3789dbe7d]]]with a probability of

[[[00000000000000000339---835b7401e2a3337f4c240eb13223365da8467ae18b0a44844b63ef789f94db65]]]get rewarded

[[[00000000000000000340---67d94fdfc421caf0c79ed6006702ed8518bfc8e9e5a36bcfa8f4468537339cc7]]]It will be. Do this for all cases to find the 'expected value of reward'. The actual calculation is as follows.

[[[00000000000000000341---859c734c891939497a37803b54cd9f3c1323531de855609dbdf46e56f8ff9eb1]]]Now we have the expected value of the reward. In this way, the calculation of multiplying the probability of occurrence of the terminal node (circle symbol) in Fig. 3-4 by the reward at that time is performed for all candidates and summed up. This will give you the expected reward value.

[[[00000000000000000342---a3b96dcdebba0c94ecadc311899f9f62622013d76c47ded4dbe32171dbd6a521]]]Next, let's express the calculation performed here in a character expression. First, let's take the result of the coin (heads and tails) as the roll of the dice. In this trial, the probability of the coin coming up heads changes depending on the dice roll. This is expressed as a conditional probability. in particular

[[[00000000000000000343---7bc6bfe78af05687bb8064b16808670a2c1ae2045f32f0a103b5e08a515fbcfe]]]takes a value like Also, the probability that and occur at the same time (this is called the 'joint probability') is

[[[00000000000000000344---ed551a0f0732edc3bec983a373f519c7feb3d783741f8f50bfb82d15cb970f96]]]becomes.

[[[00000000000000000345---fa74feda82ccd734026a450531e245b353c2d82f9e8fea4328a6266426682d9c]]]As for the reward, in this problem, the reward you get is determined by the value of . So the reward can be expressed as a function. for example,

[[[00000000000000000346---105351392f209d6a889648e47615e769d8b33c33fd8916a166da2c7df71732a6]]]It looks like. The expected value is the sum of the value and the probability of that value occurring. Now the expected value of the reward can be expressed as:

[[[00000000000000000347---009789126cc17266868b4988996aeab7024ccb875f7e5e853bf7035425ec83eb]]]The form of this formula also appears in the following Bellman equation in the same way. This completes the preparation of the Bellman equation. If you understand the above, you will understand the derivation of the Bellman equation as well.

[[[00000000000000000348---245e59d011fa797ad4de3d3ae2673dec23210a1aac0d85fb2a16d0aedd19aa23]]]Derivation of the Bellman equation

[[[00000000000000000349---6d86fac3465f566496ff129d21a2de4940ba3debf9f4bf9fd749c63997ab2aab]]]We now proceed to the derivation of the Bellman equation. First of all, as a review, we defined 'return' as follows.

[[[00000000000000000350---5bfee7ac15fa3dba628406ae0f6212cc26ae67404df4a37f3bad3e0ef07ba4a1]]]Here we consider a continuous task and assume an infinite reward. Earnings are the sum of rewards obtained after the time. However, future rewards will decrease exponentially due to the discount rate. Here, in order to investigate the structure of formula (3.1), first try substituting for formula (3.1).

[[[00000000000000000351---b26d714c6d6cd968552b8aa3bbd18b0bef5572fb69d835e5b0c92727e69d74ab]]]Equation (3.2) is the sum of rewards obtained after the time. With this in mind, we transform equation (3.1) to

[[[00000000000000000352---631d87536f2fad3cc02ec1e3e34550dd88ee9289e51054d265e2b28d19ba2f26]]]As shown in the above formula, the relationship with earnings was derived. This relationship is used in many theories and algorithms of reinforcement learning.

[[[00000000000000000353---e6c9cac39bfa8ad30fbebf8b3949c8f4635bbdc16ac375de91489629862d7392]]]Next, we substitute equation (3.3) into the definition of the state value function. The state value function is the expected return and is defined in the formula as:

[[[00000000000000000354---62d61dcb38a5c1b4b08fe9b500c2a64a5669c657217112a99081b34b8665e6df]]]As in equation (3.4), the state value function is expressed as Substituting equation (3.3) into this equation yields the following.

[[[00000000000000000355---ee23918133508559dd88ec65a736a97e12c8b7ba7112141947b09bb98c85ca75]]]The final expansion rests on the 'linearity' of the expected value. Specifically, if it is a random variable, then the following holds.

[[[00000000000000000356---126f5e82643cb4bd2453726a3232d95f6144b3c6ce4f51140fefc7e1bc0d656c]]]Here, we will proceed with the agent's policy as a probabilistic policy. This is because probabilistic policies can also represent (cover) deterministic policies. Similarly, we assume that the state transitions of the environment are also probabilistic—in the formula.

[[[00000000000000000357---51bea5442d0aa7bd85d3cb658c8fdb801dbe4c11507e76fbf7931211eed4f190]]]Now, let's find the terms of equation (3.5) one by one. First. To calculate this term, consider the following figure.

[[[00000000000000000358---5ce69a3caa2c44fff0e756fc1f4e0494fab6ab8b1a78809fa006ab1a0220dd28]]]
Figure 3-5 Relationship between states and actions


[[[00000000000000000359---86ee8d13135bdaf66ac6c572efbc35eeccf01ff0fb83595d736914537d91b8e7]]]First, let's check our situation. Now the state is , and the agent acts according to the policy. For example, if there are three possible actions,

[[[00000000000000000360---4c4032a6e2c2e5b0823f7c385639213f9b33f51571cfee0fa5538d389473e733]]]Suppose it takes a value like The agent chooses actions according to this probability distribution. Then it transitions to a new state according to the state transition probability. For example, if you perform an action and there are two transition destination candidates,

[[[00000000000000000361---7db9b75647d67f803296defe5d1ed0e29f8a9d9be329192ad9f8ed78ca7ed02e]]]takes a value like And finally, the reward is determined by the function This is our situation.

[[[00000000000000000362---f344eb045bab6df950bd5a0323b2667c5fd4102695258bcd990d17d31d82c357]]]Let's take a concrete example and do the math. For example, suppose an agent chooses an action with probability and transitions to with probability. In that case,

[[[00000000000000000363---77684980b7dc8eecb1531aac4d9e113bff9ad9f14c29d61aa6d24cc3789dbe7d]]]with a probability of

[[[00000000000000000364---0fcfe8c0a4305d853ef02f77b7e0f97a121861c69401b20a28a458ee645d58c5]]]be rewarded with

[[[00000000000000000365---9a756a1185a3b43c120b0c30db92c684447e7f1ba4a6b96148cf37baa98f3a55]]]That's what it means. To find the expected value, do this calculation for all the candidates and sum them up. This can be expressed as a formula as follows.

[[[00000000000000000366---180ef63a474d8cebb84b4466c97e7d23f2f6ce78a2fcc3503b163b0b6a3a4a5c]]]Multiply the 'probability of the agent's action', the 'probability of the transition destination state', and the 'reward function' as above. Do that calculation for all the candidates and find their sum. This has the same structure as the formula shown in the 'Dice and Coins' example in the previous section.

[[[00000000000000000367---fd7a1429878a010cc9da7400f4145180966bb5d30ee0fe9638130cee79e8bc4f]]]The notation will be collectively written from this point onward in this book.

[[[00000000000000000368---756e4c170e7f3b74adb8a9107509e67a8aaa5857992a82305fda77ad5a870c23]]]This completes the first step of formula (3.5) (Figure 3-6).

[[[00000000000000000369---61fcbdb86157cd375c106a5f6964334302ee830646b7a69cce4787456d775edf]]]
Figure 3-6 Expression during expansion


[[[00000000000000000370---c2e0113f2189dcd0d7e975b232c5425c408cf7a4f2f89f491bd38bb6f38ca2fd]]]The rest is. is a constant, so we'll look at it here. This is similar to the definition of the state-value function, except where . The state value function is

[[[00000000000000000371---42908114987a5948ca838ba795eace7d9fdaa4841e20cba5318ba9b2f91b635b]]]not, as represented by .

[[[00000000000000000372---7fafc73dc5e4df68612001096aa499329d52d6ef648216cbb02b6400caa2df15]]]To begin with, substitute in the place of expression (3.4). Then,

[[[00000000000000000373---92d4cbf6a8d7045ee693491a800e85a402e84e7c13b8f415d5b196b77a73b678]]]becomes. This is the state value function. Well, our interest is. This is the expected value of the return one time ahead () when the current time is . The key to the solution is to change the condition '' to ''. That is, to advance the time by one.

[[[00000000000000000374---7d2358a871e1e32f17a94c4dbac15259d611e6f33c3a1b9ed81c57feae9ff381]]]As before, I'll give you a concrete example. The agent is now in the state of Then, assume that the agent chooses action with probability and transitions to state with probability. In that case,

[[[00000000000000000375---77684980b7dc8eecb1531aac4d9e113bff9ad9f14c29d61aa6d24cc3789dbe7d]]]with a probability of

[[[00000000000000000376---8ccf9a99f971bdd4cdab44a06af7dde0e4ef90a9dab5c52c5ee58932dbf086ac]]]transition to

[[[00000000000000000377---a0ea4a56410a904fdd303bc46b47607d69607ef47cb823b86b04fc00ca6f4e8e]]]It will be. In this way, by “seeing” the time one step ahead, the value function in the next state can be made to appear. To find the expected value of , do this calculation for all the candidates and find their sum. This can be expressed as a formula as follows.

[[[00000000000000000378---3c51616a00a57fc0d0a0f7e07d96d8fa8bebf0f694d9493c6a66b4eb5a735ce1]]]Now you can expand the second item. Summarizing the above results in the following formula:

[[[00000000000000000379---6277f47d8f4ec5e632b567ff41f897a7346619fe436789da42f7a59a13984a1e]]]This equation (3.6) is the Bellman Equation. The Bellman equation is a formula that expresses the relationship between the 'value function of a state' and the 'value function of the next possible state'. This Bellman equation holds for all states and all policies.

[[[00000000000000000380---8121596002de4eb71301d93151c175045ed6927b87ebce56e1d25ddbeacc6182]]]Bellman equation example

[[[00000000000000000381---81dfd69472177d45653021af74406c6d2b7934aa83769f3645cfc7dcf6080c81]]]In the previous section, we derived the Bellman equation. The Bellman equation provides an important basis for solving reinforcement learning problems. By actually using the Bellman equation, we can obtain the state-value function. Here, in order to demonstrate the 'power' of the Bellman equation, let's solve a real problem using the Bellman equation.

[[[00000000000000000382---4173d13ca3f044c8d070c290c9dbd4208798ba16c59dee491ed2598da7a67af9]]]2 grid world

[[[00000000000000000383---86ca9931869edc91d00c6cb05a8a0e57ce093d86f703ecf52bd4a26d8ff471dd]]]The problem we are working on is the '2 grid world' in Figure 3-7. This time, the agent will move randomly. That is, 50% chance to move right and 50% chance to move left.

[[[00000000000000000384---7ebb005f34c7aad7ec43d65d7313d3d6136e6ff1baab5d608314f791536273dd]]]
Figure 3-7 Grid world with 2 squares (If you hit a wall, you will be rewarded for picking up an apple. Apples will appear again and again.)


[[[00000000000000000385---c9255a0feea28414287b4309f478fcae652f434f2326ee8642038aec685cff18]]]is the expected return that a state would make if it followed a random policy at . This is an infinite sum of rewards that will continue into the future. Its backup diagram is shown in Figure 3-8.

[[[00000000000000000386---7fa3b5edc1eb4636366e3560bca7bb4139af39736c84ccecd759e46f65438eba]]]
Figure 3-8 Branching and growing backup diagram (in this problem, the state transitions deterministically)


[[[00000000000000000387---8ef5dab46e550c4b5617be75d09ba09cf87b68ee1455f9c7a7b31f362aa961ef]]]As shown in Figure 3-8, it is a calculation that branches and extends infinitely. We can find this infinitely branching computation using the Bellman equation. Let's use the Bellman equation to express As shown in equation (3.6) in the previous section, Bellman's equation is expressed by the following equation.

[[[00000000000000000388---76af14bd252340f3b29199b6272cb8a70a63b63c1b5f02a5d0a8a1fc621c9d46]]]Here, in anticipation of future formula development, I will write separately. Also, in this problem the states transition deterministically. That is, state transitions are determined by functions rather than probabilities. Applying it to formula (3.7), we get

[[[00000000000000000389---2c92a23eef6ffdd9cc5f6ee351cfe0ef9678257ff64ff52d9681404d89bee366]]]In the case of,

[[[00000000000000000390---2c92a23eef6ffdd9cc5f6ee351cfe0ef9678257ff64ff52d9681404d89bee366]]]In the case of,

[[[00000000000000000391---8e62b767e247201c6caab14bb91829c0a0dac74e5b1e56b2feccf94efafd8577]]]becomes. In other words, only the term corresponding to satisfying (3.7) will remain. Therefore, equation (3.7) can be simplified as follows.

[[[00000000000000000392---3b1dabe56c2d0903f7fc8abc6e2869723a625ce594a559dad35c9379eb878bf5]]]Now, let's apply the current problem to equation (3.8). In this section, we will perform this task by referring to the following backup diagram.

[[[00000000000000000393---8d2d1bc96f8c101690118927a04de91fca466b10953d481c4f5aec4ab27192f4]]]
Backup diagram for obtaining Figure 3-9


[[[00000000000000000394---e74b93046030fd87b084e2eafdaabaa4f138e1c21b2f3697754c383d3b3f684b]]]Looking at Figure 3-9, the backup diagram has two branches, one of which selects the action Left with a probability of 0.5 and always transitions to the state. The reward for this is Here we set the discount rate to . Then, the Left-chosen term in equation (3.8) is expressed by the following equation.

[[[00000000000000000395---4a81b364cb02095434427403bcf0fcd41ce22e00ac0272d6e5bf2a4112328e9c]]]From Figure 3-9, another possibility is to choose the action Right with a probability of 0.5, transition to the state, and get reward 1. This gives us the following formula:

[[[00000000000000000396---f51611d64dbdd4614f7fae5206877ec6004c44ed133f5237ad6358d073640263]]]From the above, the Bellman equation becomes:

[[[00000000000000000397---ec10af00c99500f794ff3bd5460a3b37351128eef814d095955bc6360a352a87]]]This equation is the Bellman equation for the state. This formula can be rearranged as follows.

[[[00000000000000000398---80b6dff046eb944fa1b9ab22dcb6c7f18311bdab042bfaaed6deb1ccde0e1730]]]Next, let's find the Bellman equation for the state. If you write the backup diagram in the same way as before, it will be as follows.

[[[00000000000000000399---10820e76eabbe7ad3d9599f7d8cc5a9acbac6a0b05c520792c5909c549f69efa]]]
Backup diagram for obtaining Figure 3-10


[[[00000000000000000400---9730d6b38960d47a3d0764091dfcf05a0123f31b05d868765f374d8abd57526c]]]Now we have obtained the Bellman equation for all states. There are two variables that I want to know this time. Then, the following two equations were obtained (equation (3.9) and equation (3.10)).

[[[00000000000000000401---ca330d5a30994de55e886a32e4c22a44e623969e22d0034dc16a3f7a423f17a1]]]This is a system of equations, and a problem like this could be solved by hand. By the way, the answer is

[[[00000000000000000402---c502252baec0acc12052940c045d6e963657554e432f69662173464bbf4bb7be]]]becomes. This is the state-value function for random policies. Taking a random policy means you expect future returns when you are in the state. Sure, moving randomly can sometimes hit a wall, and it makes sense that you'll get negative rewards in the future. The value of is also greater than , because there is an apple next to , and there is a 50% chance of taking that apple on the first move.

[[[00000000000000000403---c683f590f3caf793a7d3e480ee34dd1c0e6bcdd732b5e0670a92bc0cff7d976d]]]Significance of the Bellman equation

[[[00000000000000000404---bae517e6ece4613812bd199771e17335d9518f4d2a20c8fc86df0f71f444ede9]]]As we have seen above, the Bellman equation allows us to transform an infinite number of computations into a finite system of equations. Even if there is random behavior like this time, the state-value function can be obtained by using the Bellman equation.

[[[00000000000000000405---641b3150dc558d77e803dd6e5ed78d2f3cfa9fa1581006e1546df87938c0d6d4]]]The state-value function is the expected return, defined as the infinite sum of rewards. On the other hand, Bellman's equation does not have 'infinity' as expressed by equation (3.6). The Bellman equation frees us from infinity.

[[[00000000000000000406---181080a07bbcfd61fd9a368a7c3ae55764594e9b32270a7f43591f7f56190137]]]Also, this problem is a small problem, but even if it is a big problem, you can write it out as a system of equations using the Bellman equation. Furthermore, if you use an algorithm for solving simultaneous equations, the state value function can be obtained automatically.

[[[00000000000000000407---1bce80a34d0723d6ca4d81875c0308e347f6505b7193b5315ed2caecb3d3854c]]]Action-value function and Bellman equation

[[[00000000000000000408---93bb514ae88da1f91745bb3a0393ee3f211f962d065b19999fe65594de9ff457]]]Here I will explain the new Action-Value Function. Like the state-value function, the action-value function is an important function that often appears in reinforcement learning theory. So far we have derived the Bellman equation using the state-value function. After looking at the definition of the action-value function, we will derive the Bellman equation for the action-value function.

[[[00000000000000000409---08327d3c9a173893afcd1fafea1079a347f947c493412550c3beab6464abe279]]]For simplicity, the state-value function is sometimes abbreviated as 'value function' in this book. The action-value function is also conventionally called the Q-function. Therefore, in this book, the action-value function is sometimes referred to as the Q function.

[[[00000000000000000410---94956da00068619d7e81ae34965da7f13f10fa7e11990fb03bf06211fa4ff223]]]action-value function

[[[00000000000000000411---c20cc61d275a6c4fac9e4a9a597522fb2161589ca05cb3cedc09362ca7b419ba]]]As a refresher, the state-value function was defined as:

[[[00000000000000000412---94af16ccaae220efafd90495bc5518201428d58b6a22f20c068649073aa7b543]]]There are two conditions for the state-value function: ``state is'' and ``policy is''. In addition to these two conditions, it is conceivable to add 'behavior' as well. This is the action value function (Q function). Expressed as a formula, this looks like this:

[[[00000000000000000413---2aaf274cab1a710e44fa3ae71061c78c04f75dc840b3f1747bc5ae62d4bf2fdc]]]The Q function takes action in the state at the time and follows the policy after the time. is the expected value of the profit obtained at that time.

[[[00000000000000000414---62fb32e5efa4d5b8ce40ff8196045f3b2271be775f90364806108e4d63c20d05]]]Note that the behavior of is not related to policy. You can freely decide what to do, and after taking that action, you will act according to the policy.

[[[00000000000000000415---b0d1cda9563c414acecf48e06ecc45730a5c33649ac15b3e9d7f51954db1687d]]]The Q-function is the conditional addition of action to the state-value function. This becomes clearer when comparing the backup diagrams (Fig. 3-11).

[[[00000000000000000416---769a6d9ffa4b0d2a5b0e199334c355c94d2024b559ddbadad9df9cfcd932bdcb]]]
Figure 3-11 Backup diagram of state-value function (left figure) and Q function (right figure)


[[[00000000000000000417---8ec538a0547d17f73bf021b8868859d09b2c636a1cedfe76da337f826d660fd5]]]In the state-value function (left panel of Figure 3-11), actions are chosen according to policy. On the other hand, in the Q function (right figure in Figure 3-11), the action can be chosen freely. That is the only difference between the state value function and the Q function. So if we choose the behavior of the Q function according to the policy—if we do so—the Q function and the state-value function will be the same. For example, let's say that there are three action candidates in the state, and that we act according to the policy at this time. In other words,

[[[00000000000000000418---2caffb2dd9af257baf843eb6fa3e4c20da1854d805dfdd3f19ff3a6a02f1ef8e]]]choose an action with a probability of (in that case the Q function is)

[[[00000000000000000419---2caffb2dd9af257baf843eb6fa3e4c20da1854d805dfdd3f19ff3a6a02f1ef8e]]]choose an action with a probability of (in that case the Q function is)

[[[00000000000000000420---2caffb2dd9af257baf843eb6fa3e4c20da1854d805dfdd3f19ff3a6a02f1ef8e]]]choose an action with a probability of (in that case the Q function is)

[[[00000000000000000421---4283c08b7e1ab030ce5abb0bf72c9c36f831f5d940a92f30ee847aaef29f3f23]]]about it. The expected return in this case can be found as a weighted sum of Q functions. The formula is represented as:

[[[00000000000000000422---a7ca717b10344d697f305d18e6507a52c77d7476c2f31ce4d58280cef6befa53]]]And the above is the expected value of returns under the same conditions as the state-value function. So the following formula holds:

[[[00000000000000000423---2c4124c6246503c9c0c5d9bd7a01ba0a52179b2b31900e32d00142fb60b0f330]]]Bellman equation with action-value function

[[[00000000000000000424---8266eaec6d4cef02ee099b0e068c5f537dd253b87919bbdbf90f9328d0cf6684]]]Next, we derive the Bellman equation for the action-value function (Q function). First, expand the Q function as follows:

[[[00000000000000000425---ea172d9cad987dbed3ce7a570b366612830b4cbd660a66016c329cfe6bed0a47]]]Here it is decided to be a state, an action. In such a case, the transition to the next state occurs with a probability of . And the reward is given by the function With that in mind, equation (3.12) can be continued and expanded as follows:

[[[00000000000000000426---3a5369e205ad3544d0d189a94292b7c1fe589601a967863db31e41c1e8d8e1a8]]]Here, we transformed the equation all at once, which is the same as the work done in ``3.1 Derivation of the Bellman equation'' (If you are not satisfied with the transformation of the above equation, refer to ``3.1 Derivation of the Bellman equation'' again. ). Further using equation (3.11), the state value function can be written in terms of the Q function as

[[[00000000000000000427---26088dbcceaec4555dc61f2c971923606fc98c24281f34b4e26adef0778b6518]]](3.14) represents the time-of-day behavior. This equation (3.14) is the Bellman equation using the Q function.

[[[00000000000000000428---805ee83692f1f9eed4a1eb913b6b7909a32b535ca7b249402154224b0bffe4dd]]]Bellman optimum equation

[[[00000000000000000429---ff67cd9f7a65d44ae7d713baf6f2ddf0617cf492b4388bcd4ecaa0368876eac7]]]The Bellman equation is an equation that holds for a policy. But what we ultimately want is the optimal policy. The optimal policy is the policy that maximizes the state-value function over all states. Of course, the optimal policy also satisfies the Bellman equation. Furthermore, the Bellman equation can be expressed more simply by using the property that the policy is 'optimal'. We now describe the equation that holds for the optimal policy—the Bellman Optimality Equation.

[[[00000000000000000430---5c83754e037e1817f0990ff6cd62391e1ab9ab0f1b69644905d83b82c0bf3ee5]]]Bellman optimality equation in state-value function

[[[00000000000000000431---705359f31d4d87c16afa02f408f87fa2352e9aa5d8e881c155c7d7308bb2dc4d]]]We start with the Bellman equation. As a reminder, the Bellman equation is expressed by the following formula.

[[[00000000000000000432---914fdaad3560f84665a9df1c02b705154db2f99636eb5721c524c4e40fbc26ab]]]Here, in anticipation of future formula development, I will write separately. The Bellman equation holds for any policy. Therefore, when the optimal policy is taken, the following Bellman equation holds.

[[[00000000000000000433---5b4109dcb9ac3545c4d0ebf1d229da7f1a3471a3616d29b05c9128e26dc049a7]]]In the above equation, the value function of the optimal policy is denoted by Now the question we want to consider is the action chosen by the optimal policy. What actions does the optimal policy choose? Consider the following example.

[[[00000000000000000434---d98d721ea4e268580a8fcfa2e1e52ddf2d6f02d8ff82ff4cc4cd5d8e91555880]]]
Figure 3-12 Which of the three actions should you choose?


[[[00000000000000000435---cc2c9b97aaef932cb6e6714f06b916285f14f1f3c61d468fb9a118a511390c65]]]In Figure 3-12, we assume that there are 3 action candidates and the values of are respectively. At this time, what kind of probability distribution should be used to choose an action?

[[[00000000000000000436---afa625db94c4acbe1d219182f55d3faf2d5ea6caf3c7b446ea574181733070b4]]]Of course, since it is the optimal policy, it should choose the action with the maximum value with 100% probability. In other words, it is a deterministic policy. A probabilistic policy can therefore be represented by a deterministic policy. And if the action is always chosen, the value of will be

[[[00000000000000000437---1bd605d3bbc62d037ca1da435f45b41af65eeee4befcf218c5bac4edd15390ca]]]From this example, we can see that the optimal policy picks the action with the largest value of , and leaves that maximum value alone. This can be expressed as a formula as follows.

[[[00000000000000000438---25d25179a54522b68fd4aa89e9cdee3c40464de6ae6186a814ca20383f46e52f]]]As shown above, it can be expressed using operators. This equation (3.16) is the Bellman optimum equation.

[[[00000000000000000439---865e85f5eaf14adf37f613a69f7d2c8a53aace948726a6e6aae60017c2b7b070]]]An operator is an operation that selects only one element with the maximum value from among multiple elements. For example, suppose we have a collection of four values (called 'sets' in mathematics). Then, to extract the maximum value of the function at those four values, we write:

[[[00000000000000000440---1efa5eb643baa0eaf9f12a4db02c59259a8947301676906fb4bf44a2ea9fd731]]]The function takes the maximum value when . This work can be represented by an operator.

[[[00000000000000000441---d4e7d9ab27580d7a3e1ff5dc497acba10838588289dceca50708e90b7b782cd8]]]Bellman optimality equation for the Q function

[[[00000000000000000442---1e8b688c31164781a79ea9a1dd67cc5b7c0b92aaf477c1c3145932decc114fca]]]For the action-value function (Q function), the Bellman optimum equation can be obtained in the same way. In the same vein as before, we also show the Bellman optimum equation for the Q function.

[[[00000000000000000443---ea185e3e5d498616d56617536d775dff367dad332e97b2802fa93ceb505db40a]]]The action-value function in the optimal policy is called the Optimal Action-Value Function. In this book, the optimal action-value function is written as .

[[[00000000000000000444---47ae54ab88805f6361c1c678e85c5428a91cafeba4be9f61657d7fa0e07ee2a0]]]First, we show the Bellman equation of the Q function.

[[[00000000000000000445---424d919971cb79e9c1e5163052c966fc691b4c788d91ac89d8d174b0600b5719]]]This Bellman equation holds for all policies. Of course, it also holds for the optimal policy, so we can substitute the optimal policy.

[[[00000000000000000446---c6316b8b220c1bce3b5ac9ce5cfdc0735f69cd0d3008277ffd1f04df9e9acf06]]]The next expansion is the same as ``3.4.1 Bellman optimum equation in the state-value function'' explained earlier. Since it is the optimal policy here, it can be simplified by operators. Specifically, you can replace '' with ''. Therefore, the following formula holds.

[[[00000000000000000447---1c935b5e980fad8891f277f94a1036e0adc04f5c22d76ae3c7b4ad5b3c459707]]]This equation (3.18) is the Bellman optimum equation for the Q function.

[[[00000000000000000448---4d023a5d07f45a171db386ce94e5d9d4bc7003d0148934e98281520b77ba0b02]]]There is at least one deterministic optimal policy in MDP. A deterministic policy is a policy that always chooses a particular action in a given state. So the optimal policy can be expressed as a function as Also, depending on the problem, there may be multiple optimal policies, but their value functions have the same value. From this, the value function of the optimal policy can be expressed with a single symbol. Similarly, there is only one Q-function for the optimal policy.

[[[00000000000000000449---2af36f58055ff955d7c11641e852501c5aedc8e5643da9f98371db3852bb5f86]]]Bellman optimum equation example

[[[00000000000000000450---d93a24d1b25eeb8642310a1d454df7851b1f161c6a7aa3b91d08e30db8846a67]]]Here we revisit the '2 grid world' in Figure 3-13. The reward is when the agent hits a wall as it moves from one to another. And the apple will appear again and again.

[[[00000000000000000451---c970a1b98972cf9419b36802edc30d01faa7371c49446139aa67cd866ad115f9]]]
Figure 3-13 Grid world with 2 squares


[[[00000000000000000452---28bbcbeec33e383c47c0bd9408bb75c5f6cf1882008ffbbebbf58b6c51d5e487]]]Application of the Bellman optimum equation

[[[00000000000000000453---9550889c3d2bdab1ae1d0114cedad834bd1e9f8a2144380004c011f5b4d3b9fe]]]The goal here is to apply the Bellman optimum equation to a two-square gridworld problem. As shown earlier, the Bellman optimum equation is given by

[[[00000000000000000454---d3db0e593d51e7e96eb1475902c3944088736c4b186b889434df88759cb41bef]]]Furthermore, if the state transitions are deterministic rather than probabilistic, we can simplify to

[[[00000000000000000455---8b84023c4e33de4be0c8ef358d47d377c7665cabfb78f16919829bf6ed0cd5ea]]]The reason why the above formula can be simplified is

[[[00000000000000000456---2c92a23eef6ffdd9cc5f6ee351cfe0ef9678257ff64ff52d9681404d89bee366]]]In the case of,

[[[00000000000000000457---2c92a23eef6ffdd9cc5f6ee351cfe0ef9678257ff64ff52d9681404d89bee366]]]In the case of,

[[[00000000000000000458---85307196258558431c95bd89b91b329d09af755790a1cb4a1b2808e75e7edaf4]]]Therefore, only terms that satisfy the equation (3.16) remain.

[[[00000000000000000459---1d1a9969928d14308267a477c1ffe8ad9c3e87ece98d2238352754f8f69022de]]]Then, apply the Bellman optimality equation (3.19) to the '2 grid world'. For reference, the backup diagram (backup diagram one step ahead) with state and as the starting position is shown in Fig. 3-14.

[[[00000000000000000460---2caf5efc74b084c6e1c94d4a2677cb9aefceaa12f547b4ae16047964e706ea67]]]
Figure 3-14 Backup diagram starting from state


[[[00000000000000000461---bc140d7aefccb1b6c5245a4d3380c4ffd2994aa19dad0df089a326801c85f6c5]]]Referring to Figure 3-14, the Bellman optimum equation for a discount rate of 0.9 is obtained as follows:

[[[00000000000000000462---42f62d6b807f258f0b2987562ac29c2239834d9cfc6d3ff430556febef945865]]]Here we represent an operator. For example, if and if compared, the larger value is returned. Now, as the above equation shows, we have two equations. Again, there are two variables—— and——and two equations. Therefore, from this system of equations, we can obtain and By the way, the solution of the above system of equations is:

[[[00000000000000000463---7f828fe8c72a8af0a1de050ef7d89fd806305f46f64168a787afcf76296b5386]]]In the above system of equations, the operation (take the maximum value) is performed. is a nonlinear operation. Therefore, the algorithm for solving simultaneous equations cannot be solved with a 'linear solver', but can be solved with a 'nonlinear solver'. In addition, if it is a small problem like this time, it can be obtained by hand calculation.

[[[00000000000000000464---226587dafaf2e49d508ca35c9837015cb9835f6af6426dd1a33db4cf8a4bcaae]]]For small problems such as a two-square grid world, the Bellman optimum equation can be solved directly. But what we ultimately want to know is the optimal policy. Next, consider the optimal strategy.

[[[00000000000000000465---e4773fef2405d60ad52921d031d07b0f22e8ff9ec24c2011ad22e84afa57a6b2]]]get the best policy

[[[00000000000000000466---c17aaafbef650dc8b5c882b326799bccd931858b8d7d6e6aca289500c71ed085]]]Here we assume that we know the optimal action-value function. Then the optimal behavior in the state is given by

[[[00000000000000000467---d9c4ae77349de2fd97620dd65755c5fe0dbf9f2505d5ddf7cd4d28423c5d6b78]]]returns the argument (in this case the action) that is the largest, rather than returning the maximum value. As in the above formula, if you know the optimal action-value function, all you have to do is choose the action that maximizes it. That action selection will be the optimal policy.

[[[00000000000000000468---00b6ce3d04297bbacc3b4910f1ac61803d74006a4c46618bb929b7ccff2d2557]]]In '3.3 Action-value function and Bellman's equation', we showed that the following equation holds.

[[[00000000000000000469---af01fe390cd5189df4a51431540350646ca37201c6d3f8cdce5c3bee4be49a55]]]We can replace the subscript of the policy in and in this expression with the subscript of the optimal policy. And substituting it into the formula (3.20) gives the following formula.

[[[00000000000000000470---e660a37f574a9255a0b7bde1bf1acbcda583488458d27f05952515bd60bee9fc]]]We can obtain the optimal policy using the optimal state-value function, as in equation (3.21).

[[[00000000000000000471---1b5ecc6dee883cb2853a994f322c502685a7627c07a7687528a8d5d486628c65]]]Equations (3.20) and (3.21) can be said to be 'greedy measures'. A greedy policy seeks the best action among local candidates. As in this case, the Bellman optimum equation only relates the current state () and the next state (), and simply considers the next state to choose the action with the greatest value.

[[[00000000000000000472---abf40248a1fbae69439113595f327dc85e3a5c34119b9d8a3434a164f6ef1291]]]Now, let's find the optimal policy for the '2 grid world' problem using equation (3.21). We have already sought the optimal state-value functions —— and —— in a “2-square grid world” Here, referring to Figure 3-15, let's first find the optimal action for the state.

[[[00000000000000000473---3e7eecde17fc3fdbf192dc1e79a8958e87eb09f1d58474329b7f1facfd003c8a]]]
Figure 3-15 Backup diagram and optimal state value function


[[[00000000000000000474---7a5d55ed4d07dd1bb6672d83b49691e08a99a2e2c1c500d8dd5f6e5aa01d34de]]]There are two possible actions, Left or Right, as shown in Figure 3-15. If you take Left here, you will advance to the state and get a reward. In this case, the value of in equation (3.21) is

[[[00000000000000000475---729cb2aefcdabcd34097275b50c6a53c2f4a17067ae333c6b411ebb6810327eb]]](0.9 is the discount rate). On the other hand, if you take the action Right, you advance to the state and get a reward of 1. in this case

[[[00000000000000000476---80e6ea3581a8e7a13384966c91aaa74b0189bedddae063740648c04d16cf5808]]]will be If you choose the action with the maximum value among these two values, it will be 'Right'. In other words, the optimal action in the state is Right.

[[[00000000000000000477---09f9d8b8575fd08e6b7bf62d2a0fd145fcaefeb1f8635424c08defb28ccaa939]]]Also, the optimal action in the state is Left after the same calculation. Now we have found the best solution. As shown in Fig. 3-16, the optimal policy is to move to the right when you are in the room and to the left when you are in the room.

[[[00000000000000000478---7752b30170fb7da781615517163c92b4660249968d2f71d35955ed93b20cf6af]]]
Figure 3-16 Optimal strategy for a two-square grid world


[[[00000000000000000479---13ad8c9948fbcf559b31ca2aaa739f72ba611aeecffac064b89f048f25778bb5]]]As we have seen above, if we know the optimal state-value function, we can obtain the optimal policy.

[[[00000000000000000480---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000481---03b31aa04725aea66fcf4fdd9c46c2ace88ccff9e9e7614b2d1b7e4f7ee60e9d]]]In this chapter, we learned about the Bellman equation. I did a derivation of the Bellman equation and solved a small problem using the Bellman equation. Specifically, the Bellman equation gave us a system of equations, from which we were able to determine the value function. Unfortunately, such a system of simultaneous equations cannot be used for practical problems due to the enormous amount of computation. However, the Bellman equation provides an important basis for many reinforcement learning algorithms.

[[[00000000000000000482---8883c1db75ca1eddc146f82a7453d4b1d5ccb84ed065ac382b4a740e5b8d6561]]]The ultimate goal of reinforcement learning is to obtain an optimal policy. In this chapter, we also learned about the Bellman optimum equation. The Bellman optimality equation is a special Bellman equation that holds for optimal policies. If we can obtain the value function for the optimal policy, we can easily obtain the optimal policy. Finally, here are the important formulas we learned in this chapter:

[[[00000000000000000483---2808227a50246ff0f6dc0b683ca7a1a647a28ea818d04f6c48958da1d3388ff9]]][Bellman equation]

[[[00000000000000000484---a73c4650dbb317038b101f603b2558268ef0a688ca14a36e61f0b3895119a85e]]][Bellman optimum equation]

[[[00000000000000000485---58f889966763c0b87b53f8828114861405becb979108b029355472715aed0c12]]][Optimal policy]

[[[00000000000000000486---066c6a41a8ed489b43ec07285dc093bd8202eacf454506394df85a55c7652bd4]]]Appendix A

[[[00000000000000000487---d0b3b4df6aac2755b60feca50656b419913b668ad5a854727c61d29b0b0fb160]]]Off-policy Monte Carlo method

[[[00000000000000000488---bb01068e278f43666003fcc9e01d0a076d905f56c37e9acd7a369093c1e7f1a7]]]Here we describe the off-policy Monte Carlo method. First, I will explain the theory of off-policy Monte Carlo method. Next, we will implement a policy-off Monte Carlo method for the '3 × 4 grid world' task. This appendix should be read as a continuation of Chapter 5.

[[[00000000000000000489---c25a4576b37960eaaffe52bf81121f24db9373a2aa0b939b8c31a715acda21a3]]]Theory of off-policy Monte Carlo method

[[[00000000000000000490---c4fc7961a9996e7218589b3210120815bad9b947c7608a1b875b54bebb482e1a]]]Here we apply importance sampling to a reinforcement learning problem. First, we review the on-policy Monte Carlo method. Our goal is a Monte Carlo approximation of the Q function defined by

[[[00000000000000000491---947ffb52e5130510ae3a17f8cf586b494eadf7850cf176f1499a55135c494201]]]represents the expected value of the return obtained starting from the state, action, and then following the policy. To approximate the Q function using the Monte Carlo method, we act on the policy and average the returns obtained there. For example, if you have sample data of earnings, you can approximate:

[[[00000000000000000492---284e3c40f38a7378c34eb3a94c718e975ccada7dc0636bb66468c9cea3a4fe4c]]]Now, let us consider the 'policy-off type' Monte Carlo method. Using importance sampling, the Q function is given by

[[[00000000000000000493---396878ff5d7eff44d0ec35fc3c007ec328fda48a9a224ee202cbae14f856c107]]]Two important points here:

[[[00000000000000000494---aff34b49e4db048040b379511f3ec03d6e2d69e13195343766279314a68d697b]]]represented as the expected value by the policy ()

[[[00000000000000000495---bb5f2acc3c6d7596db371fc2fb1f745fac475c6b73defea1195c22ae6eb40b4f]]]'Weight' is added to fill the gap between the two policies (probability distributions)

[[[00000000000000000496---903b4a8fcf1524434dc64388ac9d0bc53ad4d8c771aae3cded45f8d529f93a02]]]The weight is the ratio between the probability of making a profit given the policy and the probability of making a profit given the policy. Now we approximate equation (A.1) using the Monte Carlo method. This is expressed as:

[[[00000000000000000497---741c2afb6ded19d8873bfe49f1cfb6171e22361285f218aae35972cb496a5a41]]]Here we denote the weight for each item's profit by . According to the above formula, the agent acts according to the behavior policy, and uses the sample data obtained there to find the weighted average.

[[[00000000000000000498---b3d4537d0bfbf4ed439f31294daf479ff42d2b21166e99f443ca2dfc57616392]]]Next, let's take a closer look at how the weights are calculated. Here, we consider the case where the following time series data are obtained by the policy.

[[[00000000000000000499---dfc8bae1d63c546b45132bc6f770ac0645be2cd5915300b7acce59f978354cf5]]]
Figure A-1 Example of time-series data obtained by policies starting from states and actions


[[[00000000000000000500---47db361973404e4d554b1a8940270c558225d2c8793509adab1a522f2ab5fbb6]]]Here, we call the time-series data in Figure A-1 a trajectory. in short,

[[[00000000000000000501---032e38e192656af3334d4b7e7eedb85aef8a77c97536c544906b164373764d95]]]is. In this case the weight is given by the following formula:

[[[00000000000000000502---ccad8ab6c6e2395fc933a530f68964b987a781f690f7a02e698b56c8a4810110]]]Probability is represented here. is the probability of getting the trajectory when the policy is , and is the probability of getting the trajectory when the policy is . Or it can be expressed more simply by considering the Markov process. A Markov process means that the state transitions of the environment and the behavior of the agent depend only on the previous state (and actions). This process can be represented graphically as follows.

[[[00000000000000000503---ce04037e4a82ab4e3ea2d096002209b1349f2186140d31c723375df518a9561e]]]
Figure A-2 State Transition Probability and Policy Involvement


[[[00000000000000000504---22a559bb0c5bc6759241c806b5a9952adb2ce157254315fa2e32d446c452d4b4]]]Given a state and an action, the state transition probabilities of the environment determine the next state, as shown in Figure A-2. Then, given the state, the next action is determined by the agent's policy. Therefore, is represented by the following formula:

[[[00000000000000000505---568669e88e68d1351f0217d97ef01756f57fd0b85b68215065484096f9ea11c4]]]The above formula is the probability of obtaining the sample data trajectory when the policy is . In the same vein, and assuming a policy, we get:

[[[00000000000000000506---eee262864ba96d16813b8cb6bece04a2fd7e31c6c61223488dd34ed0bd7b53c9]]]The weight is the ratio of equations (A.2) and (A.3). The point to be noted here is that the state transition probabilities of the environment are common. Therefore, the denominator and the numerator cancel each other out. The formula is represented as:

[[[00000000000000000507---e59d4a1fbb5064e5cc5c13c8843e676de4599dff11a049de2a9f0b0c852e722a]]]The weights can be derived from the policy-only ratios, as per equation (A.4). The above is the off-policy Monte Carlo method. The steps of the algorithm can be summarized as follows.

[[[00000000000000000508---de7e03751de149cd7ab9b6cdbc600ec81560f09ddc5d3434d4461a2d90f9ce07]]]Sampling with behavior policy (obtain time series data)

[[[00000000000000000509---02cb9ffa2afe479bc838cc3fdbe7f105a08e37a8252bd588c22f5a9633982aa7]]]Calculate revenue from earned

[[[00000000000000000510---740835ec15a4854a964dba37cd057f28c38791f0cff2abe0fc14bb390c9ec554]]]Calculate the weights by formula (A.4)

[[[00000000000000000511---244277f686f8cce1124d71da383bfff28ca97d838cc63a68af319b2601189207]]]Repeat 1 to 3 multiple times and find the average of

[[[00000000000000000512---1f375f787726ac162406b3d211904ca3d65ca93943eff418858b315ddf43c37d]]]Implementation of off-policy Monte Carlo method

[[[00000000000000000513---3c6b58fb749c88676a1fd4b6304fd7a88f53ab663958f8b6366b11bed567e11e]]]Now let's move on to the implementation. Before that, let's talk about how to implement weights efficiently. This is the same method as described in 5.2.3 Efficient implementation of Monte Carlo methods. Let's start by looking at the following diagram.

[[[00000000000000000514---4f4b57e96ea64d0eeca7afec3b1ca9bc104ff99e0205ee10dc98a0a6ac482199]]]
Figure A-3 All 'state-action pairs' can be considered starting positions


[[[00000000000000000515---ce9b90e3019c48f263d01f40b2f8f46fd26063ccaa6ece450c936fdf724baa8d]]]Figure A-3 is the result obtained with the state-action pair as the starting position. At this time, the state-action pair data in the middle can also be regarded as sample data obtained as the starting position. In that case, it is efficient to update the weights backwards from the goal (end point). Specifically, we initialize to 1 first. Then the weight for is 1. Then update the weights for

[[[00000000000000000516---c99300b9271715f4af6c8b3bf06845759eaf5c8206e5b29fc53cbd2b0bfbe244]]]In the same way, update the weights for

[[[00000000000000000517---5c3d0c7c376bec8add5403d64c2352fdf19e2438b9951eec1dce40248a7e6879]]]In this way, it can be calculated efficiently by updating the weights in the opposite direction from the goal. Now, we will implement an agent that performs policy control with off-policy Monte Carlo method. Here is the code:

[[[00000000000000000518---d004ab0a926a06d073c98a87422bab5db458f71c70b98d07a6135b6cde4e0d10]]]# ① Behavior policy

[[[00000000000000000519---1bc8172d54af54fdb39eca68d2ac2c32a0059cd536b298372aebb444df9834f0]]]# ② Act with behavior policy

[[[00000000000000000520---e7157862a7090fd280ab2045fe3188560a245a9737697743f28950d5d209c845]]]# ③ Update Q function using sample data

[[[00000000000000000521---756609624a8c92b545f1c3b40068480945bf8cbebe0619843a441f484b438927]]]# ④pi is greedy, b is ε-greedy

[[[00000000000000000522---4038a7286bdab31655811b0bf19e026826476c901d948e6e0f7722ea66c4c314]]]The code has much in common with the McAgent class implemented in '5.4 Policy Control by Monte Carlo Method'. Here we will focus on the differences. First, code ① initializes a behavior policy with the name b. This behavior policy initializes with a random policy. Next, in the code ② get_action method, we get the action from policy b.

[[[00000000000000000523---a7a42fe20ad7f478578198822cbea9269ae0f3743158cfe2b23e20152bd74c56]]]Code ③ updates using the importance sampling weight rho. The code may give the impression of being complicated, but compared to the McAgent class implemented in '5.4 Policy Control Using the Monte Carlo Method', the changes are minor as shown below.

[[[00000000000000000524---489363d8d03b1cc2d66eb031631e4166dac82a27baf4c3889b847f68b3de2351]]]# policy on type (previous)

[[[00000000000000000525---83f9760b084a8df34ea7d059fbfb0c680dca551b34655097a6d92d4b79647896]]]# policy off type (this time)

[[[00000000000000000526---a6bf87cda5cfffefd80c7aadf453e5bd472dcd7b1af5b642857e1c91c6d7d228]]]Returns obtained as sample data should be corrected by the weight rho. So as above, we use rho to update revenue G. Finally, code ④ improves the policy. Specifically, the behavior policy b is updated by the -greedy method (1) and the target policy pi is updated by the -greedy method (). Since the target policy pi is , this is a complete greedy.

[[[00000000000000000527---c1f6787e256278c788cc5b97508c6d8ef0e9b5846fda7efd65c2e23e9904cc48]]]Since the purpose of the behavior policy is to 'search', there is no problem with a policy that selects all actions equally (=random policy). But here we update the behavior policy b with the -greedy method to reduce the return variance. The -greedy method allows the behavior policy b to approach the probability distribution of the target policy pi while still being explored. In addition, '5.5.3 How to reduce the variance' explained how the variance can be reduced by bringing the two probability distributions closer together.

[[[00000000000000000528---f3c9f936595a53458491790ef0d771e9b7d58a277beffc4a16b165f954db5e56]]]Now let's use the McOffPolicyAgent class to solve the problem. The results are as follows.

[[[00000000000000000529---5a9afa338d0fb7b212890fca1f194b93c7b4d136993e306744aa823fd6621e8d]]]
Figure A-4 Results obtained using off-policy agents


[[[00000000000000000530---443df96af5a4006d27e775dbe12f7ece497997ad2c936857ed3e85dc18b40c90]]]Results will vary from run to run, but generally good results. By the way, the policy in Figure A-4 matches the optimal policy.

[[[00000000000000000531---442f09be9458f4623162466375f05b0a173c812c47ccd8ea68fc0b9c81e8f3f4]]]For our small problem, the off-policy Monte Carlo method worked well. But the bigger the problem, the harder it is to get good results. The reason is that the variance of the sample data will be large. Because as the problem grows, it goes through many states and actions before reaching the goal. As a result, the variance of the 'weight' due to importance sampling increases accordingly. In this way, the off-policy Monte Carlo method has the disadvantage of requiring a large number of episodes to improve the policy, which takes a long time to calculate.

[[[00000000000000000532---3a0824baa1752fb7403fd9483c25c1f2909dbae9da80b9699f1c258f4cc88ea5]]]● Author introduction

[[[00000000000000000533---299ac4470df01946755c3411ec8c9fd68793965c598cb2725a530e9847394c99]]]Kouki Saito

[[[00000000000000000534---057a615642003c85648e9b7a7820ca8e488ee7a34c97f8ea9cbdeb403cc493b0]]]Born 1984 in Tsushima, Nagasaki Prefecture. Graduated from the Faculty of Engineering, Tokyo Institute of Technology. Currently engaged in research and development related to artificial intelligence at a company. His books include 'Deep Learning from Scratch' and 'Deep Learning from Scratch ❷', and translated books include 'Practical Python 3', 'Computer System Theory and Implementation', and 'Practical Machine Learning Systems' (O'Reilly Japan). be.

[[[00000000000000000535---ca67ef85c9beb2732d0b96ad2113e21ef67f973eb3711ad3b94163c5d7b4a709]]]Chapter 7

[[[00000000000000000536---fee1394fe59ca483d439a374b56586385330e0805da7b182fa21360f265d94a6]]]Neural networks and Q-learning

[[[00000000000000000537---bec1386ae8e129ebbb009590bb4214ec27c6f3e551969ffc36ba0d677e1af1bd]]]So far we have only dealt with problems with small state and action sizes. For example, in the '3x4 grid world' problem, there were 12 state candidates and 4 action candidates, so there are a total of 1 Q function candidates. With that size, there's nothing wrong with keeping the Q functions as a table (or as a dictionary in Python code). But the reality is more complicated. The number of possible states and actions can be enormous. For such problems it is not practical to still have the Q function as a table.

[[[00000000000000000538---14d6db90a2fb53f0db1ad438ce60dfe92bf5e097a764153f5730a21ccae3cc13]]]For example, in the case of chess, there are 10 to the 123rd power patterns for the arrangement of pieces. That is, there are that number of states. It's practically impossible to keep it as a table. A further problem is that each element of the table must be evaluated and improved independently. It is not realistic to experience all that huge number.

[[[00000000000000000539---d90ea3cd141beb8a0ff36709a9cc26ef43f2c8413d3bff7f2bdd3a007a2d507a]]]A possible solution to this problem is to approximate the Q function with a compact function. Deep learning is the most powerful method for this. The combination of reinforcement learning and deep learning has led to many innovations. Now we finally move on to deep reinforcement learning.

[[[00000000000000000540---ddd1c0d22d4474f26fa341fa5c567e65507d98aff612b0e875054f0a5968aabc]]]In this chapter, we will first explain how to use a deep learning framework called DeZero. Then use DeZero to learn the basics of neural networks. After that, we will implement the Q-learning implemented in the previous chapter using a neural network this time.

[[[00000000000000000541---f9ea2b11cb98aa6c786d994bd6ba662028e9cca8ec7e3906a1eee0f5eae1ace1]]]Deep learning is the deepening of the “layers” of neural networks. In this book, the terms “deep learning” and “neural network” are used interchangeably.

[[[00000000000000000542---7d70854d64d8fb8fc11f1a426e15b1592987b6d4be5c404f2b1d0f9341d5d6c5]]]Basics of DeZero

[[[00000000000000000543---5557c5c97949af594eb762980031f0bbb2cac8b8efde067689412b61e26eeb96]]]Over the next few sections, I will explain neural networks (deep learning) with code. First, we will learn about deep learning frameworks. We then proceed to solve a fundamental machine learning problem (linear regression) and finally implement a neural network.

[[[00000000000000000544---3bb91ba3c8462ff5768d57fbc022cd3ab9f60048c841cc856d243fa790508483]]]The theme of this book is 'reinforcement learning'. Therefore, the content related to deep learning that will be explained from now on will be used only as a 'tool'. If you are already familiar with deep learning, you can skip ahead and move on to 7.4 Q-learning and Neural Networks.

[[[00000000000000000545---25807e11e4b9376aabf078e4d87b8868f18632d0845866c00a555130432385e2]]]This book uses a deep learning framework called DeZero. DeZero is a framework created in Volume 3 of this book series, 'Deep Learning from Scratch ❸'.

[[[00000000000000000546---33a74d0f12df2c0275e4e9648ce172c317bcd1fb27b16edca72d6e4ee3be55ce]]]
Figure 7-1 “Deep Learning from scratch ❸” and DeZero


[[[00000000000000000547---7bacaf070ab94e403e085db09c55ab3aa681511b00cce701a3d3f8eb8192ee99]]]DeZero is based on PyTorch, a framework designed with an emphasis on 'understanding'. Of course, it doesn't matter if it's your first time using DeZero. If you have experience with modern deep learning frameworks such as PyTorch and TensorFlow, you will find DeZero up and running quickly. Here is a brief explanation of how to use DeZero.

[[[00000000000000000548---aaa4ad4ab06998bace8771e52469b3fcf2cb913478d8ecb7d4f602e1d9c86c44]]]This document uses DeZero for explanations. However, DeZero is kind of a 'dialect' of PyTorch, so it's easy to rewrite DeZero code to PyTorch code. In addition, the code for the PyTorch version is also available on the support page (GitHub) of this document. If you want to use PyTorch, please refer to that code.

[[[00000000000000000549---0137e3fd0ecac34ee784dd250e9f670d84278971a40246271658be730c4bbb8b]]]Use DeZero

[[[00000000000000000550---b14a7a7de0aafafa9090ccb8233471085492cae0911e8657a42e827283e68ce1]]]First, install DeZero. DeZero can be installed using pip as follows:

[[[00000000000000000551---a4bd42a1eae61ce271a61039c1db61ea8d92faec0e84aeb8a696eca163417a57]]]After installation, let's use DeZero. Let's start with the Variable class. Variable is a class that wraps a NumPy multidimensional array (np.ndarray). It is used like this:

[[[00000000000000000552---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000553---946eb44ab063a4723224ae0fe73e2ebedb1ecbc1999396f0335a6955ecb310c4]]]Here we import the Variable class from the dezero module. x = Variable(x_np) makes x a Variable instance. After that you can compute like you would treat a normal np.ndarray. When the code above executes y = 3 * x ** 2, it immediately gives a result of 75.0.

[[[00000000000000000554---dc4b8a561dea766f89cb28ee6b8ec85eb40ce4c93d0132ccc9a1001be0565f49]]]Now let's find the derivative. It calls the backward method. Here, following the code above, execute the following code.

[[[00000000000000000555---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000556---bd2825adfcfc547f468c948f84c03460d28ffd1ab614ea98eea8d0878a655f68]]]where the output y is a Variable instance. By calling the backward method on this output y, backpropagation is executed and the differentiation of each variable is obtained. Note that in the code above, we calculated y = 3 * x ** 2, which is the formula. Its derivative is, so substituting . Indeed, this agrees with the results above.

[[[00000000000000000557---57b55106a98f4d2afa1e664189d171f32167b9aa8086ecd6350d9d155b7389b4]]]Multidimensional arrays (tensors) and functions

[[[00000000000000000558---af04ebc9d50ec81e8ef198912d572225a2843c5ca654e8fddb7b5a8bc28673f0]]]It is common to work with multidimensional arrays (tensors) in machine learning. A multidimensional array is a data structure for handling multiple numbers (elements) together. The arrangement of elements has a 'direction', and that direction is called a 'dimension' or an 'axis'. Figure 7-2 shows an example of a multidimensional array.

[[[00000000000000000559---9e974edea19b0d1c318ab0cab4fe81b075ef79046c572d68f2395dd10d841021]]]
Figure 7-2 Example of Multidimensional Array


[[[00000000000000000560---31d1c6b0baa4958baf0fd31aaaa142e30890c47f750afbbff3d9c12fbc1775eb]]]Figure 7-2 shows, from left to right, a 0-dimensional array, a 1-dimensional array, and a 2-dimensional array. They are called 'scalar', 'vector' and 'matrix' respectively. A scalar simply represents a single number. Vectors have numbers along one axis, and matrices have numbers along two axes. Note that multidimensional arrays are also called tensors. In that case, the example in Figure 7-2 is called a 0th-order tensor, a 1st-order tensor, and a 2nd-order tensor from left to right.

[[[00000000000000000561---401a11cda855c6e7d36aa8f8fc8dcc7aed44c099ff483c5e7be384674ae0a068]]]Next, we will explain the inner product of vectors. Here we assume that we have two vectors At this time, the inner product of vectors is defined as Equation (7.1).

[[[00000000000000000562---623580f0be23282f809add88d16b7845e6ae983d027dd697243d49c1b4a137d7]]]The dot product of vectors is the sum of the products of corresponding elements between two vectors, as shown in equation (7.1).

[[[00000000000000000563---329b8f6768b4a2c3ede6138c2764cd0c18e541358bf18c2a9a2cec1cb1b41ee7]]]When notating symbols in mathematical expressions, in the case of scalars, they are notated as follows. On the other hand, for vectors, matrices, etc., it is written in bold.

[[[00000000000000000564---29a1f5f211dd622ab3d0ebedb296820c0bf2c6b53c51371953571122748dad5c]]]Finally, let's talk about matrix multiplication. Matrix products are calculated according to the steps in Figure 7-3.

[[[00000000000000000565---fa95c56bf685f43242be6ad24e0efec334a9e661f7c3ec21114bd8522d219847]]]
Figure 7-3 How to Calculate the Matrix Product


[[[00000000000000000566---2c1c97262af617f5052a31b5df14e31c8ac7b7c7f2f4d5416176fea44a8cf0f0]]]As shown in Figure 7-3, matrix multiplication computes the inner product between the horizontal vectors of the left matrix and the vertical vectors of the right matrix. The result is then stored in the corresponding element of the new matrix. For example, the result of row 1 and column 1 goes to the element in row 1 and column 1, the result of row 2 and column 1 goes to the element in row 2 and column 1, and so on. .

[[[00000000000000000567---d7dd63d29641653548c06a1d609bf148b0bb957330a05ee2f0493d859c9294c0]]]Now, let's use DeZero to calculate the inner product of vectors and the product of matrices. For that we use the matmul function in the dezero.functions package.

[[[00000000000000000568---4fe2c5043a1cc0ec95bacbf6dd20de4019b8bbbf2ecae741c618e8e9a47b342a]]]# Inner product of vectors

[[[00000000000000000569---fd28e81223ab81be7830f60b3ede70ab38848a5448c2f1debdf6481857415020]]]# optional

[[[00000000000000000570---5de35d6711aab57c6425824f4b87616644a3686a4b31b9794605871f93722ad6]]]# matrix product

[[[00000000000000000571---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000572---ae9638e55bb3090f103c1a4ba8028c48426be344faaf7279361a5005935a111e]]]Here we are calculating the inner product of vectors and the product of matrices. To do so, import dezero.functions as F as above. Then you can use DeZero's function as F.matmul. As shown here, the F.matmul function can be used to both compute inner products of vectors and products of matrices.

[[[00000000000000000573---dd9a841893f80c3bad5514bdaf808b81c67fd0a5915dd535268ae51a1e700668]]]DeZero's functions can work directly with np.ndarray instances (in which case DeZero internally converts them to Variable instances). Therefore, a and b in the above code can be input to F.matmul() as np.ndarray instances without explicitly converting them to Variable instances.

[[[00000000000000000574---cbd23caeab5f44e0eb793b803c8b9b1a8f9bb2d5f5c6c75b674f409caa143d15]]]It is important to pay attention to the 'shape' of matrices and vectors in computations. For example, the matrix product calculation has the relationship shown in Figure 7-4.

[[[00000000000000000575---2be48fe302fd872c42b67d54103d6c26c15d851a85c532eabd03c5a661bac7fb]]]
Figure 7-4 Matching the Number of Elements in Corresponding Dimensions (Axis) in Matrix Multiplication


[[[00000000000000000576---a110a0197b0e25e86d0ff3a367365f7c52d438969b6e5f1196dd90cc6f5d797c]]]In the example in Figure 7-4, the matrix multiplication with the matrix produces the matrix . At this time, as the figure shows, the number of elements in the corresponding dimension (axis) of the matrix must match. The resulting matrix then consists of the number of matrix rows and the number of matrix columns.

[[[00000000000000000577---a0a5c871a86009e00ee61cc105c3b0f9afc51570808e127b6f84c8010963af45]]]optimization

[[[00000000000000000578---b62e44f80bd6fda119507cc02addce16a927b1b353f4bf01b7b713d4c243fb79]]]Next, let's solve a simple problem using DeZero. We want to find the minimum value of the function expressed by the following formula.

[[[00000000000000000579---b32628b3aa11a7b11e527f7450377c803d01fad74dba4256bb23325c4e46ed60]]]This function is called the Rosenbrock function. The Rosenbrock function is widely used as an optimization benchmark problem because it is difficult to search for the true minimum value and the shape of the function has characteristics. Our goal is to find the minimum output of the Rosenbrock function. To answer first, the minimum value of the Rosenbrock function is at Here we use DeZero to see if we can actually find that minimum.

[[[00000000000000000580---7d72aca0f688ac49f85ea839626b90ae0b2c327a10f942ee4d891d22154330b7]]]The task of finding the 'function argument (input)' that takes the minimum (or maximum) value of the function is called optimization. Our goal here is to solve an optimization problem using DeZero.

[[[00000000000000000581---85d842fe9f7cc864be1b18c2cc06311b5e7ce1888e9099f38299aad3d1ddc329]]]As a start, let's find the derivative of the Rosenbrock function at With DeZero, this can be implemented like this:

[[[00000000000000000582---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000583---f383481830985cb0bdd03063034a1280726a6a1f28a91f34cbd665fead26c433]]]As above, if you first wrap the numerical data (np.ndarray instance) in Variable, you just code according to the formula. After that, just call y.backward() and the derivative will be automatically obtained.

[[[00000000000000000584---791ed18d0eb98a0b176fae09994eccd07d186c81c7fb1bcd79fdaeb865450dab]]]After running the code above, the derivatives (and) at x0 and x1 are -2.0 and 400.0, respectively. The sum of these two derivatives—(-2.0, 400.0) and the vector form—is then called the Gradient or gradient vector. The slope indicates the direction that increases the output of the function the most at each point. In the example above, it means that (-2.0, 400.0) is the direction that increases the y value the most at (x0, x1) = (0.0, 2.0). Conversely, it also means that the direction of (2.0, -400.0), which is the negative multiplication of the gradient, is the direction that reduces the y value the most.

[[[00000000000000000585---9aecdeab8d5532c69e7cd8c899d95357059b597b61567dff5ee97146ae1b6a93]]]Functions with complex shapes often do not have a maximum value in the direction the gradient points (or a minimum value in the direction opposite the gradient). However, when confined to local points, the gradient points in the direction that maximizes the output of the function. Therefore, if you repeat the process of advancing a certain distance in the direction of the gradient and finding the gradient again at that point, you can expect to gradually approach the target location (maximum value or minimum value). This is Gradient Descent.

[[[00000000000000000586---33fcb0793ee3256cdc7a8031f60417c8456c5ab495721ce4f242a3ea428118dc]]]Now let's apply gradient descent to our problem. The problem here is to find the 'minimum' of the Rosenbrock function. Therefore, it advances in the direction that multiplied the gradient direction by minus. With that in mind, you can implement it like this:

[[[00000000000000000587---e08811e546931f69c6555b954091aeaa55ff7dfb8a3032fc9790ff2e5d17f65d]]]# learning rate

[[[00000000000000000588---d44b9bc77be7c69e8fea9c4d0136ffc73437d96fe4abfea71f4132782012f8ec]]]# number of repetitions

[[[00000000000000000589---da3e70f4ca98e2e0ce23ee01d49896a8f77a7d3ce67a004cc998b85816c6d06e]]]As above, set the number of iterations to update as iters (this iters stands for iterations). Also, set the value to be multiplied for the gradient in advance. In the example above, set lr = 0.001. This lr stands for learning rate and means learning rate. The actual variable update is performed at x0.data -= lr * x0.grad.data.

[[[00000000000000000590---61f8589d84fda620e63ee22189a2aa402bc09a3417196f7365d56f3e07d411a4]]]Note that both x0 and x0.grad are Variable instances. The actual data (np.ndarray) is in x0.data and x0.grad.data. Since we are just updating the data here, we will do the calculation directly on the .data attribute. If you do a computation on a Variable instance, you'll have extra computation to do backpropagation later.

[[[00000000000000000591---f5d2c3a5c1a05086dc37ab66242849840fa45ded647e4d01985576fc8ce95cfa]]]In the for statement of the code above, the variable instances x0 and x1 are repeatedly used to find the derivative. At this time, the differential values are added to x0.grad and x1.grad one after another, so when obtaining a new differential, it is necessary to reset the differential that has been added so far. Therefore, before backpropagation, we call the cleargrad method of each variable to reset the differentiation.

[[[00000000000000000592---2382576b5005ca55eabca3bdf25261cbb72407eed482961673e2048290ed5b12]]]Now let's run the code above. Then the values of (x0, x1) are updated to finally give the following result.

[[[00000000000000000593---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000594---fb7be540353a212cc338278a53b6f655db956659b2736506ee9dfa140b9c31e4]]]The solution for our problem is (1.0, 1.0), so the result above isn't quite right, but it's close enough.

[[[00000000000000000595---7838bad2c377bf6b5505f1db328b1c0d8fb2273631cd86b12f8a1a3c98947add]]]That's all there is to DeZero basics. Next, we will use DeZero to solve machine learning problems.

[[[00000000000000000596---34772eaafb96acf5a61bc203e40981e67a6b9dbd9d0d79d39dc1db508430c98c]]]linear regression

[[[00000000000000000597---34d302c806390a54479d3953b8cc6e37a1f121394f6aa093efae0b766b0debd0]]]Machine learning uses data to solve problems. Instead of humans thinking about how to solve problems, we let computers discover (learn) how to solve problems from collected 'data'. In this way, finding solutions from 'data' is the essence of machine learning. Here we use DeZero to solve a machine learning problem. First, we will implement “linear regression”, which is the most basic of machine learning.

[[[00000000000000000598---fc93ca7dadd436c7a79d570773edb396d8b7f84e0d5efdd37f9f26e08a09225b]]]toy dataset

[[[00000000000000000599---279be4ea914fd1f989a375fa46cbfe23c9aa371b5f12bfa6631851037eef39bf]]]Now let's use DeZero to solve a practical problem. First, create a small dataset for the experiment. Such small datasets are called toy datasets. In consideration of reproducibility, the random number seed is fixed and the data is generated as follows.

[[[00000000000000000600---6430b7f05014e82450baca288bdf147ae668808dbdaf4044741e014347c9f9cd]]]# fix seed

[[[00000000000000000601---194a5244e10ed902c603496edf100e2b53c58a28a132e2bbfea4b21021aefbc2]]]Create a dataset consisting of two variables, x and y, as above. At this time, the point cloud is on a straight line, and random numbers are added to y as noise. Plotting this (x, y) point results in Figure 7-5.

[[[00000000000000000602---bbf909ed5b8011b7c0d3b58aa969e25ccb5b210f3a61d0729eded43eaefc10a0]]]
Figure 7-5 Noisy Dataset


[[[00000000000000000603---2f53d10a99562084b1054da67ef3407b599eb2fa4c027f072de60d9c07466f89]]]As shown in Figure 7-5, x and y have a 'linear' relationship, but it contains noise. Our goal is to create a model (mathematical formula) that can predict y values from x values.

[[[00000000000000000604---3ac7a8e3fd26caee5028989b7d45627796dbacf8168707c46e7f13c7e75f9d22]]]Predicting a real-valued y from a value of x is called 'regression'. Further, when the predicting model predicts 'as a line (as a straight line)', it is called 'linear regression'.

[[[00000000000000000605---71d36bbe6893ba144d34954f3e6709ee7dcf23c030ec3b54962c144734e68a78]]]Theory of Linear Regression

[[[00000000000000000606---4656682c702a7a52b0c7360b4359979c3710903d807af2edb5bb43ac949c79d6]]]Our goal is to find a function that fits the given data. Here we assume that the relationship between is linear, so we can write (here scalar). This straight line is represented as in Figure 7-6.

[[[00000000000000000607---a008e3d811800792e22158d2c244d70e48d0e8b923a2bf26f3b69f7c2ea2dabc]]]
Figure 7-6 Linear Regression Example


[[[00000000000000000608---14f73d715ef5cb2d5cd4c334de91d3c63cb167286f107ad7051585c765bae51a]]]Our goal is to find a straight line that fits the data, as shown in Figure 7-6. To do this, we need to reduce the difference between the data and the predicted values—this is called the “residual”—as much as possible. Here, we define a metric that expresses how well the model's predictions and data fit, using the following formula:

[[[00000000000000000609---2117612e8d3ab5b926e7c75f4c85fa98a7233edfb641396428c32e3d2556d493]]]In equation (7.2), assuming there are a total of points, find the squared error at each point and add them together. Then divide by to find its average. This formula is called the Mean Squared Error. In addition, in formula (7.2), it is defined as, but there is also a case where it is defined as. However, both definitions lead to the same problem setup by adjusting the value of the learning rate when solving with gradient descent.

[[[00000000000000000610---affd8a4678aa9e62b15f0685fa6bcc533d876f673c510d0c6e6cf1f9ee01b682]]]A function that evaluates the “badness” of a model is called the Loss Function. Therefore, linear regression can be expressed as 'using the mean squared error as the loss function'.

[[[00000000000000000611---3fbcd97a6a1d0f01bae2fc96946afc53afccf5d6e646f7c5cfd3c97d543509f1]]]Our goal is to find the minimum loss function given by (7.2). This is a function optimization problem. In the previous section, we solved such a problem using gradient descent. Again, we use gradient descent to find the parameters that minimize Eq. (7.2).

[[[00000000000000000612---696b2c2b92f9edba76d978ad927b92ecba6d0d6534e54a2456039367407788a6]]]Implementing linear regression

[[[00000000000000000613---c5c18c5b08149f77c0dc31d5943a84173a207b9308368d1f8b9f0fb4b0a5d9b5]]]Now let's use DeZero to implement linear regression. Here, the code is divided into two parts, the first half and the second half. Here is the code for the first half.

[[[00000000000000000614---37a43e14eb6a75bde7ebddcf5825f979ce93b39913ddd309e40949717793a998]]]# toy dataset

[[[00000000000000000615---fd28e81223ab81be7830f60b3ede70ab38848a5448c2f1debdf6481857415020]]]# optional

[[[00000000000000000616---86e243c870cce63a61028630f78b4e5a56cdff88f4ee816afcd8f8add3e55589]]]Here, W and b are created as Variable instances as parameters (W is capitalized). For geometry, W is (1, 1) and b is (1,). Also, the code above defines a predict function. There, we use the matmul function of matrix multiplication to do the calculation. By using matrix multiplication, you can perform calculations on multiple data (100 data in the example above) at once. The transition of the shape at this time is as shown in the following figure.

[[[00000000000000000617---8a3c789355bf15984d4a8631c80f3ff88312876b03a45960f71d984e1c617fcf]]]
Figure 7-7 Changes in the shape of the product of matrices (addition of b omitted)


[[[00000000000000000618---40c8d7dc48209dfa3e2fec075625a1e505794ca423106c8425b3f32d4262cae9]]]You can see that the number of elements in the corresponding dimensions match, as shown in Figure 7-7. And the shape of the resulting y will be (100, 1). In other words, each x with 100 data was multiplied by W. This will give you the predicted values for all the data in one calculation. Note that the number of dimensions of x data here is 1, but if the number of dimensions is D, if the shape of W is (D, 1), calculation will be performed for each data in the same way. For example, if D=4, the matrix multiplication is done as shown in Figure 7-8.

[[[00000000000000000619---68f649d4fffd3088b8dee14d90a72e782b281c60639059548d1b4b990d4d7cee]]]
Figure 7-8 Changes in the shape of the matrix product (when the x data dimension is 4)


[[[00000000000000000620---cf074fc4703ed79509ff2bb496607e75a6eab0113636bc31004799864f05f794]]]Matching x.shape[1] and W.shape[0] as shown in Figure 7-8 ensures correct matrix multiplication. At this time, for each of the 100 data, W calculates the 'vector inner product'. So here's the code for the second half:

[[[00000000000000000621---b99dbfa0331c73b9eb2337e3d9887ad272bf4dbf4cb3e707a31f9887389f0d61]]]# or loss = F.mean_squared_error(y, y_pred)

[[[00000000000000000622---8ddc6991549d2846150a01f04d38b75c4f2fd397e88bc0fc0e9b96d6159f7a98]]]# output every 10 times

[[[00000000000000000623---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000624---5d63eba7805f083b1046b654f5c5a258191ffaadcc9f16055a654ecffb5c9dbd]]]Here, we implement the function to find the mean squared error as mean_squared_error(x0, x1). We just implement equation (7.2) using DeZero's functions. Then update the parameters by gradient descent. In DeZero, the function for obtaining the mean squared error is provided as the F.mean_squared_error function.

[[[00000000000000000625---1f76b72b55a325e3b6f8e66d6bd0cac4a320ecae59cf2bf5a68ad6b58c8ba5c6]]]Now let's run the code above. Then you can see that the output value of the loss function is decreasing. And finally we get the values W = [[2.11807369]], b = [5.46608905]. For reference, the straight line graph obtained by this parameter looks like Figure 7-9.

[[[00000000000000000626---f08ef9acba7de6a9f8fb0e663e361f650fae7e334b3ca0a0b5d84488424705f9]]]
Figure 7-9 Model after training


[[[00000000000000000627---6572730709d90a98d1fc93850a333a1ce38c3c228a41d2f4ae0aac1e782758de]]]We were able to obtain a model that fits the data, as shown in Figure 7-9. We have successfully implemented linear regression using DeZero. This completes the linear regression implementation.

[[[00000000000000000628---0b0739891461f74a49a5d6b095aca75ec4b1959eec0fe5da0dd51a1bacb59217]]]neural network

[[[00000000000000000629---a8f5e7ed83932fd3f54f961d4c9fec08155e356169271aa6f2aa57213e132259]]]In the previous section, we used DeZero to implement linear regression and it worked fine. Once you have a linear regression implementation, extending it to neural networks is straightforward. Here, we modify the code from the previous section to implement a neural network using DeZero.

[[[00000000000000000630---05965b4f9c4527f894b941547753fef59e30327480eb679435ef050294064adc]]]non-linear dataset

[[[00000000000000000631---02b608b26770f8640b32896df9a043cb5699a80a884491ccf2b93c06b50af1c8]]]In the previous section, we used a linear data set. Here, a more complex dataset is generated by the following code.

[[[00000000000000000632---3673f70211097e500a5fcd1faf1c7072b5f1ee58e54a80261a48bdf0f53557a6]]]Here we use the sin function to generate the data. Plotting this (x, y) point results in Figure 7-10.

[[[00000000000000000633---b10dae9349a60bd58408b9233112a44740c4ffe5f2af25434471714ee1fa166c]]]
Figure 7-10 Dataset used in this section


[[[00000000000000000634---d84abebfe1e3b1d4b8873ffc03eb928895ad171398e19d29dd4235c4aff26f4f]]]As shown in Figure 7-10, x and y are not linearly related. Of course, linear regression cannot handle such a nonlinear dataset. That's where neural networks come in.

[[[00000000000000000635---4727a6676cc19503d5329dad8cd0ef30ef3836b5a7726f8b62a4e0fd363584df]]]Linear transformations and activation functions

[[[00000000000000000636---d19fe9c149eda04dc015189fca952e7b7102b8758f20ad51ae40ae076324966b]]]In the previous section, we implemented linear regression on a simple dataset. The only calculations I did in that linear regression were 'matrix multiplication' and 'addition' (aside from the loss function). A snippet of that code looks like this:

[[[00000000000000000637---697e5ceb84203955011f59b2b0305b295c3493c9236f5cbb7723a8a420a88a66]]]As above, we do the matrix multiplication between the input x and the parameter W, and add b to it. This transformation is called Linear Transformation or Affine Transformation. In DeZero, linear transformation is prepared as F.linear function. Specifically, it is used as follows.

[[[00000000000000000638---b4369a381a70978dcdb6457304cf2e92c6cf4a70a3ea80a8607da7b6bfe6c2d4]]]A linear transformation is strictly y = F.matmul(x, W) and does not involve adding b. However, in the field of neural networks, operations including addition of b are generally called linear transformations (this book also follows that rule). Linear transformations also correspond to fully connected layers in neural networks. The parameter W is called Weight and the parameter b is called Bias.

[[[00000000000000000639---c5312a7e5ce225271320b78074c699f81fc275f251308a687af1cdd647b16ad1]]]A linear transformation performs a linear transformation on the input data. On the other hand, neural networks perform nonlinear transformations on the output of linear transformations. The function that does that non-linear transformation is called the activation function. Typical examples include the sigmoid function and ReLU function (Figure 7-11).

[[[00000000000000000640---468889baa75b9a262ca843bafadd3102abcbc1c45a7981c4ac1e286b79e7fd7a]]]
Figure 7-11 Sigmoid function (left) and ReLU function (right)


[[[00000000000000000641---9742b3c5451bfcbcd7cfdb6307c861c6237b9c63f7b43be6a27e000320050438]]]As shown in Figure 7-11, the sigmoid and ReLU functions are nonlinear functions (functions that are not 'linear'). Neural networks apply nonlinear transformations like those in Figure 7-11 to each element of a tensor. In DeZero, the sigmoid function is available as the F.sigmoid function, and the ReLU function as the F.relu function.

[[[00000000000000000642---812f3b86916e5fdd7cc6f6c51241d8f3609917a16f540aa67a024403ab43a9e6]]]Neural network implementation

[[[00000000000000000643---7f6608ee83c73cd17ec1f3f9dcf24be20466e9c556920f11108e06ce8e82428e]]]A typical neural network alternates between a 'linear transformation' and an 'activation function'. For example, a two-layer neural network can be implemented as follows (I omit the code to generate the parameters here):

[[[00000000000000000644---c4dae6f2add294acd217536695801f6dfecbc5e8a6c3cc5af73f10ea43758947]]]As above, apply a 'linear transformation' and an 'activation function' in sequence. Here is the code for neural network inference (predict). Of course, getting this inference right requires training. In neural network training, we add a loss function after the inference process. Then find the parameters that minimize the output of that loss function. Now, let's try training the neural network using the actual dataset. Then I'll post the code together.

[[[00000000000000000645---e4381c42345ad5ca7e53cabd6ee7e91af8a0f019aede9388d49a6e12d3d003ee]]]# data set

[[[00000000000000000646---8c56a8edcd258fcd6e925d3894a06bd2eceda76521f8c45b3fb838b412926311]]]# ① Initialize weights

[[[00000000000000000647---fe3bc972ed11437dd29d7b0b1c3e829fde31b03118ff298bc292cf9cbab22aa8]]]# ② Neural network inference

[[[00000000000000000648---18417f0ad72992cf011bf9223b2bf5c6b7b2432ff8b3f02860db4c693db437ec]]]# ③ Neural network training

[[[00000000000000000649---dd643402f31d298af1671f2b55390bfa9e8cf25c9f65e963e199a1fa131b9599]]]# output every 1000 times

[[[00000000000000000650---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000651---6ad26c05db26a5c738b8a3b352a6693cec23fd9642f529c62f216f4799054630]]]First, the parameters are initialized at point ① in the code. where I (=1) corresponds to the number of dimensions of the input layer, H (=10) corresponds to the number of dimensions of the hidden layer, and O (=1) corresponds to the number of dimensions of the output layer. The value of I and O is 1. This is automatically determined from the problem settings this time—the number of dimensions of the input data is 1 and the number of dimensions of the output data is 1. H, on the other hand, is a hyperparameter and can be set to any integer greater than or equal to 1. We also initialize the biases with a 0 vector (np.zeros(...)) and the weights with small random values (0.01 * np.random.randn(...)).

[[[00000000000000000652---1bdae2596ea14a3107fe985f29d5e1c76cab15f1dbba87de603b20b43b457960]]]Neural networks need to randomly set the initial weights. For the reason for this, please refer to '6.2.1 Set the initial value of the weight to 0?' in 'Deep Learning from Scratch' Vol.

[[[00000000000000000653---59a8b04daeb40ee4c44f3a0d27bf7c0d2a36f10a20710b69f877afe621a805c5]]]Next, in the point ②, the inference of the neural network is performed. Then, at point ③, update the parameters. This code in ③ is exactly the same as the code in the previous section except for the addition of parameters. Running the above code will start training the neural network. And the neural network after training predicts the curves in Figures 7-12.

[[[00000000000000000654---2c6f87a269f398e23f6bf311ee4a02aa5ad896ffd736046f5f518c66eee2f0ef]]]
Figure 7-12 Neural network after training


[[[00000000000000000655---ac25180dd8448aa9425a33e420e6cffc1507b393c397c69696eb96c7dc1a597f]]]As shown in Figure 7-12, it gives a nice representation of the curve of the sine function. Overlapping the activation function and the linear transformation for the linear regression implementation successfully learned the nonlinear relationship. This is a neural network. Next, I will explain the modules provided by DeZero to make it easier to write the code implemented here. First, let's talk about layers and models.

[[[00000000000000000656---38e118b04a2f59683b4a8d82d1a44daff0d9975cd66cee2d6e11d9d7c992c16e]]]layers and models

[[[00000000000000000657---ca0e9fae5fa98818326bcbeb0fa2c67df5ff63a0bdffcebd8ac04e5d39f7f4bc]]]DeZero provides convenience classes to easily implement neural networks. First, let's take a look at the 'layers' in the dezero.layers package. This layer class has functions such as parameter management and initialization. Here we will use the Linear class that performs linear transformation. The initialization of this Linear class receives the following arguments:

[[[00000000000000000658---654775498b7bb37ae4b092f3f7308602468f2707b5b245069f68466658d8cf18]]]out_size is the output size (the number of dimensions of the output data), nobias is the flag for whether to use bias, dtype is the data type, and in_size is the input size (the number of dimensions of the input data).

[[[00000000000000000659---356f814ac6ff064106d30f716e28e1372f356fd81a683d746fde35d494f4f7d2]]]Inside the Linear class, the weights and biases used for the linear transform are initialized and used in the actual linear transform computation. The weights and biases are generated from the in_size and out_size passed when initializing the Linear class. If in_size is None, the input size is obtained when data is passed (when input x for linear transformation is given), and weights and biases are automatically initialized at that timing.

[[[00000000000000000660---2eeb87686493942b4d225984158aaf4f172069d26f0662bc33c981d98b2b97b0]]]Now let's look at the code that uses the Linear layer.

[[[00000000000000000661---b6bd8e2696aa545e452058562f219a3b4149f54406af311fa1b1ab2d4a5439c4]]]# specify output size only

[[[00000000000000000662---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000663---1bdf115d0a59ebed493fdc87270d1cbe067f6146927dd34c9a0aafe8c0bc03a1]]]As above, if you generate with linear = L.Linear(10), the linear transformation calculation will be done at y = linear(x). The weights and biases are inside the linear instance and can be accessed by linear.W and linear.b. You can also access all parameters via linear.params().

[[[00000000000000000664---8487b33a6b6627604ddfce0ee9b7b1d7fd38a5e9233e5854c72436be742833ad]]]In DeZero, you can use this layer to build neural networks by combining them like 'Lego blocks'. You can also define a neural network as a class, as shown below (this convention is also used in PyTorch).

[[[00000000000000000665---22e08e64ea8c03ab6969d7fe61b6579f1e31efe2d7568e185bbdfbc475eae05c]]]Implement your model by extending the Model class, as above. In the initialization, we generate the necessary layers and write the actual processing (forward propagation of the neural network) in the forward method. By inheriting from the Model class, you can manage all the parameters your model has. For example, you can use:

[[[00000000000000000666---b59fbc9bf37426e67d4181a8dbdcef5fa6bdc24a56c040768c77d259a9f87bb7]]]# access all parameters

[[[00000000000000000667---94ccb16be068ab011cf1183dd7529d6d410a3fa977f7b7281b1b8da1016a6b88]]]# reset gradients for all parameters

[[[00000000000000000668---32dc428135c36aa3c725f026405f482724f65c4dcf95e6147c45df506d640ead]]]You can access all parameters in turn with model.params() as shown above. Also, a method to reset all parameter gradients is provided as model.cleargrads().

[[[00000000000000000669---e983c469275e485879418eaf830a555348729e55e0762640e826828d7f8d8223]]]Now let's learn the sine function on non-linear data, this time using dezero.Model and dezero.layers. Here is the code:

[[[00000000000000000670---6d899f23a58765763821a8447872cc20e56e7c26a7590a09f97af22656e83c60]]]# Generate dataset

[[[00000000000000000671---ddbbec28f496e33c890fd2d8a9b84dea64ced5922d2874b57a2f380c8ffe88c9]]]# or model(x) works the same

[[[00000000000000000672---c9902ce9cab70304ac311b957fcfaa1c68ad590b12875c8fbe89cd3f862da4a7]]]The result will be the same as last time. However, this time, the neural network is implemented as a single class, so the code for updating parameters and resetting gradients is cleaner.

[[[00000000000000000673---4d7aa3a619487947be724b82d22e7406480b6ee5e16defab4146c484f3be6b57]]]optimizer (optimization method)

[[[00000000000000000674---27f15f00d87a6834dba6561582eb395958d8869efd2436f3e7319af5cd5b94ba]]]Finally, we introduce a class that updates the parameters of the model — the optimizer —. It's quick, but if you use the optimizer for the code you implemented earlier, it will be as follows.

[[[00000000000000000675---6d899f23a58765763821a8447872cc20e56e7c26a7590a09f97af22656e83c60]]]# generate dataset

[[[00000000000000000676---599721a8e6614688ba329cba25a982433f559ea82b20dd1dfdca53c8090c771b]]]# generate optimizer

[[[00000000000000000677---84ab0c7464a2fdb0529290e435aa1a6670da79e69974de1841eb075112fd619f]]]# update by optimizer

[[[00000000000000000678---d1c43decf14b3179dff8dcb4b6b3a463ee64bab4d420689c140952e1c39bf4de]]]I will only explain the differences from the previous code. First, import the optimizers package with from dezero import optimizers. There are various optimization techniques in this optimizers package. Here we generate an optimization method called SGD as optimizer = optimizers.SGD(lr). SGD updates the parameters by lr times in the gradient direction, as we have done so far.

[[[00000000000000000679---be1dde303f910f5aaab0ce02407ab019acf7653920d0ca94bdf88454af17483e]]]SGD stands for Stochastic Gradient Descent. In Japanese, it translates as stochastic gradient descent. 'Stochastic' here means to select data randomly (probabilistically) from the target data and apply the gradient descent method to the selected data. In the field of deep learning, it is common to randomly select such data from the original data and perform gradient descent on that data.

[[[00000000000000000680---726035a6216c721dbc9ceec14995827f6e30930bbcdf5dcdd8613990a63bfd20]]]After creating the optimizer, register the model with optimizer.setup(model). Now you can let the optimizer do the parameter update. Updating parameters is done by calling optimizer.update() (every time). By using the optimizer like this, you can leave the update of the parameters to the optimizer.

[[[00000000000000000681---febf9cb8f3f78b9e839ab2e5f982a1a84cbb7de649e56fcd380acb0c5833c000]]]There are various methods of optimization using gradients. Typical methods include Momentum, AdaGrad[6], AdaDelta[7] and Adam[8]. The dezero.optimizers package implements such representative optimization methods, and you can easily switch between them. For example, if you want to use Adam's method in the above code, just change the code in the following places.

[[[00000000000000000682---4e46081c66049ef7e4ae72aa1f8afe78cc945a805b0ba4beec16bdc5c0a6e9fa]]]Using the optimizer in this way makes it easy to switch between optimization techniques. This concludes the explanation of DeZero and neural networks.

[[[00000000000000000683---7c6a6c7aeb380a6feb3ea03428e5550c8a9dcc9d1bb35ab2b8d1e56e6523f193]]]Q-learning and neural networks

[[[00000000000000000684---1f17f9f01ff2da7b4370ccdb7e6cce2fb25a3bcf593f70efa5011dc6bc8f030e]]]In the previous chapter, we learned the TD method. Among them, I learned the most famous algorithm in reinforcement learning called Q-learning. The theme of this section is to “fuse” Q-learning and neural networks. The combination of reinforcement learning and deep learning has led to many innovations. Finally we enter the world where reinforcement learning and deep learning intersect. First, I will explain the preprocessing of the neural network.

[[[00000000000000000685---ba332336ff0b72b44b3cd59f7ced96fa511af2b4ea76416094892071355a4cf9]]]Neural network preprocessing

[[[00000000000000000686---8b1fdc94a56fc7cc025bc3f584bc292bce1d8500e2a8bb9642d2786d0e4f44c1]]]When dealing with 'categorical data' in neural networks, it is common practice to convert them to one-hot vectors. Categorical data is data such as clothes size 'S/M/L' or blood type 'A/B/O/AB'. For such categorical data, one-hot vectors are preprocessed. A one-hot vector is a vector in which only one element is 1 and the other elements are 0. For example, (0, 0, 1) represents L, (0, 1, 0) represents M, and so on.

[[[00000000000000000687---7ecfc2be9b2b0efc7c3df163896ec247e3b09241691280b9c25fee995cfd8252]]]In the '3x4 grid world' problem, the states are represented as (0, 0) and (0, 1). This state is represented by the data format of the agent's position (y, x), and can be considered as 'categorical data' because it corresponds to one of 12 locations in total. Therefore, we will convert the state in the '3x4 grid world' to a one-hot vector as preprocessing. The code looks like this:

[[[00000000000000000688---56aa7d9319905754ffa610f7dff80f743ebd61d752727022f4393c26123b2792]]]# add new axis for batch

[[[00000000000000000689---27fa9fea5992d948a7fdb7acdeb4b2af3dba30801064ea46ebc8ca5e78d91743]]]The one_hot function takes a state and converts it into a one-hot vector. In the one_hot function, we first prepare a vector with elements (all values are 0). Then set the corresponding element to 1.0 depending on the state. Also, assuming batch processing, add a new axis with vec[np.newaxis, :]. As a result, the shape of the tensor returned by the one_hot function is (1, 12) (the original vec was of shape (12,)).

[[[00000000000000000690---1b5611f5535c69a2071a37b7c0c524eea7b0792af7b47dd594df8b6e6828b9d5]]]Neural networks process data together in 'batches.' For example, if you want to process 100 pieces of data at once, enter data in the shape of (100, 12).

[[[00000000000000000691---2e352628329e71540c8b9a6864d761040f48a7e9ed873c265a32e1fece318873]]]Neural network representing the Q function

[[[00000000000000000692---6dba3fc6b1c8f7a324e113dab35a1fd55cf94916e8a4bba8e6eb49ede46aba13]]]As a refresher, so far we've implemented the Q-function as a table—a dictionary (defaultdict) in our Python code. For example, code like this:

[[[00000000000000000693---076273f9d89459846f4940c40d564b39be9f80a9a0c562760c921d1fdde5115d]]]Q inputs the pair data of (state, action) and outputs the value of the Q function. In other words, the value of the Q function is stored individually for each (state, action) pair data.

[[[00000000000000000694---791cb4884d3fafdb830feef4955c1720b00e13b18e9f3c4c456c1a704c1e0329]]]Now let's 'transform' the Q-function represented by the table into a neural network. To do this, we first need to clarify the inputs and outputs of the neural network. There are several suggestions for this. Two typical network structures are shown in Figure 7-13.

[[[00000000000000000695---13891002bf679b18a15c780595210359c77380d1f0d0f568b47e374dbcd49f89]]]
Figure 7-13 Structure of two neural networks


[[[00000000000000000696---da22141973ca4d588df04aa878326ef845f52f9d501351d777cdb5a4e8c99c0b]]]The first structure is a network with two inputs, state and action (top of Figure 7-13). In this case, the output is just one value of the Q function (not batches here, just one input). The other structure is a network that takes only the state as input and outputs the value of the Q function for the number of candidate actions (lower figure in Figure 7-13). For example, if there are 4 possible actions, output a vector with 4 elements.

[[[00000000000000000697---d95676c8117a6574b776e0493f38d382c19c71b716399f046d1af0984dd90537]]]I have listed two candidates for the network structure, but the first network structure has a problem in terms of computational cost. Specifically, the computational cost of finding the maximum value of the Q function in a certain state—the computational cost of expressing it as a formula—is high.

[[[00000000000000000698---4218db4e971a3dbeaae0e388a185b558cca4ee99b378ca51c2f2783f4c438a2f]]]In Q-learning you have to do the calculations. The calculation finds the action that maximizes the Q function in the state. In the first network structure, we must find the value of the Q function by forward propagation of the neural network for the number of candidate actions. If the number of actions is 4, we need to do a total of 4 forward propagations to find the Q function for each action. On the other hand, the second network structure is computationally efficient because it requires only one forward propagation to obtain the Q function for all actions.

[[[00000000000000000699---e6b7c4df90a77b7e8b5c892a6afe22a4fd23ac75c589a416e2f376519a009fab]]]Now let's implement the second network structure (a network structure with state only inputs). Here we implement a neural network consisting of two layers of fully-connected layers. Here is the code:

[[[00000000000000000700---befda2292a4ad876dcf0bbd684ebb5f47f1904fa90aab8351242ec468904ad19]]]# middle layer size

[[[00000000000000000701---7da203613adc2fd12e2252b07131e6cb6ef715b8bea2d4a904de3b13fafc7188]]]# action size

[[[00000000000000000702---e7993e910551f2758e431db13c71aa9216a75d98c892e46dd33544b36c7b1b84]]]# convert to one-hot vector

[[[00000000000000000703---51965024b80331120a0791cd546265c6c19ca219e891179f341ef9a11dbe1864]]]Following DeZero's method, inherit the Model class to implement the neural network model. On initialization, it creates the necessary layers. DeZero only specifies the output size to generate layers. The above example will generate two linear transform layers with output sizes of 100 and 4. And write the processing to be done in forward propagation in the forward method. In the forward method, we do the main processing of the neural network.

[[[00000000000000000704---6f8e647bfee2ff9e56214eb09dcaefff05b3e4c3368273720a15cf6b89a85786]]]We have now replaced the Q function with a neural network. Next, we will implement the Q-learning algorithm using the neural network implemented here.

[[[00000000000000000705---fee1394fe59ca483d439a374b56586385330e0805da7b182fa21360f265d94a6]]]Neural networks and Q-learning

[[[00000000000000000706---dba90a84270e7185e0b8d3eb4317eab2d4edcd35246c54793c455c9e428acc5f]]]To implement Q-learning using neural networks, we will start by reviewing Q-learning. As we learned in the previous chapter, Q-learning updates the Q-function by

[[[00000000000000000707---d776f27d16a775d48e9b94598276f62b033864ec9d0cc9b9eda3261a7b02ff54]]]This expression updates the value of in the direction of the target. Depending on this time, how much it advances in the direction of the target is adjusted.

[[[00000000000000000708---ddab8ef4f92d226aea441165af7dbe25c8ddae0b96e8b8833be2728cb7267db0]]]Now let's denote the target by . Then equation (7.3) can be expressed as

[[[00000000000000000709---a70bb268fc7a291e9d7e2bf6294c97009dec4c61ceea72721fd92e823dd4e325]]]Equation (7.4) can be interpreted as updating the Q function so that the output is when the input is . In the context of a neural network, this is the same as learning the input to be and the output to be. In other words, it can be regarded as a correct label. or scalar values, so you can think of this as a regression problem.

[[[00000000000000000710---7321d4ef0e007e48c9f6df1ef21f47fb157baee9e4ef59f681ba270d62ef0eb3]]]Based on the above, we will implement an agent that performs Q-learning. The code is shown here in two parts. Here is the first half of the code.

[[[00000000000000000711---b5d714c053ecaa4ad05d2da66a3d43dbfa849f5721d99d59c4cb921a13084da6]]]Initialization of the QLearningAgent class initializes the neural network and optimizer. And set the neural network in the optimizer.

[[[00000000000000000712---8df4de61de23b5f4187d68da3e438a5b86c32458e21375cae47b73584e59d6a3]]]The get_action method selects an action with -greedy. In other words, choose a random action with a probability of , otherwise choose the action that maximizes the Q function. Note that the state of get_action(self, state) assumes that the one-hot vectorized state is input. So here's the rest of the code in the QLearningAgent class.

[[[00000000000000000713---369d3393dbad5afde1129e555a18f4d44cb8cdf9f228fa8668595e9c0ce8aa2e]]]The update method updates the Q function. First, find the maximum value of the Q function (next_q) in the next state. However, if done is True, i.e. next_state is the goal, the Q function at next_state will always be 0, so next_q is set to 0 (np.zeros(1) to be exact).

[[[00000000000000000714---da5bb88abf9c068a5423d14f426142d6d9f24c305218cd59786fad8715236d33]]]next_q is used to create the correct answer label. In supervised learning, we don't need a gradient on the correct labels, so we do next_q.unchain() to exclude next_q from backpropagation (unchain means 'unchain'). As a result, next_q becomes a simple number, and even if backpropagation is performed later, the gradient calculation related to next_q will not be performed, and unnecessary calculation can be omitted.

[[[00000000000000000715---76c233dc912398f0ca6ed3a1006b0466fcdd3d1961f4e6875dcbc9251452e7cc]]]Next, find the target and find the Q function (q) in the current state. Then find the mean squared error of target and q as the loss function. Finally, backpropagation follows DeZero etiquette to update the parameters.

[[[00000000000000000716---45d7a2a66701cf313a40af2d4a7418a9dd9679e89743bd1d6b41b588e1d2b3fa]]]In the code above, the if statement is used to switch the calculation of the target according to the done flag, but it can also be implemented as follows without using the if statement.

[[[00000000000000000717---23465b21a194844a21b6d7ad40ddf528e52a9d5307f57bb610e61ce3990188c8]]]In Python, converting a bool type to an int type converts True to 1 and False to 0. By using (1 - done) as done = int(done), the code above will achieve the same result as before. This code will be useful in the next chapter when training with mini-batches.

[[[00000000000000000718---37047aba808b39d160e18b87b33d71fd28ba17f10cf2572ce4f44e536d563c64]]]Above is the code for the QLearningAgent class. Now let's run the agent. Here is the code:

[[[00000000000000000719---995b396d9e9b618ec0409178a0384b656ae55dfa603241b54aa9a27bb17f8471]]]# number of episodes

[[[00000000000000000720---a2812a5a301555bd761b1cfc03cadf09f253436e8073492227492139a3506cdf]]]Here we set the number of episodes to 1000 and record the average loss per episode. The result should look like Figure 7-14.

[[[00000000000000000721---88ff8be1bcf96ba91fec00d935c95f71e5260848a6ec5b41b5eeea116084d930]]]
Figure 7-14 Changes in Loss by Episode


[[[00000000000000000722---69819cc6ad868b0bd85ea503efb61e0a6b88bd98ea58c8b393a519bd2ab3ee65]]]In reinforcement learning using neural networks, plotting the loss often does not give stable results. Figure 7-14 shows a large range of change, but from a large perspective, we can see that the loss is getting smaller with each episode. Also, the Q function finally obtained by the above code and the measures to make the Q function greedy are shown below.

[[[00000000000000000723---768389ab3446e96e9d5a6d72583a9092be26858845d19e22ba45f4ea18993b9b]]]
Figure 7-15 Q-functions and policies obtained by Q-learning using neural networks


[[[00000000000000000724---ae2e31e80db44e64f101bd7c0467dbd0c36a32955db3c94c81b92e594964db6b]]]Results vary each time, but generally good results are obtained. Figure 7-15 is not a completely optimal policy, but by increasing the number of episodes, a policy close to the optimal policy can be stably obtained. This completes the implementation of Q-learning using neural networks.

[[[00000000000000000725---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000726---af66887601c993d980f659189e0f36a2cfd4b347a71b31eb3273fc13d68da212]]]In this chapter, we learned how to use a framework called DeZero to implement neural networks. DeZero is a framework designed with the same philosophy as modern deep learning frameworks like PyTorch. With DeZero, you can treat your data like NumPy's ndarray, and you can also find the derivative by backpropagation. This makes it easy to implement things like gradient descent that use derivatives to update parameters.

[[[00000000000000000727---59691ed575e9ec3d4b360e6933b3f6099d0bd7f96285682a246df342c7b2dff3]]]In this chapter, we used DeZero to solve basic machine learning problems such as linear regression. We also implemented a neural network for non-linear datasets and confirmed that it could train correctly. DeZero provides useful classes for implementing neural networks, such as 'layers' and 'optimizers'. By using them, you can easily implement neural networks.

[[[00000000000000000728---bacb1d4996bd885f10fd42499885fb5b133dce36719a5c6369bd367748a76c41]]]At the end of this chapter, we implemented Q-learning using neural networks. The underlying principle is the same as Q-learning, which we learned in the previous chapter. If you understand the mechanism of Q-learning, applying a neural network to it is not that difficult. We have entered the world of Deep Reinforcement Learning, which is a combination of Reinforcement Learning and Deep Learning. Now it doesn't matter if the size of the state or action grows.

[[[00000000000000000729---edb06b511c1845b562ff7af9164a2b39de6c1849f69c4de4ce8d0b32f70b20c6]]]Chapter 10

[[[00000000000000000730---6da2c2373e335a1f171768e62f0d68fb140802b01af1e2353c1e6aeb43258307]]]further

[[[00000000000000000731---585679c566c0bca72f5327cf68173c92f69eeb38aabcb2c54502a2e8d0259039]]]We've learned a lot about reinforcement learning so far. In the first half of this book, we learned the basics of reinforcement learning, and in the second half we looked at some important techniques using deep learning. Here we introduce a more advanced algorithm as modern deep reinforcement learning. First, we will show an overview of deep reinforcement learning, then we will look at the algorithm of the policy gradient method series, and then the algorithm of the DQN series.

[[[00000000000000000732---80785aeb1607edb1b4b65fc2ac65240ef3e405adbfeabe51eafa830126f8b62d]]]This chapter also introduces important research examples as case studies of deep reinforcement learning. We will introduce some examples that have made great achievements in society, such as board games such as Go and Shogi, robot control, and semiconductor design. Finally, we conclude the book by discussing the possibilities and challenges of deep reinforcement learning.

[[[00000000000000000733---9305ddc9c0ef02258f82205fb2f935daa7505cb981387d6fe374c5a233b2bc2e]]]Classification of Deep Reinforcement Learning Algorithms

[[[00000000000000000734---9e03524bc09cc5778b42fc1ce24e2f415527c3f29db9048524f4e3445f53ffd2]]]Here, we classify deep reinforcement learning algorithms according to their types. This classification method is based on reference [20]. Now let's take a look at Figure 10-1.

[[[00000000000000000735---4c21a3007a4c6350b8a599483dd89e3cb68ee8e8a80ee90d2c75308602808e16]]]
Figure 10-1 Classification diagram of deep reinforcement learning algorithms


[[[00000000000000000736---11fb7b4b9f25cd9af8f301fce580828a9f1267ceb79be457ea65b4dbae4a248f]]]Based on Figure 10-1, I will explain how to classify deep reinforcement learning algorithms. The first major fork in the road is whether or not to use a model of the environment—the state transition function and the reward function. The method that does not use a model of the environment is called a model-free method, and the method that uses a model of the environment is called a model-based method.

[[[00000000000000000737---5c6f2dcf5fb78ca76cfe61d2a1956668efe34b52fd3176340edf8516e5525e60]]]Model-based techniques can be divided into two categories: given a model of the environment and learning a model of the environment. Given a model of the environment, the agent can solve the problem by Planning without taking any action. In Chapter 4, Dynamic Programming, we accessed a model of the environment and used dynamic programming to solve the problem. Also, in board games such as Go and Shogi, a model of the environment can be tackled as a known problem setting. Again, you can use the approach of using a model of the environment. Famous algorithms include AlphaGo and AlphaZero (more details are given in 10.4.1 Board Games).

[[[00000000000000000738---2c4a48383af7f516af3da17fe4ffb87a4f1c3075d24fdcadea939642e646282d]]]If a model of the environment is not given, it is conceivable to learn the model of the environment from the experience gained from the environment. The learned environment model can be used not only for planning, but also for policy evaluation and improvement. Methods in this area include World Models [21] and MBVE (Model-Based Value Estimation) [22]. This field is currently being actively researched, and it is expected that it will be effective in realizing general-purpose intelligence similar to that of humans in the future.

[[[00000000000000000739---389dc11b0a69ec9db2f9748720264a172010dea398941d246ac4a63c9d71f707]]]There are several problems with techniques for learning a model of an environment. The biggest problem is that the agent only gets a subset of the sample data that the model of the environment produces. This can lead to deviations (bias) between the learned model of the environment and the actual environment. Even if the agent behaves well against the learned model, it may behave badly in the real environment.

[[[00000000000000000740---0cc64a3ae5cabee762a3f39e0b77205906bb4e5ff4fbb29cdc602b383c0e8183]]]The approach of learning a model of the environment has great potential, but so far model-free methods have been more successful. The main focus of this book is also model-free methods. Model-free methods can be classified as policy-based methods, value-based methods, or methods that combine both.

[[[00000000000000000741---29d921f6a1316a83d491ff7d8474db3c1d30fb65cc33bb7d83accac15ea6fb6d]]]Policy-based methods include the policy gradient method and REINFORCE. Actor-Critic is a 'policy-based and value-based' method (they were learned in Chapter 9, Policy Gradient Method). In ``10.2 Algorithm for developing the policy gradient method series'', we introduce an algorithm developed from the policy gradient method. A value-based technique is one that models and learns a value function. The most important technique among them is DQN. In '10.3 DQN Sequence Evolution Algorithm', we introduce an algorithm that evolved DQN.

[[[00000000000000000742---b9ec1511b834a89e7182b1206d43bfa9f006457c99b3d9b8ab761803b30cbc4b]]]Evolution Algorithms for Policy Gradient Series

[[[00000000000000000743---9851ca3bea2f1d571d9eaf32ccf39d228de5418413a173a0cb214594553eb8f9]]]In 'Chapter 9 Policy Gradient Method', we explained policy gradient algorithms such as REINFORCE and Actor-Critic. Here, we introduce a more advanced algorithm as an evolutionary system. The algorithms we introduce are:

[[[00000000000000000744---ee477f518b3dd3fb8bc1046419c09725315049516ec89e34d81cf8a379e492e9]]]A3C, A2C (algorithms that perform distributed learning)

[[[00000000000000000745---b2d6e286a669a8a1298c28f628aecec0c0ecce54f6eab6467a541ae30da602ac]]]DDPG (algorithm with deterministic policy)

[[[00000000000000000746---2a92aeb5830c0b048a67f92acc3bee7634cce6cf60afe0d470f4548d76a4ecd4]]]TRPO, PPO (algorithms that add constraints to the objective function)

[[[00000000000000000747---61d267f154958081f0eec85662306353302f8ba39f993bef3ed4759b6c33d9d4]]]Here, based on the characteristics of the algorithm, it is divided into three groups as above. Start with A3C and A2C.

[[[00000000000000000748---6a5c58dc3e5dde0831865273a93dca5ed9f88ec3c1fbe8684dc46d476c3be525]]]A3C[23] stands for 'Asynchronous Advantage Actor-Critic'. It's called A3C because it has three 'A's and one 'C' in its name. A3C is characterized by being Asynchronous. Asynchronous here means that multiple agents run in parallel and update parameters asynchronously.

[[[00000000000000000749---739caf7425952fc860aea330985bec890f7d9e9554b06df0a1c2e30873b901f8]]]In computer science, 'concurrent' and 'parallel' have different meanings. Parallelism is a technology in which only one job (processing) is performed at a given time, but by switching between multiple tasks at high speed, it appears as if multiple tasks are being performed at the same time. Parallelism, on the other hand, is the simultaneous execution of multiple tasks in (physically) separate locations.

[[[00000000000000000750---92e8c3084a70120b5b32035d13f9b5dc559e139ea340b5c46653d2edb81e55d5]]]A3C learns policies using Actor-Critics modeled by neural networks. And use one global network and multiple local networks, as shown in Figure 10-2.

[[[00000000000000000751---92174c00b874bec39a585ba46ab226e21f9e29a031230871fddf907dac36bb30]]]
Figure 10-2 Architecture of A3C


[[[00000000000000000752---2ecde4e717609825e4493aaced92799185b46010d8560b7d70c638d8385aed3d]]]Local networks play and learn independently in each environment. Then, it sends the learning result, the gradient, to the global network. The global network asynchronously updates weight parameters using gradients from multiple local networks. While updating the weight parameters of the global network in that way, periodically synchronize the weight parameters of the global and local networks.

[[[00000000000000000753---81c4d155e7b0fd055515653ff69477502971295367888bee360e851ef5d35812]]]The advantage of A3C is that it speeds up learning by having multiple agents running in parallel. Furthermore, since the agents act (explore) independently in parallel, various data can be obtained. This can reduce the correlation of the training data obtained overall, making the training more stable.

[[[00000000000000000754---eb9fd5e51b4f21a0f7934690fc12ff5ace0338eaee9def8305ccf46d504ada4f]]]DQN used a technique called Experience Replay to reduce the correlation of training data. This is a method of saving empirical data once in a buffer and randomly selecting multiple pieces from there. It can reduce the correlation between training data. However, experiential recall cannot be used in policy-on methods. Experiential replay also has the disadvantage of increasing memory usage and computational complexity.

[[[00000000000000000755---75ea335b3a2892885d9c4408f1cbcd20530041df37facc01b51d97d9c8570dfd]]]Parallelism is a general idea that can also be used in policy-on-policy approaches. Running multiple agents in parallel reduces data correlation (without resorting to experiential replay). Since the publication of the A3C paper, parallel processing has been used in many studies and has become a trend.

[[[00000000000000000756---7030bcbf4a43d16ad922dae539bf77822f4402e973aa85608bfb8f4abe51fc57]]]A3C's Actor-Critic also shares neural network weights. As shown in Figure 10-3, one neural network shares weights and outputs policy and value function. This kind of parameter-sharing network structure works effectively because it can be expected that layers with policies and value functions close to the input will have similar weights.

[[[00000000000000000757---c07e41f5f1b63914cc035fe9b707cd78bfd3b0dc0ea832641b8c3672db0270a1]]]
Figure 10-3 A3C network configuration


[[[00000000000000000758---2e0726b458c11211b4aba88251edc0ea81020b5fea5bc18922ecfcf6354e1740]]]Next is A2C[23]. A2C is a technique to update parameters synchronously instead of asynchronously. It is named by removing the first A (Asynchronous) from A3C. A2C can be realized with the configuration in Figure 10-4.

[[[00000000000000000759---bf3524355dfb4b56fcf56b6e92956b2f9159804fe5a07f262e1e777688be7e2a]]]
Figure 10-4 A2C architecture


[[[00000000000000000760---5d85ae9e1df1bb6f1ed8485ca5a3a50cd455efcf2a728a6a35b3916a6d51009b]]]Agents operate independently in each environment, as shown in Figure 10-4. Therefore, the state at time is different in each environment. Therefore, the state of each environment at the time is collected (synchronized) as a batch and trained by the neural network. At this time, the next action is sampled from the output of the neural network (policy probability distribution), and the sampled action is passed to each environment.

[[[00000000000000000761---7671b20cbed8c94bf2ac9328ad5d2ffbc81029349bdd1ad96c1fe4f7bfeb138c]]]Experiments have shown that even synchronous updates with A2C do not degrade performance compared to A3C. Furthermore, A2C is easier to implement and makes efficient use of computational resources such as GPUs. From that point, A2C is more often used in practice.

[[[00000000000000000762---33bce5003cdb13bd977c3cffbc9b65c14da4c221634c8f3799a51d5869554923]]]A3C needs to run a neural network in each environment. So, if you have 1 environment, ideally you should have 1 GPU. On the other hand, in A2C, the parts that run the neural network are grouped together. Therefore, it is possible to calculate with only one GPU.

[[[00000000000000000763---f1dbad8a8d006b707e4be19e4d850e9d476702502811ac90be5a8aaff449c6e0]]]Techniques such as the policy gradient method, which models the policy directly, can also be used for continuous action space problems. For example, a neural network representing a policy can be designed to output the 'mean of a normal distribution', as shown in Figure 10-5. Actual behavior is obtained by sampling from that normal distribution.

[[[00000000000000000764---5a329d06a323d967f1c5e74aeef832734dd0fc7035bce1b58bf9801819e22dd5]]]
Figure 10-5 An example where the policy neural network outputs a probability distribution of continuous values


[[[00000000000000000765---e4956e584c07c5fb9e050685a5d3ad58f9b9301e609f516500fd615c10cfbe6d]]]DDPG[24] stands for 'Deep Deterministic Policy Gradient method'. As its name implies, it is an algorithm designed for continuous action space problems. Neural networks directly output actions as continuous values, as shown in Figure 10-6.

[[[00000000000000000766---15bb9009573de8abef0dc0e685c993a8eaa78e70b6167a76e7e4114c46e9e327]]]
Figure 10-6 Neural network directly outputs actions as continuous values


[[[00000000000000000767---4b91666eba0932ee66870a9ac62a125edfa744e4b6289bb9ea03b7c60c56c6aa]]]A DDPG policy is deterministic because the action is uniquely determined when a state is entered. DDPG incorporates this deterministic policy into DQN (Deep Q-Network). Here, we denote the neural network representing the policy by the neural network representing the Q function of DQN. are the parameters of the respective neural network. DDPG then updates the parameters through two processes:

[[[00000000000000000768---0228780db74760cc16c67950c0767a827997558dfa48d59ab13ecea5b98f7497]]]Update the policy parameters to increase the output of the Q function

[[[00000000000000000769---7c03a90c411026b297c77314c03f37373a24e0f2bd872217264694cd87a745f4]]]Update Q-function parameters by Q-learning with DQN

[[[00000000000000000770---13a8af002ae82d2c74cfeedede485e5833dd8f4c2669987e3b8db2796741d45b]]]Let's start with the first study. It uses a combination of two neural networks to update the parameters of the deterministic policy to maximize the output of the Q function, as shown in Figure 10-7.

[[[00000000000000000771---c695d75ff1b7d2efa3677bddef8513247131a4a0c3612f6be772d7470402d0e9]]]
Figure 10-7 Flow of calculation by two neural networks


[[[00000000000000000772---73a28f39841ece5b3e09de99ebd0a6b8d0b4c644c68a9b40b38cc95ef1d91ab9]]]The important point in Figure 10-7 is that the output action is a continuous value, and the output is the input as it is. This allows backpropagation through two neural networks. The backpropagation gives the gradient (here the output of the Q function). Then you can use the gradient to update the parameters.

[[[00000000000000000773---19d0ed7234a216719a89485a2eec6c2c32afb28070d0e02d15ea94a94c4b3703]]]In Figure 10-7, if we were to sample an action by a stochastic policy, the backpropagation would stop at the point where we are sampling (beyond that point, only a gradient of 0 would propagate). In that case, it is not possible to update the policy parameters.

[[[00000000000000000774---b9d3d5de8609e56403a1f7f60005d1d6c19b0ae739d5ed7301695cf7d891ad8b]]]The second learning is Q-learning with DQN. The method was explained in 'Chapter 8 DQN'. However, in this case, we can compute efficiently by using a deterministic policy.

[[[00000000000000000775---1e776592996e5b032a873ceb6103e200717c32a382d5add2a0a96e704e586c14]]]The update to the Q function in DQN was to make the value of (or close to) Here, with the first learning, the policy outputs actions that increase the Q function. Therefore, we can use the following approximation:

[[[00000000000000000776---b3a998188c3cb1cf9725ee6f38a913171e9b3e149f0c0ea9350f7f8b6e1a3cfb]]]Calculation to find the maximum value generally requires a lot of calculations. Complex optimization problems must be solved, especially when the action space is continuous. In DDPG replace with forward propagation of two neural networks (Figure 10-7). This simplifies the calculations, and efficient learning can be expected.

[[[00000000000000000777---664ad648a2f6289501e143e0680fad1e62d3cf99b38b6a6e0675b279d548bee4]]]DDPG also incorporates ideas such as 'soft target' and 'search noise'. Soft target is a method that makes DQN's 'target network' 'loose'. Instead of periodically synchronizing the parameters of the target network with the parameters being trained, we approach the direction of the parameters being trained each time. Search noise is a technique that introduces randomness into behavior by adding noise to a deterministic policy. See paper [24] for details.

[[[00000000000000000778---a51015c02e21478dc6d6ee1f4998f4c9ccb7344ef425b95f98cea4130a42e7d1]]]In the policy gradient method, the policy is modeled with a neural network and parameters are updated by the gradient method. The problem with the policy gradient method is that the 'direction' in which the parameters should be updated is known by the gradient, but the 'step width' of how far to proceed is unknown. If the step size is too large, the policy will be bad, and if the step size is too small, the learning will hardly progress. TRPO (Trust Region Policy Optimization) [25] solved this problem. In Japanese, it translates to 'Optimization of Trust Region Policy'. As the name suggests, it allows you to optimize your policy within the trusted region—that is, with a reasonable step size.

[[[00000000000000000779---37dcfc80d1ba6272dd84e18c99154ecee1b4510eed452a942182e198c7e4289f]]]Well, there is KL divergence (Kullback-Leibler Divergence) as an index that measures how similar two probability distributions are. In TRPO, we use the KL divergence before and after the policy update as an index and impose a constraint that the value does not exceed a threshold. In other words, it is regarded as a problem of maximizing the objective function under the constraint of KL divergence. By imposing this constraint, a reasonable step size can be achieved.

[[[00000000000000000780---2ecd66b1f80ff44fc25f4164244b85a405f2c4c0d9e1f691cfb24f9a15e88008]]]The details of TRPO (especially the mathematical derivation) are complicated, so I won't go into any more technical details. The important point is to impose a constraint so that the policy does not change too much in one update (learning is performed within that constraint) in order to have an appropriate step size when updating the gradient. The rest is a constrained optimization problem, which can be solved using mathematics.

[[[00000000000000000781---08568f102cb99ece6e871f0aefb98bfa65ec2cb58d0eeff42dc08162f423a265]]]Solving constrained optimization problems with TRPO requires computation of the second derivative, the Hessian matrix. The Hessian matrix is computationally intensive, and computational complexity is the bottleneck. PPO (Proximal Policy Optimization) [26] is a method that has improved this problem. PPO is a simpler version of TRPO and is often used in practice because it has the same performance as TRPO while reducing the amount of computation.

[[[00000000000000000782---163f8df2f7b5f41faa89ab12811e9b4b5187ffe024d19227dd750ae911dde7c9]]]DQN sequence evolution algorithm

[[[00000000000000000783---f7562b4b5b681398bffa06e3a56215783db4f2edc0a515c8469f9cc60b196311]]]DQN is the most important algorithm in deep reinforcement learning. Even now, many extension methods based on DQN have been proposed. Here, we introduce a particularly important algorithm among the algorithms developed from DQN. The algorithm we will discuss is shown in Figure 10-8.

[[[00000000000000000784---d7f1715f9f66fad9340cd704b8f475722a23e53393d950dc8831895dc0a6d0a0]]]
Figure 10-8 Algorithm developed from DQN


[[[00000000000000000785---16fe3d1b4e656e8482bfd794ceff52af90bd07ddb8411c612c9ce69f93319059]]]As shown in Figure 10-8, modern deep reinforcement learning has developed from the origin of DQN with improvements. The following three techniques have already been explained in '8.4 Extending DQN'.

[[[00000000000000000786---f90a823790719933941b8bc7d8980cdf0fb82d8054fa1f23a79f6974dcef5126]]]Prioritized Experience Replay

[[[00000000000000000787---acbbc355fa35989d2568248530fb9c70509b97a7183e725df31157c9558168d9]]]Here, we will look at the other three methods in turn.

[[[00000000000000000788---cb2fe8623f217f654b87cf2efff74805652baac4479be0bf8ea42dc024112a14]]]Categorical DQN

[[[00000000000000000789---3a7b2d88b113ca60d754867d67ccaa5b384a394418019ccd81b6f17049bfddc3]]]First, we describe a technique called categorical DQN [27]. First, let's review the Q function. The Q function is a formula

[[[00000000000000000790---9de867c66b932dd9422daf600289f51a54a3e76238400d11530aa052d5fdfc7a]]]is represented as A characteristic of the Q function is that it represents a single value, the expected value, for the probabilistic event of profit (Fig. 10-9).

[[[00000000000000000791---72b2ca03396e3048e009b2bc4d8296fa16cf6edd7f72fab86181f9e53594b08d]]]
Figure 10-9 Relationship between Probability Distribution of Earnings and Q Function (here probability distribution of earnings is represented by )


[[[00000000000000000792---8e5bccae8c874ef0144878a1efce4c0a6d2f2cc66d956af4310200e0faa10206]]]In DQN (and Q-learning for that matter), we learn a value expressed as an expected value called the Q-function. There is an idea to develop this and let it learn the 'distribution' instead of learning the expected value of the Q function. This idea is called Distributional Reinforcement Learning. In distribution reinforcement learning, we learn a probability distribution of returns called .

[[[00000000000000000793---941856506caf6501d8f66ffd0870007fe32c392e18cd7a4b9109e2e9685be65e]]]A technique called categorical DQN is based on distribution reinforcement learning. By 'categorical' I mean modeling it as a categorical distribution like Figure 10-10.

[[[00000000000000000794---b04403770af260ad60805e0af581879a161b6e0afa55b4d651381e98f595c42a]]]
Figure 10-10 Modeling by categorical distribution (blue line is 'true distribution')


[[[00000000000000000795---9b4bd43dbeafc733d4388079a66ee37376695464a455fd2d0b7771f8f5efad7c]]]A categorical distribution is a probability distribution of belonging to one of multiple categories (discrete values). As shown in Figure 10-10, the return values are divided into regions (called 'bins') and the probability of falling into each bin is modeled as a categorical distribution.

[[[00000000000000000796---c72a6ffafcd70f94cae1810e8fbbf620b1b8305c8c4c5500d04e495208ef0359]]]Categorical DQN models returns as a categorical distribution and learns the shape of that distribution. To do so, we derive a categorical version of the Bellman equation and update the categorical distribution accordingly. By the way, this method is sometimes called 'C51' because it performed best on the 'Atari' task when the number of bins in the categorical distribution was 51.

[[[00000000000000000797---7184e1c96d29e9e61a5dc56c8ad6ed646afbd9d87f00aa7f49d4804f9401989b]]]DQN selects actions by the -greedy method. In other words, choose a random action with a probability of , and choose a greedy action (the action with the largest Q function) with a probability of . In practice, 'scheduling' is often done by gradually lowering the value as the episode progresses. The problem here is the value of . is a hyperparameter and how it is set has a large impact on the final accuracy. However, there are various candidates for the value of .

[[[00000000000000000798---a71834b77681b937dd4297f7bbe7666f05b23f305a92452b829f8898249f966f]]]Noisy Network [28] was proposed to solve the configuration of Noisy Network builds randomness into neural networks. By doing so, you can choose actions according to the greedy method rather than the -greedy method. Specifically, the fully connected layer on the output side uses a “noisy fully connected layer”. In a 'noisy fully connected layer' the weights are modeled as the mean and variance of a normal distribution, and (on each forward propagation) the weights are sampled from the normal distribution. This adds randomness to each forward propagation, changing the final output.

[[[00000000000000000799---16facfb7885bf901ca1c5731f2034fe1b25c096110814da6f89f2fa17a279f95]]]So far, we have introduced various algorithms as extensions of DQN. A technique that combines all of these is Rainbow [29]. Rainbow uses all of the following techniques combined for the original DQN.

[[[00000000000000000800---7a6d964d7e19ee3bad06b51aced886426bda8df659f07be8263ff3398bb70e81]]]Prioritized experience playback

[[[00000000000000000801---cb2fe8623f217f654b87cf2efff74805652baac4479be0bf8ea42dc024112a14]]]Categorical DQN

[[[00000000000000000802---1efaf3820942022f7009141d9703e6a3c27856d0a1d593fa51eb5c751cfd19df]]]In the experiment using 'Atari', the results are shown in Figure 10-11.

[[[00000000000000000803---94c6765a2a90c3e275a5cb1a5785892b1cc78bc53ed6afc86522d6e35dd0b0df]]]
Figure 10-11 Accuracy comparison between Rainbow and other methods (Figure taken from [29])


[[[00000000000000000804---69e2fcf5b6cee6ae7b59a1727ce031a28e80bd9a268583f9c56f61b19feb50b4]]]The horizontal axis in Figure 10-11 is the number of images used for learning (however, the number of images actually used is the number multiplied by 1,000,000 on the horizontal axis). The vertical axis is the normalized score compared to general people, and the higher the score, the better the performance. As shown in Figure 10-11, we can see that Rainbow's performance is dramatically improved compared to other methods.

[[[00000000000000000805---2c16a8a1319bf313172267e81f5f5d60f8ceff78fb0f4f57896ff76b19613fcc]]]Algorithms developed after Rainbow

[[[00000000000000000806---18177520a504bea98c7399f6708814ce9c2cc5a670f96d1ead9608832d52e308]]]Since Rainbow was proposed, distributed parallel learning using multiple CPUs/GPUs has achieved great results. Also known as distributed reinforcement learning, it trains in multiple execution environments. A well-known distributed reinforcement learning method is Ape-X [30]. Based on Rainbow, Ape-X makes multiple agents act independently on different CPUs and learns while collecting experience data. All distributed and parallelized agents have different search rates, which collect diverse empirical data. Learning progresses quickly with distributed parallelization, and at the same time performance improvement is achieved with diverse empirical data.

[[[00000000000000000807---228aefefebb652de8fadb23911c33c34b82aa8ec17c0d8bfd9ea002f0a2eb9a8]]]A further improvement of Ape-X is R2D2 [31]. R2D2 uses Ape-X plus RNN (recurrent neural network) to process time series data (actually LSTM is used). It's a simple idea, but we've seen a lot of ingenuity for learning with RNN, and we've succeeded in further improving the performance of Ape-X. By the way, the origin of the name of R2D2 is two 'R' of Recurrent and Replay (experience reproduction), and two 'D' of Distributed and Deep Q-Network (Of course, 'R2D2' is named after Star Wars. are available).

[[[00000000000000000808---e824995b10f7674c4d5584047a7207b67479d9f9bc939eba9077b61ad94ccf51]]]Next is NGU [32], which is a further evolution of R2D2. NGU is an abbreviation for 'Never Give Up'. As the saying goes, even difficult tasks—especially those with sparse rewards—can be explored without giving up. NGU adds a mechanism called intrinsic reward based on R2D2. Intrinsic reward means that the more different the state transition is from the expectation, the more “surprised” you are, the more you reward yourself. For sparse tasks with near-zero rewards, intrinsic rewards can encourage behavior based on curiosity. Along the way, we can (hopefully) find a policy that maximizes the normal reward.

[[[00000000000000000809---ecf39f51bb216062aec285a69a6e2e9fb3e966e69fff28c7a83b0bbfc96f8867]]]Children (especially babies) often play in search of something new and surprising. Act out of curiosity rather than learning for a specific purpose. Intrinsic reward aims to incorporate such curiosity-driven behavior into the agent.

[[[00000000000000000810---8f2fb728b57a817f501eb6f64ccd7d1cf02a51defbb420c19a8e304a2a39b524]]]Finally, we introduce a technique called Agent57[33]. Agent57 is a technique developed from NGU. Importantly, we improved the intrinsic reward mechanism and used a mechanism called a 'meta-controller' to provide flexibility in policy assignment to agents. The Atari (more precisely 'Atari 2600') has a total of 57 games. Agent57 manages to outperform every game. This was a first for reinforcement learning algorithms.

[[[00000000000000000811---869fd38a06a67e5ff426a33e5a00ba7aaf9cafb6ae2323bb02a7a7ccf161e640]]]Case Study

[[[00000000000000000812---b9376f051c3fb42757ace728729eef1390a9e83a70173a925fd7e71a134fba97]]]Well-known examples of deep reinforcement learning are video games and board games such as Go, but it is also producing results in various other fields such as robotics, self-driving cars, medicine, finance, and biotechnology. Here are some famous examples of how deep reinforcement learning is used.

[[[00000000000000000813---902943a66a2eb0058645e7aa36e91c04c7a53675db24ac61a43e15fe67caefd4]]]Board game

[[[00000000000000000814---a19c4b3bf723b0762019d2afce7195e0e9e6bb8aa7179c36f02a497e40074906]]]Board games such as Go, Shogi, and Othello have the following characteristics.

[[[00000000000000000815---2e9d7407f24159339cf03c429a598d673e8fdeaae29c079f3b4c569eb97bd72a]]]All the information on the board is known (knowing all the information is called 'complete information')

[[[00000000000000000816---c7e68c16fb46657790fc515d44a4e34327d68e6ebd182e4657fbaa46b8f07480]]]If one wins, the other loses.

[[[00000000000000000817---359783622e102eff226f1d85010249b7368a0d2ff6ad9a033f89fc630f2b76f7]]]There is no accidental element in state transitions (decided deterministically)

[[[00000000000000000818---28430e1b099fe2d0e9e2e1ee157aa485b4c3139eb26e4ee401f77a7bde50935d]]]The important thing in a game with this kind of character is 'reading' - if I make this move, the opponent will make that move, then I will make this move... and think about various possibilities in the future. You can find good hands in The 'reading' in this board game can be represented as a tree structure, as shown in Figure 10-12.

[[[00000000000000000819---a6e93582e75f9d2e8bee581c68e079bfce95cbb1db7b448fe93b3b5fdebca51a]]]
Figure 10-12 Game tree using Othello as an example


[[[00000000000000000820---de48f0d9e36389e0d104f5c346c932a6b607fe47350b08cb8cd03bfbe4ad528a]]]As shown in Figure 10-12, a game tree represents the board surface with graph nodes and the pointing hands with edges (arrows). If we can expand the entire game tree, we know all the outcomes so we can find the best move. However, in the case of Go and Shogi, the possible states (patterns of how stones are placed) are astronomical numbers, and it is practically impossible to expand the entire game tree. Therefore, it is necessary to search the game tree efficiently.

[[[00000000000000000821---ff20c529c46dc273059aa813cd45bb2bbfcba353ae2c5bea39c6d737122ccc7b]]]All states (nodes) of the game tree are board game dependent. It is 10 to the 58th power in Othello, 10 to the 123rd power in chess, 10 to the 226th power in shogi, and 10 to the 400th power in Go.

[[[00000000000000000822---9c07d82c4a67eefa7195c23e90d8eda7aff73fd612cb19d48bfb948b974e1ee0]]]Monte Carlo Tree Search (MCTS) is a method of approximating tree expansion by the Monte Carlo method. To assess how good a board is, we let two players play random moves until they win or lose. Repeat such trials (games starting from a particular board) and tally the wins and losses to see how much you have won. It represents the approximate “goodness” of the board. Random play until a conclusion is reached is called 'Playout' or 'Rollout'.

[[[00000000000000000823---bc9c2147020fdd9857c956f3810db1fd684d0ad749952320daacb73a603595b1]]]A game flow using a Monte Carlo tree search evaluates each possible move on the current board and determines the next move based on the result. This is the basic idea of Monte Carlo tree search. In general, when it comes to Monte Carlo tree search, in addition to the above ideas, it is also devised to develop a promising board, evaluate it, and feed back the results. For details on Monte Carlo tree search, please refer to the paper [34].

[[[00000000000000000824---e1159974762759b12064986295e33dbb5557751938e90dcf5887000050bfe7d1]]]AlphaGo [35] is a method that combines deep reinforcement learning with Monte Carlo tree search. In 2016, he shocked the world by defeating the world's top Go player.

[[[00000000000000000825---021336f7a9a61ccc8e9e5c63efe770bfc1ccf8d8db5b7154a009970e20fefb7a]]]AlphaGo uses two neural networks. One is the Value network that evaluates the probability of winning from the current board, and the other is the Policy network that represents the policy. The Policy network outputs the next moves as probabilities (eg, 2.4% chance to hit (1, 1), 0.2% chance to hit (1, 2), etc.). These two neural networks have made Monte Carlo tree search more accurate.

[[[00000000000000000826---69de1be5e35858521d76b7524bd26d2422727df5e7a95aa37dafd5d365a31ad0]]]AlphaGo also uses human game record data to train the two networks. After that, it repeats the match in the form of self-play (Self Play) and further learns using the experience data collected there.

[[[00000000000000000827---51c0b2f99c6cd48d50f9e0fbbcd797be55931dbbfc51716dcd9f79033da92470]]]Self-matching can be explained in the framework of reinforcement learning. There is an 'agent' playing Go, and an opponent who is a copy of the agent acts as the 'environment'. The two interact with each other and ultimately earn rewards for winning/losing. The method used in AlphaGo can be said to be reinforcement learning in terms of collecting data while interacting in an environment of self-competition.

[[[00000000000000000828---2feeedaa23b01d8cc0aa6d6337cc709cae09a87c69d480645d47fef7df9b4e01]]]In AlphaGo, we used human game record data as training data. AlphaGo Zero[36] does not use such training data and learns only by self-matching reinforcement learning. Another feature is that it does not use the 'domain knowledge (=specialized knowledge of the target area)' that was used in AlphaGo. Other improvements include:

[[[00000000000000000829---f52555104b7125fe6b536d935049c66dad4ad5c1b1f18ee31aee28f8f6e4eec6]]]Represent policy network and value network with one neural network

[[[00000000000000000830---03c781ab582962e70c6b325c808aaa3c5c2cfbb5e095eaeadd9be49fb05984f3]]]Evaluate each node (state) using only the output of the neural network without performing a playout in the Monte Carlo tree search

[[[00000000000000000831---ce2cdd4cbe8853bc0d23a75cbb05ec3b0cff06a614da1836f9f8dcc3c0097322]]]In this way, the previous version of AlphaGo has been improved to be simpler and more versatile. And AlphaGo Zero learns entirely through reinforcement learning, without human knowledge. Surprisingly, AlphaGo Zero, which does not use human knowledge, has become stronger than the previous version of AlphaGo. In the actual match, they defeated AlphaGo 100-0.

[[[00000000000000000832---b000012e60d66d2f96816c719bf8a1aa14c45b6eff24640d86f901cb61232cbc]]]In supervised learning using human data, the point that can be reached is up to the human ability included in the supervised data. The way reinforcement learning learns from its own experience has no such limitations and can theoretically surpass humans. In fact, AlphaGo Zero has become much stronger than humans. This is also important research showing the potential of reinforcement learning.

[[[00000000000000000833---7f4a04f8561ddb59e095a08c6526f90c210ad829598ac72385a36e93ed6d1a26]]]AlphaGo Zero only dealt with Go, but AlphaZero[37] also supports chess and shogi. AlphaZero's algorithm is just a tweak of AlphaGo Zero and can be considered almost the same algorithm. This shows that AlphaZero is an algorithm that can be used universally regardless of the type of board game.

[[[00000000000000000834---c0076cbc576b6aae7e16e66d899aa4d61215ee673ac52152a5ffef1e02d66590]]]robot control

[[[00000000000000000835---c4ccd949db704b5358df176be036612776041d8219ad30d777fd370108e2b423]]]Deep reinforcement learning is also used in real-world systems such as robot control. In the research of Google AI [38], we succeeded in learning robot operations that can grasp various objects. Specifically, as shown in Fig. 10-13, a camera is attached to the top of the robot, and the robot behaves based on the images obtained from the camera. As a result of the action, if the object can be grasped, it is rewarded as a success, and if it cannot be grasped, it is rewarded as a failure. Learning is done within the framework of this reinforcement learning.

[[[00000000000000000836---81b38b9389822afe80035ed9420a4fcc5fec32ff7043b35d515dd4476a947402]]]
Figure 10-13 Empirical data collected by 7 robots (image taken from paper [38])


[[[00000000000000000837---642d5aee53e33943545f8af2a9d5f3b49c7f3f7b066a11a0c52f6468e5aa85f3]]]Data collection is done by operating seven robots in production for several months, as shown in Figure 10-13. The reinforcement learning algorithm used there is a technique called QT-Opt based on Q-learning. Since QT-Opt is based on Q-learning, it is a policy-off method. In other words, empirical data obtained in the past can also be used. The availability of historical data is important when using real-world robots, as collection costs are high.

[[[00000000000000000838---068d1ac2f60cd72e4e9d54672e9574152322d1cd80e7df5e5d11b78c58a3c8b1]]]Experiments show that QT-Opt successfully grasps unseen objects 96% of the time, reducing failures to less than 1/5 compared to supervised learning methods previously developed by Google AI. It seems that I was able to do it.

[[[00000000000000000839---2294fd7b2d168fe91b272c0631ff0ce195ada91ee2f6e7a3763272d7ff9543be]]]The architecture (network structure) of deep learning is usually determined by humans. Designing a good architecture takes experience and involves a lot of trial and error. Recently, there has been a lot of research on automatically designing optimal architectures [39]. This is the field called NAS (Neural Architecture Search).

[[[00000000000000000840---e743012f014940d735117ff109ac0b3860713c3490f5ec723464362ce9bd6423]]]There are various methods for NAS, such as Bayesian optimization and genetic programming, and one of the most promising candidates is reinforcement learning. The paper that sparked the boom was 'Neural Architecture Search with Reinforcement Learning' [40]. In this paper, we succeeded in automatically optimizing the network structure using reinforcement learning and discovering an architecture that is equivalent to or better than human design. Below is a brief description of the methods in this paper.

[[[00000000000000000841---ac604d10ea7ba5012fc1c94eee946d7486d78e35042ef01cab77f23a1c77ee18]]]The core idea is to focus on the fact that the neural network architecture can be specified 'textually'. For example, you can specify a neural network's architecture as text data, albeit in a fictitious format, as the following example shows.

[[[00000000000000000842---b1837a28143f527cb7c4f1cc2f5417c482ef3587396710a648f763d9c688efe9]]]Also, if you use RNN (Recurrent Neural Network), you can generate variable length text. Theoretically, we could also imagine an RNN outputting text like the one above. Based on the above, it is conceivable to use RNNs as agents as shown in Figure 10-14 and search for the optimal architecture in the framework of reinforcement learning.

[[[00000000000000000843---ee6891a4ef11add8115597c26f923c747fdc82bfecc2bc3a7a9023777ebc27cd]]]
Figure 10-14 Framework for exploring neural network architecture


[[[00000000000000000844---7e261d55d7e8e8913740199c87ff6102b559daa3c4886e159d17be5e52394b57]]]As shown in Figure 10-14, the RNN generates the architecture of the neural network in text. That's what agents do. The generated architecture (neural network) then trains using the validation data and finally measures the recognition accuracy. This recognition accuracy is the reward. The paper [40] uses the REINFORCE algorithm to update the parameters of the RNN. As a result, the RNN will output an architecture with high recognition accuracy.

[[[00000000000000000845---cfe6a9740ca32c4bcafa662e3c7e90110ec019019ab598ccc2f407d9a7408a86]]]In the paper [40], some tricks are provided to limit the range of architectures to explore. One example of this is limiting to parameters such as the filter size and stride of the convolutional layers, and the RNN outputs those parameters in turn.

[[[00000000000000000846---c324f656c87e6f30097f439d60f2494aa2e4546edcbfead412d73942312992b3]]]Other examples

[[[00000000000000000847---43802901f04c3cd1056205bdf5411e1c280b7167bda503d4d23cb81391370f68]]]There are many other applications of deep reinforcement learning. Here are three more examples.

[[[00000000000000000848---2d95dc9ebba14357945533870cf1e4a072e3c9936a7d5ea8d85c8a189bc3d1b7]]]self-driving

[[[00000000000000000849---1ccf30f2be9998267741c19c593b708c2607c8f88348fc3a800e49c569d95ca2]]]Currently, research and development is being carried out all over the world to realize self-driving cars. Many of them are being worked on as 'supervised learning' using deep learning. On the other hand, there is also a movement to work on reinforcement learning [41]. The problem of self-driving cars fits naturally into the reinforcement learning paradigm, as it follows a series of decisions that observe the environment and choose the best course of action there (brake, turn the steering wheel, etc.).

[[[00000000000000000850---4d2347be3b17dd03272c12bbd70a86400fe96fcc83a207bc33f2cfb1760e446f]]]In addition, there is 'AWS DeepRacer [42]' as an environment for learning automatic driving and reinforcement learning. DeepRacer is a racing car that runs autonomously, and its control is performed by a method based on deep reinforcement learning. Users implement their own algorithms and set their own reward functions. Then use the simulator to learn on AWS. The final model can be used in virtual races and real machine races. In addition, races such as the 'Championship Cup' are held regularly, and competitions are held for prize money.

[[[00000000000000000851---28ba8a9f5eaddfd425aa35980c0ade8ce855c3918583addc9dadd681682e223a]]]Building energy management

[[[00000000000000000852---d2d451f7167fdbbd51596700dc5fafd11c2719e876ca37168560395e64e8ac9a]]]Buildings consume a lot of electricity. Reducing power consumption is an important issue, but the environment inside buildings is complex and efficient operation is generally difficult. Recently, many methods of deep reinforcement learning have been proposed to address this problem, and have achieved great results [43].

[[[00000000000000000853---235070143f79913110b30f5dc855967882888adc536d2f4bfef60b8090afae2a]]]For example, a method based on DQN has been proposed for the operation of air conditioning equipment in office buildings. In this study, it was reported that they succeeded in reducing costs by more than 30% while maintaining the comfort of workers [44]. Google's data center also uses a machine learning-based method to control the cooling system, successfully reducing power consumption by as much as 40%. Reinforcement learning techniques are also used here [45].

[[[00000000000000000854---a0af16759975b57f01697c3b158135990451ed19cef5f403d07ebb126c98a65b]]]semiconductor chip design

[[[00000000000000000855---7effb80168ff7d1dc3633d6b86ab7f78e588d7bdc82815432b0a5a2cae2f28ec]]]Semiconductor chip design places billions of transistors on a small substrate. This is a complex problem, with many difficulties. Until now, skilled engineers spent a lot of time designing semiconductor chips, but Google proposed a new semiconductor chip design method using deep reinforcement learning [46].

[[[00000000000000000856---98b3e60c6dcaecc3e2ff5b847973199bec29513f461641d48242c2c15fa419cc]]]This Google study views semiconductor chip design as a reinforcement learning problem, where the agent tries various semiconductor layouts in a trial-and-error fashion to find the best design. The results were impressive: the design was completed in less than 6 hours and performed better than or equal to expert designs on all key metrics. This technology has already been used in the design of the 'TPU (Tensor Processing Unit)' developed by Google.

[[[00000000000000000857---072f209904465e50de8c16a65f94fa2e3156be1219796868192d896db6792831]]]Challenges and Possibilities of Deep Reinforcement Learning

[[[00000000000000000858---a019cb9c6bd675fa50890c205649d9f8940f368f8ebc5360e87aebf30ddd37db]]]Here, we present challenges and solutions in applying deep reinforcement learning to real-world systems. We also discuss points to consider when describing reinforcement learning problems as MDPs. Finally, we conclude the book by discussing the possibilities of reinforcement learning.

[[[00000000000000000859---6c50c3cc9db861f7c981daba0866e088c19c3d0900848273463d31e96c6d0943]]]Application to real system

[[[00000000000000000860---39f448a6dcb5ab744afc9e4ba192a7bcd3ad5477792c93330370e8c1ba8cedfb]]]Deep reinforcement learning is used in real-world systems such as robot control. However, unlike the virtual world, the real world has many limitations. For example, since robots are expensive and difficult to prepare in large quantities, it is difficult to collect experience data. In addition, in reinforcement learning, experience data is collected through trial and error, but in the process, it is necessary to avoid actions that could damage the robot or endanger the surroundings. Here are some potential solutions to such problems.

[[[00000000000000000861---59bcc18fc7eda69c8dae66b4069ffd8e8cba10f59e05eb9d4308f07b189eb6da]]]Use of simulator

[[[00000000000000000862---489961b796f8cd51c3d625e679eb399bd0509a33f1ee65c0b2e9d7c0412cda1f]]]Many of the constraints in the real world can be overcome using simulators. On simulators, agent-environment interactions can be repeated at high speed. Also, even if the agent behaves dangerously, there is no problem if it is on the simulator. However, there is a gap between the real world and the simulator. Since simulators cannot perfectly imitate real-world environments, models (agents) trained on simulators often do not behave as expected in real-world environments. The research field that solves this problem is called Sim2Real.

[[[00000000000000000863---b6745423799c48035f03ff988bb184e5d82b091d310651ee51d515b8e435d793]]]Sim2Real has several powerful methods, but here we will introduce a method called 'Domain Randomization'. In this method, random elements are added to the simulator to create a diverse environment, and the agent is made to act and learn in that diverse environment. By doing so, it can be expected that generalization performance will improve and it will behave well for real-world tasks. For example, OpenAI has published research using Domain Randomization for the task of solving a Rubik's Cube in a five-fingered robot hand (Fig. 10-15).

[[[00000000000000000864---16aa3991fc53c4f1d56015de1140a82630e898d3d1daf5726e5726def096d6cd]]]
Figure 10-15 Solving a Rubik's Cube with a five-fingered robot hand using reinforcement learning and Domain Randomization (image taken from paper [47])


[[[00000000000000000865---61abc74a58fb16739fbca3e903c20228c3784442b445917d3e0bc9ce7ea34bac]]]In this research, we create a variety of environments by inserting random values into rendering image parameters such as light sources and textures, and dynamic parameters such as friction, and perform agent learning. As a result, even in the real world, we succeeded in solving the task without additional learning.

[[[00000000000000000866---4a94a540e5df989ace5cd691e5012da69ed6d9713f207d6e97bebfbe10b56453]]]offline reinforcement learning

[[[00000000000000000867---4777ffbbb2b976b1fb3cc67c88778cff78bb3f44fcc77fffbe1b67875eee9b70]]]There is an approach that makes good use of empirical data collected in the past. For example, in areas such as autonomous driving and robot control, there may be a lot of accumulated experience data from human operations. Also, in dialogue systems, etc., it may be possible to easily collect the history of conversations between people in the past (there are many studies that realize dialogue systems such as chatbots using reinforcement learning). It is conceivable to use such empirical data acquired in the past—these can also be called “offline data”—to perform agent learning. Furthermore, it is conceivable to use only those offline data to estimate the optimal policy without any interaction with the environment. This is offline reinforcement learning (Fig. 10-16).

[[[00000000000000000868---d13445f8dd83ead87ef028e1c482659849ce06d28daafdbf292f0575e46e90f3]]]
Figure 10-16 Normal reinforcement learning (left figure) and offline reinforcement learning (right figure)


[[[00000000000000000869---515a9da4b6c39d49966700e50c1e9e2d04ba81af7d6a42d3c5de5a0d2de0b47d]]]As shown in the left figure of Fig. 10-16, normal reinforcement learning involves interaction between agents and the environment (this can also be called 'online' reinforcement learning). On the other hand, offline reinforcement learning, shown on the right side of Figure 10-16, performs learning using only the empirical data (offline dataset) collected so far. Offline reinforcement learning is characterized by the absence of interaction with the real environment.

[[[00000000000000000870---419b4be372823b7ea1ecaabaf9b635e74e98216ba09b3bbf9c3d36cebf3ca537]]]Offline reinforcement learning is related to off-policy techniques. Policy-off type collects empirical data by 'behavior policy' and uses the empirical data to update 'target policy'. Therefore, off-policy methods can be used to realize offline reinforcement learning. However, in the general off-policy method, a behavior policy is created according to the target policy, and then the agent learns while interacting with the environment (let's think back to Q-learning). On the other hand, offline reinforcement learning uses only offline empirical data to learn without any interaction with the environment.

[[[00000000000000000871---a80fc5834fa403bb2ba9a242c1ca8b4c9ad57c9237f45bac4ab1470d326efaef]]]Offline reinforcement learning has a number of technical challenges. For example, how to evaluate policies using only offline datasets, and how to improve policies using only offline datasets. For such problems, the methods described in the off-policy method (weighted sampling, etc.) can be used. In addition, various other methods have been proposed and are currently being actively researched [48].

[[[00000000000000000872---04ee804e003c3d357389285bbc946a711b4d3ddfca2d29d40a4fc76eeb4e53bc]]]imitation learning

[[[00000000000000000873---e742f0a2799fd8d6deb1f76a5d44ac63ae384f43def2602c42c8dd951c96e17d]]]We humans have much to learn from experts. For example, young baseball players practice to imitate the bat swings of professional baseball players. In the same way, it is conceivable to learn policies by imitating the movements of experts. This is imitation learning. In imitation learning, the robot learns strategies with the goal of imitating the movement of a demonstration by an expert.

[[[00000000000000000874---27b7c2acef2c6b9fb616ea53c3c38f32107406aa79ba11c949a6f72230dee78a]]]Various methods have been proposed for imitation learning. For example, the paper 'Deep Q-Learning from Demonstrations' [49] uses an expert play demo of 'Atari' to run DQN. Specifically, the time-series data of 'state, action, reward' obtained from expert play (hereafter referred to as 'expert data') is added to the DQN's experience playback buffer, and then Learn by DQN. At this time, it makes it easier for expert data to be selected from the experience playback buffer, and adjusts the DQN update formula so that it is closer to the expert data. As a result, DQN that emphasizes expert data can be realized.

[[[00000000000000000875---2b66ead820748e00935a2b7b832c114cf62413595025cd0f43646e0afd4188ef]]]Tips when formulating as MDP

[[[00000000000000000876---8d06c01d482c4619d898dd59a65c24e528bf44cfdd50f91bc14676723362f6d5]]]Many theories of reinforcement learning assume the Markov Decision Process (MDP). In this book, we assume that the reinforcement learning problem is given as an MDP, and then explore various algorithms. However, when trying to solve a real-world problem with reinforcement learning, it is first necessary to formulate the problem as an MDP. At this point, how the MDP is formulated has a great impact on the final result. Here we discuss what to think about when formulating a reinforcement learning problem as an MDP.

[[[00000000000000000877---43e6bfaded9c4dda0744e8adfa33771c310a79ea07ac494f43aa4d57d4cb5a17]]]Flexibility of MDP

[[[00000000000000000878---a96c01938ae43c788963b3ec56d3a8376467a183fd6a4f676dea59028050e711]]]Not all real-world problems can be applied to the MDP question format. However, it turns out that a surprising number of problems can be formulated as MDPs. In MDP, the environment and the agent “throw” each other three pieces of information: state, action, and reward. At this time, detailed information such as what kind of sensors to use, how to control behavior, and how to set rewards can be flexibly decided according to the problem. This flexibility makes MDP more widely applicable than you might think.

[[[00000000000000000879---6beec2038f102dcb63716eb66cbd74ead06b89d5431f4dedc84b3e8047e35a1d]]]Also, agent behavior can be considered from high-level to low-level. For example, when controlling a robot, a high-level decision could be 'throw away the trash' or 'recharge'. A low-level decision could also be an action such as 'pass x volts of current through the motor'. States can be considered similarly, from high-level to low-level.

[[[00000000000000000880---7185b80518486c1d8def4d7568e05050c6db42ca470b630fb4cd5c29172fdcb7]]]Units of time (timesteps) can also be based on when agents make decisions. For example, a step can be 1 millisecond, or 1 minute, or 1 day, or 1 month. In this way, it is possible to flexibly change the state, action, time unit, etc. according to the problem, so the scope of application of MDP is wide.

[[[00000000000000000881---f5b47774fa4a83f18ade1480390a1a77ad3deed337e7276ddb351827ac5efe68]]]Items that need to be set in MDP

[[[00000000000000000882---264de493e91ebc6721e4bd28b4a81930935416ae9b02fdb54a86c19aa57619a7]]]When trying to solve a real problem, the key is how to formulate it as an MDP. To formulate a new problem as an MDP, we need to decide:

[[[00000000000000000883---955029b8b1f32e7f8862849a59643e557cb5c0659bed9868f9f3811d2bf2616c]]]Is the problem you are working on episodic or continuous?

[[[00000000000000000884---33144087c9ed5c66267feb0b5171913cff5d8eeeccd7bc29e0f3fae6d538d261]]]What is the reward value? (Set reward function)

[[[00000000000000000885---87e0100dd572e6ccd09da25935615e6a002af6e3a644699eee34a43cfddcca55]]]What action can the agent take?

[[[00000000000000000886---e56407142583f6a122f42a94351729435df4dabe40991c3fde3da665038e7592]]]What is the state of the environment?

[[[00000000000000000887---736bf314906a670ff3ff310124d21edc03a796a56135e8ce8cbbdbc821a1701d]]]What is the profit discount rate value?

[[[00000000000000000888---2304ba1e41fca127563e545250cd6b36a701fbd01ac7cf075bae1d88db8f0a43]]]How far is the environment, and how far is the agent?

[[[00000000000000000889---88345a72761f7e9d492da382dba24a3b8970fb1bc636ea7660babc2478dec36e]]]Sometimes these are determined naturally by the problem, and sometimes they are difficult to configure. For a board game like Go, for example, it's easy. It's an episodic task, and the reward can be set like if you win in the end and if you lose (other moves are rewards). Also, the action taken by the agent is 'where to hit the Go stone'. Environmental conditions can be thought of as the placement of stones in the foundation.

[[[00000000000000000890---4abd77d139ba3eb9e13ae987f3c3f41ed5e761beea2acb2df6042e938ecbee82]]]When formulating a problem as an MDP, it can be unexpectedly difficult to decide how far the agent should be and how far the environment should be. As a general rule, think of an agent as anything you can control at will. For example, consider manipulating a robotic arm. In the case of a robot arm, each joint has a motor, and the current (or voltage) that flows through the motor can be freely controlled. On the other hand, it is difficult to control the physical body (hard) such as arms and hands as desired. Therefore, when it comes to hardware, it would be better to make it outside the agent—in other words, the environment. In this way, the general rule is that an agent should be able to control everything as desired.

[[[00000000000000000891---b1227d9c965af7873e4ff2a815452b8e9a5d4a059625daf465926df982cbbbe1]]]The design of the reward function is often the key when trying to solve real-world problems using reinforcement learning. Therefore, it is common in practice to iteratively reset the reward function while checking the design of the reward function and the results of the post-learning policy.

[[[00000000000000000892---0c52c5346eca6286d5bc23b9a7412935ee653feede5b9c5e25a1ee96f186a376]]]Tips for good formulation

[[[00000000000000000893---de093c1304d570b492f7cdd039959339fcce2cc98999126e80d81e34f4659aaf]]]A good way to formulate an MDP is more of an art than a science. Experience and intuition are more useful than theory. One way to learn about good MDP formulations is to look at the well-known work in the field of reinforcement learning. In doing so, let's focus on the above points—how they are formulated as MDPs. There will be hints for formulation there.

[[[00000000000000000894---3fdd5ad429628f431ec8ea821818e6cba95276395abd88c6ec44aa5dab99b580]]]general artificial intelligence system

[[[00000000000000000895---ad3721bf30bd09770b08725ae21bb574c92d6341ccfb45d9d9b2312f2443ffb1]]]The field of AI is currently advancing rapidly. Various systems have been developed day and night, and have produced great results. However, it is also true that almost all of them are systems confined to specific tasks. Systems such as recognizing images, playing chess, and driving a car are developed specifically for that task. On the other hand, we humans have a wide variety of skills. It can perceive its surroundings, understand words, drive a car, and play chess. The term Artificial General Intelligence (AGI) is used with the aim of realizing such a wide variety of human skills.

[[[00000000000000000896---c9535a41093c726fcc104cf2adfc23a110fa3b4a7a5957b274f5bd48537becf0]]]Some researchers see reinforcement learning as a key area in achieving (or at least getting close to) artificial general intelligence. For example, there is a paper entitled “Reward is Enough” by researchers at DeepMind [50]. In that paper, the hypothesis that ``intelligence and related abilities can be fully understood by maximizing the sum of rewards'' is discussed. On top of that, the possibility of realizing artificial general intelligence is described by reinforcement learning technology that aims to maximize the sum of rewards.

[[[00000000000000000897---a589163adfda3089d8e3ea6381c21d667749586c0a3586cb317a2b7463f8e173]]]Through this book, we have learned about reinforcement learning. In reinforcement learning, the agent learns a policy from interacting with the environment in order to achieve the goal of maximizing the sum of rewards. The goal of maximizing the sum of rewards may be sufficient to exercise intelligence-related abilities. And methods based on reinforcement learning may realize general artificial intelligence in the future. They are hypothetical and unknown at this time. However, I think that just knowing that there is such a perspective will enrich your way of looking at things.

[[[00000000000000000898---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000899---a5decb47a0c5c4dfd9e3a53f91dad2a773b13e1a710d8a4c18c3c9f6b2391c37]]]In this chapter, we started by classifying deep reinforcement learning algorithms. And we have seen some famous algorithms of policy gradient method series and DQN series as advanced algorithms of deep reinforcement learning. In addition, I introduced some important research examples as case studies of deep reinforcement learning. Just by looking at these examples, I think you can feel the magnitude of the impact of deep reinforcement learning.

[[[00000000000000000900---2f8bfd2fc6aa34d90de052f8aaec13881d52f80535653167c63f32f7bc757ebd]]]We have learned a lot about reinforcement learning so far. In the first half of this book, we learned the basics of reinforcement learning, and in the second half, we learned modern deep reinforcement learning techniques. Through this book, I hope you have understood that even the most advanced deep reinforcement learning is based on the foundation of reinforcement learning that has been cultivated so far. With the knowledge you have acquired so far, you will be able to move forward with a firm step into the world of deep reinforcement learning.

[[[00000000000000000901---16747166737df69ed4527127114763a122997885b4ac3d2f2ee5fdc2285f6379]]]The technology that forms the basis of reinforcement learning will not change significantly. In the world of AI, which changes daily, I would not be happier as an author if readers could learn about such 'things that do not change.' Thank you for reading this far.

[[[00000000000000000902---e1b3e6d1018f3a48202006d7c1ead9d579ea3a5c56d5bfae576ac16e118d237b]]]Chapter 5

[[[00000000000000000903---494f0b18982dca491992bb5c86b715699de5f56b8e4c7cc52fac8e23f69780f0]]]Monte Carlo method

[[[00000000000000000904---258f8a7fa8c57771f0568a1dab72fbbfbe75b9af2a478394243e8272eac2ade9]]]In the previous chapter, we used dynamic programming (DP) to obtain the optimal value function and optimal policy. However, to use that method, the 'environmental model (state transition probability and reward function)' must be known. Unfortunately, for some problems the model of the environment is unknown. Also, even if it is possible to know, it is not realistic in many cases due to the enormous amount of calculation required by the DP method. The field of reinforcement learning mainly deals with the problem of models of environments finding better policies in the unknown. To do that, agents need to take action and learn from their experiences.

[[[00000000000000000905---947f0583d9d911955c37887553f3128d877ce2b51430a32260408f5b791aaab5]]]The topic of this chapter is the Monte Carlo method. The Monte Carlo method is a general term for methods that repeatedly sample data and estimate from the results. In reinforcement learning, the Monte Carlo method can be used to estimate the value function from experience. The “experience” here is the data obtained by actually interacting with the environment and the agent. Specifically, an experience is a set of “state, action, reward” data. Our goal in this chapter is to estimate the value function based on the experience gained by the agent. Once that goal is achieved, we will continue to look at how to find the optimal policy.

[[[00000000000000000906---24253fc47b571f413707bf78169ac73eb9619da34eb0aa0133fb5befda1a7be8]]]From this chapter, we finally move on to the real problem of reinforcement learning. Over time, we've learned some important fundamentals about reinforcement learning. With what you've learned so far, approaches to reinforcement learning problems and Monte Carlo methods should come naturally to you.

[[[00000000000000000907---fc535d3b68d534459db8ee54296f2f195838d74bf5c815633b35877181b8102c]]]Basics of the Monte Carlo method

[[[00000000000000000908---8939e3be6e901aca1682d683da28c60a36a66438df8baa0f071895661f562368]]]So far we have dealt with problems where the model of the environment is known. For example, in the 'grid world' problem, the next transition destination and reward were obvious for the agent's actions. In mathematical terms, we had access to the state transition probabilities and the reward functions (if the state transitions were deterministic, they could also be expressed as a function). For problems with a known model of such an environment, we can simulate the 'state, action, reward' transitions on the agent side.

[[[00000000000000000909---1e7fd2fb3c5f8e5596a7feae31e45ee4822c99c4dbc872bcf5e7cb716a906f78]]]But for many real-world problems, we don't know the model of the environment. For example, consider the problem of 'managing product inventory'. In that case, 'how many products can be sold' corresponds to the state transition probability of the environment. However, since product sales are determined by a complex intertwining of various factors, it is practically impossible to know everything.

[[[00000000000000000910---c15f7dc784f07c5b1b85354ebcba109ad68232175838331bbcd7c614a971b3bc]]]Also, even if we could theoretically know the state transition probabilities of the environment, the calculations (or coding) required to do so would be difficult in many cases. In order to get a real feel for how difficult it is, this section will start with a simple task using 'dice' as an example.

[[[00000000000000000911---d2876dfe752b9751c8afe353cbe66cd4f9e2741910c2ac1047cde84886cc8c66]]]sum of dice rolls

[[[00000000000000000912---96c900f2f9d72d9bef9c8ee3de7484d445d95552446f8120a6ca79cf159ebe9b]]]Now consider the problem of rolling two dice. Let's say that the dice have a probability of rolling each roll. At this time, let's express the sum of the numbers of the dice as a probability distribution. One way to do this is to draw a transition diagram like Figure 5-1.

[[[00000000000000000913---c00d218d85565eb50474b9f3e1d7825dd73c02d71465b28eb79b448d1d9e06f4]]]
Figure 5-1 Transition diagram of the sum of numbers when rolling two dice sum)


[[[00000000000000000914---0c6a381d5451eccbeff7586d99d9df9593b2990068629ede6352a350462ff676]]]If you draw a transition diagram like Figure 5-1, you will see all the 'sums of outcomes' at the bottom. There are 36 of them in total, and the probability distribution can be obtained by aggregating them. For example, in Figure 5-1 there are two cases where the sum is 3, so the probability is Aggregating this over all cases gives the probability distribution in Figure 5-2.

[[[00000000000000000915---0e6df07cd55326f9d39580f4992e891a0d5ff2ca8208d235cd2a227397f7459f]]]
Figure 5-2 Probability distribution of the sum of outcomes when rolling two dice


[[[00000000000000000916---0e206fb3127b8d5d843afc6a49e2860f62d97530d5f86d66120f67a95fbc7fa8]]]As shown in Figure 5-2, the sum of dice numbers is represented as a probability distribution. Now let's calculate the expected value using this probability distribution. This can be implemented like this:

[[[00000000000000000917---c9ef244ef4fd55bfc70d9d7c4ec4bc2a5773520bb179077388743a0d7161028c]]]As shown above, enter the probability distribution (the sum of the numbers on the dice and its probability) and find the expected value. The result is 6.999... (in this case, due to numerical error, the exact value of 7 is not obtained). In this way, if we know the probability distribution, we can calculate the expected value.

[[[00000000000000000918---0b8856950b0a1b0f9da1417df99a67326cb129a2e53a1505b77ef61caad2b6b1]]]The reason why we calculated the expected value here is that the calculation of the expected value is the main part of reinforcement learning. As a review, the purpose of reinforcement learning was to maximize 'profit'. Earnings are defined as the 'expected value of total rewards'.

[[[00000000000000000919---7246f1b797e8fd1925320ccc4fed887f6a04bd59378b9434558a92ba11af9f02]]]Distribution model and sample model

[[[00000000000000000920---51d7b658d0f6a567da021fff707daf991bb77d1cbd88bb238eb2bbe75bf49304]]]We represented the sum of the dice rolls as a 'probability distribution'. In other words, we modeled the trial of rolling dice as a 'probability distribution'. A model represented as such a probability distribution can be called a distribution model.

[[[00000000000000000921---f1491e20ad9e6c2360acddf3a85b91c606af2298459927071fc6badca7e7c110]]]Distribution models are not the only way to represent models. In addition, there is a sample model (Sample Model). A sample model is a model that only needs to be sampled. Sampling, in the dice example, means actually rolling the dice and observing the sum of the dice rolls. While the distribution model must be able to hold the probability distribution explicitly, the sample model must be able to sample (Figure 5-3).

[[[00000000000000000922---44afd7b54613b9106c8aadb7318b4df0bc0de9438f07bec04091dd764deed1fd]]]
Figure 5-3 Examples of distribution model (left) and sample model (right)


[[[00000000000000000923---023c413917eb99025dfda428714f2ccf1ee4c98cd1a6272d93f95d191c37b388]]]The sample model in Figure 5-3 gives specific sample data such as 6 and 4. A sample model does not require an explicit probability distribution, it just needs to be sampled. However, if the sampling is repeated, it is required that the distribution becomes a probability distribution as shown in Figure 5-2.

[[[00000000000000000924---1504ee6f4143b78d66a38c8bdfdf4c50721c626b84522c6f8216987328c29c6c]]]Let's actually implement the sample model. As before, let's implement 'two dice' as a sample model. Here is the code:

[[[00000000000000000925---567a11ea467457776c561b74a6632d1a4aebc9a69b3a308db5557ef68ce9b36e]]]I will briefly explain the code. First, np.random.choice([1, 2, 3, 4, 5, 6]) selects one element from [1, 2, 3, 4, 5, 6] of the list with equal probability. This is the result (sample data) when rolling one die. This time we roll two dice, so we repeat that twice. Now, let's use the sample method implemented here.

[[[00000000000000000926---05494c7a625f964fdc1ca207319b32e3398a8f6dc56bc75a7468d76744ccb4e8]]]As above, the results change from run to run. Now we have implemented two dice as a 'sample model'. You don't need to prepare a probability distribution, so you can easily implement it like this.

[[[00000000000000000927---b9748b93f03981785fd0e93c522f836b97668267118e1dbcd94307640bfa79ed]]]Also, if it is a sample model, it can be easily implemented even if there are 10 dice. In that case, simply change sample(10) and the argument. On the other hand, implementing 10 dice with a 'distribution model' is not easy. If you try to do it simply, you need to think of a combination of streets.

[[[00000000000000000928---860c7474565cf12b411bf5fe287f473843ca7ff8a0051b1c4956fd9d68dda135]]]Let's calculate the expected value using a sample model. The only way to do that is to take a lot of samples and take the average. This is the Monte Carlo method. 'Put the number and take the average'--It is a simple method, but when the number of samples is infinite, the average value converges to the correct value according to the law of large numbers.

[[[00000000000000000929---d9c53e7436e0fff150ed13573b550d958f45da2cccc1c3e4afa042f0830f2331]]]In the bandit problem in chapter 1, we actually played slot machines and estimated the goodness (value) of slot machines based on the average value. What we did there was exactly the Monte Carlo method. The theme of this chapter is to apply the Monte Carlo method used for the bandit problem to the reinforcement learning problem.

[[[00000000000000000930---119f460898ead65af72682c6088f672ac713bd43ee51e37f75aac6451fed1cdb]]]Implementation of the Monte Carlo method

[[[00000000000000000931---5a5d214cf345b04bfb448b0813b0c19ae002e880feb6ffe8b4fcbdfd733deeea]]]Now, let's write a code to find the expected value using the Monte Carlo method.

[[[00000000000000000932---ac810e161b12299f0bbc98bff8b59c12534ba6a03dcf1c965e3a63f597e31567]]]# find the average

[[[00000000000000000933---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000934---1633acbca80a7255b27ac4a984e94d35226f517ab5839f574b82991b135b79f7]]]Here, we will sample 1000 times and find the average value. Its implementation appends the results of each trial to the list samples and finally averages them. Looking at the result above, it is 6.98, but 7 is the correct answer, so it can be said that it is approximately the correct value. This result varies from run to run, but should give a value around 7.

[[[00000000000000000935---3dfd8354cd686890104452c0f25ee6741bea2d2ae1c020453c0a97344afa5361]]]The Monte Carlo method becomes more reliable as the number of samples increases. In technical terms, it is said that 'variance becomes smaller'. Variance is, intuitively, the amplitude of deviation from the correct answer. Variance will be discussed in more detail in 5.5.2 Importance Sampling.

[[[00000000000000000936---0d7df28f85b736ff7218ea85c593e9a4904457acbb62d87d05e079d85ca03998]]]Next, consider the case where you want to find the average value every time you get sample data. In that case, you can simply implement it like this:

[[[00000000000000000937---75dfb18a7150adf87f7619fd7f83227bfa8336088968fe98c64175831b1da09a]]]# Calculate the average value each time

[[[00000000000000000938---b255bf07fb66e6e2de4cdfc9d85817ae7b5b6365230172c1bb731592f9786e96]]]As before, add the sample data to the list and find the average from that list. This time, the calculation of the average value is performed in the for statement. This is correct, but a more efficient implementation is possible. It is the 'incremental implementation' that you learned in '1.3.2 Implementation for finding the average value'. In a nutshell, it looks like this:

[[[00000000000000000939---c26af2c23b922d8ca0f5983652e5fcb4fda975ec29a1ed38bbb20ca05f2cb629]]]
Figure 5-4 Comparison of calculation formulas for calculating the average value (The first sample data obtained is written as , and the value function at the stage when sample data is obtained is written as )


[[[00000000000000000940---29b915569ad88b26e6e2d0f14a6a9325dbb86fa87ed2d28013b8574e32ea930e]]]The same result can be obtained whether the calculation of the average value is performed using the upper formula in Figure 5-4 or the lower formula (“Incremental method”). Incremental methods are more efficient if you are averaging each time you get a sample of data. Now, let's find the average value in an incremental way. Here is the code:

[[[00000000000000000941---8d42d2712cac4800394f398c344ebec21b24bd2df3a4f56fd5f1ea1e8369b303]]]# or V = V + (s - n) / n

[[[00000000000000000942---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000943---3fa6e3a9f373a2908482aa86d24c1612e6dfa8308daa6ae55b4bd4601ba8d357]]]Running the above code will calculate the average value each time sample data is obtained. Looking at the results, you can see that as the number of sample data increases, the correct answer, 7, is approached.

[[[00000000000000000944---458940eb8e5894e15c4dc102a3cb80b42ed2237a6a0ad518305c233e6621daf9]]]This completes the basics of the Monte Carlo method. Next, we apply the Monte Carlo method to a reinforcement learning problem.

[[[00000000000000000945---7e0a4eb70de303c89fc7468b9be9aed42fc3f81e8156fc750911b845e88eb99d]]]Policy evaluation by Monte Carlo method

[[[00000000000000000946---f2b9e286430c4877cdaed355f3fd3a29527e92279e93226e6c40203a9baec592]]]In the previous section, we learned about the Monte Carlo method. Monte Carlo methods actually sample and calculate expected values from the sample data. Of course, this technique can also be applied to reinforcement learning problems. Specifically, the value function can be estimated from the experience (sample data) obtained by the agent's actual actions. Here, given a policy, we compute the policy's value function using the Monte Carlo method. In this section, only 'policy evaluation' will be performed, and 'policy control' for finding the optimal policy will be performed in the following sections.

[[[00000000000000000947---bcca6c7ee9185b59351d8365c77701e2b193c4628f69e7952803e821bd6b6849]]]Find the value function using the Monte Carlo method

[[[00000000000000000948---84c7fd548f46fbb44179ae2ce6ac36c8badf0bc6970121f578a16a01128b016e]]]Let's start by reviewing the value function. The value function is expressed by the formula:

[[[00000000000000000949---290ec7d915a1a056b12012fb31561f647542afd080147e976546fc8855147f22]]]Here, we denote the profit obtained starting from the state (return is the sum of rewards with a discount rate). The value function is defined as the expected value of profit obtained by following the policy, as shown in equation (5.1). Here, we assume an episodic task and consider the case where the goal is reached at a certain time.

[[[00000000000000000950---40f58446c214ed7156805988f50a4c0e1673d9ed933f8160503064c147c4a79c]]]Let's calculate the value function of formula (5.1) using the Monte Carlo method. It does this by having the agent actually act according to the policy. The actual revenue generated is the sample data. The Monte Carlo method collects a lot of such sample data and finds the average. Written as a formula, this looks like this:

[[[00000000000000000951---c6ac519e137f41c49e0e0bcd9f966ba54223033b00d6855f6f1f45c109a0de91]]]Here, we denote the profit obtained starting from the state with , and the profit obtained in the first episode with . To calculate with the Monte Carlo method, repeat the episodes and find the average of the sample data obtained there, as shown in equation (5.2).

[[[00000000000000000952---91777a975cb39c2d6586dafce9b20e56dc85dadc5546ff9337b7e64d8b282b33]]]Monte Carlo methods can only be used in episodic tasks. Since there is no 'end' to the continuous task, the return sample data--such as in Eq. (5.2)--are indeterminate.

[[[00000000000000000953---d18e5b52926b560abd174a885b899e4b00efd292026f56f23b90a15279f039db]]]Let's take a look at the Monte Carlo method with a concrete example. Here, we consider the case where the agent behaves as shown in Figure 5-5.

[[[00000000000000000954---167dd223dfdb9156b65de517af64c15ab6246a2592503213018bb6bee369bac2]]]
Figure 5-5 Attempts of the agent (○ represents the state, ● represents the action, □ represents the goal, and the numerical value represents the reward actually obtained)


[[[00000000000000000955---95483eb5ff5e7c2ad1a5816464828bd63c42d3b31da53e2c3cfbabfc4d4d5970]]]Figure 5-5 depicts the result of an agent starting from a state and acting on a policy. In the example of Figure 5-5, suppose that the rewards obtained are 1, 0, 2. Assuming a discount rate of 1 on this task, the return on state is:

[[[00000000000000000956---566451a69bb3f8261f48dc08e21af92369f3ca88aae8d2bb781cc01dec455e52]]]Here is the first sample data. at this point

[[[00000000000000000957---4d5a7dec8832ae86fcdc1f9a90b2cc19afbb496adb6d740b1c3bcbcf34ce2663]]]can be estimated. Suppose this is followed by a second trial, as shown in the following figure.

[[[00000000000000000958---8ca820f1de95fc52d6c5212208400665ccfa87c8b0a52ec4f992b03f4223904b]]]
Figure 5-6 Second Agent Attempt


[[[00000000000000000959---48bb60dd3526fbbbd8892fa97820f453f3e4be611dd8014d8b729e5e2a2bf302]]]This time, we started from the state and got the reward of . Even if you start from the same state as last time, the reward you get will be different. This is because the agent's policy may be probabilistic and the state transitions of the environment may be probabilistic. If either one of them is stochastic, the reward obtained for each trial will fluctuate stochastically. We use the Monte Carlo method for such stochastically fluctuating values (rewards).

[[[00000000000000000960---31473a3cf6b9f478a83144d7ac85ebb01dea3eabdbf029c5d31c76f5c6882ac9]]]From Figure 5-6, the revenue for the second episode is

[[[00000000000000000961---4e9dedc22842bbfe341149c16e11cb55668b42c6e9de2a53598b6b259a79c4e3]]]is. The first return is 3 and the second return is 2, so the average is

[[[00000000000000000962---313dbf389e8c7b1845fc34b35cf28882faeafee78d6546805c1012c40fe10f9f]]]becomes. So at this point it will be

[[[00000000000000000963---e3cb63cf08089d5bfdc6da6ef2f79cde6478a6a2764ea3ccb4c9904dfaf92fad]]]In this way, we can approximate by actually acting and finding the average return. Increasing the number of trials also increases the accuracy of the approximation.

[[[00000000000000000964---65891817c7ff01e4475f64f6c2a7a9430d8fc061583961e72733c4756c7731be]]]find the value function of all states

[[[00000000000000000965---1d2fcb27248d9ce4080e864464ebe3cfd66fb145fff6b675c58f791c0be3067b]]]So far, we have focused on only one state and used the Monte Carlo method to find the value function for that state. Next, we will find the value function for all states. Simply put, this could be achieved by changing the starting state and repeating the process above. For example, if there are 3 states in total, the value function for each state can be obtained as shown in Figure 5-7.

[[[00000000000000000966---bd002a2a72794b7c398b10445a6d3d5e89196e37bdeb0c314a7785ffec1e38c8]]]
Figure 5-7 Example of calculating revenue starting from each state


[[[00000000000000000967---232a97a06a0df11bab8724f30cf06281dc1fc23f383ddc998deb11bc3f8373c5]]]As shown in Figure 5-7, start from each state and actually act to collect sample data. Then, the value function can be obtained by taking the average of the returns in each state. However, this method suffers from computational efficiency. More precisely, there is room for improvement in that the value function for each state is obtained independently. For example, the returns obtained starting from the state (sample data) are only used to calculate , and do not contribute to the calculation of other value functions.

[[[00000000000000000968---eebee33af6371ae65051f10915bb26e05033e0b70956f17546adf1b71777fa7d]]]Methods like Figure 5-7 are applicable only to problems that can start from any state. For example, in a game or simulator problem, it may be possible to start the agent from any location. But in real life it is almost impossible to start from any place.

[[[00000000000000000969---75bfe17db33284b584bafe40114012df6cb41e07e296dd137c4a2ba60b78ad76]]]Now let's think about a more efficient way. First, consider the following example.

[[[00000000000000000970---c1082f4c4b7d2fd85a64d36cb15bb0afc0516668dd406559a1563788877175d3]]]
Figure 5-8 Example of starting from a state and acting according to a policy


[[[00000000000000000971---3b1c1f9d118c3576341f60e7add8cdca7a1cb00f4e052f6c7553deaacfd96490]]]Figure 5-8 shows the result of starting from the state and acting according to the policy. Here, it is assumed that the goal is reached through the states in the order of provisional. And what is the reward for that time? Given the discount rate for this task, the return you get starting from the state is given by the formula:

[[[00000000000000000972---7fd97e0a59b44231c283cdd746518d6bac47cf7fef65a97a3fefa5348ec5cad2]]]This is the return when the state is the starting position. Next, let's focus on the transitions down from the state, as shown in Figure 5-9.

[[[00000000000000000973---4b0ec10db43d9f24ee17a68984663ee47cbf73497cbf6d03f53ba6194ad2b7cf]]]
Figure 5-9 Transition Down from State


[[[00000000000000000974---889a60008dcbbd8ed17fb87b0f6490c6bbb4e72e396669e6c252b8f3cef7a6e2]]]You can think of it as sample data for revenue when starting from the state, as shown in Figure 5-9. In that case, since we have earned a reward, the profit obtained when starting from the state is

[[[00000000000000000975---70a99dd2da9325d1a2cc6acd67904baf3aadcb4cdab322c47076639fadb160f1]]]becomes. Similarly, the return obtained starting from the state is

[[[00000000000000000976---ba62abe31a34f83c9dbc3b99d89b8554ad22dbff9b5f0e91ca36b1cbfa8a66f9]]]It is considered. Thus, only one trial in Figure 5-8 yielded returns (sample data) for three states.

[[[00000000000000000977---f0b01abebcb3fa0defb388ad6ec3c6cb82bbb9550d24080c983b24abf0fb61f9]]]Even if the starting position of the agent is fixed, if all states can be traversed during the repeated episodes, it is possible to collect sample revenue data for all states. For example, if an agent takes a random policy, over episodes it may transition to various states and go through all states. In that case, you don't need to set the agent's start state anywhere.

[[[00000000000000000978---ae77e4832060a26a3385d05a7eb265dcea94c8067fbdce7603ffbdeb90a9f150]]]Efficient implementation of Monte Carlo methods

[[[00000000000000000979---23ecc282108031c90fe10d9bac8b106a988ff8f08b99a5b3f8c7eeb2b07e828f]]]Finally, I will add an efficient method of calculating earnings. From the example in Figure 5-8, we need to calculate the following three returns.

[[[00000000000000000980---a57334c5a6ff51d9c7d5a3c55315f4e0e4992a6cf62bac5485434022ad2ae40a]]]It's not a particularly difficult calculation, but the calculation can be devised. To do so, we first transform the formula as follows:

[[[00000000000000000981---71606dd0277ba1561841554763ab6851c4818672239d52d19f3fb675443db5e2]]]Note that we use to compute . and using to compute This pattern can be used wisely to avoid redundant calculations. It is to calculate from back to back -- in order --. Written as a formula, it looks like this:

[[[00000000000000000982---2fb82960cc4a3c69e5391d78949d7cf487d4303af0256f295f94a9685b1c54c9]]]As in the above formula, first find . Then use and then ask for using . In this way, we can eliminate duplicate calculations by calculating the earnings in order from the back. The above is the explanation of the policy evaluation by the Monte Carlo method.

[[[00000000000000000983---119f460898ead65af72682c6088f672ac713bd43ee51e37f75aac6451fed1cdb]]]Implementation of the Monte Carlo method

[[[00000000000000000984---29b019ee7f7cdbf202c057eece0e66f7969d25537a82d51f43fa0abf7880b0f2]]]Now, let's solve the '3x4 grid world' problem solved in Chapter 4 using the Monte Carlo method (Figure 5-10).

[[[00000000000000000985---d133e41a2300994b8cd180294c829eb90f12a5cb73508d155d71a0f94ca25a5d]]]
Figure 5-10 3x4 grid world


[[[00000000000000000986---f7377dc9daf3ae1eb7e01bd100df3229089e7e5f638a6689fec767c0e0ec7490]]]This time, we will evaluate the policy without using the environment model (state transition probability and reward function). To do that, we need a method to actually make the agent perform the action. In this document, the GridWorld class has a method called step. First, I will explain how to use the step method.

[[[00000000000000000987---e2dcbad3316a48f781f274cede43e54ee1778a2447cd06b60e8edb9a6b366ead]]]step method

[[[00000000000000000988---c92f0c7cba2d822ce9921d3065925252d4497bdc3e0af2aee64da6c5bd2e16a0]]]The GridWorld class has a step method. This step method can be used to make the agent perform an action. An example usage would be:

[[[00000000000000000989---23eb4a2b34523203aa07c1af1f81d1db155967d97797e3446dc71d641d6e86bf]]]# dummy behavior

[[[00000000000000000990---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000000991---80f4eaecde80e538eff8f3e53a3ad6d11f8e075d9c04bfdd6b0bf7f766e972f2]]]The step method takes an action as an argument. When you write env.step(action), you perform an action on the environment, and as a result you get three values: next_state, reward, and done. next_state is the next state, reward is the reward, and done is a flag indicating whether the episode is finished. The relationship between state, action, reward, and next_state will become clearer when looking at Figure 5-11.

[[[00000000000000000992---507e70dc6d50e0f28226c38ec20094e0fbd1e9bc1ad234cc926a6c9a3e65c588]]]
Figure 5-11 Correspondence between code and formula


[[[00000000000000000993---fdee01e7137342174622eae6021063ffef97a8eb203a9024559e227a23fe4b5a]]]As shown in Figure 5-11, corresponds to state and action to the current time. If the agent takes action at the time, it will get a reward and proceed to the next state. The reward obtained at this time corresponds to reward, and the next state corresponds to next_state.

[[[00000000000000000994---a653969e78997292e784636d8f1f62ac6380315a351cacbf1f58693719e22544]]]In the 3x4 grid world, state transitions are determined deterministically, but they can also be determined stochastically (for example, when an agent moves to the right, there is an 80% probability of moving to the right). and 20% chance of staying in place). In the case of probabilistic state transitions, the value returned by the step method changes every time it is called even if the same action is performed in the same state.

[[[00000000000000000995---613a4b317ba18728727e9b9c38d190df560ca7760cd610c7695f214315714e56]]]In the GridWorld class, we use the step method to make the agent perform actions and get sample data. The GridWorld class also has a reset method. Using this method resets the environment to its initial state. An example usage is:

[[[00000000000000000996---c452b8ee97c111c9f3f4c405d9675ce98ef72bbc2210fa9d41c628448c1be059]]]As shown above, the reset method returns the initial state. The above is an additional description of the GridWorld class.

[[[00000000000000000997---935b9fe798c61841b04ed50a48bc878b5869f61be5c484f0aca2c09fd47c5309]]]Agent class implementation

[[[00000000000000000998---370554a12f6d0eba7e7f2519a0218933a3da765bc058e6e4696ec30fe7894fc1]]]Next, we will implement an agent that performs policy evaluation using the Monte Carlo method. This agent shall act according to a random policy. Here we implement the agent as a RandomAgent class. First, here's the first half of the code.

[[[00000000000000000999---59d9483bdaaba8cdb7f4738ef85707b6f503934cd01fb79e7789184c348b4cf1]]]In the __init__ method of initialization, set the discount rate gamma and the number of actions action_size. Then create a probability distribution for random actions as random_actions and set it to the policy self.pi. self.V represents the value function, and self.memory holds the experience obtained by the agent's actual actions (data columns 'state, action, reward'). self.cnts is used for averaging returns by 'incremental implementation'.

[[[00000000000000001000---5dfab97d7359a29092ba2cf06989c301f737bf65c87205979d5d982b8216a438]]]Next is the get_action(self, state) method. This method retrieves an action in state. The important part is np.random.choice(actions, p=probs). This will sample one action according to the probability distribution of probs.

[[[00000000000000001001---67014be2ea845df0383f99f43113ca69a99788dd46af15d378272ec5b8dfb9de]]]Since the Monte Carlo method requires the agent to choose an action--that the action can be sampled--there are other ways to do it without keeping the probability distribution of actions like self.pi. The agent implementation method used here is based on the 'distribution model', and another implementation method based on the 'sample model' is also considered. Implementation using the agent sample model is described in 6.5 Distribution Models and Sample Models.

[[[00000000000000001002---32e69068c4096322739ac64fc575f03a9fd8abf22eb1c4a3156de2a7833f7456]]]Here is the rest of the code (second half) of the RandomAgent class.

[[[00000000000000001003---aba1c11a7fdd31a967196a4367b7701e91d6d4c6bc9b128c418fd19a19be107e]]]# traverse backwards

[[[00000000000000001004---282b3efca3bb0b3bd126cb6f9754dc39358190ee19fe6a04e8094a58703782ca]]]First, I will explain the add method for recording the actions and rewards actually performed. Calling this method will assemble the state, action, reward into a tuple (state, action, reward) and add it to the list self.memory. I would like to add a little bit about grouping into tuples. For example, consider the following time series data.

[[[00000000000000001005---4dadc6ea0a9ae000e7a3e5ad09df547f578346ccd176d4f189456ce29e8f98f7]]]Given the above data, the code holds it in the following format:

[[[00000000000000001006---cf12f41d8e8f2ba52fd5bdf1f4712220a6fc98037c57bf2afda1ea2463abdf0b]]]Since it is summarized in units of (state, action, reward), the data format is as above. The caveat is that the last state (in this example) is not kept in the self.memory list. The reason we don't add the last state is that the value function of the last state (the state in the goal) is always 0. In other words, the last state doesn't need to update the value function, so we don't add it to self.memory.

[[[00000000000000001007---ea4816435d1debdde1c1c86a74e092e7f2e0dec6f473cc05b4546d76aabb48f0]]]It is the eval method that performs the Monte Carlo method in the RandomAgent class. First, the profit G is initialized to 0, and the profit obtained in each state is calculated by tracing back the actually obtained self.memory. Then find the value function in each state as the average of the returns obtained so far. Here we implement the calculation of the average value in an 'incremental fashion'. The above is the RandomAgent class.

[[[00000000000000001008---2f9fee2a2fe115ed39a06a4778159714a1b48084af528168edfb6ccb46597858]]]Run the Monte Carlo method

[[[00000000000000001009---93309c2f196335694d1d525523150d7a419a20df352b1d4ed14dd819eca9b6be]]]Now, let's work with the RandomAgent class of the agent and the GridWorld class of the environment. Here is the code:

[[[00000000000000001010---fd8912bb9891116275e4ea1d0c93b7b5196519f0b0de634085e318082d2cc0f2]]]Here we run 1000 episodes. When the episode starts, we reset the environment and the agent and let the agent act in a while loop. Then, record the sample data of 'state, behavior, and reward' obtained in the process. After reaching the goal, the value function is updated using the Monte Carlo method based on the obtained sample data. Finally exit the while loop and start the next episode. And after 1000 episodes, visualize the value function with env.render_v(agent.V). Now let's run the code above. Then we get the following value function:

[[[00000000000000001011---5d2dc1aa9a0471d16fed499e83faf1eb50b8841df31376f68d48cfe9a19bfbf2]]]
Figure 5-12 Value function obtained by Monte Carlo method


[[[00000000000000001012---6918935489986697e1da8906022fb28a60d34dbd533b978d703dc49c3f7b3d31]]]This time we are evaluating the value function for a random policy. The starting position of the agent is fixed at one place in the bottom left, but since it is a random policy, it goes through all places. It allows us to evaluate the value function at all locations (states).

[[[00000000000000001013---02078c77271a923d2564bf38b9a447438a3b5338eeb0bc9e535b57fc6e782401]]]If the agent's starting position is fixed and the agent's policy is deterministic, the agent will only go through certain states. In that case, it is not possible to collect sample data of returns for all states.

[[[00000000000000001014---c0eeb77fc1285c356b4b23dc7ff50926eb642494f548231627078c85aae0970d]]]By the way, if we compare the result of Figure 5-12 with the result of evaluation using dynamic programming (DP), we get Figure 5-13.

[[[00000000000000001015---4cb16b3b2bce3b8710de431777566e92306f204f4384bde6ebee29b2cbf0b674]]]
Figure 5-13 Value function obtained by Monte Carlo method (left) and value function obtained by dynamic programming (right)


[[[00000000000000001016---1cd568cab95ae35ef96e8700eb4405e36b21d1d778216748d6140b0d1f23f499]]]The dynamic programming in the right panel of Figure 5-13 is the correct result, but you can see that the Monte Carlo method gives almost the same result. So we were able to get the policy evaluation right without knowing the model of the environment!

[[[00000000000000001017---bf307d873e28ba49dfa0945fb003c69266febeeb956ee2770784207cf50d8615]]]Policy control by Monte Carlo method

[[[00000000000000001018---3edbf5498edf47551dc814cffce687d0a23cccc699e31cafd2d353c029eab400]]]In the previous section, we performed 'policy evaluation' using the Monte Carlo method. Policy evaluation is followed by policy control to find the optimal policy. This section describes policy control using the Monte Carlo method. Having said that, there is not much new to learn. We already learned the key idea in ``4.3 Policy Iteration''. It is an alternating cycle of evaluation and improvement. Start by reviewing it.

[[[00000000000000001019---f62800a2b7386d374cff02c566bc29c6403c6a720d86c34c238de099f8049573]]]Evaluate and improve

[[[00000000000000001020---e3b7a1606a9c9434f54f4a09824889cf17721c5eb3cac4f99df2cf130880733e]]]The optimal policy is obtained by alternating between 'evaluation' and 'improvement'. In the 'evaluation' phase, the policy is evaluated to obtain a value function. And in the 'improvement' phase, we improve the policy by greedying the value function. By alternating between these two processes, we get closer and closer to the optimal policy (and thus the optimal value function).

[[[00000000000000001021---3c9a98ebba5d6d277520569b4e1a25d0d9bd803854666ede21a838d9c5fe3687]]]In the previous section, we evaluated the policy using the Monte Carlo method. For example, if we had the policy, we could get by the Monte Carlo method. Next is the improvement phase. The improvement phase is greedy. This is represented mathematically as:

[[[00000000000000001022---bb61f25b3951f6d4065dc13f95b2d936cc52f8ce4ea1c2aaa97394163534f265]]]In the improvement phase, we choose the action that maximizes the value function (this is called “greedying” in this book). For the Q function (action-value function), choose the action that gives the maximum value of the Q function, as shown in equation (5.3). At this time, one action is determined, so it can be expressed as a function. It can also be expressed using a value function as shown in equation (5.4).

[[[00000000000000001023---11bafb627a2c130335a32cb4ad26156c8a58ff8f58f741230f8fa252b6130519]]]Up to the previous section, we have evaluated the value function. If we use the value function to improve the policy, we compute equation (5.4). But there is a problem with equation (5.4). In general reinforcement learning problems, the models of the environment —— and —— cannot be known. Looking at equation (5.4), it cannot be calculated without using a model of the environment. Therefore, we use the Q function of equation (5.3) to improve the policy. Equation (5.3) does not require a model of the environment, since it simply extracts the action that maximizes .

[[[00000000000000001024---9065361d9f44d30fb371056df2b6328df8f2724910567b5015a38c4bfa730de1]]]If you want to improve the Q function, you have to do an 'evaluation' with respect to the Q function. Up to the previous section, we evaluated the state value function using the Monte Carlo method. I need to switch it to a Q function. To do so, switch the Monte Carlo update formula from to . Expressed as a formula, it looks like this:

[[[00000000000000001025---29ba6a081a32e931b25ec3b6d75a3030dda7a246ad5758617169eabcea66f2e8]]][Evaluation of the state value function]

[[[00000000000000001026---407f78e2ebf310d47e47a7b8253c469477112dff1810912f3340422e0a425e63]]][Evaluation of the Q function]

[[[00000000000000001027---947fd6df1d90d7aef21fbd85b7c01eb3f393639d724448f8bce7e1dc83623ea2]]]Here, the profit earned in the 1st episode is the estimate of the state value function at the end of the 1st episode. Similarly, let be the estimate of the Q function at the end of the second episode. As shown in the above formula, whether it is a state-value function or a Q function, the calculation itself using the Monte Carlo method does not change, just because the target has changed.

[[[00000000000000001028---3d3e29089e26d6d93ee9ab615ea679700eac1c0ec0d7bb057c21c0e2ee08b642]]]Implementation of policy control using Monte Carlo method

[[[00000000000000001029---7f5eafb5bbce92bd43976044676ba90e8fd57d4d1ad7015932397110d0913934]]]Now, we will implement an agent that performs policy control using the Monte Carlo method. The first half of the code for the McAgent class is shown below.

[[[00000000000000001030---d93c410808fb01ff405a4f396d2f7dfe9e93574f6f83d8c8d7cc20069b7e1bd7]]]# Use Q instead of V

[[[00000000000000001031---369eabbaca803cec5d3e6f6cea104972a90bfa410247a17c19b8d4e51fcf9983]]]This code is almost identical to the RandomAgent class implemented in the previous section. The only difference is the renaming of self.V to self.Q. Next, implement the main policy control. Here is the code:

[[[00000000000000001032---368df70c3999d55effdbc52f870e78886cdde42d867c000dbc6feafe542c16fe]]]# At this point action_probs are {0:0.0, 1:0.0, 2:0.0, 3:0.0}

[[[00000000000000001033---5a472d98c023ac60262005436bb0b5a19907071b86ed81adcc4d3bbbaa162bba]]]First, prepare a function called greedy_probs. Implement this function as an external function, not as a method of the agent. The reason is that we will use this function from other classes in the future. The greedy_probs(Q, state) function returns the probability distribution of actions. As for what kind of probability distribution, it is a probability distribution that takes greedy actions. In other words, it is a probability distribution that takes only the action with the maximum value of the Q function in the state state. For example, the greedy_probs function returns {0: 1.0, 1: 0.0, 2: 0.0, 3: 0.0} if the 0th action of the Q function is the largest in a given state.

[[[00000000000000001034---10157bf574cc82022a2852a7af9e49dc29fb860d677703937f3be76fc37f597c]]]Inside the update method, update self.Q. Note that the key of self.Q is a tuple (state, action). Here we update self.Q in an 'incremental fashion' according to equation (5.5). After updating self.Q, greedy the policy in state.

[[[00000000000000001035---ee3f6fd7082ebf12a279a8e0cf6ed3ef5058fa08d420e589dee53788556d5446]]]The above is the McAgent class, but actually this code doesn't work very well. There are two improvements. It is next.

[[[00000000000000001036---f54aba7b5e4f6c25db689843488ab4cc18da02c3c9118b0c8bb57ec49270520b]]]Code ①: Make it -greedy instead of full greedy

[[[00000000000000001037---28b248bcb955eab2fed803e88707e00c91033c63796094e938e8fbf538d50036]]]Code ②: Update Q using the “fixed value method”

[[[00000000000000001038---cb1c2392ae638a872b8b1032fbab43451ba84cb4e4e7e028981004d173ff7cc7]]]Now let's talk about these two fixes (and why).

[[[00000000000000001039---29751c1944dcb0cd6b6d670cbd7e7592a4006ca817a016767e5e92eb13893a64]]]ε-greedy method (first modification)

[[[00000000000000001040---20ccfc70723a376ca30017cb4ed379fa9c88d0267c069a52559dc45830e4656c]]]Agents greedy policies during the improvement phase. By greedying, the action to be taken in a certain state is fixed to one (if the Q value is the same value, it is possible to take multiple actions). For example, let's assume that the policy is greedy and the behavior shown in Figure 5-14 is taken.

[[[00000000000000001041---540a4589010a6f2b9a2d826e7eaca16b449fd3664a8f0d2f40f4fcff0c034d26]]]
Figure 5-14 Greedy policy and agent route


[[[00000000000000001042---3fa451fc155b650b0658fab2a6f33e04b5686ee8727610a0efadc87eb2ece5b2]]]If only greedy behavior is performed as shown in Figure 5-14, the route taken by the agent is fixed to one. This doesn't allow us to collect revenue sample data for all state-action combinations. To solve this problem, we need to let the agent do the 'probing'.

[[[00000000000000001043---d1b8cb01a58793500dd8a171ab91309c625f7616df391de3de140dcf8c591798]]]As explained in the bandit problem, there is a trade-off between exploitation and exploration here as well. “Utilization” is the action that seems to be the best based on past experience, and “exploration” is the act of experimenting and increasing new experiences. There is a trade-off between exploitation and exploration.

[[[00000000000000001044---353a8733375976d19e5d2647a06b2d0c243cbed6b6198681e914a3b95262f6c5]]]One way to make the agent 'probe' is the -greedy method. By adding a 'bit' of randomness to the agent's actions, we basically force the Q-function to choose the action with the highest probability and choose a random action with a low probability. Doing so avoids the problem of only certain states and actions being chosen (hopefully going through all states and covering all actions). Yet, in many cases, greedy actions can be taken, resulting in near-optimal policy results.

[[[00000000000000001045---235dde1b6a33cc305ae8ce7b14c7e905be0c861dcd897b0d11080224da0ba110]]]Now, implement the -greedy version of the greedy_probs function. Here is the code:

[[[00000000000000001046---e8da5bf11d4b69c505bb664725f8ffe507f684d4db45f8c34d0c975bfbf39a9b]]]# At this point action_probs are {0: ε/4, 1: ε/4, 2: ε/4, 3: ε/4}

[[[00000000000000001047---d1be0a1c8f201fdd3496c32e40fcccc16ec64cdbc2d38b8ac82db27bdad9265f]]]Here we change the previously fully greedy probability distribution to -greedy. To create a probability distribution with -greedy, we first set the probability of all actions to , since there are 4 actions in our problem. Then add the probability of to the action with the maximum Q function (Fig. 5-15).

[[[00000000000000001048---c2af38d9ac23efd832c620a55b5ffe62598e82a9d5468637e7ce0ed64b7a67dd]]]
Figure 5-15 Probability of each action being chosen by the -greedy method


[[[00000000000000001049---071f3c575e90de4c4a06c6675107bb4082ecefaa19fd1daf40154ba80b58523c]]]In addition, since the greedy_probs function implemented here will be used repeatedly from now on, the same code is also placed in common.utils. From now on it can be imported by from common.utils import greedy_probs.

[[[00000000000000001050---24f713f1e50b1e654e04a4ea89aa765ea3dc082c781d33fb976df44d61904209]]]Fixed value

[[[00000000000000001051---07ee33271b691f71e4d57d28dcadba50e2bd7f19e6aa40c6da6e595822cd18e3]]]To the method (second modification)

[[[00000000000000001052---1b71098a740bf3731d40504637c37169e6f5e08a73babc9eaccfd0380f502fda]]]Here is the second fix. Here's what the code looks like to fix it:

[[[00000000000000001053---0990efddf480c0ba93c7aa605801aa66083528cb3046147c0ce9fe66135b01ec]]]# Before correction

[[[00000000000000001054---29c4f9df54aa1c7ce82398a57785c1315422ef7b285eba1145e5e0ba9359f8c5]]]# Revised

[[[00000000000000001055---5e125a51265c819c65f407ad259f806e7049f0fde85a9ee1474be68a6a07414d]]]As shown above, the code ② is updated with a fixed value alpha. There are differences between the pre-correction and post-correction methods, as shown in Figure 5-16.

[[[00000000000000001056---a401aa2078b67e1ea0b8fe0c647e9d7fa883710e6f85a7e78b4b337eb1f893e6]]]
Figure 5-16 Weights for each data of 'Method before modification (left figure)' and 'Fixed value method (right figure)'


[[[00000000000000001057---26a3a19dd51808145285a4173685169dfd5dc54305d4f546c0098276e413b577]]]The unmodified method finds a 'uniformly weighted average' over the sample data () so far. This is also called the 'sample mean'. The sample mean is all about the weight for each datum. On the other hand, in the fixed value update method, the weight for each data increases exponentially as shown in the right figure of Figure 5-16. This is also called an “exponential moving average”. An exponential moving average gives more weight to newer data.

[[[00000000000000001058---e1263d3304dd635ae05fc17da57d128ab6af8f6d8914a02d46c799044e9d149a]]]Exponential moving average is suitable for policy control by Monte Carlo method. The reason for this is that the probability distribution from which returns (that is, sample data) are generated fluctuates over time. More precisely, as the episode progresses, the policy is updated, so the probability distribution of revenue generation will change. To use the terminology of the bandit problem, it is 'non-steady state'. As explained in ``1.5.1 For solving non-stationary problems'', exponential moving averages are suitable when sample data (returns) are generated from non-stationary probability distributions.

[[[00000000000000001059---57cb9e9641a736c595fca1a676994f691f2785dfc8bcfd88c22c4d50839a4a28]]]In the process of revenue generation, two probabilistic processes of 'environmental state transition' and 'agent's policy' are repeatedly passed through. If the probability distribution of 'environmental state transition' does not change and the probability distribution of 'agent policy' does not change, then the distribution of sampled returns is 'stationary'. However, if either one changes, the return probability distribution is 'non-stationary.' This time we will iteratively improve the policy. Therefore, the strategy will change from episode to episode. As a result, the probability distribution of earnings also changes.

[[[00000000000000001060---017461c250a36e72729949b60f1398c4a9b30bf9fd84aad9a0d75771650b79ea]]][Modified version] Implementation of policy iteration using Monte Carlo method

[[[00000000000000001061---0031acf761487b120eb9a9dff0380fef13c15ab03e73e10f8673042f1cfdf7dd]]]So here's the modified McAgent class:

[[[00000000000000001062---670156f5f74525d874ea051696e80e4bac1d038cbf7e4b6dde6827c488152c49]]]# ε in ε-greedy

[[[00000000000000001063---a2c4edefa6e4be4e280c99331b5d699a4a3d6330fce35484899391b53b2cde81]]]# Fixed value alpha when updating Q value

[[[00000000000000001064---3af46f0acfac36026a1db6bf9361554d92a2822580384382f21a9438ca462a6d]]]# (1) Fixed value

[[[00000000000000001065---d69ff85fe8b74908c30e8a63b71440d3c07b0edf42744adecb3d33008bdc0bd1]]]First, add the parameters self.epsilon and self.alpha when initializing. self.epsilon is a parameter for -greedy. For example, if self.epsilon is 0.1, there is a 10% chance of choosing a random action and a 90% chance of choosing a greedy action. Also, self.alpha is a fixed value used when updating the Q function. Code 1 updates the Q function by a fixed value, self.alpha.

[[[00000000000000001066---9542ca79f405fbeb96db385681001c64dfe16672fbeb8e9a1eb22540db53a057]]]This is the modified McAgent class. Using this McAgent class, you can use it together with the GridWorld class as follows. Here is the code:

[[[00000000000000001067---6d33d7d0e4dc33bace14496dffa6f06bec2389e9907213343082047b148acc0e]]]Here we train with 10000 episodes and finally visualize the Q function with env.render_q(agent.Q). Running the code above gives the following image.

[[[00000000000000001068---6082eafa992e5efdf847d5fa80380cabf698ea01280a8601c627437a734d06aa]]]
Figure 5-17 Visualization of the Q function


[[[00000000000000001069---8d10b678769d29ff4a79f11d71f36e40d27e8e7922f384c8e6ba3de0c0e4e6e0]]]In the visualization of the Q function, there are four actions in each square, so each square is divided into four and drawn as shown in Figure 5-17. The figure shows that behaviors that avoid negative rewards and gain positive rewards have a larger Q function (this result changes from run to run). Extracting greedy actions from this Q function results in Figure 5-18.

[[[00000000000000001070---a2213bfda972e57993757f5df4354654e236b496660543391d6f1e8c7f860606]]]
Figure 5-18 A greedy policy resulting from the Q function


[[[00000000000000001071---ad1ff7a1691e656b2d80f2aa23cd322466bb6a4f73e7248230a0b6adc907f72e]]]As above, the greedy policy obtained from the Q function is close to the optimal policy. In reality, the agent takes random actions in each square due to -greedy, but greedy actions account for the majority, so even with this policy, generally good results can be obtained. The above is the implementation of policy control using the Monte Carlo method.

[[[00000000000000001072---49a566e891e2af7703cca635471bbf68bd6fe7c79974e446a5e69d6c2ef5585c]]]Off-policy and weighted sampling

[[[00000000000000001073---663889110eed7620cb48237d228259f8b2a44a7d4b1bc714df5fbe77ece3df28]]]In the previous section, a near-optimal policy was obtained by combining the Monte Carlo method with the -greedy method. But it's not a completely optimal policy. We want (if possible) to only take actions with the largest value of the Q function. In other words, we want to “utilize” it. But you can't 'explore' it. So I did a 'search' with a small probability. The -greedy method is kind of a 'compromise', so to speak. Here we consider how to learn a perfectly optimal policy using Monte Carlo methods. In doing so, I will first explain the policy-on type and the policy-off type.

[[[00000000000000001074---3cebc6ca939ddcc6096431f84c29764e00e6cce077f8b6ba48d6571aa0fa6acd]]]Strategy on and off

[[[00000000000000001075---3136155ef1005568d2446ccf83a09d83e63a91d546013c4d27d7d98f401d1d0c]]]We can observe the behavior of others and use it to improve our own skills. For example, you can watch other tennis players swing and use it to improve your own. To use the terminology of reinforcement learning, it means improving your policy from experience gained elsewhere. This approach is called off-policy in reinforcement learning. On the other hand, when you improve your policy based on your own experience, it is called on-policy.

[[[00000000000000001076---89cba73b22cfa4c579f4d385b8675fc1d593fcf167dc014db917c655e14f3f70]]]The agent strategy can be viewed as having two strategies from a role perspective. One strategy is the strategy that is subject to evaluation and improvement. We will evaluate this policy and make improvements. This policy is called the Target Policy. Another strategy is the strategy that agents use when they actually take action. This strategy generates sample data for 'state, action, reward'. This policy is called the Behavior Policy.

[[[00000000000000001077---0f5626c7ea06719c8a1aeca99597d4b8a595ffd1b0560191c7493a5091d563b3]]]So far we have used the terms 'target policy' and 'behavior policy' interchangeably. In other words, the 'target policy for evaluation and improvement' and the 'behavior policy for actual action' are the same. As such, if the target policy and the behavior policy are the same, it is called policy-on-type. On the other hand, if we consider the target policy and the behavior policy separately, it is called off-policy type. Here, on/off is used in the sense of 'connected/separated'. The off-policy type means 'the target policy and the behavior policy are separated'.

[[[00000000000000001078---d07f2f8d9c36d6917068c8d91310ff3b5312fb903cfa40c293d325c9c3d289f9]]]The theme here is 'policy-off type'. Evaluate and improve your own policy (target policy) based on the experience gained from another policy (behavior policy), like the example of a tennis player. With policy-off type, you can have the behavior policy do 'explore' and the target policy just do 'exploit'. However, using sample data from the behavioral policy to determine the expected value associated with the target policy requires computational ingenuity. This is where a technique called Importance Sampling comes into play. Next, we will discuss weighted sampling.

[[[00000000000000001079---3415da3a01d9ce233533367d8ed215f835eaab2a4abebed4cf29231bdb4f6630]]]weighted sampling

[[[00000000000000001080---1ed2ce709b96d3518497e58ede6bb6b314fc46c5d87141c336d45582274d0bf7]]]Importance sampling is a technique for calculating the expected value of one probability distribution using sampled data from another probability distribution. To illustrate importance sampling, we consider the expectation calculation as a simple example. Here it is a random variable and the probability of is represented by . This expected value is expressed by the following formula.

[[[00000000000000001081---6c909a1275819c1ca9476f0a7445b3a65802b721026b6d606ee31844a8f9461e]]]As a refresher, we can approximate this expectation using a Monte Carlo method by sampling from a probability distribution and taking the average. Written as a formula, it looks like this:

[[[00000000000000001082---1002d6002d7e447d057fdbe53d1d5734fb0f7e781fd364e9ff173c2235d27f1e]]]Here, the notation means that the th data is sampled from the probability distribution.

[[[00000000000000001083---4966f5bb848df6b55be0f3269659714f9d8b892274ed9c9d493e14cc6ef24c75]]]Now let's get down to business. The problem we want to consider here is when is sampled from a different probability distribution. For example, let's say is sampled from a probability distribution that is (not). If so, how can the expected value be approximated? The key to solving the problem lies in the following transformation of the formula.

[[[00000000000000001084---53065d52acb1b14871e2f03d6e42f604d75ab7e4e21d09f34b7d78a49dad7ad8]]]The point here is to include the . is always 1, so the equality holds. And if it is in the form of (5.6), it can be regarded as the expected value in the probability distribution. Actually transforming equation (5.6), we get the following equation.

[[[00000000000000001085---fa367ac89b3e61bca83deed0a47d4dc9eb1f8003a80f71dd40b7538c703d6874]]]The point I want to pay attention to here is that it is expressed as the expected value of the probability distribution. It is also important to note that each is multiplied by . Here, each can be viewed as being multiplied by as a 'weight'. From the above, the Monte Carlo method based on Eq. (5.7) is

[[[00000000000000001086---0050058318b3b9d620a10781f552477a8b32e07d2b92522a92a897cf48399a76]]]We can now compute using data sampled from different probability distributions. Now let's implement importance sampling. Here, weighted sampling is performed for the probability distribution shown in Figure 5-19.

[[[00000000000000001087---8ab516e7145cf58dbb55d4a5ad7b7c9918e42aa7b14c86ec6b5a662d5fc9b11e]]]
Figure 5-19 Probability Distribution and


[[[00000000000000001088---598e439d78a0cf7bb0ed773e798ac912efb9d004da5eb37703daf44578d7eb53]]]The goal here is to find the expected value. First, let's find the expected value of the probability distribution using the usual Monte Carlo method. Here is the code:

[[[00000000000000001089---3cbdd94991f1effcd7863681904ea5826b654b97b32cde8941eb3ab24b1a4a80]]]# Expected value

[[[00000000000000001090---974b44a5a6415e6525b179ff5fe8e6596177fd1505230d551e5cb8320de5c492]]]# Monte Carlo method

[[[00000000000000001091---c1aab742565e43e84226666d10638bee7c443f2c1e2141ec0965836b410b5f57]]]# sample using pi

[[[00000000000000001092---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001093---24e4dca78c4126956776fb6f155a5ae85d62605f1980b109d09c28f0b205c1bf]]]First, find the expected value by applying it to the definition formula. The result is 2.7 (this is the true value). Then use the Monte Carlo method to find it. Here, only 100 data are sampled from the probability distribution pi and the average is obtained. For that, I used NumPy's np.mean method to find the mean. The result is 2.78, which is close to the true value. FYI, I'm also asking for 'variance' using NumPy's np.var method. The variance has a value of 0.27. Use this value as a reference when comparing the following importance sampling results. Variance refers to how scattered the data is. Expressing the relationship between the expected value and the variance as a formula is as follows.

[[[00000000000000001094---b1a6fbaf71336db379bbb65d3b44d800e6d3b8653def48358e038d3980d8e62b]]]The variance is the expected value of the squared difference between the data and the mean of. Intuitively, it represents the 'variability' of the data, as shown in Figure 5-20.

[[[00000000000000001095---02f3f76288fb8bd8c6006ff1bb2a0f59be9207e64310c9b52c9d3a2d7358533e]]]
Figure 5-20 Image of variance when each data is a point on two dimensions (the center of the circle is the average value)


[[[00000000000000001096---6e6611ecafae9b3014941259b534564a610bcf02abb6319379fd7c4b045b2661]]]Next, we use importance sampling to find the expected value. Here is the code:

[[[00000000000000001097---2dce9a3cbfd7ad4f8aefc7db9f9ce1adc20c741d7b5ce1e9d8518226618c50e7]]]# sample using b

[[[00000000000000001098---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001099---a38fd178db21bcf395383459789c008f60640833bc3cbeb8af94228fddc9e858]]]Here, we sample using the probability distribution b. However, the sampling target is 'b's index ([0, 1, 2])'. This is because we use the sampled indices when computing the weights rho.

[[[00000000000000001100---1e9fbc0fcf112229222cab2dfe705b69c47fa5336532964c890c760335d82eea]]]Now let's look at the result above. The average is 2.95, which is a little far from the true value of 2.7, but close to it. We can also see that the variance is 10.63, which indicates that the data are more 'dispersed' than with the Monte Carlo method (the variance was 0.27 with the Monte Carlo method).

[[[00000000000000001101---a618d96e6ac9a30f03afaa05ac9800738e3ab23990ac714e872cec37826f982e]]]to reduce the variance

[[[00000000000000001102---75f99ecae58c097f5cfefa9169338f5ca446c79b5af5bef0ebb2fb086e21c032]]]The smaller the variance, the better the approximation with fewer samples. Conversely, the larger the variance, the more samples need to be taken for a more accurate approximation. Now let's see how importance sampling reduces the variance. First, I will use Figure 5-21 to explain why weighted sampling has a large variance.

[[[00000000000000001103---704af6872f99ac5ec019508579095fb47d3a735accbc8538fa75396edad1b57e]]]
Figure 5-21 Example of sampling from probability distribution


[[[00000000000000001104---f251e86fc1579e1b2cf72983e54a256691d0bf11340420790fdbf8d5156dc410]]]Figure 5-21 shows an example where is chosen as the sample data. Then the weight is Therefore, the value of will be doubled. In other words, even though we got the value of , we got the value of . That might make you feel like you're doing something wrong, but it makes sense. Here's why.

[[[00000000000000001105---403814ac598c51a821d4c73a5554fcc360fa97e0e418c02cb14d0823acaf783c]]]Since it is a representative value for the probability distribution, it should be sampled a lot (originally)

[[[00000000000000001106---744c50dc37d38baf197ecb62ab695f72be1084dc6381b33a64e096acd19e74e7]]]However, in the probability distribution, the value does not appear so often

[[[00000000000000001107---e8039baf00f09ac7865690872b38475b59b9a6932400f434d3f4aa8ec273dd7d]]]To fill this gap, when the value of is sampled, it is adjusted by multiplying it by a 'weight' to make it larger.

[[[00000000000000001108---2bf2ca554033eb9b0db9d4ec89da9a1611ff07b78061b90eeedc54c15dcebcf3]]]In this way, it makes sense to adjust the sampled values by weighting them to account for differences from the probability distribution. However, even though the value was sampled, treating it as means that if it was the first sample data, the estimated value at that point would be . While the true value is , it deviates greatly from the true value. In this way, the greater the weight correction for the actually obtained value, the greater the variance (variation from the true value).

[[[00000000000000001109---d48a260cbb5cd9374f3f82ad2c4d831cf1db8e98cf03500bcc20c2cd5ef673a7]]]So how can we reduce the variance? One way is to bring the two probability distributions (and) close together. Doing so brings the weight values closer to . Now let's do an experiment. Here we only change the value of b in the probability distribution relative to the previous code.

[[[00000000000000001110---e04211aee50691bb1dbe5c5ead60954bf3acd2ddcd3f8f75aa65d0b000d89940]]]# change probability distribution

[[[00000000000000001111---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001112---1f8d9c3d3916e73165a0f589677edff13c46d1d4bb019bb0d4def2860820e7df]]]As shown above, the probability distribution of b is set to [0.2, 0.2, 0.6], and the shape is approximated to the probability distribution of pi. The result is an average of 2.72, which is closer to the correct answer. Also, we can see that the variance is 2.48, which is smaller than before.

[[[00000000000000001113---2fefd1228041a7a127baebb02f311f3b98222b32085e318d2a2a1a5fcfe577ac]]]In this way, when performing importance sampling, we can reduce the variance by bringing the two probability distributions closer together. However, the main focus of reinforcement learning is to have one policy (probability distribution) perform “exploration” and the other “exploit”. After satisfying that condition, the variance can be reduced by bringing the two probability distributions as close as possible.

[[[00000000000000001114---d0f8629d580bf4aaba5f1a2288eedb0514e9280a3d0e51b812f55be0fa81285d]]]This is the weighted sampling. With importance sampling, policy-off type can be achieved. Specifically, data sampled from probability distributions called behavioral policies can be used to compute expected values for target policies. The detailed method is described in 'Appendix A Off-policy Monte Carlo method'. It will be a developmental story, so please refer to it if you are interested.

[[[00000000000000001115---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001116---8e2c63d83c8acf2e27b0e90d5b1bccc8885a3bbc667ce578603926effdd9d131]]]In reinforcement learning, the agent actually acts on the environment and finds a better policy based on the experience gained there. The field of reinforcement learning is characterized by such interactions between environments and agents. The technique you learned in this chapter is the Monte Carlo method. The Monte Carlo method allows us to approximate the value function based on actual experience.

[[[00000000000000001117---af740ecbe6872ad3c5df9a5705647a4d871bba3abb828f88bf2820afaec91635]]]If we can evaluate the Q function by the Monte Carlo method, then we will improve the policy based on the Q function. By alternately repeating this evaluation and improvement, a better policy can be obtained. However, in improving the policy, if it becomes completely greedy, it will not be possible to 'explore' it after that. In this chapter, we've updated to -greedy instead of full greedy to balance exploitation and exploration. In reinforcement learning problems, it is necessary to find the best action while balancing exploitation and exploration.

[[[00000000000000001118---3e677a1b53e292924e3b6efd456ecf4752177282c72d1af5adbef79d213e333b]]]I also explained that there are two types of agent policies: target policies and behavior policies. Behavioral policies perform the actual actions and use the experience gained to update the target policy. When the target policy and the behavior policy are the same, it is called the policy-on type, and when they are considered separately, it is called the policy-off type. In the off-policy case, we need to take action with the behavior policy and use the result to compute the expected value for the target policy. Importance sampling makes this possible.

[[[00000000000000001119---a66fc8ffc5427c249ed59925da9020f42c09148dc5dbe0cbeeb81fd6d305193e]]]Appendix B

[[[00000000000000001120---11754b3778843f66258fca5819848f0ba68c9d9abe2275a37796bb187842f0cb]]]n-step TD method

[[[00000000000000001121---582da33534973eeb19fbd4949cb071b40b95f282b7dc5b04657fc9b86f17cc13]]]In the TD method described in Chapter 6, TD method, we used only one-step ahead information as the TD target. This TD target can be extended to use 2 steps ahead, 3 steps ahead, ... and even further information. This is the idea of the 'n-step TD method'. In mathematics, the update formula for the value function is expressed as:

[[[00000000000000001122---fcde99b2c8e36512aded981b6327ecef0dd416afb9ed5b994453a0da9bb41475]]]At this time, it takes the following values depending on the value of .

[[[00000000000000001123---fddc0d025085eb319a622bb859871cd2702e2901eaeade9ee5f40a79c4e165a2]]]As above, the TD target changes depending on the value of . When , the TD method is used. When , it means that the goal is reached, which corresponds to the MC method. In the n-step TD method, the extremes of and correspond to the TD method and the MC method, respectively, and can be interpreted as a gradation between them.

[[[00000000000000001124---10dd38025ee87421add3e3029082ca8c8f7aa3d8599a72328794986d40f90cf6]]]So how should we choose the value of ? Of course, the optimal value will vary depending on the task. A smart move is to use all TD targets instead of trying to pick the best one. One might then represent it as a 'weighted sum' of each TD target. This is a method called TD(λ) (pronounced 'tee dee lambda'). The TD target of TD() is given by

[[[00000000000000001125---375f3d98800beb6e79961bcd064a6aa0f63c323f615feea2f2d6143b8e015fa0]]]Use all TD targets from to as in the above formula. And set the weight of each term to be multiplied by increasing (set a real number between 0 and 1). By the way, the weights are all summed up when (see the next formula).

[[[00000000000000001126---b185df5e2a5c6a0a4e97d32a3334ba2e57352b0e401aac56970ee1d43a9e50f9]]]The above is the explanation of the n-step TD method and TD().

[[[00000000000000001127---1c730b625c1e38795f23a2ad626ab6abb0cf204565b688e7387052eb57ca49c1]]]Chapter 8

[[[00000000000000001128---e7d989b356c31c7dab6a93bec71799c867d9c085cd3ae802c5f11c2e23adf23c]]]The theme of this chapter is DQN (Deep Q Network). DQN is a technique using Q-learning and neural networks. In the previous chapter, we learned how to combine Q-learning and neural networks. DQN adds new technologies---'experience reproduction' and 'target network'---to be used there. In this chapter, we will learn about those techniques, implement them, and verify their effectiveness. In addition, we also cover DQN extensions ('Double DQN', 'Prioritized Experience Replay', and 'Dueling DQN').

[[[00000000000000001129---8005b45dedc4766d067c29b90b8a849acd1628cd2524d86f7e125c00842579c9]]]DQNs are able to perform admirably on complex tasks like video games, and that is how the current boom in deep reinforcement learning started. In that respect, DQN can be said to be a monumental research in deep reinforcement learning. DQN was announced in 2013, which is a little old, but even now, many DQN-based methods have been proposed. DQN is still one of the important algorithms.

[[[00000000000000001130---9acfdedea1c7dfcd0c230bc23a16c0c5e97b9f590a213d5f32e09fccd8846bab]]]Also, from this chapter, we will graduate from the previous 'grid world' and tackle more practical problems. Specifically, we will use a tool called OpenAI Gym to tackle the “cart pole” problem. First, let's look at how to use OpenAI Gym.

[[[00000000000000001131---b64d3ec41ac3583c89d2980a32ace6f7bf369757e93c040f92cf2e8c15dea251]]]OpenAI Gym is an open source library. As shown in Figure 8-1, various reinforcement learning tasks (environments) are prepared.

[[[00000000000000001132---b7a6e9a67631bdc18d9ff613264bf253387ad4cb572413e78ebf912ceca64cc8]]]
Figure 8-1 OpenAI Gym[9] task list screen


[[[00000000000000001133---3187f8e83fa1ab5f2f5e348919bd2530eee188790f372902e9b7a68feecd1bde]]]OpenAI Gym has a common interface for many tasks. Therefore, it is easy to switch reinforcement learning tasks. Also, in papers on reinforcement learning, OpenAI Gym is often used as a benchmark (among others, OpenAI Gym's game 'Atari' is often used as a benchmark). Here you will learn the basic usage of OpenAI Gym.

[[[00000000000000001134---1ed725423fab79e4efdcc1e0bbf9e19438f79c8b114f51a7152ba63076a88477]]]Basic knowledge of OpenAI Gym

[[[00000000000000001135---84c27f5984efb17ad294360aeaff211998b2bf9f7dda24e07500648025d98182]]]First, install the gym module of OpenAI Gym. Installation can be done using pip as follows:

[[[00000000000000001136---e6a860dbf9fc73b66cac78dab53c5646be2128fbaa6f8df44fae9c7af7fccd0f]]]Installation is completed by executing the above line in the terminal. Now let's use the gym module. Various environments are prepared in OpenAI Gym, but here CartPole-v0 is specified. Start with the following code.

[[[00000000000000001137---2866ab41159bae7dd1f19fe1938759ec38e52ced2f293eca3c9ffa656d334aa3]]]This created an environment called 'Cartpole'. 'Cart pole' is a balance game where you adjust the pole so that it does not fall over, as shown in Figure 8-2.

[[[00000000000000001138---1fcdd069e5714654ad3940d0162f7694bd2039af5fa399904e21695dcc93d039]]]
Figure 8-2 Cart pole


[[[00000000000000001139---b7a38c81896f243d53c7b6b466eebb5d66566297eff8b8362c8f19d4a0ac7a70]]]Balance the pole by moving the cart left or right as shown in Figure 8-2. A kart pole exit condition is when the pole becomes unbalanced (pole angle exceeds a certain angle) or the cart position moves beyond a certain range.

[[[00000000000000001140---9e65b222f2f26110fdddfc556037c45af19b0f8a63b2b8f74ddc5de64e2d5aad]]]CartPole has version 0 (CartPole-v0) and version 1 (CartPole-v1). Version 0 is limited to 200 steps. If you can keep your balance for 200 steps, the game ends. Version 1 has an upper limit of 500 steps.

[[[00000000000000001141---8f0f108a11de0fd18f416797fffc0c47324ada24af9eba7a188c9d8197f21920]]]Now continue with the following code:

[[[00000000000000001142---136ef6c3f3ce47b19308aeadcdd7b98f89ce5e53f5144a36d9d9524af1e011a6]]]# initial state

[[[00000000000000001143---3ce96e008df228d1115c3e25fe2cdcd335c17fba27f7c364af1efc009c118a32]]]# number of dimensions of action

[[[00000000000000001144---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001145---ada3932ebc956920e5eeb86766d2a2e4e82dc218bfd97b3f6c84f55fedd57137]]]Here we get the initial 'state' by state = env.reset(). Looking at its output, we can see that it is an array with 4 elements. For reference, the four elements are

[[[00000000000000001146---80e524a4c5a6045997ad0d5206844dba7494f1c2af47338c396a4abaf33c62dd]]]cart position

[[[00000000000000001147---c5e4201ddf3650bec9c2e25489a503d1f9c9d9c2e4f1ac2600d5fe2a63d5e7d4]]]cart speed

[[[00000000000000001148---a2a70e898464fc9e8ba02c78623f4bef5100500b923f6baa00752b954882b550]]]rod angle

[[[00000000000000001149---741baa8b7b1763a33d7abdbda23ef73b37a22f7789a736e0329e41d63f1e0294]]]angular velocity of rod

[[[00000000000000001150---6c4352678c1286a2e071b37ca0645b178a4a5dd62046c1726fbe4d8f5ca00dd7]]]represents Also, the number of dimensions of actions (the number of actions that can be taken) can be found from env.action_space. Its output is an instance of its own class called Discrete(2). This means that there are two possible actions. Specifically, 0 corresponds to the action of moving the cart to the left and 1 to the right. Now, let's actually take action and advance the time by one.

[[[00000000000000001151---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001152---d2c13c35aff3cea98f8ffd77d58d60eb4a2827333b1f639d28b63daca535cb1c]]]As above, the action is executed by env.step(action). And as a result, we get the following four pieces of information:

[[[00000000000000001153---7de2411441510762b9b42b26a4a665d7dacef6d62ad12d3b6f4e3c2fa44450c1]]]Next state (next_state)

[[[00000000000000001154---a561a208bc3a58f0dcc369e88d9ea4419b807a024c09aaa4f26cffaa7aeeaa67]]]reward

[[[00000000000000001155---39f447e0ad09dbadf17b119d12b9192fb9027cf4e4109a3d85ea9585d561fcf9]]]Flag for completion (done)

[[[00000000000000001156---e687a97bf24810eb091b7479687cebdcb83dfd9776a8c4cdf79d8c171d3b72e6]]]Additional information (info)

[[[00000000000000001157---1ceda29f36fd31bf305f1212205d9c3c6be98a77881c092cc3e2ec7fc841b145]]]reward is a scalar value (float). This task now always rewards 1 as long as you are in balance. info contains useful information for debugging (for example, it contains a model of the environment). However, info is basically not used when implementing and evaluating reinforcement learning algorithms.

[[[00000000000000001158---fd2e2a6bd0996a919c973d362cd9899feaf188959a49271d7ead94a08725eab7]]]random agent

[[[00000000000000001159---09f8c3477a0bcc9085efa4613d5a9a05ee00e1ffaa1e63c3ab46740e0c1e4b68]]]The above completes the necessary knowledge about OpenAI Gym. Let's put the code together and run it. Here, let's assume a random agent (an agent that acts randomly) and run one episode. Here is the code:

[[[00000000000000001160---13e042acb07520be988c34e86cdd52f8f28cb42249047f4e90d38a5173281b91]]]Here we use a while statement to continue the action until the episode ends. The behavior is to randomly sample 0 or 1. Also, OpenAI Gym can visualize tasks with env.render(). In this case, you will see a window like this:

[[[00000000000000001161---5de7fb0fa186672539b6559ce3dc25de82c9b1f04bb66a54623acb7cfc5affce]]]
Figure 8-3 Cart Pole Rendering (Drawing)


[[[00000000000000001162---ec773be56fec1591eed166b2ac38c29cf47bd3934e16bc9e585eeb856f140261]]]This time it's doing random actions, which will quickly result in an imbalance. In the next section, we will challenge the cart pole using DQN.

[[[00000000000000001163---52de6f5f7ec139bb9732f06bf39524508e7919b272cadecca9d6dc2a6491cf3a]]][Supplement] About states and observations

[[[00000000000000001164---53f3855fc2305f0b8e807047a66f72de871214ec22fccbe6f7834e921e6c6e93]]]The OpenAI Gym documentation uses the term 'observation' instead of 'state'. APIs in OpenAI Gym are also named on an 'observation' basis (e.g. env.observation_space, etc.).

[[[00000000000000001165---6a669e66dc0e01db753a576cd069e073f34f3f77b65133d546b6f9efd8d266dd]]]States and observations are different. A state is a “complete description (information)” about the environment. Once the state is known, the Markov decision process completely determines the next state and reward probability distributions. An observation, on the other hand, is a 'partial description' of a state. This can be best visualized by imagining a problem (such as poker or mahjong) where the agent only sees part of the problem (the world).

[[[00000000000000001166---872cb599585fe9a1d4389b9f0eeec9c5c9676be4518f39136f48bd3e517955f2]]]In some tasks, states and observations may be equivalent, but given the variety of reinforcement learning tasks, it is more appropriate to use the term 'observation' rather than 'state'. Therefore, the term 'observation' is used in OpenAI Gym. Since this book deals only with problems where states and observations are equal, we will continue to use the term 'state'.

[[[00000000000000001167---4e7682d4929416a32cb89f39e207c3edb0f73d87ab2936775021ce0a039afaef]]]Core technology of DQN

[[[00000000000000001168---c7561513651bec6b585829eee61517301554f0d98988dd877a94893db748cc09]]]In Q-learning, the guesses are used to update the guesses (this principle is called 'bootstrapping'). Q-learning (or TD method, broadly speaking) tends to be unstable because it updates existing estimates using estimates that are not yet accurate. Add a highly expressive function approximation method such as a neural network, and the results become even more unstable.

[[[00000000000000001169---05bdb38a9e36aba5d95fa564416ac91da20d3151a012df156cc51aed83ac31d4]]]The good thing about neural networks is that they are highly expressive, but that can also be a bad thing. One is (potentially) overfitting to the training data. This is called overfitting.

[[[00000000000000001170---0a94d07fcc66ac362cb2edcb67b84d92955c5a6209d542d80e2b9627fc3e892c]]]DQN is a technique that combines Q-learning and neural networks. The feature is that it uses the technologies of Experience Replay and Target Network to stabilize the learning of the neural network (there are other tricks, but I will explain them later). . With these technologies, DQN is the first to successfully perform complex tasks such as video games. Here, we will explain the two core technologies of DQN in turn. Start with experience playback.

[[[00000000000000001171---b2c21951ed1dd87b5874561db67790ed3d73e7b81c383ac9cb275d5a44974f3e]]]Experience Replay

[[[00000000000000001172---cf2e29559f3d4f40f52c4f3dfacbfdc5b3fe4fe78ed7a8c5a92b2cf5708abb9b]]]There are many examples of successfully solving 'supervised learning' using neural networks. However, until DQN was announced in 2013, there were almost no examples of successfully solving 'reinforcement learning' problems using neural networks (the only successful example of learning backgammon using neural networks [ 10] is). Why is it difficult to apply neural networks to reinforcement learning algorithms (especially Q-learning)? QHow can I use learning and neural networks well? A hint for the solution lies in the difference between 'supervised learning' and 'Q-learning.'

[[[00000000000000001173---56bd3311a8ea07dd67fcd2ba9a08d916095176aca080a783120d523315865e5b]]]First, let's review supervised learning. Here, supervised learning is explained using MNIST of handwritten digit images as an example. MNIST is mainly used for classification problems. The contents of the dataset are given as pairs of image data and correct labels. The general flow of learning with a neural network using MNIST is shown in Figure 8-4.

[[[00000000000000001174---b31d9e919d11df30ee7c6825bad7611ad834ca4767b82d56a4e1d03063c4dbff]]]
Figure 8-4 Flow of supervised learning


[[[00000000000000001175---2a06b79c150b6b1d4301a6d90d1cf1947cbbc034e3025ddea76c44df316979f9]]]Randomly pick some data from the training dataset, as shown in Figure 8-4. This retrieved data is called a mini-batch. Use that mini-batch to update the neural network parameters. When creating the mini-batch here, care must be taken to ensure that the data is not biased (for example, only '2' images are included in the mini-batch). In neural network training, it is common to randomly draw from the data set to prevent data bias.

[[[00000000000000001176---9ce18f56f62e205cee678996a2637e219243cede76635cd8d9241e69cc8f3d5f]]]Next is Q-learning. In Q-learning, data is generated each time an agent performs an action on the environment. Specifically, we update the Q function using the obtained at some time. Let's call it 'empirical data'. This empirical data is obtained over time, but there is a strong correlation between the empirical data (for example, there is a strong correlation between and). In other words, in Q-learning, learning is performed using highly correlated (biased) data. This is the first difference between supervised learning and Q-learning. A technique that bridges this gap is experiential reproduction.

[[[00000000000000001177---d0aed529496b6b27188d5eab0203a3dcf3f88281364d6f208bcace0bb9b387af]]]The idea of experience replay is quite simple. First, the data experienced by the agent is saved in a 'buffer' (a buffer is a storage device that temporarily stores data). Then, when updating the Q function, we randomly pick empirical data from that buffer and use it (Figure 8-5).

[[[00000000000000001178---811fc4650b248b4db7f221f1f3748bd72dca656c5eeb9fee35685388e1117357]]]
Figure 8-5 Flow of Learning Using Experience Replay


[[[00000000000000001179---2b66a368197d9ae954578516f65353a258b91261df26ff21da752c5d1cca6913]]]Empirical recall weakens the correlation between empirical data, resulting in less biased data. In addition, empirical data can be used repeatedly, resulting in better data efficiency.

[[[00000000000000001180---68897ec43a637f53fc3130f1afe119c0700be5808eaa6c1cf985b0ff5fabdee5]]]Experience recall can be used not only in Q-learning but also in other reinforcement learning algorithms. However, experiential recall is only available for off-policy algorithms. Policy-on types cannot use experience recall because they can only use data from the current policy—in other words, they cannot use experience data collected in the past.

[[[00000000000000001181---d7ed38ae4d1fabb0ebcabccea70405bd0abba9803a54433d5821b41dc7fa38c3]]]Now let's talk about how to implement experience playback.

[[[00000000000000001182---cf9b465bf567e3cbafdc6adcd1ddfddee96c10510372ddb6e4ac6a54346c9883]]]Implementing experience playback

[[[00000000000000001183---b6a2bae266fb47bfcea6a73bcef8f7b0347e7fbb7894a1236051078cf5351c7c]]]The experience playback buffer cannot actually store infinite data. Therefore, the maximum size is decided in advance. Set the maximum size, for example a buffer that can hold up to 50,000 pieces of experience data. And when data is added beyond the maximum size, the oldest data is deleted. That way, the latest data will be stored in the buffer. Collections.deque in the Python standard library is a good choice for storing data in this 'first in first out' format.

[[[00000000000000001184---9aa2f1aa7425b3d189494651ae7070568409050e0684e7c87657f27a8dc65014]]]Now, implement the experience playback mechanism with a class name of ReplayBuffer. Here is the code:

[[[00000000000000001185---4b97a0c5b7b083a05e2c75a8f6b795d9215b4c15f61cfd744ac105e2eac4870f]]]First, it receives buffer_size and batch_size as initialization arguments. buffer_size is the size of the buffer and batch_size is the size of the minibatch. Initialize the buffer as self.buffer = deque(maxlen=buffer_size). By using deque, you can add data like a list. Furthermore, if you add data exceeding the maximum size, the oldest data will be deleted.

[[[00000000000000001186---e89235876155c305d231ad3117902ac5fd56700606fa4913106c0ba174cecedc]]]The method for adding empirical data is the add method. The data to be added to the buffer is (state, action, reward, next_state, done) in one 'chunk'. Then there's the __len__ method, which lets you use the len function to get the size of the buffer. For example, if replay_buffer = ReplayBuffer(50000, 32), you can get the data size currently added to the buffer by len(replay_buffer).

[[[00000000000000001187---1599d0ca9358d3ce89ce9a14e21274b4dda60bb590f22583515053fe768f5063]]]Finally the get_batch method. This is a method for creating a mini-batch from the data in the buffer. Specifically, it takes random data from self.buffer and converts it to an np.ndarray instance for easier processing by the neural network. The code inside is shown in Figure 8-6.

[[[00000000000000001188---ba1c60db3dd5144f5db017c110113bbc64655371796195e5a3f182405f1fa9ed]]]
Figure 8-6 Code example for converting minibatch elements to np.ndarray


[[[00000000000000001189---51adc4e9667714890a29deef5ec6891f7cb872bab74a9d59d65ee069bc71f66f]]]Now let's use experiential playback in the cartpole environment. Here is the code:

[[[00000000000000001190---2e1752b0dd73f4e0559cf6ed3b19d5e7fe5ec3d982782ebccdb11dd93d3c0db2]]]Here we do 10 episodes. Each episode always does only the 0th action and adds the resulting data to the replay_buffer. And finally retrieve the mini-batch with replay_buffer.get_batch(). As the output shows, you can see that the batch size (here 32) data is retrieved as an np.ndarray instance.

[[[00000000000000001191---35dbc5832812de1045eb4745ba28b15d6799907d42f55e7496f6d590d18dfe28]]]This concludes the experience playback implementation. Next, I will explain another core technology used in Q-learning, the “target network”.

[[[00000000000000001192---dee6d393ded3b0c4496a8ab13c074714e4631fc76dbb2f3c9ff3e9d78cdf25ca]]]Target Network

[[[00000000000000001193---3338e927719ae368f5170ddf03f8870ff08b7de4194dc8afec5c1e82b783124c]]]Again, we will compare supervised learning and Q-learning. In supervised learning, correct labels are given to training data. At this time, the correct answer label for the input is unchanged. For example, suppose you have an MNIST input image and the correct answer label for that image is '7'. Then its label will always be '7'. Of course, the label will not change from '7' to '4' in the middle of training the neural network.

[[[00000000000000001194---6e78baa87bfd6feaa31313c8db00c90fda3224841a4fb1469508ed16b8183cd5]]]So what about Q-learning? In Q-learning, we update the Q-function so that the value of is -- we call it the 'TD target'. TD targets are equivalent to correct labels in supervised learning. However, the value of the TD target fluctuates when the Q function is updated. This is the difference between supervised learning and Q-learning. To bridge this gap, we use a technique called target networks, which fixes the TD target.

[[[00000000000000001195---14c7908278845073da560eacfb11046c597a2ac20e40434e8f708b6f6d085f58]]]I will explain concretely how to realize the target network. First, prepare the original network representing the Q function (let's call it qnet). Separately, prepare another network with the same structure (this is called qnet_target). qnet updates by regular Q-learning. On the other hand, the qnet_target should be periodically synchronized with the qnet weights, otherwise keeping the weight parameters fixed. After that, if you use qnet_target to calculate the value of the TD target, the fluctuation of the TD target, which is the teacher label, will be suppressed. As a result, the TD target, which is the teacher label, does not (always) fluctuate, so we can expect stable neural network learning.

[[[00000000000000001196---c4c6643c29b2baaaa590c2768d85d40408c1b99b562e44144247fcb2d2d426e3]]]A target network is a technique for fixing the value of a TD target. However, the learning of the Q function will not proceed unless the TD target is updated at all, so try to update the target network at regular intervals (say, every 100 episodes).

[[[00000000000000001197---51b436ca0b6b720513047b3751fc920f2e5a056c6e78cd948433b4b1b540500e]]]Target network implementation

[[[00000000000000001198---fbcfa0624858dff32eda25a4fece5669b942387f8822be65528ae45193f98a9b]]]Now let's look at the target network in code. Here, in anticipation of the implementation of the entire DQN that will be done in the next section, I will show a part of the code of an agent called DQNAgent.

[[[00000000000000001199---8460b252a1469205333476c244168223f5d3b94e1d65ea497a158c6f13c5d57e]]]# configure qnet

[[[00000000000000001200---0edd61ff1ce7bb1ab4d217ea324ef92e96cf6cc5ddfca52c8407aa2548aa455c]]]# add batch dimension

[[[00000000000000001201---fad598fdf309b0ca55bbf0ed232825eee3b58a98a4b7891702a19242625f0a1c]]]First, implement the QNet class for neural networks. And the DQNAgent class of the agent will have two neural networks, self.qnet and self.qnet_target (both networks with the same structure). And only register self.qnet with the optimizer. Now the weight parameter update is done only by self.qnet (self.qnet_target's weight parameter is not updated by the optimizer).

[[[00000000000000001202---08d91a303f870137d2e393d3753ef3f59ffb1044edccaabd96285912039cfa0b]]]Next is the sync_qnet method. This is a method for synchronizing neural networks. There we use the copy.deepcopy method in the Python standard library. Deepcopy means 'deep copy' and is used when you want to completely duplicate all data. Here we make a full copy of self.qnet and call it self.qnet_target.

[[[00000000000000001203---ff1a73bfa2e3dd3feba458224e5296febaa9860a5e77bd7b05ede8eaeda0e52c]]]The copy module has 'shallow copy (copy.copy)' and 'deep copy (copy.deepcopy)'. A shallow copy is for data efficiency where only the 'references' to the data contained in the object are copied. If we were to use shallow copies here, the two neural networks would share the same weight parameters.

[[[00000000000000001204---214f9a21ee5ba4d17caa7bec9f1575c8ffac512d2296c31f353312c49f957913]]]Finally we show the method for updating the weight parameter in the DQNAgent class. Here is the code:

[[[00000000000000001205---94fd016436520b6c1ab9bcc876059b6ce2ce5d7ec26b51aa5121ecd97e8e8b36]]]When the update method is called, experience data is first added to the buffer (self.replay_buffer). Then, when the empirical data equal to or greater than the minibatch size is stored in the buffer, we retrieve the data from there as minibatches. Codes 1 to 4 are explained in order below.

[[[00000000000000001206---09b7dab5163330a23b2a8dfd9e0044fa3343d455d07ab1892fc31bd294b0a9f1]]]state is an np.ndarray of shape (32, 4) (batch size is 32 and state size is 4). 32 pieces of data are collectively given to the neural network (self.qnet). Then the output qs will be of shape (32, 2). In the Cartpole task, the action size is 2, so the Q function for each action is output.

[[[00000000000000001207---afcbdb8f63976c7559590aec956a8f7f60cb30f44b123a36c3c668aa2f8ebf19]]]action is an np.ndarray of shape (32,). This action contains the action taken by the agent. For example, data like [0, 1, 0, 0, ... , 1]. Here we retrieve the corresponding element of action from qs as shown in Figure 8-7. This can be handled with the code qs[np.arange(32), action]. Figure 8-7 Processing performed by qs[np.arange(32), action]




[[[00000000000000001208---e81745780b564f8c70b96e7a865bd7f368aa8f5c7fe85a3b5f01d1d9bf86afc8]]]Find the value of the Q function in the next state by next_qs = self.qnet_target(next_state). Note that we are using self.qnet_target (rather than self.qnet) for the calculation. After this, we retrieve the Q function in the next state by next_q = next_qs.max(axis=1). By setting axis=1, the maximum value is extracted for each batch data.

[[[00000000000000001209---0b08c752313de8fd43cfdbb7c144e17c67c8bd42ac1f4ba674b6c79e49890ea8]]]done represents a finished flag. However, it is (implicitly) converted from bool to int in the calculation 1 - done. Use this 1-done as a 'mask' to calculate the TD target.

[[[00000000000000001210---bac6e944693f6cdba10edbef736ae1e1a2df37d6ab68a1c81f81ceb34b797154]]]The above is the code for the DQNAgent class. This concludes all the explanations about the core technology used in DQN. Next, let's actually move DQN with the cart pole problem.

[[[00000000000000001211---be570e23bc9143cf4e99c2d34882aaed03341aedd66a4429f073cff53dbf32ad]]]Move DQN

[[[00000000000000001212---dbd3c4aa4a5b333c0e3b55ddbf953c3f533f0a669f20b08071f9b01919832362]]]Now let's use the DQNAgent class to run the cart pole problem. Here is the code:

[[[00000000000000001213---c99c83cd64a1cf3885a110c605d92599430bc614cfaeda0447fcfb7086d4b24c]]]I'm going to let you play 300 episodes in total here. It also calls agent.sync_qnet() every 20 episodes to sync the target network. The rest is pretty much the same as the code we've seen so far.

[[[00000000000000001214---1d988927d5ba8b900f513c8c5052d41e14eb0f9328e47889b46da784702a39ef]]]Now run the code above (it will take a few minutes to finish). The reward_history records the sum of rewards obtained for each episode, so let's plot it. Here are the results (this result will change from run to run):

[[[00000000000000001215---05aba51e1757e8841ea199021b5b3b33ceb8b115d0c77b784a938bbcceb48dcf]]]
Figure 8-8 Changes in the sum of rewards for each episode in cart pole


[[[00000000000000001216---299693b457892872c6e79360026e69bf11016a7f87569de9f8dd0364652efab9]]]The horizontal axis in Figure 8-8 is the number of episodes, and the vertical axis is the sum of rewards. For our task, the sum of the rewards is the time (timestep) the pole was kept in balance. As shown in Figure 8-8, we can see that the sum of rewards increases with each episode. However, the variation in the graph is large, and it is difficult to judge from this alone. When evaluating reinforcement learning algorithms, it is dangerous to judge based on the results of a single experiment. A better approach is to repeat the same experiment and average the results obtained. The following figure shows a graph of the average results of 100 repetitions of the same experiment.

[[[00000000000000001217---e3640e238b7661bc69b0e35176e98e1510ad25ba254c29ab2af76adeab25d64b]]]
Figure 8-9 Results averaged over 100 experiments


[[[00000000000000001218---ff52de54d60ee3286d9b6f7cf119563e64ab5ebb6030ff454ad9e4e03b5781c9]]]From Figure 8-9, it loses its balance quickly at first, but after about 50 episodes it starts to get the hang of it. Learning progresses smoothly up to 150 episodes, but after that it slows down a bit. Overall, however, learning seems to be progressing in a positive direction.

[[[00000000000000001219---d33ed7483a9a507cee8a519d7d7fd9681c1370f6d53a24681b6ee46a188d13d6]]]In addition, the agent in the middle of learning will act according to -greedy. In other words, it will act randomly with a probability of . Let's make the agent perform greedy actions after learning. Here is the code:

[[[00000000000000001220---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001221---94a53dfd417ec652def51b59fac837a5b47e3fcb431693227a2f39a7b47abd7d]]]From the above results, the agent after learning was able to balance up to 116 steps by performing greedy actions. The results will vary each time (cart poles have slightly different initial states each time), but they are generally over 100. It's not perfectly balanced yet, but we're learning in the right direction. Also, tuning the hyperparameters—especially increasing the number of episodes—will yield even better results.

[[[00000000000000001222---3ce3ef48c27e2adc89fddd434a495462208188ebf70ede4edd54fd399f54c0b2]]]Hyperparameters are values preset by humans. In our code, the following items correspond to hyperparameters:

[[[00000000000000001223---55f5e830ab1cef13634cd987ef29dfc34bf52b6e4bd89b0eeff391f53e45f273]]]discount rate (gamma = 0.98)

[[[00000000000000001224---5512a520a49d335fdd25a0e58dad872f1117e457e3999787cf02a189ffbe9f81]]]learning rate (lr = 0.0005)

[[[00000000000000001225---4f7849737091a3dbcb50c1e5dd39c13e583a73528a4a0f8cbc6986ae91a93edc]]]-greedy method (epsilon = 0.05)

[[[00000000000000001226---17c8c9ddf3d369f5c3d03be1fb0f6a6438c8da6f824189cbe371e52fadeb876c]]]Buffer size for experience playback (buffer_size = 100000)

[[[00000000000000001227---386d0993f8a40f818a171f694f19fdb50e7d2377640a226e9542b06936a074c8]]]Mini-batch size (batch_size = 32)

[[[00000000000000001228---3ccc2ac9fc972c6fd9aa81c1b6db02df20439193a4ca8b49538fabcf866db5a7]]]number of episodes (episodes = 300)

[[[00000000000000001229---fabdc3a09a18307040c2909825e4b86d4edfc8216278f9961c4e88a9b13c07d3]]]Synchronization timing (sync_interval = 20)

[[[00000000000000001230---63098f3173005eb86462727a15ad83062d2c88e32ed8c90b052203417233edcb]]]Neural network structure (number of layers in neural network, size of nodes in Linear layer, etc.)

[[[00000000000000001231---e506777dc6e9bed22fe9d9f6fd09804e1d0982cb72c1806d12f2b2a4302f8c5d]]]DQN and Atari

[[[00000000000000001232---f69902bb905a1a3bf7722a136ce8a18942bf995e76b715071120925067339732]]]DQN is a method proposed in the paper 'Playing Atari with Deep Reinforcement Learning' [11]. Atari in the title is the name of a company that makes computer games. In the field of reinforcement learning, the old game software produced by Atari is called 'Atari'.

[[[00000000000000001233---a685888f56634d9e369abebf7622c54e3c74d6176c0bb9adde6a92af00840484]]]After DQN was announced in 2013, a paper on DQN (titled 'Human-level control through deep reinforcement learning' [12]) was published in the scientific journal Nature in 2015. In that paper, it was shown that DQN can play at the same level as humans.

[[[00000000000000001234---ba434772d3f1b185b8df79f179776b2f4c87666de69e28a470f20c7a4129f5cd]]]We were able to train cart poles using DQN. But what about a game like 'Atari', which is a more difficult task? I have good news and bad news here. The good news is that by making some changes to the DQN code we just implemented, we can learn Atari well too. The bad news is that Atari takes a long time to train (you can train it faster with a GPU, but it still takes a full day). Therefore, I will not provide the DQN code for Atari here, but will focus on the differences from the DQN in the previous section.

[[[00000000000000001235---9cb7ce2ef6f99c9f2a10fdf243e04ab036c1dca49927ef273878b56bed88d9df]]]Atari game environment

[[[00000000000000001236---a7b71b738c6f660ed7f6b29e18b43f384784b6beea83905511d91fe5beea231c]]]OpenAI Gym also has an Atari game environment. To use Atari, a separate installation is required (for details on how to install, please refer to the OpenAI Gym documentation, etc.). In addition, Atari has a variety of games. Here, we will focus on the game 'Pong' in Figure 8-10.

[[[00000000000000001237---d0025bb970ea35847a4b7a8c05205ddb36d09dcbdfc253f88b37479986acf767]]]
Figure 8-10 Screen with 'Pong' drawn


[[[00000000000000001238---005b6fc4f629ea8daa27f30f6b4571adcf9f9ac847222b0bcbdaf74f25adfd25]]]'Pong' is a ball-hitting game as shown in Figure 8-10. The enemy moves the board on the left side, and the user moves the board on the right side up and down. If the ball goes beyond the opponent's territory, points are added. The agent is given an image of the game as its state (specifically, a 210x160, 3-channel RGB image).

[[[00000000000000001239---4ea6ca0a974c535e2b8d1ea1cab17e49d222546c24913b2fa04f76cd075d0669]]]This game 'Pong' can be solved similarly using the DQN in the previous section. However, it is a more complex problem than cart poles and requires some ingenuity. I'll get to that point now.

[[[00000000000000001240---299ca99b8abf80e9998011a9b447dbea49510bba086afc662faf5f538d2f862e]]]Preprocessing

[[[00000000000000001241---eef87d5bb0c0bdc486ff3e75a8a9d22281c08475e91ab49fcd3ce39ed1bd0c00]]]The theory of reinforcement learning discussed so far in this book was premised on the MDP (Markov Decision Process). In MDP, the 'current state' contains the information necessary to determine the optimal course of action. Unfortunately 'Pong' does not meet the MDP requirements. This is because we cannot tell which direction the ball is traveling from only the image shown in Figure 8-10. Of course, the optimal action cannot be performed when the direction of travel of the ball is unknown. Such a problem is called POMDP (Partially Observable Markov Decision Process).

[[[00000000000000001242---cd12a2e846c7898d8a8fe911cffef1a1551fcb5f88b6704c5a7cda3e47f1c05c]]]Converting POMDP to MDP is easy for video games like 'Pong'. The method is to use consecutive frames. The DQN paper [12] superimposes 4 consecutive images and treats it as one 'state'. By using a sequence of images, we can see the state transitions (and see the movement of the ball). Now you can treat it as MDP as before.

[[[00000000000000001243---28c27c1438f2a6a552645b007df698545b533e090951ee6d4ec372ca04e4a0ef]]]Reinforcement learning for POMDP is also actively researched. In POMDP, current observations alone are insufficient, so past observations must also be taken into consideration when making decisions. A powerful method in POMDP is the method using RNN (Recurrent Neural Network). By using RNN, it is possible to take over the data input in the past and perform calculations.

[[[00000000000000001244---bd404ee820a8fcaae8daca1a9a9ddd0324222f140f67304b2278629213eeb990]]]Also, in the DQN paper, fixation is done before overlapping the frames. Specifically, the preprocessing is as follows.

[[[00000000000000001245---adf6bee19f328d0231bc1d6a9c85cd763e3ec8f5e3af5fc6c7ab3e9e409b72a5]]]Crop around the image (because there is wasted material around the image)

[[[00000000000000001246---0b57f89f614f1bd4df1505a643e3804282355593e1a7ed99b2c4f25275e53c82]]]Convert to grayscale (black and white image)

[[[00000000000000001247---9e6f1bf72198fb2cb6f7585a100f2a2aab8733bf0a5a66638d09239466d6e30b]]]image resize

[[[00000000000000001248---de37fd8db25cd85ff2779eb01f08864174de36f70d22ec10b0b892cb4303563c]]]normalization (converting image elements between 0.0 and 1.0)

[[[00000000000000001249---b485553c2048a65ba331fb3735aea51c6f146d970980e321b6b31aa323f9cc70]]]After performing these preprocessing, the work of stacking four frames is performed.

[[[00000000000000001250---931621185043369ce8d70ddcda78805699f92c4d045fa45fcb8ae3d5538bcc55]]]Cartpole in the previous section used a neural network consisting of fully connected layers. On the other hand, CNN (Convolutional Neural Network) is effective when handling image data like Atari. A CNN is a neural network that uses convolutional operations (convolutional layers). The DQN paper uses a CNN with the structure shown in Figure 8-11.

[[[00000000000000001251---6e284165ce33baff9186a858d7ab5669ccadd0f0214ff12a3198ccf594301255]]]
Figure 8-11 CNN used in DQN


[[[00000000000000001252---f2c9e8ebe9eb3ca0c02e1540a66089dc4ecc9d0da2ad68844422868337b21199]]]As shown in Figure 8-11, the layers near the inputs use convolutional layers and the layers near the outputs use fully connected layers. The final output layer outputs the number of action candidates according to the task. The ReLU function is used as the activation function.

[[[00000000000000001253---42f00c69bd522ae4cdd86d826f9e7abd3f82fa8b5f6b00c7be3fe6627c086115]]]Other tricks

[[[00000000000000001254---15dcf68b7ebe0bdae88e931359d517bf3ac88ab62671f58e988366b31288c3c4]]]In addition to the tricks discussed so far, the DQN paper also uses the following techniques: These are valid not only for DQN but also for other reinforcement learning algorithms.

[[[00000000000000001255---a0d44305753125163e6f1be77d0d73a3537dc6d59093b3df3f3913c572927674]]]GPU usage

[[[00000000000000001256---622c61b01122574bc4b5f9af1faa01a27aff7e016e42e36bd86c7ba4d1ee8c5e]]]Games like Atari handle image data, so the data size is large, and the amount of calculation required for learning is also large. In order to speed up this calculation, it is effective to perform parallel calculation using GPU (or TPU). By the way, DeZero also has a function to calculate using GPU.

[[[00000000000000001257---24031a07bd67e6d7d515a3186360e9fdc3b9b64009a98a0157e30d65d41f102a]]]Adjusting ε

[[[00000000000000001258---ab32cd096f49fff1e10404625456869739fea13a36a06cec9bb7210fe1a045df]]]In reinforcement learning, the balance between exploitation and exploration is important. Given that the value function becomes more reliable as the agent's experience increases, it makes sense to reduce the exploration rate proportionally to the number of episodes. In other words, let the agent explore more in the early stages, and explore less (increase utilization) as learning progresses. In order to realize this idea with the -greedy method, it is possible to reduce the number as the agent repeats actions. The DQN paper adopts a method of linearly decreasing from 1.0 to 0.1 for the first million steps and then fixed at (Fig. 8-12). Figure 8-12 Changes in DQN




[[[00000000000000001259---2f294e0ce13e59ae8d6fa20efbb7b26a287cfbff3ad4a51629f1a97c8f78f0c8]]]Reward Clipping

[[[00000000000000001260---bc0f03335440fcb0646795e80bdb552a973dad537195083dd45a9be7f7d5fa90]]]In the DQN paper, the rewards are adjusted to fit in the range from , and the reward scale is aligned to facilitate learning. However, for 'Pong', the reward is either , , or . So the reward scale is reasonable and performance is not impacted without reward clipping.

[[[00000000000000001261---e658f3994591e9010c31fcc927ae06e8ef515394d66ccd354f8344552fd51c63]]]By adding the above ingenuity, DQN will be able to play 'Pong' well. In the thesis, it grows to the point where it can beat the computer opponent with ease. Also, not only 'Pong' but also other games (such as 'Breakout' and 'Pac-Man') can be run without changing the code. And grow up to play beautifully. From this point, we can see the high versatility of DQN.

[[[00000000000000001262---742fa4e94444332732bd65e399bf335d5b307e9a0950637effe8f9a0a3889367]]]Extension of DQN

[[[00000000000000001263---f3a71150a89adcd36503602548d369b287ff685dc5a9cd8500c49390b35bc9c0]]]DQN is one of the most famous algorithms in deep reinforcement learning. Since DQN was announced, many methods have been proposed to develop DQN. Here are three of the most popular methods.

[[[00000000000000001264---b85f1561405270648750661271f8577d812617ca3f695f758e9810877ce8c11a]]]The first technique is Double DQN [13]. Before explaining, I will start by reviewing DQN. DQN uses a technique called 'target network'. This is a technique that uses a network with different parameters (= target network) in addition to the main network. Let us denote the parameters of the two networks by and respectively, and denote the Q function represented by the two networks by . At this time, the target used in updating the Q function is expressed by the following formula.

[[[00000000000000001265---2399af26d30b60302b2e671db3a23b7cf0935f2bddfc2113fee83e75ce694408]]]DQN learns the value of to be close to the value of the above equation—this is called the 'TD target'. Here's the problem. Specifically, using the operator on the error-filled estimate () is overestimated compared to computing with the true Q function. Double DQN solved this problem. Double DQN takes the following expression as a TD target:

[[[00000000000000001266---f2e7765e343b32d4133a8cf949255eae1dfa3e9687c4f065577f9375240385f3]]]The point is to use to pick the action that gives the maximum and get the actual value from . By using the two Q functions differently in this way, overestimation is eliminated and learning becomes more stable. 'Appendix C Understanding Double DQN' explains what exactly overestimation is and how it can be eliminated. Please refer to it if you are interested.

[[[00000000000000001267---7a6d964d7e19ee3bad06b51aced886426bda8df659f07be8263ff3398bb70e81]]]Prioritized experience playback

[[[00000000000000001268---49f01716f83ff0ad8426bab5a9c00e10d04a10441bd6943b1535a4d02fbe01ea]]]In the experience playback used in DQN, the experience is stored in a buffer, and the experience data is randomly extracted from the buffer and used during training. A further evolution of this is Prioritized Experience Replay [14]. As the saying goes, instead of choosing experience data at random, we make it easier for them to be selected according to their priority.

[[[00000000000000001269---933a33529c0af4249f68207cab01b6a10b482962aff5d3063f5452d9b21522cf]]]So how can we prioritize empirical data? A natural conceivable expression is:

[[[00000000000000001270---dd2e392ce7191d67cfc365e1c6401390ad65ec97a22239f19b9757faea3ad444]]]As shown in the above formula, take the difference from the TD target and find the absolute value. The bigger the , the more you have to fix – which means the more you have to learn. Conversely, if is small, you already have good parameters and you have less to learn.

[[[00000000000000001271---7a6eddd13672ca96174eb6258b30c28343e26e64524fd3fe036f99ae9d40855d]]]Prioritized experience playback also calculates when saving experience data to a buffer. Then add the data also included in the empirical data to the buffer. When retrieving empirical data from the buffer, use to calculate the probability that each empirical data is chosen. If empirical data is included in the buffer, the probability of choosing the empirical data is expressed by the following formula.

[[[00000000000000001272---f6fb96a2a1e7338e0a99c9ba2c28bde6c4a138a6e07054a38d2114ee466d4660]]]Empirical data is picked from the buffer according to this probability. By using prioritized experience playback, the data that has more to learn is prioritized and used, so learning can be expected to progress faster.

[[[00000000000000001273---59bb09482cf8781bc3c9b21264489cce433fb11174e63817956b454def566ae5]]]Finally, we introduce a method called Dueling DQN [15]. Dueling DQN is a technique that devises the structure of neural networks. The key to this method is the Advantage function. The advantage function is defined as the difference between the Q function and the value function. The formula is represented by the following formula:

[[[00000000000000001274---e9d5fc405c509fd0455741fb316c035a069d814408dfd9fa82189062a156e932]]]The advantage function in equation (8.1) describes how much better (or worse) the action than the policy. The reason for this can be understood by considering the following points.

[[[00000000000000001275---5630048aba290cad91d3bbef35e316c00bbf138cfe332fa6588ef060e6cc4f94]]]is the expected value of the profit obtained when performing a 'specific action' in the state and thereafter acting according to

[[[00000000000000001276---8d638f0add41a42759b52723cf08a62a0b39a97e993af8ec4c58a4ac23fdba9d]]]is the expected value of profit obtained when all subsequent actions are taken in accordance with the policy in the state

[[[00000000000000001277---f098498291b2ead215f646e8356cf94a9cbb6ab1c5309624b6596fcea3381665]]]The difference is whether we act in a state or choose an action according to a policy. Therefore, the advantage function can be interpreted as a measure of how well the 'behavior that is' is compared to the 'behavior that is chosen by the policy.'

[[[00000000000000001278---5c3664105cc3d5831a8a9fa0808e23505bde7913abaa0d551e1a2caa45f79945]]]Also, the Q function can be obtained based on the advantage function of formula (8.1). To do so, transform the advantage function in Eq. (8.1) as follows.

[[[00000000000000001279---2acffb7a689666c1d0967aa6a559fcff28ffe2c5f23a416d8529f3a4de8e2c7b]]]Dueling DQN expresses equation (8.2) with a neural network. Its structure is represented in Figure 8-13.

[[[00000000000000001280---56c0ed5cd64e651b46d1992181a33d0c57f5e7da088187f0d20a79ec53740a48]]]
Figure 8-13 Comparison of DQN (upper figure) and Dueling DQN (lower figure) (Figure is taken from the paper [15])


[[[00000000000000001281---f1ed1b43c78aeaa0ecb13827ab33617b54d05f8c010160aa237cdde901e2c4fc]]]As shown in Figure 8-13, share the calculation halfway as a network and separate it so that it branches into the advantage function and the value function. And finally add the two together and print . This structure is a feature of Dueling DQN.

[[[00000000000000001282---902d3723d53846b41dc52137336725f11c04f8583622d0cb0c7dada5eeb0ff1e]]]So what is the advantage of learning the advantage function and the value function separately? This is mainly seen as an advantage in situations where the outcome of any action has little effect. For example, consider the case in Figure 8-14 in 'Pong'.

[[[00000000000000001283---171363aa17f2a3e3ccf5e467a9df6d66393382701efb8aae91b3d80c420dfac1]]]
Figure 8-14 Screen just before the ball enters your territory


[[[00000000000000001284---bdb191b8e987595ba11865fc41a0bddfac6aaed81e29722ab77104d302750f5a]]]In the case of Figure 8-14, the result is a loss (negative reward) no matter what action is taken. In the case of DQN, it learns about actions actually performed in a certain state. As shown in Figure 8-14, even if the outcome is determined regardless of actions, learning is not possible unless all actions are tried. Dueling DQN, on the other hand, goes through a value function. A value function is the value in a state (does not consider behavior). Therefore, experiencing the states in Figure 8-14 will learn and thereby improve the approximation performance without trying other actions. This is expected to facilitate learning.

[[[00000000000000001285---a69b669df20f8d9100b7ed2954b54af2101819df8e1517dd71c98d774a8029eb]]]So far, we have introduced three methods for extending DQN. In '10.3 DQN Sequence Evolution Algorithms', we introduce further DQN-based algorithms.

[[[00000000000000001286---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001287---a8b48e9b9bf5d85b97021ffa01b034fb6dba2c52448132ee77be088610c51ee2]]]In this chapter, we learned about DQN. The key technologies in DQN are Experience Replay and Target Network. Experience playback is a mechanism for repeatedly using experience data. Empirical replay improves data efficiency and reduces the correlation between samples. A target network is a technique that computes a TD target from another network. Using a separate network stabilizes the training of the neural network because the TD target is fixed.

[[[00000000000000001288---786b80313650de239b5875303a3b1efc3fa99b91dbb9bbdf4d5a2511c4784f6b]]]Although some time has passed since DQN was announced, DQN is still one of the important methods. Various methods based on DQN are still being proposed. In this chapter, we introduced three extensions of DQN—Double DQN, Prioritized Experience Replay, and Dueling DQN.

[[[00000000000000001289---cf53f8ba70231c87c9b5c508d2e8c00c13ce13851dab4a0facde2729ba7af63f]]]You also learned about the OpenAI Gym in this chapter. Various tasks are prepared in Gym, and since the interface is common, you can easily switch between various tasks. In this chapter, we solved the task 'cart pole' in Gym using DQN. I also explained the techniques needed to get Atari to play.

[[[00000000000000001290---dba6f0d126a635bb9165e613e13503eec4ebe84a53f470a1bd360d8a7fbea171]]]System names and product names used in this document are trademarks or registered trademarks of their respective companies. Note that the ™, ®, and © marks may be omitted in the text.

[[[00000000000000001291---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O'Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000001292---54e0dd1ef64452a08578a4f166022525e061fac7980ad8281b0609224b2e27d3]]]Chapter 9

[[[00000000000000001293---a366443da28171667b836c033a96b7c44cf18805f5a70500d8730bbc49be05d6]]]policy gradient method

[[[00000000000000001294---6d561a1bf0fb6c2761b80ba2625cb46c741fb26635f77670cb5d8f2f08a791d7]]]So far we have learned Q-learning, SARSA, Monte Carlo method, etc. These methods can be broadly classified as value-based methods. 'Value' here refers to the action value function (Q function) or the state value function. Value-based methods model and learn value functions. Then we get the policy 'via' the value function.

[[[00000000000000001295---abc20889d79be3ef13df3ece3a76ce5364e01690eb24c9b609707f3c1a5f97f3]]]Value-based methods often rely on the idea of generalized policy iteration to find the optimal policy. Specifically, by repeating the process of evaluating the value function and improving the policy, the optimal policy is gradually approached.

[[[00000000000000001296---3009ce4775a29a921cbcbe2d8511cae5b0aeb51b0a464e53b68f9fbe956816dc]]]In addition to the value-based method, we can also consider a method that expresses the policy directly without going through the value function. This is the Policy-based Method. Among them, a method that models a policy with a neural network or the like and optimizes the policy using gradients is called the Policy Gradient Method.

[[[00000000000000001297---648957dafdae2139a5d2319ffd0c9ae4b6f1bfd7319a70bd61d4f4bcae364759]]]Various methods have been proposed for algorithms based on the policy gradient method. In this chapter, we first look at the simplest policy gradient method. Then, in the process of improving the simple gradient method, we derive an algorithm called REINFORCE. Furthermore, in the flow of improving REINFORCE, we derive a method called REINFORCE with baseline and Actor-Critic.

[[[00000000000000001298---80b490407b881a14cdfeee956857e53c6cf0e17eda792c890e178dd3692262e9]]]Simplest Policy Gradient Method

[[[00000000000000001299---6816e9bb69cb5d1feb10041a691f1818c91dbc84e881ee751d53581de4526ac0]]]The policy gradient method is a general term for methods that update policies using gradients. There are many policy gradient algorithms, but here we derive the simplest policy gradient method. From the next section, we will show new methods while improving based on the methods learned here.

[[[00000000000000001300---17f92cbd8c5116dc25e5b34850c7eb8c44ce9a56b41ff111ba59ccb1291541f1]]]Derivation of Policy Gradient Method

[[[00000000000000001301---23d628a3ce30bdcf4d6e8ee6d93edf62364e17439216f298e97721ad2af0d9f6]]]A probabilistic policy is represented by the formula is the probability of taking action in the state. Here, we model the policy with a neural network. To do so, we collectively denote all the weight parameters of the neural network with the notation . And we will represent the policy by the neural network by .

[[[00000000000000001302---ca48c261cd13afc4d68bf8f5d2f80733aa239d24d8f61c35bb5a4613049d19c5]]]Then use the policy to set the objective function. After setting the objective function, all that remains is to find the parameters that maximize the objective function. This is called 'optimization' and is the usual neural network learning process.

[[[00000000000000001303---6356dd1afe4c5499018d3351f75af3ebe4e4c14491e402c509bc60a241073dd6]]]For optimization problems, here we set the objective function instead of the commonly used loss function. For the loss function, find the minimum by gradient descent. On the other hand, the objective function finds the maximum value by gradient ascent. The gradient descent method updates the parameters in the direction of the negative multiplication of the gradient, and the gradient ascent method updates the parameters in the direction of the positive multiplication of the gradient. However, if the objective function is minus, it can be treated as a loss function (and vice versa), so the loss function and the objective function essentially play the same role.

[[[00000000000000001304---2548f72e324c619d94e69f0fcc99740df32e4cdda4720806b31a2d79c0f759fc]]]Now use the policy to set the objective function. First, clarify the problem setting. Here, we consider an episodic task and consider the case of choosing an action according to the policy. In that case, assume that the following time series data consisting of 'state, action, reward' was obtained.

[[[00000000000000001305---316de4dbb0dfaf8d557a5fe3a0b4a4623becbddcf9be791ff480ac4efbb40d80]]]is also called a trajectory. At this time, the profit (return) can be set using the discount rate as follows.

[[[00000000000000001306---a146fd33a89acfdd27f3a18c94330de4806d748d907902c5b4a23f608e75458b]]]We use the notation here to make it clear that revenue can be calculated from At this time, the objective function is expressed by the following equation.

[[[00000000000000001307---eee4e3177a89e269620972c2861dac9fa1e0f42a23921bcf77ae2f99628d0c2a]]]Since returns fluctuate stochastically, their expected value is the objective function. In the above formula, '' is written as a subscript of the expected value. This notation indicates that is produced by

[[[00000000000000001308---1acedfb9b3392c03c9b4e5d148298d6594a94b2491b4ea180d231610dd5bf74e]]]In addition to the agent's policy, the generation process of is also related to the environment. But we can only control the agent's policy. Therefore, I will write it only as ''.

[[[00000000000000001309---f9c9708657b22a063c2b6cd9a735e07676e24a7bc3a0a4f7f2d4ecb9ee267727]]]Once the objective function is determined, the next step is to find its gradient. Here we denote the gradient with respect to the parameters by . Our goal is to seek In this chapter, the derivation process is omitted, and only the result is shown in Equation (9.1). The derivation process is explained in 'D.1 Derivation of Policy Gradient Method'. Please refer to it if you are interested.

[[[00000000000000001310---c6f8abb009bd655359e49ff66342e8d2cb50d71b4edbf10e280105d4e52e8258]]]What we want to notice in equation (9.1) is that is in (the gradient computation is done as ). We'll look into that in a minute. is found, then update the parameters of the neural network. There are various optimization techniques, but a simple one is given by the following formula:

[[[00000000000000001311---0159a4437cd3ba5aed9bf6d910d5aa192393aa6cbadfb0688234ee333be65c31]]]Update the parameters only in the gradient direction as in the above equation. Here we represent the learning rate. This is a method that belongs to the gradient ascent method.

[[[00000000000000001312---36094bc6a5f478a522eaa2776f56ef12a8ae1fd75408f60249ea2467ca57ba5f]]]Policy Gradient Algorithm

[[[00000000000000001313---1e22ff10684ddbc1ffb38667e7f9c1d991721027c31445c59dcf55779c21e2f0]]]is expressed as an expected value as in Eq. (9.1). This expected value can be obtained by the Monte Carlo method. The Monte Carlo method takes several samples and averages them. In this case, let the policy agent actually act and obtain a trajectory. In that case, we can approximate by computing the content ( ) of the expected value of Eq. (9.1) at each and finding the average. The formula is represented as:

[[[00000000000000001314---1a8f7a75d3926b65feee1f62ff3197d5f68ac87eb43e27847ba31d14e0cbba89]]]Here, let us represent the trajectory obtained in the th episode, the behavior at the time of the th episode, and the state.

[[[00000000000000001315---a4eb8d1a2aec258a89dc4c5fc35483f417b0a3fd0364311e937bbcd1719af2de]]]Let's consider the case where the number of samples in the Monte Carlo method is 1, that is, the case in the above formula. In that case, it can be simplified to:

[[[00000000000000001316---3da42780c2dcbf869662efeef0b75dda22c8dfc91b8dce50d8cc4615c15d0231]]]For simplicity, this chapter deals with the policy gradient method for Eq. (9.2). Now equation (9.2) finds at all times (), multiplies each gradient by the return as a 'weight', and sums them. Visualization of this calculation process is shown in Figure 9-1.

[[[00000000000000001317---6cac68187d2300733ee690bd9bdca55275d21398fbfad9594a01eb1946d652a9]]]
Figure 9-1 Calculations performed by the policy gradient method


[[[00000000000000001318---d89b551a96ba2f670510f744e4cd4fc8f37a84582d959575964c7e47bcebde1a]]]Now let's consider the 'meaning' of the calculations performed in Figure 9-1. The first derivative yields the following equation:

[[[00000000000000001319---7bff7e18258ea80211f463bfc04e46d0a2e3a439e9969e97a5638c53d026a92c]]]The above formula is multiplied by the gradient (vector) of . You can see that it points in the same direction as this. is the direction in which the probability of taking an action in state is the highest. It refers to the direction in which the probability of taking action in the same state increases the most. A certain amount of weight is applied to that direction.

[[[00000000000000001320---d93560cb925100e0f569cfc962548974d70576e0534812dc946338b8ebe7cbfd]]]For example, an agent earns 100 in revenue. In that case, a gradient is sought that makes the action taken during that time more likely to be chosen, and it is strengthened by a weight of 100. In other words, if you do well, the action you took so far will be strengthened by the amount you did well. Conversely, if it doesn't go well, the action taken in the meantime will be weakened by that much.

[[[00000000000000001321---4629ed40c6fc8d0ad53c7922cbf4f464d484dfcef4e315ce66ee3ef8baa5087e]]]Implementation of Policy Gradient Method

[[[00000000000000001322---8313c83128758d3af64b80667282eb9b52c51715dad35fdd2592de294e42e379]]]We then move on to the simplest implementation of the policy gradient method. First, here is the code for the neural network that represents the import statement and the policy.

[[[00000000000000001323---a66f1fe759582c0dcb84bfaf45eb35d13627c228e31c1c63b59feb62dc7cfb1d]]]Here we implement a model consisting of two layers of fully connected neural networks. The number of elements in the final output is set to the number of actions (action_size). This final output is the output of the softmax function, so we have a 'probability' for each action.

[[[00000000000000001324---66246b2756116e668c140ae45a46e73cc0dd01013790b86e639aa01dcb79f753]]]If you input a vector with elements into the softmax function, it will output a vector with the same elements. At this time, the second output is expressed by the following formula.

[[[00000000000000001325---b7a2e47e0a67ce6630e0ecff0087b4cda191aa71cfd55c7a255f225b8b83e698]]]Here it is the Napier number (followed by the real number). The output values of the softmax function are all real numbers between 0 and 1, and the sum is 1 (). So the output of the softmax function can be used as a 'probability'.

[[[00000000000000001326---eb5af244128527e72487ea2444a796ba7615ccc8305f9ff565b482094d55d45b]]]So here is the code for the Agent class. First, we show the initialization and the get_action method.

[[[00000000000000001327---85b4a7696709649b0487aea5ad3fed6160cbb3c8f6460c5548ba90b06d061e69]]]# add batch axis

[[[00000000000000001328---d2f4f364822082dc4c354efb0a8ccfb87feafff726f6c305e4842df6dda38544]]]The get_action method determines the action in the state. To do so, forward propagating the neural network by self.pi(state) and get the probability distribution probs. Then sample one action according to that probability distribution. At this time, it also returns the probability of the chosen action (probs[action] in the code above).

[[[00000000000000001329---b9bcf1aeab401c2fbc3a7ba7dbee7648b9cd8875e2c8f187aff8129e3f4ccaa3]]]Let's try using the get_action method. The code looks like this:

[[[00000000000000001330---39461dfb6b0e7d089541213e406df13575306b314cb88caffe3c40fba085c7a6]]]# dummy weights

[[[00000000000000001331---4fa129145e8162559782acb2e95f9b1476fb32694c81ac76a603624f3ee581a5]]]# find the gradient

[[[00000000000000001332---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001333---fb26427fcdc5547274b9e523fc1a3587ec95aaea2a4b07eea697a4490ace112d]]]The code above retrieves the actions and their probabilities in the initial state. We also show the code, using dummy weights, to find the gradient given by

[[[00000000000000001334---081655a485b2ded11e56689f0c67465e9930fd5aaec300b5a08c361778c1410e]]]For reference, the variables appearing in the above code are compared to the formula as follows.

[[[00000000000000001335---a4d40be57b6f131b2a6c192eeae97c1c4567889297a05b69d67c2bf08319a726]]]If J is found, it is found by J.backward().

[[[00000000000000001336---4bbb1f78250d3334906c07c0eb27862fa5819c0783d5188a98542eae05bcd5ed]]]Here's the rest of the Agent class code.

[[[00000000000000001337---d409011a63bee7e2e4f9d11aa549a94d7763aa0db1ad9837e336db2948686253]]]# reset memory

[[[00000000000000001338---31beb64b123a8c0d08c4a30d4930fb8ab69f9ecce1da2cb6166a0b8f7b01e35d]]]The add method is a method that is called each time an agent performs an action and receives a reward. It stores the reward (reward) and the probability of the action taken by the agent (prob) in memory (self.memory). The update method is the method called when the agent reaches the goal. There, we first find the profit G. Earnings can be calculated efficiently by tracing back the rewards obtained (this principle was explained in ``5.2.3 Efficient implementation of Monte Carlo methods''). Then calculate the loss function. To do this, find -F.log(prob) at each time, multiply it by the weight G, and sum them. After that, it's the usual neural network training.

[[[00000000000000001339---5fc46068516f61e8e41480cdc751b450973f426f5cdfd583dedd88b7654b3495]]]In neural network training, it is common to set a loss function. In that case, multiply the objective function by a negative number. If is the loss function, the parameters can be updated by gradient descent optimization techniques (such as SGD and Adam).

[[[00000000000000001340---79cca65dad88cc814f76efbbc529a2cbaec4d029142013ff2484d61b4f8716ed]]]Finally, let's run the agent in a 'cart pole' environment. Here is the code:

[[[00000000000000001341---095e61605161db31494e0c8efd058c5f991670bbc3d0602cb51213fdd1b908f0]]]This is familiar code. In the while statement, we add the reward received by the agent (reward) and the probability of the action (prob). And when we exit the while statement (when the episode ends) we update the policy with agent.update().

[[[00000000000000001342---5d06a14f82db7898d048d8a9a959f3cee9c43448da96b6eaa0742eac72b1a2be]]]When you run this code, the rewards you get will increase as the number of episodes increases. A graph of the results looks like this:

[[[00000000000000001343---246215a5f5850076aeeefd1ebf70a2c80efac13076d040502cc410b6d9a4ef8c]]]
Figure 9-2 Changes in reward sum for each episode


[[[00000000000000001344---3565ba418a89b8c0937ec9f6f453beb083ff6d0e6cdec6471a72c83e989c7fb5]]]As you can see in Figure 9-2, there are large fluctuations, but we gradually get better results as the episode progresses. However, Fig. 9-2 is the result of only one experiment, and there is a problem with reliability. Therefore, we conducted 100 experiments and averaged the results shown in Figure 9-3.

[[[00000000000000001345---78399da420c62382b5641a8c99948f4f5366b61667a710577788af89d2bc5d0c]]]
Figure 9-3 Results averaged over 100 experiments


[[[00000000000000001346---f7f99d5ce262daf91fe394f3529d7d049dbefe7156faeedb2bbf59e0ba9f0dad]]]Looking at Figure 9-3, we can see that the sum of rewards improves with each episode. However, even after 3000 episodes, we have not reached the upper limit of 200 for this task. This seems like there is still room for improvement. We then move on to improving the simplest policy gradient method derived here. The improved algorithm is a well-known algorithm called REINFORCE.

[[[00000000000000001347---fd3c9bedb37478d3b674ae4d42874e089828cf711506faf6f323d8ce6b548c3f]]]REINFORCE [16] is an improved method of the policy gradient method in the previous section. Here, we derive the REINFORCE algorithm on a mathematical basis. Then, implement REINFORCE by modifying the previous code.

[[[00000000000000001348---2f24d77d641f7f5555261fadf3487eb117f0e0f62ad564b0919fe6af15ed8945]]]The name REINFORCE is an acronym for 'REward Increment Nonnegative Factor Offset Reinforcement Characteristic Eligibility'.

[[[00000000000000001349---7af545a7bb826dd8c7c8a619266f4acf4b778d745388a920e2aa59050e152658]]]REINFORCE algorithm

[[[00000000000000001350---c35c26ff608aabc8e142a650f74d34029647c51dfba1ec83734f26bb95f3c5a2]]]Let's start by reviewing the previous section. The simplest gradient strategy method is implemented based on equation (9.1).

[[[00000000000000001351---f3d065911641e39e3d49d48c3c0c8e4a21afe8ff8060334696bc80ed8ad5a3c3]]](9.1) is the sum of all rewards obtained so far (more precisely, the sum of 'discounted' rewards). The problem we want to consider here is that we always use a constant weight to increase (or decrease) the probability of taking an action at any time.

[[[00000000000000001352---f85427aeef574c7e6eea0363af61c37a393c491c295b61f4c37baa0eaa73ab28]]]The goodness of an agent's action is evaluated by the sum of the rewards obtained after that action (recall the definition of the value function). Conversely, the reward received prior to performing a behavior has nothing to do with whether the behavior is good or bad. For example, in evaluating actions taken at a certain time, it doesn't matter what you did or what rewards you received before that time. The good or bad of an action is determined by the total rewards obtained after that time.

[[[00000000000000001353---945d3e08bad9dd261f96094d3f0f47dd228c70a8456477cfb7d0947c06cd6ae3]]]As can be seen from equation (9.1), the weight for action is . This weight also includes rewards prior to the time. In other words, the originally irrelevant reward will be included as noise. To improve this (remove noise), one might consider changing the weights as follows:

[[[00000000000000001354---f121c95da19a82aa319309e279c744487f5fda5a9ea2800b1f5f799d0df31a54]]]As above, I changed the weights to . The weight is the sum of the rewards obtained up to the time. Now the probability of an action being picked is strengthened by weights that do not include rewards prior to time. This is the idea to improve the policy gradient method in the previous section. An algorithm based on this equation (9.3) is known as REINFORCE. Note that this document does not prove that equation (9.3) holds. If you are interested in the proof, please refer to references [17][18].

[[[00000000000000001355---f9567b9e519af10dfae3420a02c40944183f63d643d2f098c7c4d92eeda2dce9]]]The algorithm based on equation (9.3), REINFORCE, outperforms the simplest policy gradient method (algorithm based on equation (9.1)). Equations (9.1) and (9.3) converge to exactly (which can be described as 'unbiased') by increasing the number of samples to infinity. On the other hand, 'variance', which is the dispersion of samples, is larger in formula (9.1). This is because the weights in equation (9.1) contain irrelevant data (noise).

[[[00000000000000001356---2b197fafb2a4f703217383df94c3f4b504ddcf528a16664e69e40ca2d36cc55b]]]Implementing REINFORCE

[[[00000000000000001357---13c84a08bd6462f52ae57b1762e08d0ce7cc2e7bb76ae5c105107c63d976e21d]]]REINFORCE has a small variance, so it can be approximated with good accuracy even if the number of data samples is small. Now let's implement REINFORCE and verify its accuracy. The REINFORCE code is mostly the same as the code in the previous section. The only difference is the update method of the Agent class. Only the differences are shown here.

[[[00000000000000001358---4417238a119a6a80b584dfe1265b85e8dccf053e4de212faa1fd00a396c8085c]]]self.memory is a list containing the agent's reward (reward) and action probability (prob), in that order. Here, G at each time is obtained while tracing the elements of self.memory in order from the back.

[[[00000000000000001359---0f0c7d02ccbdeffaaba39cfd332894933e4378713c146a55dae32eda9d95a066]]]Now let's run REINFORCE. Figure 9-4 shows the graph obtained by running the code once and the graph obtained by averaging over 100 iterations.

[[[00000000000000001360---cb5e51aa1fb81e85b69d572cbe9079a1c1d3495b2a7bacca01d956a9240932ef]]]
Figure 9-4 Changes in reward sum for each episode (left figure is the result when the code is executed once, right figure is the average of 100 times)


[[[00000000000000001361---aac9b5e68c42d45b37c6d5b7492da449ddc87d139d2a90371c7f2eb881ad548b]]]As shown in Figure 9-4, the total reward increases with each episode. Furthermore, the right panel of Figure 9-4 (average results) approaches the upper limit of 200. You can see that the learning is stable and the learning speed is improving compared to the previous result.

[[[00000000000000001362---4bbaad26f40c3613c7dfa4f09b9e525fbdccc40250582f31825410cb6c974b26]]]Base line

[[[00000000000000001363---1d09f98431eb751426fb3c904f299cf800158e777db0281738ba79893fd08f84]]]Next, we introduce a technology called Baseline to improve REINFORCE. I'll start with a simple example to illustrate the idea, then apply a baseline to REINFORCE.

[[[00000000000000001364---66c5186310a7cd0501303bb5114e67b9170bca6cace555d2957f1e1881f2188e]]]baseline idea

[[[00000000000000001365---b77a84f6eb277957075ceb3e5b4ad525d4c205c6fe87afe5ce0a963e549f9659]]]I'll start with a simple example to illustrate the idea of baselines. Suppose three people (A, B, and C) took a test and scored 90, 40, and 50 points respectively.

[[[00000000000000001366---48b9b066032a6e71bea5093efdfd24b9b60183e3cd5d957179aae29fd132376e]]]
Figure 9-5 Test results for three people


[[[00000000000000001367---8a664ef341fef6a40ab9f35e2926f61a357a0d8ec35d17de74dd59b6c1c3c4c3]]]Find the variance for this result. Using NumPy you can get:

[[[00000000000000001368---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001369---bb8c2b6f8d33e0de1a5177ace889cec16dab6cf043aad997cab80f0b1f331c8c]]]As you can see above, the test result has a large variance of 466.6666666666667. Variance represents the degree of scatter of the data, so there will be a large scatter in the test results. Think about ways to reduce this variance.

[[[00000000000000001370---d6ed7f42a3999e7e685aee11281070a4b9be73c3c7daff3fb5505e3733023525]]]Here, let's assume that we can use the test results of the past three people. For example, suppose you have the results of your tests so far, as shown in Figure 9-6.

[[[00000000000000001371---6dc0849641f52a50dffb323bac3b396055af17c5b6aa320c7fdb638898866a46]]]
Figure 9-6 Test results for 3 people so far


[[[00000000000000001372---8a684e39814024ea450ca0cee8b48ef466cee3447d7f091546f7b9cdf8f9c159]]]As shown in Figure 9-6, if you have past test results, you can predict what your score will be in the next test. One way to do that is to average past tests. Then the next test result can be predicted as a 'difference' from the past average score.

[[[00000000000000001373---ad8d534c5185753287490d93e6993cf3cb6dfa1d93a6d10ae83a11f2231ac70a]]]Suppose that the results in Figure 9-6 are averaged to 82 points for A, 46 points for B, and 49 points for C. Use this as a 'prediction' and think about the difference from the target test result.

[[[00000000000000001374---e3be61ecf6f203c6cfbcf26ab6a36905f0ea51dd743c70bee7b9ec58b5cfd91c]]]
Figure 9-7 Difference Between Actual Results and Predicted Values


[[[00000000000000001375---dc2426ca0bd354d25e97443d9559ca015a13d2620e4e69210ab2c9e6a81a4646]]]Now let's find the variance for the difference in Figure 9-7. This can be calculated as

[[[00000000000000001376---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001377---c8550deb02ca876b6370073f2ccc1e29df32d014b82a96c8c1e20b8f958419cb]]]As above, the variance is now 32.666666666666664. We were able to reduce the variance significantly compared to the beginning. As this example shows, you can reduce the variance by subtracting the predicted value for an outcome. The more accurate the predicted value, the smaller the variance. This is the idea of the baseline method. Then apply the baseline to REINFORCE.

[[[00000000000000001378---5e5cc9b688d7a8cee1801e2567f098ff110561178a172ec40bd99b87c87e7f36]]]Policy gradient method with baseline

[[[00000000000000001379---f796eb66ca0dff9bae08b3851a02a85e138b3aa12dbe8b8f341ec272f0e23b62]]]REINFORCE is represented by formula (9.3). Applying the baseline to this REINFORCE results in equation (9.4).

[[[00000000000000001380---2202598cd75ea806e702a090818f2d4dea559bc4ae8368b11bc236ff5bf85fcd]]]can be used instead of as in equation (9.4). Any function here. In other words, a function can be any function as long as it has an input. This is the baseline.

[[[00000000000000001381---42797f4b2b7eddb857ae2b4bf692c91d27f015f18823a8951fe1e5eac8b5d2bd]]]The proof that the formula transformation from formula (9.3) to formula (9.4) holds (that the equality holds) is given in 'D.2 Baseline Derivation'. Please refer to it if you are interested.

[[[00000000000000001382---6db6a0511e848588255bdd74f80f876bcab67bc44947a4adf48067f8ff838bae]]](9.4) can use any function. For example, in a state, we could use the average of the rewards obtained so far as: Value functions are often used in practice. Written as a formula, it becomes If the baseline can be used to reduce the variance, then sample-efficient learning can be achieved. Note that if we use the value function as a baseline, we cannot know the true value function. In that case, you should also learn about value functions.

[[[00000000000000001383---e7925f36a5406d8965fbc82d4805bf6074856a5df070607c591f600edd216fe6]]]Finally, I'll add an intuitive explanation of why you should use baselines. Let's take a cart pole as an example and consider the situation in Figure 9-8.

[[[00000000000000001384---329ee7437e652961c2ae957912a07bea4f7a246239859caee768c35407b9ef3f]]]
Figure 9-8 Pole unbalanced


[[[00000000000000001385---6628e57c2d7308128e6c7c0c3f7bb9c17c06a219842f2f1b1a6bbe6608df4412]]]Figure 9-8 shows the situation just before the pole becomes unbalanced and the game is over†1. In this state, no matter what action you choose, the game will be over after a few steps.

[[[00000000000000001386---44531a8916ee440d95f5b5982ad45987ad736b65e92c522a35dfe0cf23d49d4f]]][†1] In the cart pole of OpenAI Gym, the game is over if the pole crosses 15 degrees from the vertical line, but here we assume that the game is over when the pole reaches a parallel line.

[[[00000000000000001387---d186c0634f92971b412d4aa10441e4d6a1a0c9b1b2ae769afd98a97095641c15]]]Let's assume the state in Figure 9-8 and the action there. And after a few steps from the state, for example, after 3 steps, the game will always be over. In that case, the return on the state would be 3 (here the discount rate). If it were REINFORCE without a baseline, the action in the state would be strengthened with a weight of 3 (higher probability that the action would be chosen in the state). However, no matter what action you take, the game will always be over after 3 steps, so increasing the probability that such an action will be chosen is a meaningless task.

[[[00000000000000001388---ba133cd873185d3a2a5b86a8fb0ca93434bb7536fa3c45176791f79c92928b07]]]That's where baselines come in. Here we use the value function as a baseline, and assume that it is known from the example in Figure 9-8 (actually, it should be learned and estimated by Monte Carlo method, TD method, etc.). The weight in this case is '', so it is 0. Since the weight is 0, whatever action you choose will neither increase nor decrease the probability of that action being chosen. By using the baseline in this way, you can expect to eliminate wasteful learning.

[[[00000000000000001389---d7cef34806bbea979a53ef83f4b3c0d1bd669ba19e8070dbcea9eb2f119afcc7]]]Reinforcement learning algorithms can be roughly divided into value-based methods and policy-based methods. All the methods we have seen so far in this chapter are policy-based. DQN and SARSA are also value-based methods. A method that uses both (a “value-based and policy-based” method) is also conceivable.

[[[00000000000000001390---754d42b2ee4c088422c738491cce3f9f684530f4241b90dcedfb7cd454f28e23]]]
Figure 9-9 Value-Based and Policy-Based Approaches


[[[00000000000000001391---41d875255404f37505a305f6c09298e28f9b5da48fad550564dbdeaf39585291]]]The REINFORCE with baseline described in the previous section can be considered “value-based and policy-based” if a value function is used for the baseline. Here, we take REINFORCE with baseline a step further and derive an algorithm called Actor-Critic. Actor-Critic is also a “value-based and policy-based” method.

[[[00000000000000001392---d52f3959359cb65b701d493e97e0bce358e0b8129c2827b0c574cf863cc41c41]]]Actor-Critic Derivation

[[[00000000000000001393---75ab3674480717b781e0bbf21e8e8d2adc15e863b0f29bc13680c314b2675825]]]First, start by reviewing 'REINFORCE with a baseline'. In REINFORCE with Baseline, the gradient of the objective function is given by

[[[00000000000000001394---871a7a7f5ef36744c174491681eed7e1103afa4d4e2c6bdf6d9bef10153098d2]]]In equation (9.4), is revenue and is baseline. A baseline can use any function. As a baseline, we use a value function modeled by a neural network. In doing so, we will use the following new symbols:

[[[00000000000000001395---e1ec5980ed6f69f7be93bb85e209584dfc428023fe7785c20d340a66d5cbb0d7]]]: all weight parameters of the neural network representing the value function

[[[00000000000000001396---b6863f3836fd7819dfa96b43d69144280a1207365c76e29a78013f4bd5936ed3]]]: Neural network modeling the value function

[[[00000000000000001397---193eec11fecfe391a3396023e66dae66bed9a0945ba7dd1de4e583b6efae66d8]]]The gradient of the objective function in this case is given by

[[[00000000000000001398---ef3821bc7005d6a7178171f30563eba53a2901869642be037caa7df236f9b482]]]There is one problem with expression (9.5). The problem is that revenue has no value until the goal is reached. In other words, the policy and value function cannot be updated until the goal is reached. This is a drawback of any Monte Carlo-based approach. The TD method discussed in Chapter 6 eliminates this drawback. When learning the value function with the TD method, it can be updated using the results after one step (or after a step), as shown in Figure 9-10.

[[[00000000000000001399---56f3eca1a85d73848a8d5e81567e8d53dabe040c3922c5851278a54a65e5a49f]]]
Figure 9-10 Comparison of Monte Carlo method and TD method


[[[00000000000000001400---7c2efacaab20f71de3091921f2665acf8a5c583c4df3f41be84507aba58ccbe5]]]When learning the value function, the Monte Carlo method uses returns, as illustrated in Figure 9-10. On the other hand, the TD method uses

[[[00000000000000001401---321a218b793f028a358cb7175b084ddb61cab2434e8e774fbed6b271b6183b3e]]]When we model the value function with a neural network, we learn to approach the value of . Specifically, we update the neural network weights by gradient descent, with the mean squared error between and as the loss function.

[[[00000000000000001402---59421e585681338bd797ac2b2c65a5822a0484576dbb9bbcb5be324a3de17e9e]]]Now, let's switch the Monte Carlo-based formula (9.5) to the TD method. For that, use instead of . In that case, we get the following formula:

[[[00000000000000001403---0f43a71130eab4505e1e0c11a07aaa8e9d3cf8e79f575b7435c6207680e054e2]]]The method based on equation (9.6) is Actor-Critic. The policy and the value function are neural networks, and we train the two neural networks in parallel. Specifically, the policy is learned based on equation (9.6). And the value function is learned by the TD method so that the value of

[[[00000000000000001404---b5907c667cbe1d9a7692c8f821a9d60bd8bd88876d4a98e799de4c94299a2b00]]]Actor in Actor-Critic means 'doer'. This is a person who acts and corresponds to a policy. On the other hand, Critic means 'critic', which corresponds to the value function. In other words, it means that the action (act) determined by the policy is criticized using the good or bad of the action.

[[[00000000000000001405---9a1673cf5f0f1f20bb94cdfbf6432146a74454a49ac6e36952b8efd8653372a8]]]Implementing Actor-Critics

[[[00000000000000001406---15d7f28f5678de44547225538950990d9e1e7927c141722ec56464c5e2d42750]]]Now let's move on to the Actor-Critic implementation. First, here is the code for two neural networks, the policy and the value function.

[[[00000000000000001407---17a44de5cfde65fce4d0a7e73319dbaf39256400214268fcfcbe428f2e2f66e9]]]Here we implement the PolicyNet class for the policy and the ValueNet class for the value function. The final output of the policy is the output of the softmax function, so the 'probability' is printed. Next is the Agent class.

[[[00000000000000001408---834a1e855bda4e56af67cd8f37ba005878ea3c8b3bf6b4ccfe8061d8c51c3197]]]# add batch axis

[[[00000000000000001409---834a1e855bda4e56af67cd8f37ba005878ea3c8b3bf6b4ccfe8061d8c51c3197]]]# add batch axis

[[[00000000000000001410---c232e5c15258948282ced071450608e79710b8b3e3f8344001d4510d8d5c8c26]]]# ① self.v loss

[[[00000000000000001411---d27f9648f0c57cea220c2f8c1a5a2b09fb96fae0f32e201dd221fa7024e98d96]]]# ②Loss of self.pi

[[[00000000000000001412---cc0c83bf8c3d835ef1f22cc54d23f6ad95704d3cf6f5f1a5f31b446cc98c213c]]]The get_action method retrieves an action according to the policy. Note that the input data to the neural network is processed as a mini-batch, so if you want to process one data (state), you need an axis for the batch. Also, the get_action method returns both the selected action and its probability. We will use the probability of an action being chosen later in calculating the loss function.

[[[00000000000000001413---467433d3b99cd14607933d07c8dce6ddf9b87f3ff549c2fe1af87f8a64e98c7d]]]The update method performs value function and policy learning. In code 1, we find the loss for the value function (self.v). To do so, we compute the TD target (target) and find the mean squared error with the value function (v) in the current state. Then in the code ② we find the loss for the policy (self.pi). Based on the formula (9.6), the value obtained by multiplying it by a negative value is the loss. The rest is the usual neural network training code.

[[[00000000000000001414---e28342ee63b2ef6b94f00dbd47c6124e1f85e1203166808cadc029970a1b3a83]]]The code that drives the agent remains the same. Therefore, the explanation is omitted. And we get the result in Figure 9-11.

[[[00000000000000001415---f319ed60617b7a14af2f6e263e290bf29a3c9213cb0dbd71e84b69e9b92c09dd]]]
Figure 9-11 Changes in reward sum for each episode (left figure is the result when the code is executed once, right figure is the average of 100 times)


[[[00000000000000001416---6477e01e791314571b0eb57417d6bdf24beea8820bb4f05818f73fe3a63b022b]]]As you can see from the figure above, learning is progressing smoothly. This completes the implementation of Actor-Critic.

[[[00000000000000001417---6c19401ac528dc39b9f578b8e1111693146eebaef12b0509a2d4d1c74ccf88d2]]]Advantages of the policy-based approach

[[[00000000000000001418---ca73fec2e8991ba62430c93f74e198e86c959c8adb7dfea83477e48ef0110d02]]]So far we have seen policy-based methods. Finally, we discuss these policy-based advantages. Here are three advantages and an explanation for each.

[[[00000000000000001419---6daabd173a679fee966022501334fe6b7b2e659abdcded638416c6a313047dd8]]]1. Efficient because it models policies directly

[[[00000000000000001420---0be757357851e0275f726533899d9e689eadbbfba6bb54722c050c0d0c9747dc]]]What we want in the end is the optimal policy. Value-based methods estimate a value function and base policy decisions on it. Policy-based methods, on the other hand, estimate the policy “directly”. Depending on the problem, the value function may have a complex shape, but the optimal policy may be simple. In such cases, policy-based methods are expected to learn faster.

[[[00000000000000001421---420bb7266acaa378cc83862af84cccd5872d10bba315276bdbacf05f76bfa9e7]]]2. Can be used in a continuous action space

[[[00000000000000001422---390564fc6ee56a145d6d211956a5aafa0b4239c3d7ba572285ac1b7846288e67]]]All the reinforcement learning problems we've seen so far have been discrete action spaces. For example, for a cart pole, you choose between two actions, right or left. In such a discrete action space, an action chooses one among several (rather than continuous) candidates. On the other hand, a continuous action space is also conceivable. For example, OpenAI Gym's 'Pendulum' is a task to lift a pole by applying torque to the center of the pole (Fig. 9-12). In this case, the action space is a continuous value of how much torque to apply (for example, 2.05 or -0.24).

[[[00000000000000001423---80ac67d891f586a35a924d3b02ee3123e9a4fb5e1b105bd06572c4ea28680cbc]]]
Figure 9-12 'Pendulum' in OpenAI Gym


[[[00000000000000001424---4ff566381f14a749d6ed568fa041c73c59a57f85b3e096308ee755be2524254e]]]Value-based techniques become difficult to apply when the action space becomes continuous. There are several possible countermeasures. One way is to discretize the continuous action space. However, how to discretize - this is called 'quantize' - is a difficult problem, and different tasks are better suited for different tasks. Finding a good discretization often requires trial and error.

[[[00000000000000001425---1aa5c562877e8f5262aec2d200a51df4294474b639b59dae7af597bba20a4532]]]On the other hand, policy-based methods can simply accommodate continuous action spaces. For example, if the output of a neural network is assumed to be normally distributed, the neural network may output the mean and variance of a normal distribution. Continuous values are obtained by sampling based on the mean and variance (Figure 9-13).

[[[00000000000000001426---9b08d9908e76440baeae92d88ba7eebc0c3c4cee455ffe891140d9ff3fd5d720]]]
Figure 9-13 Example of a neural network that outputs continuous values


[[[00000000000000001427---11ce896241c99a1bae30a623b7c6909803ac8e855dd0a8a5a858168fa0b88400]]]3. Action selection probability changes smoothly

[[[00000000000000001428---cdf70ca44c88c92d1ff96dff3212cc8b751942fe662c0e871548c05dc74ea139]]]In value-based methods, agent actions are often chosen by a -greedy method. In that case, basically the action with the largest Q function is chosen. At this time, if the action that becomes the maximum value changes due to the update of the Q function, the way of taking action will suddenly change. Policy-based methods, on the other hand, determine the probability of each action by a softmax function. Therefore, the probability of each action changes smoothly in the process of updating policy parameters. Thanks to this, learning policy gradients tends to be stable.

[[[00000000000000001429---a45964d560dddbd2bf54c90ebf5ff6e8ef34ff97ce610e661480d231ad7a46e4]]]These are the benefits of policy-based. The point to note here is that the policy-based method does not always outperform the value-based method, and there are pros and cons for different tasks. Also, whether policy-based or value-based, there are various ingenuity in algorithms to realize it. Algorithms should be selected with this in mind.

[[[00000000000000001430---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001431---13606fef0a444d04da092553258ed701ee170645813e7a90a8aaa814820200ac]]]In this chapter, we learned about the policy gradient method, which is a policy-based method. Specifically, I learned four algorithms for the policy gradient method. They are (uniformly) represented by the following formula:

[[[00000000000000001432---3ead83cb8745a461c5446029ab3cbfcf88845b3678bb829328509bc82b2c2985]]](simplest policy gradient method)

[[[00000000000000001433---d5eb9efd318ec3b024e0a939aba2ea6ffc774cd7eb3d5fc46cb3a94a9276c4e8]]](REINFORCE with baseline)

[[[00000000000000001434---200c3f1d61885148818f83455fd68832de9dc75bef97ea0a254408f537356d54]]]The above four methods differ in their weights. The simplest policy gradient method has weights all the time. REINFORCE is an improvement to this, and evaluates the weight at the time according to the return. Furthermore, REINFORCE with baseline reduces variance by adding a method called baseline. The fourth, Actor-Critic, is a method of modeling not only the policy but also the value function with a neural network. And as the number goes on, the techniques get more sophisticated and you can expect better results.

[[[00000000000000001435---873a87facb1d73d8ecb75d1fef7b34381b933a7afd52e9a8c5ab466bfdaa217d]]]In addition to the above four options, the Q function can be used as follows (for proof that the equality holds even if the Q function is used, see reference [19], etc.).

[[[00000000000000001436---e426ddc761170cb84335abad5b214360f78f2fdfdc37e2354d6f0ef4fb4cf2b2]]]Additionally, using a value function for the baseline, we can set

[[[00000000000000001437---2cba339a7cd7cb994a976dba853628d1ed96bfb4a76de2d5d2cd6d45aa0bca80]]]As in the above formula, the value obtained by subtracting the value function from the Q function can be used. This is the advantage function and can be written as

[[[00000000000000001438---cf76f02501ccb0f5061f30864bd3ed4d6065bcf38520d121593a39a47b53d245]]]Deep Learning from scratch ❹

[[[00000000000000001439---7c05b957c25fe05f05721a109656bcae8205d1146d50e1049584e909e746010d]]]Reinforcement learning

[[[00000000000000001440---f18f6f9783a1c8de9a24cf9c9210b59e4d725b724e6c50464d4714e65306977a]]]Written by Yasuki Saito

[[[00000000000000001441---25776fc8b2e92d3d94c0cea93957def950b0a750b090f7c92cc11575b1ce6318]]]Chapter 2

[[[00000000000000001442---ff744ff7a4204eef2a70921433911255bad736d06939892fd045bb0a959591a9]]]Markov decision process

[[[00000000000000001443---d8e7c52b5ea3881a6bf12049fcd284531866f59b95fb861466245ac1adede19c]]]In the bandit problem, whatever action the agent takes does not change the next problem to tackle. Agents are challenged with the same slot machines every time, looking for the best slot machine among them. But the real problem is different. For example, consider the game of Go. In Go, when an agent makes a move, the placement of stones on the board changes (and the opponent strikes a stone, and the placement of stones on the board changes). In this way, the situation changes from moment to moment depending on the agent's actions. Agents are expected to make the best of the transitions.

[[[00000000000000001444---25b21615ef015151491e7286f0baa9961bb6e59b05914447f09b53bdb5ae9ef7]]]In the previous chapter, we also explained the 'non-stationary problem' in which slot machine reward settings (probability distribution of rewards) change over time. In non-stationary problems, the 'probability distribution of rewards' changes over time, regardless of the agent's actions. The problem we are considering here is that the state of the environment changes depending on the actions of the agent.

[[[00000000000000001445---2fd776b7e0b5c301d9316db1d66833790753c60f417a21ec0493386fb78abfe2]]]This chapter deals with the problem that the situation changes depending on the actions of the agent. Some such problems are formulated as Markov Decision Processes (MDPs for short). In this chapter, the terms that appear in MDP are first explained and expressed using formulas. After that, after clarifying the goals of MDP, we will look at the process of achieving the goals while actually showing simple problems of MDP.

[[[00000000000000001446---9310ab140907a9cb995b9acf15438e21ad211b3cb20a4008ee4126cde618228e]]]What is MDP

[[[00000000000000001447---95b1ab5a582db7bf09aca7e325281914108dbf42d12143a12f3caa589b41e7d8]]]MDP stands for Markov Decision Process and is translated as 'Markov Decision Process' (The meaning of 'Markov property' will be explained in '2.2.1 State Transition'). The decision process is the process by which an agent determines its behavior (while interacting with its environment). In this section, we will look at the properties of MDP with specific examples.

[[[00000000000000001448---4daca56f1d90e1581c41784b0e1f1cc799a5ebde1d9c3f603b640f47191d1994]]]Examples of MDP

[[[00000000000000001449---055f5e1bf66d16f39f4f5ff92751f7de50650d71a71523c9276b2220d09126be]]]First, let's look at Figure 2-1. The world is divided into grids, with a robot in the middle. The robot can move right or left on the grid. In this book, the world shown in Figure 2-1 is called 'grid world'.

[[[00000000000000001450---1b873a2947819323f3afaa293b558f16b38890d3a67ae743c973824c38d5eded]]]
Figure 2-1 Example of MDP Problem Setting: Grid World


[[[00000000000000001451---27855fcc066fb907be83ecd3ba06ed86dd55bcc7c0016ce775b95c28e0e62fe5]]]In terms of reinforcement learning, the robot is the 'agent' and the environment around it is the 'environment'. Agents can take two actions, going right or left. Also in Figure 2-1, the leftmost grid has an apple, and the rightmost one has a bomb. These are the 'rewards' for the robot. For this problem, let's say the reward is for picking an apple, and the reward for a bomb. Also, the reward for empty places (blank grids) is 0.

[[[00000000000000001452---258f96855a2a8453f7d0ededc683a5bdab8acd9ce0818c6d7fcaba183f6ee1b1]]]In the problem in Figure 2-1, the situation changes as agents act. As an example, consider an agent moving left, then left again. In that case, the agent gets a reward of As another example, if the agent moves right, then right again, it gets rewarded. These agent transitions can be represented as shown in Figure 2-2.

[[[00000000000000001453---0007b7d1be62541487868bbd098013af00d7b93fbdf269cf3877323a850a8807]]]
Figure 2-2 MDP state transition


[[[00000000000000001454---47549f1000c73bbd6f15ab1cdde14d6c0bf80e6bda8cc98ce97a7cd7317fe897]]]As shown in Figure 2-2, the agent's situation changes depending on the agent's actions. This situation is called a 'state' in reinforcement learning. In MDP, the state changes depending on the actions of the agent, and the new action is performed after the state transitions. By the way, as you can see from Figure 2-2, the optimal action for the agent in this case is to go left twice. Doing so will give you the best rewards.

[[[00000000000000001455---0f9e15f02409383f95c3320ff8f96c75bb0ec3636c6ae57a24be7f3e5db34fb5]]]MDP requires the concept of 'time'. At some point in time, an agent will act, resulting in a transition to a new state. This unit of time is called a 'time step'. A timestep is the interval at which an agent makes a decision, so the actual unit depends on the problem.

[[[00000000000000001456---7a1f9c2eca509f0aa66a6c067f1ed7a387d917312adf426ff8fc9adb1745863a]]]Now let's consider the problem in Figure 2-3.

[[[00000000000000001457---013dfa0677e9910899555db571019a22c9004171007f0e0bb2993b94316c17af]]]
Figure 2-3 Another World (Environment)


[[[00000000000000001458---3a215caad8d3ba32be3cd399efa72bbd8ac0ec5184216901ef674ef4808bc7d9]]]In Figure 2-3, there is a reward apple on the far left of the grid and a reward bomb just to the right of the agent. And on the far right of the grid is an apple with a reward. In this problem, moving to the right will give you an immediate negative reward, but then moving further to the right will give you a reward apple. Therefore, the optimal behavior for this problem seems to be to go right twice.

[[[00000000000000001459---5b17ed3f8e616dc494c7777b4daeaff9f0709a748bba6f98864faf80869cd389]]]As the example in Figure 2-3 shows, the agent should consider the total future rewards, not the immediate rewards. In other words, we want to maximize the sum of rewards.

[[[00000000000000001460---a764db9d354080eca4eda1f2fb30d66a3dcb0e4fa6b19b3c7572582a316cb481]]]Agent-environment interaction

[[[00000000000000001461---ed60aac41b2374dbd0664dd970f9bb80804677e6ff180e20345fbb312108288a]]]MDP provides interactions between agents and the environment. The important point is that the state transitions as the agent takes action. As a result, the rewards you receive will also change. The diagram below shows such a relationship.

[[[00000000000000001462---9dcd24b62e8c85a322a7769333d36a36d7e1019c8800c9c6c3ec5dcf5616a824]]]
Figure 2-4 MDP cycle


[[[00000000000000001463---49581836aa874f8635b1251e560dcfffdeea3e7dd9e76e4620715762dbbd5904]]]Figure 2-4 assumes that at time the state is . Based on that state, the agent takes action, gets rewarded, and transitions to the next state. This agent-environment interaction actually produces the following transitions:

[[[00000000000000001464---c7815832cd0e2ac7db4d99fee740cfc304d1793a162a0833526f357f74f093a4]]]This time series data starts as the first state. In the state, the agent performs an action and gets a reward, and the time advances by one and the state advances to. Next, the agent takes action based on the state, gets a reward, moves on to the next state, and so on.

[[[00000000000000001465---8f0714750141cf22053714e644a65058d49c91a2116d2aabe5733b76098bee05]]]In the field of reinforcement learning, we see two conventions for reward time timing. It's the difference between rewarding and rewarding. A more precise writing would be:

[[[00000000000000001466---12be2ef6e1266e3d10d2b817638ee04d8d166d42c114293334b04bb1c19f671c]]]Perform an action in a state, receive a reward, and transition to the next state

[[[00000000000000001467---12be2ef6e1266e3d10d2b817638ee04d8d166d42c114293334b04bb1c19f671c]]]Perform an action in a state, receive a reward, and transition to the next state

[[[00000000000000001468---8bb17bbea14f14fb9551193a46969db1661fba3641da502de439a62c206d8deb]]]As above, there are two ways to treat the reward as or as. In this book, we will adopt the former (method) in consideration of compatibility with programming.

[[[00000000000000001469---58f53df13caceaaf115a595bda5d3d5fa5179bc3b85cf3a534650c40da8c7748]]]The above is an overview of MDP. Now let's describe the MDP using formulas.

[[[00000000000000001470---6b9e889328d545bc816c18807831675f09b0b4c3f79900e941d7314edefd88c0]]]Environment and agent formulation

[[[00000000000000001471---6ee84d2c1aae00ec25adc107be78ba08543a916362b81065ebcf22c9f8a6a847]]]MDP formulates the interactions between the agent and the environment through mathematical formulas. To do that, we need to formulate three elements:

[[[00000000000000001472---c025aff066624cc163acb46d5664452a48c500cd7f5ab5bae5e5976f2e95fb63]]]State transitions: how states transition

[[[00000000000000001473---539aa7dbbb35af1fc35387c663a4eaa1441083cb6fc1243f8c99f829317eca5a]]]Rewards: how rewards are given

[[[00000000000000001474---5e13873b331f650624306ec84b995f6650660b438570def66c905df745acbaef]]]Strategies: how agents decide to act

[[[00000000000000001475---eca887948f3d31c3f2d8252f86b29e3917090cd045007bce25b241bcc7a0644f]]]If these three elements can be represented by a formula, it will be formulated as an MDP. Let's start by looking at 'state transitions'.

[[[00000000000000001476---5510b0977e769f253e9340efd5db07cdc44913a10576daa8213090123f179afe]]]state change

[[[00000000000000001477---5ed529e38a1db206494318ddb4185bddf0f01aca9e312e44abddd77825dc6024]]]First, let's look at Figure 2-5 as an example of state transition.

[[[00000000000000001478---f1293f43f217b506cd69daad564823851c9fbe5294894f655b5547795248a36d]]]
Figure 2-5 Agent actions and state transitions (left figure is deterministic, right figure is probabilistic)


[[[00000000000000001479---25b95cdc2f2903618455c74f6f174028f7373ab6e56d46f42498b5030e158041]]]The left image in Figure 2-5 depicts an example in which the agent chooses to move left, and as a result, the agent 'always' moves to the left. Such properties are said to be deterministic. For deterministic state transitions, the next state is uniquely determined by the current state and actions. So it can be expressed as a function as

[[[00000000000000001480---8504815cc94caff5c2ea14b297eb33388c0881de576bbaafb0450038c4bac9f6]]]is a function that takes a state and an action as input and outputs the next state. This is called the State Transition Function.

[[[00000000000000001481---88d5b0ba58ddd7c50285a3cba3afcd2b149f524315345b3a01380f2f9ac2a3ac]]]On the other hand, the right side of Figure 2-5 depicts stochastic behavior. If the agent chooses to go left, it has a 0.9 chance of going left and a 0.1 chance of staying in place. This stochastic behavior could be caused, for example, by a slippery floor, or by the agent's internal mechanisms (motors, etc.) not working properly.

[[[00000000000000001482---370f84910d64c55070da96937c73fe988b7ae99a08ba5e601cdc9924605c9e69]]]Even if the state transition is deterministic, it can be described probabilistically. In the example of Figure 2-5, if the agent chooses to move left, the probability of moving left is 1.0, and the probability of transitioning to another state can be described as 0.

[[[00000000000000001483---7da9b2d2341401a384dfe30950748ed03a6a5f76d4777fd616814a1031e73abd]]]Now, I will explain how to notate probabilistic state transitions. Suppose an agent is now in a state and has performed an action. Then the probability of moving to the next state can be expressed as

[[[00000000000000001484---202dcfc2f5f9666051b03769797566ec922381d05e56ac90b01255006467c715]]]On the right side of is written a random variable that represents the 'condition'. In this case, it corresponds to the condition that the action was selected in the state. Given the two conditions, the probability of transitioning to is represented by . is called State Transition Probability.

[[[00000000000000001485---c150eb0227aada0cb4b1e8bf2451e73a0fe270585ff9309888f9b660cc69c744]]]Then, a specific example is shown in Figure 2-6.

[[[00000000000000001486---1e2adbf542ecc6fbb48f9ecf2a5506ad6bb5f580c42d8989d7bc9eb8f550e5df]]]
Figure 2-6 Example of state transition probability


[[[00000000000000001487---8e34f71879cc3f71b6c3a346121d931d741a3b61732671a4a607e0bd0fc18163]]]Let's call the five squares from left to right, as shown in Figure 2-6. Also, regarding the agent's actions, we denote the movement to the left as the movement to the right. Here we assume that when the agent is in (when the state is ), the action is selected. In that case, state transition probabilities are expressed as shown in the table in Figure 2-6.

[[[00000000000000001488---4799d56bad5fc42c755dff3f9144f62127c9b0a9c0d184f9820090bc91bcee8c]]]The table in Figure 2-6 is more precisely the 'probability distribution' of state transitions. This is because probabilities are expressed for all possible values of the random variable. In other words, it is a 'distribution' of probabilities.

[[[00000000000000001489---eb0d50906a7860b35a8e2fbe4f36b92301d571859e8aa47e8c7769eb6afbcbb2]]]depends only on its current state and actions to determine its next state. In other words, state transitions don't need information about the past than the present—information about what state they were in and what actions they took. This property is called Markov property.

[[[00000000000000001490---de9e5a517e402f3f65ade3d1cff7146068ecae9cc2a24858ddddca13d7d70153]]]MDP models state transitions (and rewards) by assuming Markov properties. The main reason for introducing Markov property is to make the problem easier to solve. If we do not assume Markov property, we have to consider all past states and actions, and the number of combinations grows exponentially.

[[[00000000000000001491---e0eb34c059daf7705651ad04522c3cea34246161660a1dc52c59b19c50f5673b]]]reward function

[[[00000000000000001492---9b955c59ea9759acf3c977c714b863ec557e1a69bca9fcb7e9387c2269110673]]]Next, think about rewards. In this book, we prioritize simplicity and proceed with the assumption that rewards are given 'deterministically'. Specifically, the function defines the reward obtained when the agent is in a state, performs an action, and enters the next state. is called the Reward Function. Let's take a look at Figure 2-7 as an example of a reward function.

[[[00000000000000001493---6a87cb0f06fcf688abff056c8e1fb23a7d94bcf8b4765fe79286b324be749a7d]]]
Figure 2-7 Reward Example


[[[00000000000000001494---391234d8501da5e1013a7c81acba02d3f1c25f783e8a674dc7523d39024d418b]]]Figure 2-7 depicts an example where the agent is in (that is, the current state is in), chooses an action, and transitions to as the next state. The reward in that case is obtained from the reward function.

[[[00000000000000001495---a85d23406130abf2251284ce31e9914d3f115609f4f8fe4e234185adae5db365]]]In addition, in the example of Figure 2-7, the reward is determined only by knowing the next state. This is because in this problem, the reward is determined only by the apple to which it moves. So the reward function can also be implemented as

[[[00000000000000001496---174daf52080792446f962a9e25dbbf4b42cb8513a742e56f2f57df5b1e4a9d04]]]Rewards may also be given 'probabilistically'. For example, if you go to a certain place, there is a 0.8 chance that you will be attacked by an enemy and you will receive a reward of -10 (0.2 chance that you will not be attacked). It is also possible to receive rewards stochastically like that. However, by setting the reward function to return the expected value of the reward, the theories about MDP that follow (such as the Bellman equation) hold as well as if the reward were deterministic. In this book, we will proceed assuming that rewards are given deterministically.

[[[00000000000000001497---3277743d3c9bd73be64ac9684348796542348919ba52e0a1c96044123bc65e30]]]Agent strategy

[[[00000000000000001498---d39f68a1c7dcd7e96d0a7206044cfd29d6f4d805e3e219267c2f3f5e5f8fe035]]]Next, consider the policy of the agent. A policy describes how an agent decides to act. The important point about policies is that the agent can make decisions based solely on the 'current state'. The reason why the 'current state' is sufficient is that the environment transitions based on Markov properties.

[[[00000000000000001499---fd5d15a7a123c6b651f4283a6a62d8d4906c40aa9b623a9e63095602f0e979ae]]]An environment's state transitions are conditioned only on its current state and behavior—without needing any previous information—to determine its next state. Similarly, the reward is determined by the current state and action, and the transition destination state. What this means is that all the necessary information about the environment is in its 'current state'. As such, agents can make decisions based solely on their 'current state.'

[[[00000000000000001500---4cfca101d77eb8a76c0547ae2d2a83293c39d465c5e854b7530b4e8ac1b00895]]]The Markov property of MDP can be viewed as a constraint on the environment rather than the agent. In other words, it is necessary to keep the 'state' so that the environment side satisfies the Markov property. From the agent's perspective, they have enough information in their current state to make the best choices and can act on it.

[[[00000000000000001501---809c3974f5f4d54a4ddda2d621a14719d554e33c5ec0514b4eb2b70cac0f5c6f]]]The agent's strategy can be determined based on the current state. At this time, the decision of action can be either 'deterministic' or 'probabilistic'. A deterministic policy always performs a fixed action when in a certain state, as shown in the left diagram of Figure 2-8.

[[[00000000000000001502---b332e7ef1893d27b52801ebf0b3ffe9584c168f5032769adc220da89b1ea45fc]]]
Figure 2-8 Deterministic and Probabilistic Policies


[[[00000000000000001503---b5dec72d327c2115850de74cac8368e62264962d04bab90ab338097eb08726ad]]]In the left diagram of Figure 2-8, the agent always moves to the left when it is in a certain place. Such a deterministic policy is a function of

[[[00000000000000001504---4206d4f6561b122a3e337fb41358a8637a52cb937465757379f1bf7bb4907102]]]can be defined as is a function that returns an action given the state as an argument. This example returns .

[[[00000000000000001505---fc29302920dc3484df1ef0ddadb8785ce6c7e3321c87a0c63b89cb37bc31d615]]]The right panel of Figure 2-8 depicts an example of a probabilistic policy. In this example, the probability of the agent moving left is 0.4 and the probability of moving right is 0.6. A policy that probabilistically determines an agent's behavior in this way is represented by a formula as follows.

[[[00000000000000001506---6f73a76d9359ff1e41c45374b3041950efc5e0b9eb087c79958003ea5445ca10]]]represents the probability of taking an action while in a certain state. In the example on the right side of Figure 2-8,

[[[00000000000000001507---ed551a0f0732edc3bec983a373f519c7feb3d783741f8f50bfb82d15cb970f96]]]becomes.

[[[00000000000000001508---ba176f251e09ed93a5eb0a2ea16c61b18a4b4a97cbf4977e5fb292f3ffb98621]]]A deterministic policy can also be represented by a probabilistic policy. For example, a deterministic policy can be represented by a probability distribution with a probability of 1.0 to take an action in a state and a probability of 0 to take any other action.

[[[00000000000000001509---328f966d4313a3fc29456905f8e39f8d31eb7cc3eda9c0dc04152cc58425d1f5]]]With the above, state transitions, reward functions, and policies can be represented by formulas. These three are then used to define goals in MDP.

[[[00000000000000001510---96f94804103e1d1846979479c9779f73c6060758c8a4c797fccb1d44e00b42e3]]]Goal of MDP

[[[00000000000000001511---d9a42e90cdd28637eab0108da26d5c5dd2280a3126730cf648981a90ec5a411b]]]So far we have formulated the environment and agent behavior. As a quick review, agents act by policy. It transitions to the next state depending on the action and state transition probability. Then rewards are given according to the reward function. The goal of MDP is to find the Optimal Policy within this framework. The optimal policy is the one with the highest return (we will discuss returns in a moment).

[[[00000000000000001512---47da287dfc4dd585d0033caf69746d1d225393e263cf4713f8e60cac071ae3bf]]]If an agent has a deterministic policy, it can be represented by the function However, since a deterministic policy can be represented (covered) by a probabilistic policy, we will assume a probabilistic policy here. For the same reason, we assume that the state transitions of the environment are also stochastic.

[[[00000000000000001513---0f58daba16a777e8abbb121e9dc9769b40269afdd2fce21c4731af1b171e210b]]]In this section, we will prepare for the rigorous formulation of the optimal policy. In doing so, I will first explain that the problems with MDP can be roughly divided into two. The two are 'episodic tasks' and 'continuous tasks'.

[[[00000000000000001514---1abfa3a47e3ffedebe76a02c9760334dde649812760732c7ac416b929651a36d]]]Episodic and continuous tasks

[[[00000000000000001515---0c59ea90b3f59e6a48d8b94c579d228dab96f2b55112caa5329bc12d195d1479]]]MDP can be divided into episodic tasks and continuous tasks depending on the problem. An episodic task is a problem with an 'end'. For example, Go is classified as an episodic task. The game of Go ultimately comes down to a win/lose/tie. Then, in the next game, the game starts with no stones placed (initial state). In episodic tasks, a sequence of trials from beginning to end is called an 'episode'.

[[[00000000000000001516---3f9a84f3eeafdd9d9558aa584ee023055c32b55f8fdc88d000113cf1f1913115]]]Another example of an episodic task might be a grid-world problem as shown in Figure 2-9.

[[[00000000000000001517---e8253931ce78ad870d90edab425be1bcdf532cfca5a277992fe78ddc97e5df56]]]
Figure 2-9 Example problem with goals


[[[00000000000000001518---b194ebe44dd58ccdb58cb107bd0a8eec336d24cf7e0b62acdcc1a4ce0c344aa2]]]In the example in Figure 2-9, there is a goal at a certain location, and when that goal is reached, it is the 'end'. And the episode starts again from the initial state.

[[[00000000000000001519---61ec1a16f6f48e1801ccd06f89e0f3b67fc645b2661af38ca4fe0ad2ce4beb7a]]]A continuous task, on the other hand, is a problem with no 'end'. For example, the problem of inventory management can be thought of as a continuous task. The inventory management problem determines how many items an agent stocks. It is necessary to determine the best amount to purchase according to the sales volume and the amount of inventory. Problems like this can be defined as problems that go on forever, with no 'end'.

[[[00000000000000001520---8dd4eef50814999ada238c8d1672b023cea2c7b872efc96f7e5fe7ca7837f440]]]Earnings

[[[00000000000000001521---12b002102b231856201f07daa56f21aaa710a5b477975332678dee33c62e3f51]]]Then we introduce a new term, Return. Maximizing this revenue is the agent's goal.

[[[00000000000000001522---fb264cc67de6cdd73cd54d92991a29ec7937ae3084cd1a36adbb31849bfe0120]]]In defining revenue, the assumption here is that the time is and the state is (this can be any value). Next, consider the case where the agent acts according to the policy, receives a reward, and transitions to a new state. Revenue is then defined as:

[[[00000000000000001523---6d89a52b63dd28b5bfc1a5eeb4e52f34b2b7bc380dd2259b07881eca675456f2]]]As per equation (2.1), revenue is expressed as the sum of rewards earned by agents. However, the reward decays exponentially as time progresses. This is called Discount Rate and is a real number between 0.0 and 1.0. For example, if the discount rate is 0.9,

[[[00000000000000001524---60582c0d825863d0b36ab731b649c364c3f28b8ff88324e802f3024969941de9]]]It looks like.

[[[00000000000000001525---adf81762f3df859153a84997c9296beab0b51bbbc91c050b1ece5ad24e2afcc1]]]The main reason for introducing a discount rate is to prevent infinite returns when assuming a continuous task. If there is no discount rate (or otherwise) for continuous tasks, the returns diverge to infinity. Therefore, by setting a discount rate, divergence of earnings can be prevented.

[[[00000000000000001526---98be3f17cf2e77c70929a31d7242094961d180bf98560a241429deab9789e349]]]A discount rate can also make near-term rewards appear more important. This explains many of the principles of human (and even biological) behavior. For example, would you choose to receive 10,000 yen today or 20,000 yen in a year? Given that discount rates exponentially reduce future rewards, immediate rewards are more attractive.

[[[00000000000000001527---9929411b5a97711349b9b628bc7f2d79b6891dd91c41257b2d32af1248966c73]]]state value function

[[[00000000000000001528---6c5494b3d2632095769e91b146b50bc65b9b84f7a10dafa47eda7dc8702a5497]]]So far, we've defined 'revenue' anew. Maximizing this revenue is the agent's goal. There is one caveat here. That is, agents and environments can behave 'probabilistically'. Agents may decide actions probabilistically, and state transitions may be probabilistic. In that case, the profit you get will behave 'stochastic'. For example, even if you start from the same state, the earnings you earn in one episode will stochastically fluctuate from episode to episode.

[[[00000000000000001529---9f699fdba18e62bc422a566b51880811bbaa3d60dd9ca25a5f96f7611a146fdc]]]To account for such stochastic behavior, we need a measure of expected value—or “expected return”. The expected value of profit is expressed by the following formula:

[[[00000000000000001530---bdc84a65a5f88b7a3618c1f544e941b50de16a750da5c30f4343fb1a05e22f9c]]]In equation (2.2) we condition that the state is and the agent's policy is (where time is any value). Under that condition, the expected value of the agent's profit is expressed as equation (2.2). Here we will denote the expected value of return with the special symbol . is called the State-Value Function.

[[[00000000000000001531---67c7c4c5079d3c2e201811e3cd1fbe9027217f8ce964c862c12c6938af9de89d]]]On the right-hand side of Eq. (2.2), the agent's policy is given as a condition. This is because if the policy changes, the reward that the agent gets will also change, and the total profit will also change. To make that point explicit, the state-value function is often written below . Equation (2.2) can also be written as follows.

[[[00000000000000001532---22bc3dbd5e9e4f716bd313c8bc62573da2a6893afa3d4f99943e807243b2b226]]]In formula (2.3), the position to write is . The implication is that the same policy as in formula (2.2) is given as a condition. From now on, we will write in the style of formula (2.3).

[[[00000000000000001533---6167fa4d392504ea866ddf7cb3e150c743e7717954b0d64f3083b74fd0c2b510]]]The notation for the state-value function may be is the true state-value function and means the state-value function as an estimate.

[[[00000000000000001534---c229459080c9aded55949331b10dc6fd089ecdf440985173ccd10d54ae626f4c]]]Optimal policy and optimal value function

[[[00000000000000001535---f0fc8f33c5bb96b18284d14ccedea7b9f7b434bf6da6d39b91d31fc95ea55515]]]Our goal in reinforcement learning is to obtain the optimal policy. Here, we consider what an optimal policy is, and how we can express 'optimal' in the first place.

[[[00000000000000001536---20e98c5592cd219de8788958b2912e3c4e8141f46f5b53325644ebc602490630]]]First, consider the case where there are two strategies here. Let's call one and the other. At this time, the state-value function is determined for each of the two policies. Here, we assume that the value function is determined for all states, as shown in Figure 2-10.

[[[00000000000000001537---b3150b5c8fbeb653276739a0399517eec4129cf34430daf30d17260e172ba1f3]]]
Figure 2-10 Graph of state value function in each state with policies (there are five states in total)


[[[00000000000000001538---ed233646e0fce7b1c917dddf9387bcafb0a55867e31e82736ae8e40e1de1bb26]]]The point to note in Figure 2-10 is that in one state and in another. For example, when the state is , the expected return is higher if the policy is followed. However, when the state is , the policy will have better results. In cases like this—where the state-value function changes in magnitude depending on the state—the two policies cannot be superior or inferior.

[[[00000000000000001539---daa06fa6522b9a3c6c06f5313bf418c1883a333da34c1f78ee5c183f23416669]]]In what cases can the two strategies be superior or inferior? That's the case in Figure 2-11.

[[[00000000000000001540---44ae92cfa6903cc67aaadaca037cd04d40d673851b76f57bf223c847ee34a884]]]
Figure 2-11 Graph for all states


[[[00000000000000001541---960f6c3c62e5c5823a0f0f16df338c8a7079dbc8eacb3a09e63e87bece2552e9]]]In the example of Figure 2-11, in all states. In cases like this, policy is better than policy. This is because the expected value of the sum of rewards obtained is greater than (or equal to) no matter which state you start from.

[[[00000000000000001542---7eef87f79e8d142e12a25654f39f15b3d0b97c28999811f5c3e3d57ee34aad39]]]In this way, it is necessary to satisfy 'all conditions' in order to determine the relative merits of the two measures. When this condition is met, is a better policy (if in all conditions it is an equally good policy).

[[[00000000000000001543---02b1ac84d6f23b9de6c6c7e81b47031a858a338bca97dbd7495761fb83a69299]]]Now we can weigh the two strategies. Taking this idea forward, we can define what the 'optimal' policy is. If we denote an optimal policy by , it means that the policy has a large value of the state-value function in all states compared to any other policy. This can be represented graphically as shown in Figure 2-12.

[[[00000000000000001544---b498947ae2029ec35e43171dfd0597eaa24b38bfc7a4f983b6d0170739d234a1]]]
Figure 2-12 Optimal strategy and other strategies


[[[00000000000000001545---1240586d9bb2b142d7b5d098a40b00f0ef6f5b9b668022b18c484da27669519f]]]As shown in Figure 2-12, a policy is the optimal policy when the value of the state-value function is greater than any other policy in all states. An important fact is that in MDP there is at least one optimal policy. Moreover, the optimal policy is the “deterministic policy”. A deterministic policy uniquely determines the behavior in each state. In mathematics, it can be represented as a function that inputs a state and outputs an action, like

[[[00000000000000001546---b6a6b171d3d23662a26ac3b475ccbb8e2ec77f62cdf4dc3073b82ebde8fd2c2d]]]It can be proved mathematically that there is at least one optimal deterministic policy in MDP. We omit the proof in this paper. If you are interested in the proof, please refer to [4]. Also, the reason why the optimal policy is deterministic is explained with a concrete example in ``3.4.1 Bellman optimum equation in the state-value function''.

[[[00000000000000001547---c1c85e0502bfe25b52d57908e2b0b42026cf631c62c413270750b5bf63d48574]]]The state-value function in the optimal policy is called the Optimal State-Value Function. In this book, we denote the optimal state-value function by .

[[[00000000000000001548---9e3c211ff89448a39db3f2f50a3a2b9b2e0126d6a844fc9e7400166aad1c046b]]]MDP example

[[[00000000000000001549---6ae5f7fcb1678cd57c069f8e015e88a461746d58174ce64c36398b5825a4fd5e]]]Now let's look at a specific issue with MDP. The problem this time is a two-square grid world as shown in Figure 2-13.

[[[00000000000000001550---880f3e4ad585ffff524ac3ff2ece1a44c2b82110034260bdf8ee49d80728978c]]]
Figure 2-13 A grid world with two squares


[[[00000000000000001551---ae057e5540bf1a08587fdb7525db376c9b87cb108704652cf6d2ba29c075c524]]]As shown in Figure 2-13, there are two squares with walls on the left and right edges. Here are the settings for this issue:

[[[00000000000000001552---ce018862e30e9a22f296a964e3c67423ad3aade81b1be1d3982b08c859fdb413]]]Agent can move left or right

[[[00000000000000001553---66b08429f4b1235daa48dfb382549d75e32c32782f632ab68420fcb9cc8d9628]]]State transitions are deterministic (when the agent performs an action to move to the right, the next state of the agent must move to the right).

[[[00000000000000001554---d5c07e7fcde0cd7b27ffaa3c7379a640587d141348cf9d18627370b5635dd0f5]]]Receive an apple and get +1 reward when agent moves from

[[[00000000000000001555---46fdfe22f7c7eb863e9f16058fd7a0db4e1e9b1c9593213bc30894297fa2cd29]]]The apple reappears when the agent moves from

[[[00000000000000001556---95df61c82e64ea2b668dfb509d23b744530ddb9c8983db2e4b103b93f7b40bba]]]Get rewarded (punished) for hitting a wall. For example, if the agent is standing and performs an action to move to the left, it will get a reward of . Also, if you go to the right in the state, you will get a reward as well (At this time, apples will not appear)

[[[00000000000000001557---4ad8ba15118c97d8028b72e4570a9a4fe52df7359cee264e767c41c6a2552f81]]]This time, set a problem with no 'end' as a continuous task

[[[00000000000000001558---3db4af369128a8eae88af7d7c4dc1fc817cf31c4b813516534bc8bce84aec017]]]Now let's solve the problem in Figure 2-13.

[[[00000000000000001559---9f9546e59603a8440e7ca19db22697153b6cd2bc5bc54e5d226fd2b4f25cc2ee]]]Backup diagram

[[[00000000000000001560---3d0fe664361001ffd8e7c0adf06c3912fa1a03185310a8357d66e0817e6167f5]]]Start by drawing a 'backup diagram' to organize the problem in Figure 2-13. A backup diagram expresses the transition of 'state, action, reward' by 'directed graph (= graph composed of nodes and arrows)'. First, an example of a backup diagram is shown in Figure 2-14.

[[[00000000000000001561---a9c5acc84a73f061140d195cedf4345a2b65cf2133bf60afe9c7d34a74898b6c]]]
Figure 2-14 Backup diagram example


[[[00000000000000001562---36ec488f11f0beb8b1c5a46fc1f1f7cf8caa797c81c0295487eac5c1e6fe965c]]]Here we consider the case where the agent always moves to the right no matter what state it is in. In such cases, Figure 2-14 shows the agent's actions and state transitions. In Figure 2-14, time progresses from top to bottom. The transition from the initial state is drawn.

[[[00000000000000001563---6e6dc951302133b05d84a0e75d9b386e488b934063c7bce90b1a69ef2f5525c6]]]Figure 2-14 shows that the agent's policy is deterministic. In other words, it always takes a fixed action. Furthermore, since the state transitions of the environment are also deterministic, the transitions of the backup diagram are linear. For reference, the backup diagram when the agent has a 50% chance of moving right and a 50% chance of moving left is shown in Figure 2-15.

[[[00000000000000001564---94c45d5fe5e722ba6cb8026d02549e985b7a777b03e235a0bd3e79a03a78993d]]]
Figure 2-15 Example of backup diagram (when agent's behavior is probabilistic)


[[[00000000000000001565---634cec811847b91f3b7aa857cd59171cdc2c6d4b5f82a7c0f66054541fa0aa78]]]As shown in Figure 2-15, there are two possible actions, right and left, in each state, so the backup diagram expands and grows. However, this time we will consider the case where the state transition of the environment is deterministic and the policy of the agent is also deterministic. In other words, it is a case where the backup diagram extends straight as shown in Figure 2-14.

[[[00000000000000001566---32d7ece26705d7e9b0c5cefb1a143997cc0e3fea464cbeeaf711a77dec6d20fc]]]find the best policy

[[[00000000000000001567---bf22aea7ae68e56c9a661f1f1cdd05688bdb428379e15c2b0bae7a811de63978]]]So, what is the optimal strategy in this 2-square grid world? We know that the optimal policy exists as a deterministic policy. A deterministic policy is represented as a function as Our problem has a small number of states and actions, so we can come up with all possible deterministic policies. Specifically, since there are two states and two actions, there is a total of deterministic policies. Those four measures are:

[[[00000000000000001568---57d2e36b50f0cd82db94538651a1e86eb4cc1411ca7a7fb0c70e99ccc42f9bd7]]]
Figure 2-16 Deterministic policy pattern


[[[00000000000000001569---433256d13e18108bd294ce1c68ec53463c8842f23e5e6ee4726ccd88268a7e9f]]]As shown in Figure 2-16, the actions to be taken in each state are indicated (since it is a deterministic policy, there is one action to be taken in each state). For example, the policy moves to the right in the state, but moves to the right. There is an optimal policy among the four policies in Figure 2-16.

[[[00000000000000001570---df08a8ad99144b87138ce1d382cc901fc004e63a014c74bee77ea3150c572f01]]]Now let's calculate the state-value function for the policy. This calculation method will be dealt with in detail in the next chapter, but this time it is easy to find because the state transitions and policies of the environment are all determined deterministically. For example, if you are in a state and move right according to it, you will get a reward. After that, move to the right and hit the wall, getting a reward each time. Assuming a discount rate, the state value function can be calculated as follows.

[[[00000000000000001571---71da9f6e6c9e01750f3c1b1a3325254a9481027ad3b2f95b86648ad556cc16cb]]]Here we used the formula for infinite geometric series. The formula for infinite geometric series is (when ). Alternatively, the code can compute it approximately as follows:

[[[00000000000000001572---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000001573---93e592e7186d6adeffd9f2f39790068a0b3685cd944ae29e077b614cc28b9311]]]Here we use a for statement instead of an infinite number of computations to approximate it. The results are approximate, and you can see that they are almost the same as the theoretical values.

[[[00000000000000001574---2ebb0a6d839f07fc2ba049996bd0851c6582c8e15348d78527196522c5d3f5a5]]]Then next is the value function in the state (where the policy is). This time you keep hitting the wall on the right, so you get a constant reward. So the state value function can be calculated as

[[[00000000000000001575---c3b783916fea3854778b0b59997b4848db3b5814120380f82d34ce56e934b177]]]From this, the policy value function was obtained. Repeat the above steps for all other measures. Then you get the result in Figure 2-17.

[[[00000000000000001576---d75ce1c616d612bf83409f263158e9a567059e3d282ebd656f3260522e023362]]]
Figure 2-17 Graph of state-value function for each policy


[[[00000000000000001577---ac75b2f8b69092a71a3111fafde8975c273e9ebdbb0504467dcd218906c4e22f]]]From this figure, we can see that the policy has a higher value of the state-value function than the other policies in all states. So policy is the policy we seek. The policy repeats left and right actions without hitting a wall. Keep picking apples repeatedly in the process. Now that we have the optimal policy, the goal of MDP is achieved.

[[[00000000000000001578---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001579---9843b23d644f854b795f4119e8f3805f9215478a381ea415400ed18e175b3729]]]In this chapter, we learned about MDP. MDP is a formalization of the interactions between agents and the environment. Environments have state transition probabilities (or state transition functions) and reward functions, and agents have policies. The environment and the agent then interact with each other. The goal of MDP is to find the optimal policy within such a framework. The optimal policy is the policy that has a state-value function greater than or equal to any other policy in all states.

[[[00000000000000001580---1aeeaeb40f2531a370948ddf1c6a554b9fac514e493b870515bec61c890b8651]]]In this chapter, we were able to solve the “2 grid world” as a reinforcement learning problem and actually find the optimal policy. Specifically, we proposed all policy candidates and obtained the state-value function for each policy by hand. And I found the best policy among them - the optimal policy. Unfortunately, the methods used in this chapter are only valid for reinforcement learning problems with simple settings such as a 2-square grid world. In the next chapter, we will prepare so that we can deal with even more difficult problem settings.

[[[00000000000000001581---26c6ff2fad7c7e7f39253adc724df20238ba4686447a74fc14408027340d7a52]]]Chapter 6

[[[00000000000000001582---8e86281c818f18b05d78b4b48855a868587eaf1d5cf312c486cb575dacfafcf0]]]TD method

[[[00000000000000001583---ee64a948cf3091cc7a2cd498bc6871e1bcb523bece72ef8650545416db93e8b9]]]In the previous chapter, we learned about the Monte Carlo method. Monte Carlo methods allow us to evaluate policies without a model of the environment. Then, by alternately repeating evaluation and improvement, an optimal policy (or a policy close to optimal) can be obtained. However, the Monte Carlo method cannot update the value function until the 'end' of the episode is reached. Because the 'earnings' are finalized only at the end of the episode.

[[[00000000000000001584---9dc2a777470fcbdb11288673092c380294a67b5204b868c13eec256266942ee3]]]For continuous tasks, the Monte Carlo method cannot be used. Also, even if it is an episodic task, if it takes time to finish the episode, the Monte Carlo method takes a lot of time to update the value function. Especially at the beginning of the episode, the agent's tactics are often random, so more time is needed.

[[[00000000000000001585---3d1d176d7c66f0fb844a2b40303e5d49036feb62cdee9ebe3197a6ca5b34525f]]]Here, we will explain a method that does not use a model of the environment and that updates the value function each time an action is performed—the TD method—. TD in the TD method stands for Temporal Difference. In Japanese, it means 'time difference'. Instead of waiting for the end of the episode, evaluate and improve your strategy over time.

[[[00000000000000001586---49172906f7d28292b59ed2f861fa521d78706fe597aff3af164bb20c83e4f8e3]]]Policy evaluation by TD method

[[[00000000000000001587---5fc165d9bf3373975062eaf923b818fa872de9e371d2bfa21b54d2ef27d50ee1]]]The TD method is a combination of the 'Monte Carlo method' and 'dynamic programming' that we have learned so far. Therefore, we will review those two methods first, and then derive the TD method. For simplicity, the Monte Carlo method is abbreviated as 'MC method' and dynamic programming as 'DP method' (dynamic programming is generally abbreviated as 'DP', but here TD method, I will write it as 'DP method' to match the MC method).

[[[00000000000000001588---1dea8fdb42c90a9a77ede8c5033573bdb62db8d69e8d6daf90476a4fe4985be1]]]Derivation of the TD method

[[[00000000000000001589---97c6b8004b19c7ff3872ee9feaaad7f57073bf2893a60f4b80a4185aa5122508]]]Let's start with a review of 'revenue'. We defined revenue as:

[[[00000000000000001590---d55799a3cc86178d378cae2f3d96439535c75f1b7d399501374fe5884b37ac2d]]]Here, assuming that it starts from the time, we will consider the case where the reward is obtained. At this time, the profit is expressed as the sum of rewards with a discount rate, as shown in equation (6.1). And we can recursively express — by — as in Eq. (6.2). Using this return, the value function is defined as:

[[[00000000000000001591---a94b3a6d5584f52f318e350e04c8b5d52df5926e28c65e2e24476c6168909836]]]The value function is defined as the expected value as above. This section will show

[[[00000000000000001592---926fc26c98a54c2169dfd06610d5e57dc279762820563338b7089c1ed575b5ba]]]The method using the MC method can be derived from equation (6.3)

[[[00000000000000001593---203672892d4210618ae30f6c3786daa50e48042e04296a6baf4e11440c79e676]]]The method using the DP method can be derived from equation (6.4)

[[[00000000000000001594---3bf093e06f10781af26d37a43c9a87c0008f9a8f43defaf2df90b1e01adeb8ae]]]about it.

[[[00000000000000001595---e4c050fcd75b1649918226a9c282171335b155d90f799c9c9b7a0c6bb5ff4713]]]Let's start with the MC method. Instead of calculating the expected value, the MC method approximates the expected value of equation (6.3) by averaging the sample data of actual earnings. There are two types of averages: sample averages and exponential moving averages. To achieve an exponential moving average, update with a fixed value each time you get a new profit. This can be expressed as a formula as follows.

[[[00000000000000001596---d9688b48a7d09e44fc4150aad0f953d9d7d48dac1f08f3875fc04f92e7fb8692]]]Let the current value function be the updated value function. Equation (6.5) updates the current value function towards Adjust how many people you want to update.

[[[00000000000000001597---75aae3626f494ef3727a9a5195b53494e2673242fecc2a2ec0ced0572dc33e92]]]Next is the DP method. The DP method calculates the expected value based on equation (6.4). Unlike the MC method, the DP method calculates the expected value according to the formula. Expressed as a formula, it looks like this:

[[[00000000000000001598---917da87dc81cc19eea227cee4dad2add74958739a18db940b129c5891c4fdb23]]]As above, we use the state transition probabilities and the reward function to compute the expected value. By the way, equation (6.6) is the Bellman equation. The DP method iteratively updates the value function based on the Bellman equation (6.6). The update formula looks like this:

[[[00000000000000001599---3ba4c39f2444b9f0f3c03343c7311f061bbffe6276aea6f19ac1785ef45588bb]]]The important point in equation (6.7) is to update the value function in the current state with the value function in the next state. At this time, the feature is that all transitions are considered. This feature of the DP method becomes clearer when compared with the MC method (Fig. 6-1).

[[[00000000000000001600---d8fc0e741a8f0cdbee03d07dec23f27200e68f480c6f6eeec81470c202c3d4f1]]]
Figure 6-1 Comparison of DP method (left figure) and MC method (right figure)


[[[00000000000000001601---f792f30d4d3000252b6b113741ee7108157b71486abd0b6314daad6a288c628e]]]Figure 6-1 depicts all subsequent transitions starting from a state. The DP method updates the current value function estimate with the next value function estimate. This principle is called 'bootstrap'. On the other hand, the MC method updates the current value function according to the actually gained experience. The TD method in Figure 6-2 is a fusion of these two methods.

[[[00000000000000001602---62acd4d0334d19fb6969e7e0e5d191bd817b871b7162261ceaf1c593cade1ef6]]]
Figure 6-2 Idea of TD method


[[[00000000000000001603---d4cc4c63a0884e00508d53f4ab072319695775a37fcb6641e0919b06e449b353]]]The TD method updates the current value function using only the next action and value function, as shown in Figure 6-2. Two important points are:

[[[00000000000000001604---7470a73ae9ac76c81987a2c07ff9f3408131b73dc8e8897fcd378e54a654c733]]]Like the DP method, bootstrapping can be used to incrementally update the value function.

[[[00000000000000001605---9da7079c762f5afa674417f7ea9de23d6bd778fefc64e618147a1cdd73e769dd]]]Like the MC method, sampled data can be used to update the value function without requiring any knowledge of the environment.

[[[00000000000000001606---f0b0eba96c3e2a02ba7a5ef77290b7c8031fdbb8a66eb43f01f6f028cf28013b]]]Now let's derive the TD method from a formula. We start with the following expansion:

[[[00000000000000001607---53385bb9a39466c1063e3769ee66485acf44707ea9efc6f61ce7b73a38a8043a]]]Equation (6.6) computes—the reward and the following value function—for all candidates. Rewriting this in the form of the expected value gives equation (6.8). In the TD method, when updating the value function using formula (6.8), the part is approximated from the sample data. Expressed mathematically, the update formula for the TD method is:

[[[00000000000000001608---26be75fb3613a64564751b93288fcb3f01b9b944dddae0050543ca35cd594493]]](6.9) is an estimate of the value function, and the destination (target) is . This destination is called the TD target. The TD method updates the direction of the TD target.

[[[00000000000000001609---67b6484592b440fbcac1fae3d48fc9339fd16b6ed986449cf2ac36802c67879c]]]Here we used one step ahead information as the TD target. Extending this idea, it is also possible to use the information of n steps ahead, such as 2 steps ahead, 3 steps ahead, etc. This is called the 'n-step TD method'. The n-step TD method is explained in 'Appendix B n-step TD method'. Please refer to it if you are interested.

[[[00000000000000001610---585f0cc9dbd147484eb5fad89653bc9faf33a8b669092deff1308249ce8bae13]]]Comparison of MC method and TD method

[[[00000000000000001611---f635f902702ecea4c7359c0ce13e8415b62cc50851f84627a17385e04f85a0e8]]]There are two tools that can be used when the model of the environment is unknown: the MC method and the TD method. The question here is whether the MC method or the TD method should be used, or which one is better. Since the MC method cannot be used for continuous tasks, the TD method will be used. But what about episodic tasks? Unfortunately, it's not always theoretically proven which one is better. However, for many real-world problems, the TD method trains faster (the value function updates faster). The reason for this becomes clear when we look at what the MC and TD methods target (Figure 6-3).

[[[00000000000000001612---64145043dad38204272ea4f09f98c5cc17555137f0c65d18148c6d8b04cf788c]]]
Figure 6-3 Comparison of MC method and TD method


[[[00000000000000001613---841b0049ad6b81c4a861450e32612f112cd3379e159f0a40e4cd88df152bf8cc]]]The MC method targets and updates in that direction, as shown in Figure 6-3. is the sample data of the income obtained after reaching the goal. On the other hand, the target of the TD method is calculated based on information one step ahead. With the TD method, the value function can be updated each time time advances by one step, so efficient learning can be expected.

[[[00000000000000001614---b06e63d3b3a863046a40bc9dd7e124593182a51a62636cc6973e63e46ef5b20c]]]In addition, since the target of the MC method is the result obtained by accumulating a lot of time, its value has a large 'dispersion'. In other words, the variance increases. On the other hand, the TD method is based on one step ahead data, so its variation is smaller. This can be visualized as in Figure 6-4.

[[[00000000000000001615---df515d41fee29bed294c5d2fc1e60bf8f66544eacee001731894d19bac0097a4]]]
Figure 6-4 Image of increasing “variation” over time


[[[00000000000000001616---8ca34dc7a372df7f52f8d8de694dbe650664e2479c88274f7d068ea6f6a9324f]]]Figure 6-4 depicts the motion of the car. The car moves forward, but the driver turns the steering wheel to the right or left (or does nothing) at random. A few extended lines depict several possibilities. The point I want you to pay attention to here is that as time passes, the fluctuation of the destination increases. This image also applies to the MC and TD targets. The target of the MC method has a large dispersion because it overlaps a lot of time. On the other hand, the target of the TD method (=TD target) has a smaller variance because the time is one step ahead.

[[[00000000000000001617---fb4ba297442c846aabd9e895d1e3c471ec3f2fc1f51b23378fc3b0989784d454]]]The TD target is , but some of this TD target uses an estimated value. The TD method is 'updating the estimate with the estimate', i.e. bootstrapping. TD targets contain estimates and are not exact values. Technically speaking, this is called 'bias'. However, that bias gets smaller with each iteration of the update and eventually becomes 0. On the other hand, the target of the MC method does not include estimates. In other words, there is no bias in the target of the MC method.

[[[00000000000000001618---8d966646ce1ff95e89367ac7f1a3f89b031c6bae186234312a58654ab6b94551]]]Implementation of the TD method

[[[00000000000000001619---1ee03f9f884b389ee6debadd3f936680a50f8e9ebbef258735d57e54b12bd4e2]]]Let's implement the TD method. Here we evaluate the policy using the TD method for agents with random policies. Here is the code:

[[[00000000000000001620---1747e5f6d547c869d9d68aa6e244af3a3e6303e5df411b8da8614ce45d8e426b]]]# the goal value function is 0

[[[00000000000000001621---c3545702b14863285f7fb515130b8f3f0a2efebc9cf24d4af976e31485779f63]]]The TdAgent class has much in common with the agent classes we have implemented so far (such as the McAgent class implemented in Chapter 5). Therefore, only the eval method for policy evaluation using the TD method is described here. The eval method, with its arguments, is eval(self, state, reward, next_state, done). This method is called when the action action is performed in the state state, the reward is obtained, and the next state next_state is reached. It also receives a flag done as an argument to indicate whether the episode is over (whether next_state is a goal).

[[[00000000000000001622---2e9766fecaf685f795b2e9231add474630615ec44972b0fff0c1eacd5ae6ec8d]]]The value function is the sum of future rewards. The value function at the goal will always be 0, since there is nothing beyond it.

[[[00000000000000001623---275aea1d143ce0733e37de5f06adb2d7e8d8ada6bed24dde86a9974c8328696a]]]Now, let's actually make the agent act and evaluate the policy. Here we run 1000 episodes. Here is the code:

[[[00000000000000001624---4b752073296ec6bf9a15c94dfd0d7c88503e8926c47ce1b73bb404ee817c7982]]]# call every time

[[[00000000000000001625---2a2ec15b0ba8922ecfb7578ce8c3e34205744c7a239310ea20a42421b8b4ead8]]]This code is almost the same as the code (ch05/mc_eval.py) for the MC method in the previous chapter. The main difference is that we are calling the agent's eval method each time. The TD method updates each time the time advances by one step. On the other hand, in the MC method, the eval method is called only after reaching the goal. Now run the above code. The result looks like this:

[[[00000000000000001626---eac3f83e02abadb2026e6b3610fb9ee2ce978306a2e072a7f0bc2343246b1029]]]
Figure 6-5 Value function obtained by TD method


[[[00000000000000001627---93486a179b5ee61e77c33e33eca7bf1ff0a5f975fe2d0a83272f9157efdd3768]]]We were able to evaluate the value function of agents with random policies, as shown in Figure 6-5. This is approximately the correct result. We have successfully performed policy evaluation using the TD method.

[[[00000000000000001628---7fa7242cefbc384daa13133fbc856f50fadf22eaa54dc24f2c5e7a5e37fc549b]]]In the previous section, we performed policy evaluation using the TD method. After policy evaluation, the next step is policy control (we've already seen this flow several times). Again, by repeating the process of evaluation and improvement, we get closer to the optimal policy. Here, we will work on a method called 'policy-on type' SARSA. SARSA is pronounced 'Saasa' or 'Sarsa'.

[[[00000000000000001629---c8d4027a41c177785c12bdaf8d4e52c962387b0f474093ecf926d96625dda9ce]]]As explained in '5.5 Policy-off type and weighted sampling', there are two types of policy control methods: policy-on type and policy-off type. This section deals with on-policy methods, and the following sections deal with off-policy methods.

[[[00000000000000001630---129c69583270e64a9381f70f9915cb10ae520afd20c10d3d89f5eeb0df0d009c]]]Policy-on type SARSA

[[[00000000000000001631---01259066821706e942cd4141d56fcbc35132d84bbdecae941048a970af6c7ffd]]]In the previous section, we evaluated the value function. Policy controls should target instead of . The improvement phase requires greedying the policy, and in the case of , a model of the environment is required. On the other hand, if

[[[00000000000000001632---70f4a422e849aa4b771e5c0a69b9c2e3c67b0c937fc24f123f89ba72a5d5af1e]]]and does not require a model of the environment. This was explained in '5.4.1 Evaluation and improvement'.

[[[00000000000000001633---c331567f2ae698bb908b531456fbc3dd278c39edd696e82e5e3cb6061488f440]]]In the previous section, we derived the TD method for the value function. The update formula is given by formula (6.9).

[[[00000000000000001634---f1aa2c8a40985600fda0023577521af28907e3eca61732ed7d78ed817a419c10]]]Now let's change the TD method from the state-value function to the action-value function. To do so, replace , and , in equation (6.9).

[[[00000000000000001635---52e788a1c8ee89c1e834169a01785951d382b178fb9642646bde76ac55d60718]]]Formula (6.10) is the update formula of the TD method for the Q function. Next, I will explain the method of policy-on type policy control. In policy-on mode, the agent has only one policy. More precisely, the policy that actually takes action (=behavior policy) and the policy that evaluates and improves (=target policy) match.

[[[00000000000000001636---23cd71d55f10d42449f28d3773c55939eabd6138c6bfa2a27b6d57f3a1300218]]]For on-policy methods, the behavior and target policies are the same, so they cannot be fully greedy during the refinement phase. If you do, you won't be able to 'explore'. So we (compromise) use the -greedy method. That way, you can 'explore' but often do something greedy.

[[[00000000000000001637---a0950ca285b8c5fe884ec435dde9362423a462ea3a5265ca644d9b648c3177c0]]]Here, we assume that the agent is acting according to the policy. Specifically, let's assume that the behavior shown in Figure 6-6 is performed at different times.

[[[00000000000000001638---a576c6687245b89851a8770cd6c422737b310d3013a270ff2126c30c9ac3634b]]]
Figure 6-6 State and action transitions over time


[[[00000000000000001639---dbc154a2a58db13325a9921458b2964f9fecce50f5d131f794988bd8a3a37e7b]]]The Q function takes state-action pair data as one unit. Therefore, in Fig. 6-6, the paired data of state and action at time is grouped as the paired data when advancing the time by one step. If we obtain data like Figure 6-6, we can immediately update according to formula (6.10). And as soon as this update is over, you can move on to the 'Improve' phase. In this example, is updated, so the policy in the state may change. Specifically, the policy in the state can be updated as follows:

[[[00000000000000001640---c007ba7b16147c86ab4f9ff77fffbb33aef6439d79a7e0bfb8314c11c9d85cd4]]]As shown in equation (6.11), choose a random action with a probability of , otherwise choose a greedy action. Greedy behavior improves policy, and random behavior drives exploration. The -greedy method updates how the state chooses actions.

[[[00000000000000001641---6e3742203bbba6bf007a4b65da767fa618790ab5fb513d334cdfa6413d347b00]]]In this way, the evaluation by expression (6.10) and the update by expression (6.11) are alternately repeated. Doing so yields a near-optimal policy. This algorithm is SARSA. The origin of this name comes from the initials of the data used in the TD method.

[[[00000000000000001642---e74520cf94044dd3ddb54b6b0e6bc0147794a61a4b65b6c51989152fcce97481]]]SARSA implementation

[[[00000000000000001643---5246655c9d2aa0da00bfa807b43f5f651e2f12898a4134cd42a2e792066213ce]]]Now let's implement SARSA. Here, it is implemented as a SarsaAgent class as follows.

[[[00000000000000001644---b029cfd98931f04128e886d41a8f7b17c23d4e0f74cb82db07f50caf4d166aac]]]# ① use deque

[[[00000000000000001645---f1b1c1f95bad6420a517667138ed9ec83c539b32d3a179187aa26054bfe1216a]]]# Select from ②pi

[[[00000000000000001646---ff8afde104dbdb715191cb1eb75cd879efe4195bf35306982146b9364d2c5d02]]]# ③ Next Q function

[[[00000000000000001647---092e6c7a4e013139f2570138d5518486942e8df9b4fed93ac3a054cff08d37ba]]]# ④Renewal by TD method

[[[00000000000000001648---6becf6ee0d82b39baab9de877c42110bd9fa07bfe46962f3a53419e693627d40]]]# ⑤Improvement of measures

[[[00000000000000001649---ea2d869b05163a5b012b594c461cc14e29f0c6ed4220dcba9759b6b6bf2cb766]]]The SarsaAgent class has much in common with the agent classes we have implemented so far. Here, we will explain code 1 to 5 in order.

[[[00000000000000001650---d465a97c7f25ad6e434186447f4beba3eb40937fbfd912008d51692865454baa]]]Here we use collections.deque from the Python standard library. A deque can be used like a list. Additionally, if more elements are added than the specified maximum number of elements (maxlen), the oldest elements are removed 'first in first out'. By using a deque, you can only hold the two most recent experience data.

[[[00000000000000001651---abed09cc1faae4aa223d965d7cdebde9de1ef057ccdc17daf95e610378e2b9e6]]]The SarsaAgent class is policy-on type, so it has only one policy. The get_action(self, state) method retrieves an action in state. Its actions are picked from policy self.pi.

[[[00000000000000001652---999015e98097092ee14328a380788442b3c0496d348e6975c6397bba918d8f4e]]]If the done flag is True, the goal has been reached. The Q function at the goal will always be 0. Because the Q function is the sum of future rewards. Being at the goal means there is nothing beyond it.

[[[00000000000000001653---5d93be1f220a92a1942cd46f3886b75b41b47f6168191ba2cf0eac501ace9360]]]Update self.Q by SARSA formula (6.10).

[[[00000000000000001654---127dcaf1a9739eb5ac7bcc476287d1be2c7cccef2a243b4ec6f3f23da1d4e250]]]We will use the greedy_probs function implemented in the previous chapter to improve our policy. Now the action in the state state of the policy self.pi is -greedy.

[[[00000000000000001655---918e11ee2c4d60250c30307401f430d1ac9dda505bd2ebb6ca99817ce40fc886]]]Let's get this SarsaAgent class working. I will challenge the task of '3 × 4 grid world' as before. Here we train with 10000 episodes and finally visualize the Q function with env.render_q(agent.Q). Here is the code:

[[[00000000000000001656---4b752073296ec6bf9a15c94dfd0d7c88503e8926c47ce1b73bb404ee817c7982]]]# call every time

[[[00000000000000001657---92d217eff039858c29ba0e9b98d557a4decf0181cda077da0bc460d2a9a87f2a]]]# also called when the goal is reached

[[[00000000000000001658---107e3ad209822c10a17cb7a8c058f16b6fdc0c5ce5eeedc4538f569aa112600b]]]The point I want to focus on here is the timing of calling the agent.update method. It calls it every time inside the while loop. Also, the agent.update method is a set of two calls to update the policy. So, when the goal is reached, we additionally call agent.update(next_state, None, None, None). Now run the above code. The results are as follows.

[[[00000000000000001659---1e226d752253c6ac6dc48fe4b4fc904a99c86401a7b9a8f9bc76ca394ee9d2fe]]]
Figure 6-7 Results obtained with SARSA


[[[00000000000000001660---b29d9000c7a615bf6d3bdddaa49cac6238e381dad74fac468c1c41e7658ddf2a]]]The results in Figure 6-7 vary from run to run, but are generally good. In Figure 6-7, only greedy actions are drawn with arrows, but random actions are entered. Due to the randomness of the policy, it will move as far away from the bomb as possible. This is the end of the policy-on type SARSA implementation.

[[[00000000000000001661---e085072f3ad80915fe5efc48302be671b11733dbc279d5e10259ee1cf624ee65]]]Policy-off SARSA

[[[00000000000000001662---fc8f085f39dbbdb155941dbd3255bd4efe520bb17c20cdbdd44ab93739713d7c]]]In the previous section, we implemented policy-on type SARSA. Now let's move on to off-policy type. Usually, 'Q-learning' appears here, but in this book, 'policy-off type SARSA' is derived first. After that, proceed to 'Q-learning'.

[[[00000000000000001663---49a566e891e2af7703cca635471bbf68bd6fe7c79974e446a5e69d6c2ef5585c]]]Off-policy and weighted sampling

[[[00000000000000001664---9cc207172523dd8917b6c7f4b4394c3e389e19e53b575dd6da376e7728f42a74]]]In policy-off case, the agent has two policies, the behavior policy and the target policy. A behavioral strategy collects a wide range of sample data by performing a variety of actions. Then use that sample data to update the target policy to be greedy. Here are some things to keep in mind:

[[[00000000000000001665---7458b58d83e98733c27baf4bc17993ceec19deeefc90b7ac512c8954e9344504]]]The results are more stable when the behavior policy and the target policy have similar probability distributions. Considering that point, the behavior policy is updated to -greedy for the current Q function, and the target policy is updated to greedy.

[[[00000000000000001666---280bdfd995741f1878fa14c8cecd47dc89c76bbc61f866e5c87351e72514c5e5]]]Because the two strategies are different, we use importance sampling to compensate for the weights

[[[00000000000000001667---bab42c0f7ceb6a4a9eed68d7741e58bf80dcc42cc23481cf33a47105f1c97ed9]]]Now let me explain in detail. Now suppose you want to update the . Then the SARSA update formula is:

[[[00000000000000001668---1b2d152073ef5ea15706d00524fd742314e7e884ed15f34d9c8d9adc582e8d4a]]]Also, the backup diagram corresponding to this update formula can be drawn as follows.

[[[00000000000000001669---498f1d4a60c6991b7b8df2f6407b5024a9b93302d57947e0c8e51042ae92f469]]]
Figure 6-8 Backup diagram for SARSA update formula


[[[00000000000000001670---0dd455022be3a31e5e28903543f96b0ecd263f8fa18aa9efb3b945165259f279]]]This is the target for which the pair data is updated, as shown in Figure 6-8. This update target can be selected arbitrarily. Consider the next time transition for the selected pair data. Then the next state is sampled by the state transition probability of the environment. The action taken in the state is then sampled by the target policy (or behavior policy). Using the sample data thus obtained, update according to equation (6.10). Then, if we explicitly state that the action is chosen by the policy, the SARSA update formula can be written as follows.

[[[00000000000000001671---24a46b393cb2f3451491fb851b5ad8fbb83a867a2607c55b8c4052315ec1036b]]]Equation (6.12) expresses updating in the direction of . This is called a 'TD target'.

[[[00000000000000001672---f7841760895a4eb84c06770f59ba0fb6aa7cd7715556ab41227148d0eacf4377]]]Now consider the case where behavior is sampled by a policy. In that case, we correct the TD target by weights (=important sampling). The weight is the ratio of the probability of getting the TD target when the policy is and the probability of getting the TD target when the policy is. Expressed as a formula

[[[00000000000000001673---8bc2616c4e6d79e9285e445cf8cac6c3dbf34886a61ff145dd5d7c9eab094db2]]]becomes. Therefore, the policy-off type SARSA update formula is expressed by the following formula.

[[[00000000000000001674---709e878fbacf89ae899c8f91e8fb2e10cf6d3ef3525670760cb999a8b374d2c1]]]Actions are sampled by policy and corrected for TD targets by weights, as shown above.

[[[00000000000000001675---83d385a7d430195b4f5de8916381c6f83e096e5b516b1a1e5a4faff0d3afa33d]]]Policy-off SARSA implementation

[[[00000000000000001676---cfaf6a97b5de8328c9573002b43b701d1894961a106f77cc471ed3668f958f69]]]Now, we will implement the off-policy SARSA. Here is the code:

[[[00000000000000001677---9a3b0d5c45833434835baefdb4e71a2ba6ebef25982510c5c440d8002bd224b4]]]# (1) Get from behavior policy

[[[00000000000000001678---2e4cff9f1e8a01cb23778527f925af62872b7fb6dee3e3ab26e34d651eed5d28]]]# ② Find the weight rho

[[[00000000000000001679---114c4db9fa348e7feb8f5018dea394926d0a0a86fa21183ead269d30f4290054]]]# ③Correction of TD target by rho

[[[00000000000000001680---dc06a72e249e9082f651913bcee6e2c4e73417049551b6d9570aa709e41bfdaf]]]# ④ Improve each policy

[[[00000000000000001681---e57c92d32f8cab1a8b9fa81fed1bd0b496aeaf20301e40f9e09aba4d0b4021cc]]]In this section, I will explain steps 1 to 4 in the above code.

[[[00000000000000001682---9a9af088bb1ae3b43db36e0862458d0e73069e746853ff464fe6eeb454bceb12]]]In the get_action method that retrieves the action, we retrieve the action from the probability distribution of self.b.

[[[00000000000000001683---364d1e47add90d474f7741965cfb14df6b645af3ba0bf1bb4742c6f9601f161c]]]Find the weight rho by importance sampling. This weight rho is computed by the ratio of the probabilities of the target policy self.pi and the behavior policy self.b.

[[[00000000000000001684---96ec84819d6e50c44edfee8ff324371b6396204c0d5f91ec16016b66fddf3f49]]]Multiply the TD target (target) that the function updates to by the weight rho.

[[[00000000000000001685---be9ffe880a70fe7b78759ca53bdf539e7ec399f76e43e3cbd63fa5474139e929]]]The target policy self.pi improves to greedy and the behavior policy self.b improves to -greedy.

[[[00000000000000001686---42129a954e8aacefd7326dd1e47a843f4204005d7bc49660d17dcd66335c4cd4]]]Now, let's use the SarsaOffPolicyAgent class implemented here to solve the grid world problem. The code that drives the agent is the same as in the previous section. Here are just the results:

[[[00000000000000001687---4caf53da541ba78c50b32efc8023fa18d45bfd6fe7672155d0f593fb03dcffcb]]]
Figure 6-9 Results obtained with policy-off SARSA


[[[00000000000000001688---e98913bc177aca304f7d8487643313f610e188197f0a9159a88a09081d606e69]]]Results will vary from run to run. Looking at the results in Figure 6-9, it seems that there is still room for improvement. Let's end the off-policy SARSA implementation and move on to the next technique.

[[[00000000000000001689---7a1e0833598280b53fb24b8648d4ce7bf3dd9c06b0b4126012dba3918b2f8a08]]]Q-learning

[[[00000000000000001690---ac213ceacb299f98c40f551c386910bf7d0d70aafd6daf562fa30168c0ef9f6e]]]In the previous section, we implemented policy-off SARSA. Being off-policy, the agent had two policies, the behavior policy and the target policy. By sharing the roles of the two policies, we can let the behavioral policy do 'explore' and the target policy do 'exploitation'. This will (hopefully) give you the optimal policy. However, in off-policy SARSA, we need to use importance sampling. This weighted sampling is a technique that you want to avoid if possible.

[[[00000000000000001691---3117ae389b3b9997b8da3e13e4494d0d280cb0785ad176a4107c8f8d16b53575]]]Importance sampling has the problem that the results tend to be unstable. In particular, the more different the probability distributions of the two policies, the greater the variation in the weights used in importance sampling. As a result, the target in the SARSA update formula also fluctuates, making the update of the Q function unstable.

[[[00000000000000001692---3fa6962170178fcf6ba12976bdd34a46a487d68489d6218b3f15e871ca5f92a4]]]Q-learning solves this problem. Q-learning has the following three characteristics.

[[[00000000000000001693---8e86281c818f18b05d78b4b48855a868587eaf1d5cf312c486cb575dacfafcf0]]]TD method

[[[00000000000000001694---fda8035cb8a6ee1535f62372ad1c9f732214e8b92248cd2b7a4e72dbbc44f849]]]policy off type

[[[00000000000000001695---96c479692aa21d128801e59850e8af274701e7ee3f574d1f00abd69a8a694dfd]]]Do not use importance sampling

[[[00000000000000001696---3d6a6383c9926f0f75f40284a3b1926dbba0f9ed3ca896d6882b20f8f4c5ecde]]]Here, in deriving Q-learning, we first confirm the relationship between the 'Bellman equation' and SARSA. After that, we derive Q-learning in a form related to the 'Bellman optimum equation'. As a flow, SARSA is derived from the Bellman equation, and Q-learning is derived from the Bellman optimum equation (Fig. 6-10).

[[[00000000000000001697---705c1719bb4fbabc540cd4c39837533d1a5c75a5fbc80b12f65c84ed1776e43f]]]
Figure 6-10 Relationship between Bellman equation and SARSA (upper figure), relationship between Bellman optimum equation and Q-learning (lower figure)


[[[00000000000000001698---6e60f059b5913ae8e15cf2cf1ffe7036ccee4b8ed5ddf673f135a930ed6ceba2]]]Bellman equation and SARSA

[[[00000000000000001699---a9dc9f3c658082883ddc9ade1be4e951872d30d0df660320c8c77c4cd79acb52]]]First, let's look at the relationship between SARSA and the Bellman equation. As a review, the Bellman equation is expressed by the following formula when the Q function in the policy is used.

[[[00000000000000001700---8b349574e64ea125092a168b4180376db6ca167bff82013ea565eca8f527d89a]]]There are two important points about the Bellman equation:

[[[00000000000000001701---b099ebd36ead61876aa0e78ab811838bc3bb055820c6fcceb07fd35fc6208b2a]]]Considering 'all next state transitions' according to the state transition probabilities of the environment

[[[00000000000000001702---6297db8c79a21dc01d1c61b3f55ad5632ef6e84c45089ff321127df3bccf2513]]]Agent policy considers 'all next actions'

[[[00000000000000001703---d7490d4badfb9538986d0cfcefb6951541091ba1d21cbd4093669b28a8935761]]]This becomes clearer when looking at the following backup diagram.

[[[00000000000000001704---f5700286de219c0d3550e54eaefc2cf91788c0a2244f8b4336f8be16e134c6bf]]]
Figure 6-11 Backup diagram of Bellman equation in Q function


[[[00000000000000001705---1add011bab319b1250b252ec4ae6d4a8ec4456432554bdbb7d94d2edeb052092]]]As shown in Figure 6-11, the Bellman equation considers all possible next states and next actions. Looking at SARSA with that in mind, SARSA can be seen as a 'sampling version' of the Bellman equation. By 'sampled version' I mean using one sampled data instead of all transitions. A SARSA backup diagram is shown in Figure 6-12.

[[[00000000000000001706---efa64a78e8d43a36850ab7bf94ae4f14f6a35886f78e0f7d8af9851bdcef3f67]]]
Figure 6-12 SARSA backup diagram


[[[00000000000000001707---1fd3b2d893507e535071835ea3e6be339f2b3df20cdae1ffcf1ddc565080fe91]]]SARSA samples based on the following states as shown in Figure 6-12. Then the next action is sampled based on the policy. At this time, SARSA's TD target becomes . Slightly update the Q function towards this target.

[[[00000000000000001708---b00fde9de1bd62dff6cce2383ee150fe7257a38d589eb29c40b33402b06a088e]]]Now, let's get to the point. If the Bellman equation corresponds to SARSA, then we can also think of a Bellman optimum equation. That's right, that's Q-learning!

[[[00000000000000001709---dfb9adb802afc63147a13195a0df520252fe6bc02a552c7ceeef761200c71d2d]]]Bellman optimum equation and Q-learning

[[[00000000000000001710---5cf8ba868c5fc860d006155f9ec69a914f6f2ddac3345f1829ecba8bbcd8c3cf]]]In '4.5 Value Repetition Method', we learned the value repetition method. The value iteration method is a method that integrates the two processes of 'evaluation' and 'improvement' to obtain the optimal policy. The key point of value iteration is that the optimal policy is obtained by iterating just one update formula based on the Bellman optimality equation. Here, we will consider a method that uses the Bellman optimum equation for updating and a 'sampling version' of it.

[[[00000000000000001711---e2c9f5eb7ea0207d1112f252c7aefa8ed5690eb68b714e2f4f067c6117c591bb]]]We will first look at the Bellman optimum equation for the Q function. The Bellman optimum equation is given by

[[[00000000000000001712---c821ef749d6173a5cb4cfe836af6ba80d4ff8a93dd241046de7caaba73755063]]]Here we denote the Q function for the optimal policy by . The Bellman optimum equation differs from the Bellman equation in that it uses an operator. This Bellman optimality equation can be expressed as a backup diagram as follows:

[[[00000000000000001713---1de214998ef8a730911a7f3fcd2d456044a0f744755a898c75af38d6cd1befbc]]]
Figure 6-13 Back-up plot of Bellman optimum equation for Q function


[[[00000000000000001714---282d452071313f9ce401cb51255c3ddbc1dc2268d733343c04ab454d076e0b04]]]As shown in Figure 6-13, the action is the action with the largest Q function. Now let's rewrite Figure 6-13 as a 'sampling version'.

[[[00000000000000001715---258f068269e6af020553bb241df152feeb6d3d1ee6879776b22ee120414d81f1]]]
Figure 6-14 Backup diagram of the sampled version of the Bellman optimum equation


[[[00000000000000001716---a54461197f28ce732b85fca123879c92aac397c3f752eb2400f065c517a882d5]]]The technique based on Figure 6-14 is Q-learning. In Q-learning, the target of the estimate will be Update the Q function towards this target. The formula is represented as:

[[[00000000000000001717---a627a6a5b916098d063a93e4dcc3b95f6af968bad78709bd817155abed216825]]]By iteratively updating the Q-function according to Eq. (6.14), we approach the Q-function for the optimal policy.

[[[00000000000000001718---b94c011d7bda1b61a41099babbf56d08819476d0be574ae4e01ce28bd1d91798]]]The important point in Figure 6-14 is (again) that the action is chosen by the maximum value of the Q function. Actions are selected by the operator rather than being sampled by some policy. Therefore, it does not need to be corrected by importance sampling (although it is an off-policy method).

[[[00000000000000001719---b059febf4e62601738bbe34657e864539a8d313a659017b8dd1f1d7461bd7f10]]]So let's talk about Q-learning. Q-learning is an off-policy method. It has a target policy and a behavior policy, and the behavior policy performs 'search'. A commonly used behavioral policy is the -greedy version of the current estimate of the Q function. Once the behavioral policy is decided, actions are selected accordingly and sample data is collected. Then, whenever the agent acts, we update the Q function by equation (6.14). The above is Q-learning.

[[[00000000000000001720---035c42b974aeaea61656df14bf29f07ce8976a9a7960c487cd4a2fcde7a654c3]]]Implementation of Q-learning

[[[00000000000000001721---7124751d1e163f3c3c10ae7827d0e5ed2940eca69560cd663de6e8c0148173c4]]]Now let's implement Q-learning. Here is the code:

[[[00000000000000001722---e89a121e839c288ebefe637ff6d3c1875ae9123355e427e97163048c5dae92f7]]]# Behavioral policy

[[[00000000000000001723---30a329dc3d769ea0af50abb35222d303ca81a45f67684ac90d9e22b66cbfa5b0]]]# get from behavior policy

[[[00000000000000001724---3343285e925d121a8bcaec2c157638e6af25141670e217e176929a7ceb4977a8]]]The important part here is the arguments to update(self, state, action, reward, next_state, done). In Q-learning, we can update the Q-function with only these five pieces of information --state, action, reward, next_state, done--. In the update method, we retrieve the maximum value of the Q function in the following conditions. Then update the Q function according to equation (6.14) based on the Bellman optimum equation. Update the Q function to update the behavior policy self.b to -greedy. The target policy self.pi is updating to greedy.

[[[00000000000000001725---0ae33b045edad099700d28b8a1ec88baf03490c69b6a8cd8456fe22cbceacd11]]]Now let's get the QLearningAgent class working. Here is the code:

[[[00000000000000001726---7f059966d5ffb97341af58a4c26952fb42cd2ed783c0b439af828a9052e0edaf]]]Running the above code will plot the value of the Q function and the target policy of the agent. The result looks like this:

[[[00000000000000001727---2e2148d8fefc90bc214f615d32d79ed49ffd3f850eea1be6915afc3a5d90f1c2]]]
Figure 6-15 Q-function and policy obtained by Q-learning


[[[00000000000000001728---97b3bfd8f523393e3af7d46cfa9b917da18f43e5b5193b5215e3d0bac80c1b30]]]The result looks like Figure 6-15. The results will vary each time, but in most cases the optimal policy will be obtained. The result in Figure 6-15 is also the optimal policy. This completes the implementation of Q-learning.

[[[00000000000000001729---7246f1b797e8fd1925320ccc4fed887f6a04bd59378b9434558a92ba11af9f02]]]Distribution model and sample model

[[[00000000000000001730---d02397d9ce6cff032107b35a1244b74a2666d85abeccf4a567dab2b0849cf0f0]]]So far we have learned about the TD method. Specific algorithms are SARSA and Q-learning. This is the end of the reinforcement learning algorithm that we will learn in this chapter. This section supplements how to implement the agent. First, I will explain that there are 'distribution models' and 'sample models' regarding how to implement agents. And we will show that the implementation we have done so far in this chapter is a distribution model, and that the sample model can be implemented more simply.

[[[00000000000000001731---7246f1b797e8fd1925320ccc4fed887f6a04bd59378b9434558a92ba11af9f02]]]Distribution model and sample model

[[[00000000000000001732---ae5910211ccb4afde22e3ef1a3c56ca11ff21131b49c562bb8fa16b9efb15ce1]]]There are two ways to achieve probabilistic behavior: 'distribution model' and 'sample model'. In 5.1 Basics of Monte Carlo methods we explained that there are distribution models and sample models for the environment, and the same is true for agents. The method of determining the agent's behavior can also be implemented using either a 'distribution model' or a 'sample model'.

[[[00000000000000001733---3be52fa4bf494b163e2c4c6deaad49ff1bb1e072c3c0f347814c1e79fea1e32a]]]A distribution model is a model that explicitly holds a probability distribution. For example, implementing a randomly acting agent as a distribution model would look like this:

[[[00000000000000001734---0bfdf40c47076db537576053a21475ce8a983744b6bfc581007b3328de130c7a]]]# probability distribution

[[[00000000000000001735---c4d230798f69d17c96b00b7d2b7ec720c29c02c071c5103dff173af63f02a603]]]# sampling

[[[00000000000000001736---d94b6ace9773631b3bf22c603ca2dbb7a8d237a468ebe03196d360d6e2249042]]]As above, we store the probability distribution of actions in each state as self.pi. Then, when performing actual actions, sample using that probability distribution. This is how the agent as a distribution model is implemented. In this way, the characteristic of the distribution model is that it explicitly holds the probability distribution.

[[[00000000000000001737---28d5542d9e05c67cb2bce9f722b01cc11535660e8d15685ae61017763ed5a75d]]]An alternative to the distribution model is the sample model. A sample model is a model whose only condition is that it can be sampled. They are simpler to implement than distribution models because they don't need to have probability distributions. For example, for an agent with random behavior, a sample model could be implemented like this:

[[[00000000000000001738---ef7fbb9f204736daf7e2e9d0b24122b01df2cb594cfc9d82e2291721fb4fa9c3]]]Here we do not have a probability distribution, just randomly select one of the four actions. Even with this code, we can achieve an agent that performs random actions. You can write less code because you don't need to have probability distributions explicitly.

[[[00000000000000001739---4162b5ba18671be870437e647cb7867d69812aced16f7756ce278a8e9442fcb6]]]Sample model version of Q-learning

[[[00000000000000001740---5013e07180eb22545e3411a7f177d412e00f87005573acee3c0c578dcbb09f25]]]Next, let's look at Q-learning. We will start by reviewing the Q-learning implemented in the previous section. So we implemented the agent as a distribution model. Here is the code again.

[[[00000000000000001741---826fccae68b250461866201b3cb6c4254e6c1a30d12eddf0fe2adffe2045577a]]]# target policy

[[[00000000000000001742---e89a121e839c288ebefe637ff6d3c1875ae9123355e427e97163048c5dae92f7]]]# Behavioral policy

[[[00000000000000001743---4f72aa9158ea4bab49f2a57032ff4def1e914cef47ff3f5e5ad8781c94125316]]]# pi is greedy, b is ε-greedy

[[[00000000000000001744---68b3e099e833086fe5355ad85ce3f8d67dd48d7e1b5a718432514b024c505dd5]]]The two things I want you to notice in this code are self.pi and self.b. Those two policies are held as probability distributions. So this is a distribution model. Also notice that in the update method self.pi and self.b are updated.

[[[00000000000000001745---37c8aa233c8249b71b422c718f9b467b8fcfa1fcf96df73650060825f6b339d6]]]In the update method, we update the probability distribution in the policy state. At this time, the policy that updates the Q function (self.Q) to greedy becomes self.pi, and the policy that updates it to -greedy becomes self.b.

[[[00000000000000001746---f9f40bc860f2ff59be94fea09ba5a633348e9779f8199e5de8472cac02aeb0b8]]]Before implementing the sample model, let's make the above code simpler as a groundwork. The following two points have been changed.

[[[00000000000000001747---7276da99a138e10045c1ff75db9521861db27db2ecfa559a44780f111d7e5bd3]]]delete self.pi

[[[00000000000000001748---1fff02972c746364cfa6b5cc513dcb116a82410846f62b4c0514a71cea4044fc]]]update self.b in the get_action method

[[[00000000000000001749---fc622e5a77a98917f30fc652a51ab82af3be5780d4b731bfbafc105cf085ccff]]]Before explaining, let me show you the code.

[[[00000000000000001750---0d3a22a3b4c146cb0a64a4fda9e232b74214dabc79af63254c8485a73025c755]]]# self.pi = ... # don't use self.pi

[[[00000000000000001751---cb6162cc37081bbb2e8acca432eb7f1bc289f728bd7bdd056ea3aa547cfb5ce1]]]# ε-greedy at this timing

[[[00000000000000001752---61c4115a376527cbe0bfe20de56ccbf332aa9f69b48f5bcb58cce26839057831]]]First, I deleted the target policy, self.pi. self.pi is a greedy way to update the Q function (self.Q), which used to update every time the update method was called. However, self.pi is currently not needed by any other code. So self.pi can be deleted. If you need a target policy, you can create a greedy Q function when you need it.

[[[00000000000000001753---cbb508123a3f8d0fc49efef78707c0c9fd9e4dd395158c71077b739b9298a30c]]]Next is self.b, which is a behavior policy. In the previous code, we were updating in the update method. Here, change it so that self.b is updated when the get_action method is called. Self.b is a -greedy version of the Q function, so you can always create self.b if you have a Q function.

[[[00000000000000001754---73dab5e22428e7cc850281660bf52a1fcd92002f4f99c8f02edfa0e55107eeb6]]]Now change the above code to 'Sample model'. Here is the code:

[[[00000000000000001755---a7e5622642ee17756312e34d57afd06fa5d53a10378f7574a84f591709dbfe1a]]]The change from the last time is that self.b, which is a behavior policy, has been deleted. And in the get_action method, instead of using self.b, directly code the -greedy action selection using the Q function. Specifically, it chooses a random action with a probability of self.epsilon (=0.1), and otherwise chooses the action with the largest Q function value. With this, action selection by -greedy can be realized.

[[[00000000000000001756---4af6ad78787558a67dc744b91615fe9074128b7e239a2c8444c21d65e87d4006]]]As you can see, the code above does not store the policy as a probability distribution (or, more precisely, the policy itself). Here is the implementation as a sample model. It's simple to implement because you don't need to store probability distributions. From the next chapter, we will extend Q-learning using neural networks, but we will proceed based on the implementation of the sample model shown here.

[[[00000000000000001757---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000001758---87d05606cdf3b4f522767de47ef60b04fce2142d8c0367b3bdb10911552d91e0]]]In this chapter, we learned about the TD method. The TD method (similar to the Monte Carlo method) evaluates the value function from the results of the agent's actual actions. A feature of the TD method is that the value function is updated only from 'now' and 'next' information. On the other hand, the Monte Carlo method updates the value function only after the agent reaches the goal. As a result, the TD method can be expected to update the value function faster.

[[[00000000000000001759---7b3a9d0dec55ba38151e59e9747c2628b679d79fa88651a94015ce7f149b7302]]]There are two typical algorithms for policy control using the TD method. One is SARSA and the other is Q-learning. SARSA is (generally) an on-policy technique. The Q function is evaluated by the TD method and policy improvement is achieved by -greedyization. With this -greedy, you can 'explore' and 'utilize'. We also showed that SARSA can be extended to the off-policy type.

[[[00000000000000001760---91840a5ad2ee9ba1cb3a09fe4f96d64db27a506423ac963d42fecaf22160c660]]]Furthermore, in this chapter, Q-learning was introduced based on the Bellman optimum equation. Q-learning is an off-policy method that updates the Q-function without importance sampling. Q-learning allows efficient and stable updating of the Q-function. In the field of reinforcement learning, Q-learning is a particularly important algorithm. We finally have an important weapon in our hands: Q-learning.

[[[00000000000000001761---d3076c9ecabd5c8dfe5fe91d3fb0ad49efaf30da2df7e295d7c897ee2c9da150]]]in conclusion

[[[00000000000000001762---208ad3da2007f7636b403df0ed6ca9397f8556d43a0be7e73fdb9988585b3adf]]]May the Force be with you - from the movie 'Star Wars'

[[[00000000000000001763---bd76d97509dafa87d1fe265f65da77375e27f9e57a9fd26b1b290f7065467eea]]]When I finished writing the first series of Deep Learning from Scratch in 2016, I knew that my next book (if I had the chance) would be about Reinforcement Learning. . At that time, DQN, which played video games, and AlphaGo, which beat the world champion in Go, were attracting attention, and I was also fascinated by those technologies (deep reinforcement learning). That was the first time I really got serious about reinforcement learning, and I immediately saw a lot of potential. The reason is that I felt that the framework of reinforcement learning, which learns through trial and error in 'interaction' with the environment, is a more versatile technology.

[[[00000000000000001764---715b35fee1983f90c9d23c30b5169cb4d25d3d13f35546b55ff514579555e2fc]]]Shortly after I started studying reinforcement learning in earnest, I realized that it would take me a long time to reach the level of writing a book. And while I was thinking about this and that, I decided to proceed with the next book by further advancing the contents of the previous work and making it possible to handle time series data. Specifically, I decided to write the second book in the series (❷) with the goal of understanding RNNs, LSTMs, and Attention, which was beginning to attract attention at the time. As an aside, ❷ did not initially focus on natural language processing. One day, when I was halfway through writing ❷, I came up with the idea that the theme of natural language processing would make a good story for the book as a whole, and I could learn important techniques consistently. I still remember sending an excited email that night to Mr. Miyagawa, the editor, saying, 'Next time, I'd like to work on the natural language processing part of deep learning.'

[[[00000000000000001765---b377fed63a967e5d94ff48b018c9b1de338f96dd93971a5ebaff05881b0e87b8]]]I finished writing ❷ safely, and around 2018, I was preparing to go to 'reinforcement learning' next time. But life doesn't always go according to plan. By chance (while talking with Mr. Tokui, a developer of Chainer, and members of Preferred Networks), I came to think that it wouldn't be a bad idea to create a more full-fledged deep learning framework next. Therefore, I decided to write ❸ as 'Framework Edition'. As an aside, Chainer was ahead of its time. The core code was simple, beautiful and functional. Furthermore, from an educational point of view, it was also excellent as a subject for learning algorithms, data structures, programming test methods, etc. Unfortunately, the development of Chainer has ended, but I am a little proud that I was able to leave Chainer's thoughts in a book.

[[[00000000000000001766---54e5f7b988fe170201ebf129390b2650f8a6358ed4dc5635448d37f2d5eeb71b]]]It's been a long time since I thought of writing a book on reinforcement learning. Compared to back then, I think I've acquired a little more knowledge, and I'm now able to talk about reinforcement learning in my own words (although I still have a lot to learn, and I'm still learning). The field of reinforcement learning is a difficult theory, and there were many points that I could not understand at first. It took me a lot of detours (in hindsight) to understand them. But it's also a good experience. Above all, I was very happy to experience the fun of reinforcement learning for the past few years. I hope that I can share the fun with as many readers as possible.

[[[00000000000000001767---89c55a59b08520a95e2ae8b045799d275597d464ba50b5fbf0bc92128d21bad3]]]This book series is also the 4th work (4th). It is undeniably thanks to the support of many people that we have been able to continue with one technical book as a series. This book would not exist without the researchers and developers pioneering this field. And this book only exists if there are people involved in the creation of this book and readers who pick up this book.

[[[00000000000000001768---c1def4fe993a3a9c7b55b12e2819f75cd28ebc850f48a56a4405051e4c628c51]]]Thank you for reaching the 4th (Force). May the Force be with you.

[[[00000000000000001769---e5b16f5452a486d8d6f956d08e7e55936509a3b56d9772be82dfc8d4c6007746]]]January 1, 2022

[[[00000000000000001770---b035edd8672762cb270a81ac48bf45f2b02c7a631a1fe74d3a58fed6b8a8c4bd]]]Yasutake Saitoh

[[[00000000000000001771---e196c210c5aca71f09475a94ca1750789fda03bb1b9bf85bd1fe22554073460b]]]peer review

[[[00000000000000001772---9f64fd9b1733ea3504c0d3c7bc05f5a3792828fb15971971334a31ee9ff24140]]]Yoshihiko Mochizuki

[[[00000000000000001773---86099b5da4026c19b3425562595a51f690abe00770ec4dadc5a4ba311f79bc03]]]Toyyoshi Ryuichiro

[[[00000000000000001774---0aa802dc9b560e1ae27116e347511678c745b1b395348d9b31c5551ddff22556]]]Mototaka Tanaka

[[[00000000000000001775---49d3c8dbb3a8a1cdfe332c634b5b51a97f7f96482953e4801ede229a61d84d74]]]Rintaro Mizutani

[[[00000000000000001776---d1289864d4f0103302bd4b14a91152d4d8656ad1af361dda84f40f3fb5d7dcb5]]]Shusaku Sone

[[[00000000000000001777---41db0baeefeaff4979b42f60203d9c98f71fe0f0fd137b5591a170913f62b489]]]Toshihisa Kawamata

[[[00000000000000001778---4594dfb008c0fc76decfbeba24f58ef29b8549d28748c268f2cb888a90b15249]]]Yushin Kuroiwa

[[[00000000000000001779---321156c63e2fbc91aebc9f345de66be7469ec5b22a118ba143425d84b1e7c226]]]Nobuaki Kuwahara

[[[00000000000000001780---f08b8cdcb913d2b7e840baea9f47ad70dc7ec0619d9bc7510f10ea5232dedefc]]]Satoshi Okiyama

[[[00000000000000001781---9bcc27979aa7770b4a22d53948c7028bb53441f85faa16b886ea1e7f34f70824]]]Ryosuke Yamaguchi

[[[00000000000000001782---a36182d3252048fcc5460958c680b225913beea83fa618f0eea2ac7c5900c1d5]]]Yasuaki Uechi

[[[00000000000000001783---52b8ba4f9e1cd9c5cdd8b8eac315ba0db48a0fa8dd9389f615e91cba7009ee7b]]]Takuya Hayashida

[[[00000000000000001784---57b049a627ef37c26a7072e092358c4bd2cc3d00f8fc2dd548241fb5ee4758f4]]]Yasushi Fujinami

[[[00000000000000001785---464da9f8f7855a3659dd35906d495a9415402152059cbfca96c56dd4ed0ab8fd]]]Kazuhiro Watanabe

[[[00000000000000001786---756c6a58d4d5275bf0432cfd5e4bdbaa196247c3516523b88d2ae1d9746e9738]]]Kiyoyasu Fukuchi

[[[00000000000000001787---776db53eb40b1a2c9b43f2f297593bbb988468a6fdcd291650dee435b48f8087]]]Atsushi Hirata

[[[00000000000000001788---4c3a0852eb62bbf2ad21eb73191fdefa6c515856f50f6b170544b163bd81bde8]]]Tomoaki Ando

[[[00000000000000001789---70b62326c0b2fd706d111b074c481c2fa62d8a9559d1b8d9ca9439b2da7cb51c]]]Mizuhashi Taiyo

[[[00000000000000001790---0ffea9f10415a6dcdd3a3dca154e6821126b11880a4a1f970216e7a3b7b0077a]]]Naoki Haraguchi

[[[00000000000000001791---fbd860d8e5d3d46ee9eb365d872a3ddaab40987199ba1625fc783f2eddafe52c]]]Han Qumi

[[[00000000000000001792---c8f4b5aa5046c9b6957f8b79755f83cb444f02a17261eae4f83d4525982514ec]]]Yasuo Inamura

[[[00000000000000001793---b131bc764e4a2d2ece6ab8ffc77acd9a1c164541aea69e11e6fb653faf57966e]]]Noriaki Ohshita

[[[00000000000000001794---c20f90093c2a0001e7577ecf401d3f9f77268883170b058a8cb7b4f703a00685]]]Seiya Yamazaki

[[[00000000000000001795---c4b033637a54bad07dfa8ab67fcf7fcc62ce54db1a7f7d35a9e8a1b434699c73]]]Nobuyuki Oishi

[[[00000000000000001796---fa8743f7d17f8d2cfb8cf4e31b703c211e59849db267f645936506dc00a9d037]]]Kazuhiro Harada

[[[00000000000000001797---a848883640f3bf135dfb58497cd212b51fe0ace0eb71b573ed9e34b5aad108d7]]]Keiji Takeda

[[[00000000000000001798---085b7fb89a5cf29314a716d1b07dce58355535c5d14828c2e5acf10bff541e10]]]General Tabuchi

[[[00000000000000001799---ecf3c4aec20dc4fef336d41c5f143cca8557f0ac6e9dcfe96f6694b696d17f28]]]Takumi Tanabe

[[[00000000000000001800---cfb1c661df91b9238c7583075046877e3963c8b64bc7b3ebb8d59cc10e110462]]]Masato Atsumi

[[[00000000000000001801---2b956602d29c1aaf03d1c66409f8f13d78e552f7423506bd95a8522d40fef5bf]]]Yuji Okunishi

[[[00000000000000001802---60a6a79fc3bcca5eb3a12c22e80649804b562c84ae16a3f973a50771c2e385d7]]]Toshikazu Karube

[[[00000000000000001803---73f950b8f87302ea5099f7ee65b5a610a842accb184b11ce472f561481d19398]]]Shinya Wada

[[[00000000000000001804---420e162d3cd2b5ae732b363dabc5cc49effa9706b33428194426bd821799b961]]]Daiki Shintani

[[[00000000000000001805---9db78a9a78a63df730cfff2a3d60747dfdb5610ab2a3ec6e64229b0c7f1fe06e]]]Kaiki Niwa

[[[00000000000000001806---9176f623fefbbe76c39342671e08aae9e3b702f2b401bb70d0bbe294b991ed0b]]]Yasuhiro Okawara

[[[00000000000000001807---152b76097c26b9f343ddc60688f8026b453bbd7c8c5232668005a231e3690355]]]Ryosuke Korekata

[[[00000000000000001808---0df1ab59cd5cd5cd2297dab10988f32b5806f411416a0a8fcae01b1272920e1e]]]Mika Terasaki

[[[00000000000000001809---1e03a27266dbd3d8e8d0e01eeb140b03de59741f932ccf8b105d66f018fa127f]]]Ryoma Murai

[[[00000000000000001810---a52b0b8dbb6b3b3b2424a24619e3c5dc35ac269c714b48bdbf6da7cdd788e04e]]]Shizhi East

[[[00000000000000001811---21a3ac9b41e2715153569a984be9ff1c97a424285a5d8e3a123f97d742a358a7]]]Hiromichi Suzuki

[[[00000000000000001812---0ab41d39e691456d7a6752c2e731d4ba67d44bc5cab3914a41b3b990e32eba8f]]]Ryusuke Haruta

[[[00000000000000001813---b87777eaab4f10fc1a8f0de51d0a4357c76859f95ee25dba87deb7c513bc3503]]]Goichi Kimura

[[[00000000000000001814---e8530d0468ed24a879353699e2ff71f3bfa9dd896a9d82696fb05a4b7e2acf83]]]Keisuke Kobayashi

[[[00000000000000001815---313a77b092174a5a4b72663acd23014f44215d0eab878b70077f59e04356f76c]]]Kenta Uezuki

[[[00000000000000001816---83bc5757005814dd99b8130026347f16f1df5896a72eda4b6934b686dbef9a18]]]Shoji Iwanaga

[[[00000000000000001817---54a2efc3447d71c469a3a79573dbdec756d94bf86a4b0fc3caf1d95f8f4e4312]]]Mitsunori Shimoshige

[[[00000000000000001818---9d90be90977784f3b740eb2a8362167abc11f26c335a13c79ddadcaf5911b1fd]]]Takato Okudo

[[[00000000000000001819---3ec1036377c3c9dde5223f47acf8f001e38535c02d5363e60c09836980f88961]]]Hiroyuki Fujii

[[[00000000000000001820---e71eadc9da46e909f82ca22fb7bf4ffb78f15e18c7142ae53efe994f6652c6c9]]]Mikiya Morisaki

[[[00000000000000001821---c776c7dd32a2e9534a7ffca2f5d93e86f898df0ee689b2a8e63190450e08f9f1]]]Haruki Okada

[[[00000000000000001822---eaae2b367d325bb2d8288ed6aa1c348aeb80795649fb84dbf31d45b3e94b94e4]]]Ikuhiro Yoshihara

[[[00000000000000001823---01c1e89b28a9f7f47afda023c56bf763671df8fb3cb09d888d225fb22dfeac79]]]Shintaro Haraguchi

[[[00000000000000001824---bacdca40de64c8448f47ceb7e4f65a9d76e21f703d628e92a449184ee5746546]]]Daiki Sakanoue

[[[00000000000000001825---761f32e3f6e9d026c486445725e168a3fb53e847de8a32b2b60b398e2ca8ffba]]]Takashi Kawashima

[[[00000000000000001826---34ff85ef597f872b884efbe3c33999dc4c44fc843cdfaf56e4ae53b6d28765e0]]]Takashi Kanazawa

[[[00000000000000001827---60e2615b5384abd471665e5f803b31e897736e9c620295f45970f582c0fa909b]]]Yuki Watanabe

[[[00000000000000001828---7ec29a5e9aacb409f93d35c4e5dbc9320bcd104733d74c4e3f878c14c79de7be]]]Junichiro Kanazawa

[[[00000000000000001829---cab52ff6f887e80deb61e461ad605dc35759dd6eba87bda1e50701bd493baaf1]]]Masayoshi Todorokihara

[[[00000000000000001830---f638c22b8e2795dfb2677a4fcd330d67fe681d6a39ceb10ea64323fc702f0a6b]]]Kento Fujii

[[[00000000000000001831---25db67e6b6b6adc1d47661baf3ca941c796f887a197f82ee22f5b56fa74381e1]]]Tomohiro Ohno

[[[00000000000000001832---9d2120c4cdd2dba089d82ca97b82181f7797ccb91828b4d0e8875f2222f8cf74]]]Ken Kurihara

[[[00000000000000001833---d2ce03b51ab15bc9febcca0a4ac779fc65ff8d1d7a2c96bc1ffbd5cd56876cdd]]]Takahisa Senaga

[[[00000000000000001834---e2cd6df153e04f1aafbedef53a378e41b28c5458fd6ee4fa8bc98b317c61563d]]]Rantaro Sawai

[[[00000000000000001835---5ad6376c3e2b9a733264d1edd562e067bc098328f4f57c32616bc3605e518986]]]Shiro Oinuma

[[[00000000000000001836---d01d3022bf7084391e9e2faf0f396f342cfaacedd1568d3d049e0f2abf7835dc]]]Junya Suzuki

[[[00000000000000001837---a55a98154b55ea6f2133003cdec8bb0a02daa26a2a147ddc19497d2c12557268]]]Yoshio Ichinomiya

[[[00000000000000001838---44f831591d783de085a06fc49e9c30d69ebd018bf7feb0603d2face91b944ee8]]]Ryokichi Ochiai

[[[00000000000000001839---c98421d6be5c743ab44ed62a777eec408ca646662bc6abbeab7c25593b76f970]]]Yu Saito

[[[00000000000000001840---74a23fc77596c767e173ccd5aee93b84f9b2e62e93ad5f992d96d015a6922857]]]Naoto Nishida

[[[00000000000000001841---8dce988e5b90c8d207a20cdb246f50fbb565979549c9576f0a4dead8f29ba854]]]Shuhei Ariyoshi

[[[00000000000000001842---89ab4b634460c2b01864ddbce411d714b6b28f075e76a5ed5853adc46af1397b]]]Shuichi Isoguchi

[[[00000000000000001843---282257200372328c09055d82677ab3457d89d54f82526a3de910470780300482]]]Natsuma Matsunaga

[[[00000000000000001844---227793555fda4b9f46d97ee547f2c73294bcdee900007ea3433233a19339ad28]]]Otohiko Mori

[[[00000000000000001845---d007b4b1f5e6ace0b94d0a6105d9cf8f37e6ad5cf4ec81445d2f11f424eb5645]]]Yuta Tamura

[[[00000000000000001846---e9e40efe87a0283b4010f64da3f89c93c749c868e9fd924a30c028e1fe81d0a7]]]Kenta Goto

[[[00000000000000001847---5a4eeb64fd8a67eb333c4a7688392297c545ea9a438bec64956ea32dc820b9bd]]]Akio Ueki

[[[00000000000000001848---a50d3bb3d0bdd15d1a9a268385b394830b2195cff6946b96133c0871d26481da]]]Gota Morishita

[[[00000000000000001849---e7881cb995dfaf830a719f4c67508da3163ec04fdc8d58aafd0c1c94d53ed8e2]]]Takefumi Yamazaki

[[[00000000000000001850---fc315dd880f315d160e9c3bc0d9e3af26bbafd119ea8f9457dd7cc02e153dbec]]]Kazuki Ikoma

[[[00000000000000001851---740ca60f0064bda113e5647e37d3c43b391574a29b6cb1372811316e7b8a0023]]]Ryosuke Ueda

[[[00000000000000001852---ebe55a8d9aa215430fb314a2b27be7997e6a58fe194fdcd0eab5c8a94f647f21]]]Kojiro Hirokane

[[[00000000000000001853---b865c12e16a75670f4ae079df1cfe524ee17752064da0a5f66ddc4d7c4818ea4]]]Shohei Meitoma

[[[00000000000000001854---6f0281d2c42607b9e3b2595a530c545b2718ffc82d76ec78e9533e342533cedd]]]Kentaro Yoshioka

[[[00000000000000001855---c405a680640a71d2a49998a7c60a236c9703cb81ad05f3c008b80af393fc9003]]]Tetsuro Watanabe

[[[00000000000000001856---ea999be15d2a79cb87d465d1c1bb2a60837decbab0be087304dc4dd3b86e7d41]]]Soshin Togi

[[[00000000000000001857---e96cd6f89160b4ab5614d05c59a039fbd8792e11dabd2f2c43acdcf19551f75c]]]Ichiro Sugiura

[[[00000000000000001858---d2d1278c320e363a8378de03cc913b1b4e2c6e434a1949c53279cf29bb529822]]]Kouki Hayashi

[[[00000000000000001859---311a24a14a2e7ae92e469e08ef52eaaf0e2c59e5a34073b1f06925cfd3269297]]]Kazuya Harada

[[[00000000000000001860---82b805574edca09e93df3944bdb1f178505a43b1d6cd8a3de54fe1a40dee4808]]]Koshi Yoshida

[[[00000000000000001861---9424c98ffb15844fbcd68b48dbd4033dc89f501c5a321fe64ada88c98d6e9a25]]]Kohei Nishi

[[[00000000000000001862---3c9611ab9e15bd8f9cecc738da456243138e189be3755b60d3fadeefe24c8e10]]]Takuya Fukushima

[[[00000000000000001863---9432e1d687b324f50224802f712950274ca2578a190eda2a2a6bde6af9ef4c8b]]]Shunsuke Tsuzuki

[[[00000000000000001864---9d41094f0c7874b51ed682f83c6501ed74a6828f2b7eef4fd49a66b5605fe36b]]]Shosei Takeda

[[[00000000000000001865---08c3b44f3ec9d068d90c0ba7f5e1ede19ce0a4c6bcaefdccc3b50d4254c47a01]]]Yuta Sato

[[[00000000000000001866---0b21ed5ad790cb710051e48b57f96085dd80d33575def35977ed9698d0c3b5a0]]]Masato Akiho

[[[00000000000000001867---69c382dbe971810339680050b67eace576a318c01f659ae2af39349ea3e83091]]]Yuu Matsumura

[[[00000000000000001868---a14debfcbefec368a74268b4229a5883404d09ae2c20ca6d21b3d9709ea2cafc]]]Masaya Watanabe

[[[00000000000000001869---50b3852baba5b574f6bbc244025579c3d0e635fb09e8738f0bf46545d4663e30]]]Ranmaru Kitamura

[[[00000000000000001870---358cb4b3b1d5f553f25d7702dd453af9b3de4528a46288e92d25bd6370cfe836]]]Tomoaki Kimura

[[[00000000000000001871---ef45d5805d2a3f6b2fd30b68773ce54089d00ace02152b57607581aa04edc680]]]Nobuko Yamada

[[[00000000000000001872---66f23adf1fceba9ef2c2cc473f7497ffc2c472266aa03aafbb31b3ea72cf56aa]]]Shinichi Yano

[[[00000000000000001873---cd9588a12cd1d0c2f9c8d896229bb344e5e38b188c5a25c97dfc99b74c3d26d8]]]Takahiro Tominaga

[[[00000000000000001874---7b00bab83e593de0dec81b656067b9a393d2ab821f5fc6ea0110ecdd08af20f1]]]Kazunori Koga

[[[00000000000000001875---92047cc750fc46a3cfe7c9998504930e6046615e5c3fa92a687a895e7cadc024]]]Yudai Takahashi

[[[00000000000000001876---c0ad5ac95b4fddb4fc862074d9de6f5253a8ddf1b29751716682d1aa72070b35]]]Yoshinori Saeki

[[[00000000000000001877---e906d43e5e861c753aff8de3967808297a39f84304678d25b42a930c99097817]]]Kazuki Igeta

[[[00000000000000001878---7762971c8eeaeea40833f8f1f20163ecfd045d06ec98b889a682cd495c5c557c]]]Yasuyuki Miura

[[[00000000000000001879---706c31896e1c59695e0a81bd73ccbbf01895de7e76dff4e3e74c84ed121ae726]]]Yosuke Saito

[[[00000000000000001880---fa8fa4e93cb493e4c4b597b999b51fec5225edac13ce30b34bf67cf282416784]]]Kohei Kamata

[[[00000000000000001881---73a542c456ed6b9c45fb9a74f44b70d4cabe5a77b21b0ba18d5936cce15bd966]]]Yuji Goto

[[[00000000000000001882---d52afc10d921c17e3516e8b488544a73e6df50e5f3a1697b248f3b72aa351c99]]]Takuma Uozumi

[[[00000000000000001883---deaf9c02aa6ad96ea170727a260421648a11fc003e77644e545ca4161d0b17b8]]]Soichiro Hiroi

[[[00000000000000001884---0a2db6c1eb78640b5d03ab070994733b76fa441ddf40506ce53bb02ec073afd5]]]Keisuke Kuniyoshi

[[[00000000000000001885---e3aa0be55d11bd5e6f56937b940323cd4fc60d20582ddf18fbee992211e5982a]]]Tadashi Yokokawa

[[[00000000000000001886---057bc9ea814433cd27ebfba9aa40a77e37b7c5c0c759264154b56dd37ea7e971]]]Takahisa Yamashita

[[[00000000000000001887---47bd1c467d79921068a522082010be9efbc8437861d76bf7e376e27993b003c7]]]Naoki Fujiwara

[[[00000000000000001888---d77069c2f6244027d39cbc6dc4eb71fadf1dc40d5e8713a12c8f35b78ed17860]]]Ryo Kohno

[[[00000000000000001889---561722852f8964bde1e542e1917defa63f97fca87dcc85c4f739f88fc4c3afd7]]]Yoshihiko Mochizuki

[[[00000000000000001890---e6dc9df4947144b2d7f3fb1c98d18bb7169afac181a559620868a9faa756f94e]]]Akifumi Goto

[[[00000000000000001891---c7f2f7d5ffd99d1b381ff4183fcf90e01feef4d205d810c1c67bda6ece25f4b4]]]Kazuki Arashi

[[[00000000000000001892---c2d1ec7eab3561a9a14ea1c2fb6d9afc60a70b1e438755374a4da3345ad08527]]]Miki Kuronuma

[[[00000000000000001893---36c7758071a2da9169aa75d38bbea068635bdfb1e9a0e83464123cce2fd5bead]]]Rikuya Fujimoto

[[[00000000000000001894---4a0bb8442724ac76656abbe0f860c950cbb61130b510d17a7ea824dff7802970]]]Daisuke Hirahara

[[[00000000000000001895---0e3e7d0a702f3acda711405fceb32e840951786be8a814a3556cdee063140857]]]Masayuki Tomoyasu

[[[00000000000000001896---f1767e599ea9cc6d365b4eb8dd3564be0aff060e4c0e11a01ed349273522548b]]]Ryota Shima

[[[00000000000000001897---ce0b6f1c59aa2ddafbe6de26bcd606edff10b2b70bdc87e1ec3a468ab327cc04]]]Yusuke Shimizu

[[[00000000000000001898---3e60120567e248c09eb892326eb91f9c495340bc970d1c1c1ab4d1b6da321b5e]]]Hiroki Nakashima

[[[00000000000000001899---efbc683f1aa9b1fa55c4307fbe8b13664971eaf4cd7cf95adcb3e14b009dfbf0]]]Masataka Nishihira

[[[00000000000000001900---4cf29d199cb39bd4d4f397fedae09a7cccda436fd410e026c1e73ab656b996d9]]]Karaage

[[[00000000000000001901---d31f4926dc242ea7597267edc5aaab91319f4c53fa7f18d82c7e85c71ccc92af]]]Takaaki Inada

[[[00000000000000001902---57ed0744423bb55fb08cd765d826e489e8a73de37b8fecd5c7e4f966929115c7]]]Daisuke Okutani

[[[00000000000000001903---7e35fbb488cb8246664da8b6eefaf245d6eb3baa53c27e16ec1cb3f98df69841]]]Norifumi Watanabe

[[[00000000000000001904---e5264e9ec0f8fc83ed42c60b35710cb9f4c6fc4be4228b1d173f816785509bf3]]]Yuta Nakagawa

[[[00000000000000001905---fe93af32ebe177e2e880392a648e1e157246f165482010ad00e1938eb54c2162]]]Yasunori Nakamukai

[[[00000000000000001906---44922838010e2dec3c58780aa62d7f5f7cb8186cf398fb22c64bd8b37c5f9006]]]Katsuji Suzuki

[[[00000000000000001907---6f8d76072c5f02783081a3b7628b4b6e5e13aa803e1c3da9f99d239a7aab705e]]]Yuusuke Sakabe

[[[00000000000000001908---a2014357f2bc06b75541dae5005d063dfcb22516b9848fbce819301f7f634953]]]Yuji Koike

[[[00000000000000001909---60930836781fa0cbb61767ac031090882380c2f6fc5e18078df74f15dd93d4ef]]]Tomohiro Miyakono

[[[00000000000000001910---c6a70f390489bcbc393f131d84910c8f1562b57ff5088270b831d1d5727634d4]]]Azuki Maei

[[[00000000000000001911---0ffe8717605e039cf0701ff6b62e620bc736c3f69a98690cd57678178d9c8a9d]]]Takuya Maekawa

[[[00000000000000001912---424393621dd8cf5dd53e45a9272ee9f1c0b6ea9e8f39ab135f7efc1eae7c6dc6]]]Kota Tatsumi

[[[00000000000000001913---e75097fcedadbd5c8d0db6f3151e4c6a1ed4a9991187b1c656eed601a085a68a]]]Takuma Ikeda

[[[00000000000000001914---5f3817f96ca759cc8ee452ada06c874800a1ee0b4ef9b9bd4529670d4eb53c0f]]]Mitsuhiro Haneda

[[[00000000000000001915---8fe9018fd879c9c8768198b00c5fcc84b53d2039b3ea993eb1dce754ea7c0b7b]]]Junichiro Morita

[[[00000000000000001916---3d3e5f7f9946c613a164a8655d5981b49c1be721704efe69e6d1a46fb5715316]]]Shugo Matsuzaka

[[[00000000000000001917---84f916fbb18ce8670168608f814aebc8c0d42410083db6c1eb722b490621cd59]]]Makoto Morinaga

[[[00000000000000001918---efe4ca8a0fd049ce9b7d26ce8181dfbd6ae58617ac629fd74eb20ebb23b90583]]]Naoya Kumagai

[[[00000000000000001919---3a2347cfa640fde6e42d209dc1348e0ca7d8bed29ca55a5850acfd1dc554d988]]]Shouta Suzuki

[[[00000000000000001920---a18f1b39a4b29335446efc72cc49d7188cc82414c22a6e035b9ac79cb396acf1]]]Tatsuya Matsushima

[[[00000000000000001921---2f64303b23b80b86667263934b4e5dc767bfd7f5f91e97503b33256b65f3260d]]]Toshiki Matsuki

[[[00000000000000001922---400917c0626827f49036df6091c37546ab18c20228719b924b555cf83b432257]]]Tetsuya Mori

[[[00000000000000001923---30aa45934ab37d1b026ad73ba457347ec939b7e2aa99f625e94c212571ab8bb6]]]Ichiro Shibasaki

[[[00000000000000001924---6f4a1edc2aaa07814383a03f170cd455d3a3c07a45598714e3cc2431dde00579]]]Ren Saito

[[[00000000000000001925---5eae521c244137b6dc49f7fd14ab05dd5d246761b96562edaef4109b413c51e3]]]Rintaro Itokawa

[[[00000000000000001926---42e9329fc300072aca4952f79cfc99e98183d0c601f75853b0caf533f1a0268f]]]Kunio Goto

[[[00000000000000001927---44e8d331b2a01018d67eb1fc88326881faf84e779cfb994b90e3a3d54ebe6c7a]]]Toru Tanaka

[[[00000000000000001928---01cb0e6a0ab86185d4eb4a571d015bd49ad2c65bca1e5d19ab460b8e691fb3d6]]]Kouta Itoda

[[[00000000000000001929---78541d14c24ee46e98be021104290c43304ad322ae2fe06ea888d520dfcc5851]]]Tatsuo Ohkubo

[[[00000000000000001930---cdb71655dab5f671ef99733540709626dbb387fa57f81f93039984964bd9968c]]]Kazuki Ota

[[[00000000000000001931---b5fd078b06f9259a406c4e2f8c3f37f4b5f55e46d604e1366d12fc0fc249b378]]]Wangetsu Hamano

[[[00000000000000001932---d5227bc6e8484799c85d5b7507a07721813cb2e5e33d8e4c17c118958f8f2178]]]Yusuke Hirukawa

[[[00000000000000001933---09772e03bbee100417c968645eac54f9670840f49539694d1ea1391132621475]]]Yu Kobayashi

[[[00000000000000001934---08839cf6b7642bdb01ab94cdda453491c84a1f9f1fdbb5f5c041f941a99799fc]]]Amemori Chishu

[[[00000000000000001935---9134328741ea1f1ddc53cc62f99fdd281cdb1eae261b41923d05c90a43ba90d2]]]Hikaru Takatori

[[[00000000000000001936---9debfa096c017dc9177b167d653a9238aef44d7748a7064eb09ccbefc4ff328c]]]Kentaro Okuno

[[[00000000000000001937---cafa5a476398b7af80deff29bbaf0490372462f9beaa11b05a53cd121a3dc9c8]]]Hidetomo Takaya

[[[00000000000000001938---78ae9f06c501dfe042aa7a717cc728f0f1dbbc1c1ad1710312eb41aef9c3dca2]]]Kohei Mori

[[[00000000000000001939---622c3a88ae657c656d63184fe4a1b3dc8a783e16f305bdec2c040aad6d550948]]]Takayoshi Endo

[[[00000000000000001940---2bcd3df385d18c7b9d6571df7c66acbc1dd6a77f2a47c2225f954e4c2fa8850d]]]Naoki Sunada

[[[00000000000000001941---43b5f3847960f34f3a68a9b91d99620c55eb86c8f6dffc469f6a3ef478f370fc]]]Kotaro Iyanaga

[[[00000000000000001942---9fc7094c0052d65e504ee04f8f124d8908a96e7f89b137f0466d34ccaf8c5df4]]]Shunsuke Miwa

[[[00000000000000001943---73b9088f86cdca4947dafcb5ce60bc8243ddc22b401dd5ebf9a090b49cc0cc6b]]]Kenta Yamauchi

[[[00000000000000001944---2cab246385c7f54fb50014e849d5d2d21194b17833f5d69702f575b04e8befc6]]]Toshiki Shimizu

[[[00000000000000001945---be1722c48616a2e71ffa2eddb0c43a3e64e0484254a54ff3eb21b6dcdadaef08]]]Yuri Akita

[[[00000000000000001946---da2dadcd3ff9c5bc455a1fb290f57841ab906e42d94613ffa9e978a8770e3c9c]]]production

[[[00000000000000001947---316b62b31be496c93aa368122b16d3102451dd5ff29861bf5c5389a630d0e9fb]]]Takeshi Mutoh

[[[00000000000000001948---fad57992d6fd1dd6c738c1cb96193c1656aebb97f28bd57db816e4ff5273569f]]]Moe Masuko

[[[00000000000000001949---7ce60ccaf976e36047b954a1b4b93e5a2e44ee0049c6ef3fb769404ff5dd226d]]]Kumiko Katsuno

[[[00000000000000001950---11f9049ddab593adbbeccbad6d070f959cb4d60f41351acce2a9ead4281028b8]]]edit

[[[00000000000000001951---2ed590e436542778edbed46bcadd8fe8162eaa0de4a6abe060a194f3fc863e0e]]]Naoki Miyakawa

[[[00000000000000001952---c882f036e3715fc8ab51e4cfa1cad2f96b626cc3ecc42252e2f4c4b1ebeb920d]]]Mio Iwasa

[[[00000000000000001953---7e30891d89bcfa73947df871d7de8cfcd15bf2b2cfd998167599e638f6e1288a]]]Sara Koyanagi

[[[00000000000000001954---6763aa62211b44a20c71505eed18421683192c2f2f37668012e8aaa008367120]]]table of contents

[[[00000000000000001955---8436e0b0ef902ebd6da9f6f0ec21e99f841c4c2565715de6ee9ef693060fba69]]]　large door

[[[00000000000000001956---c4cf23daec95a26831302f10c2c2ac50a27c1900d0b8edb5d828fb23984bdae2]]]　credit

[[[00000000000000001957---a714161224255075961929dbba0d0997d0781ec435a517f55ecf5a7aee11f0e9]]]　Foreword

[[[00000000000000001958---c7d60edeef43612c700f835a8fa27e800488f787725930ccc870c7f301b088c0]]]　Chapter 1 The Bandit Problem

[[[00000000000000001959---f7f5eb6073dec9ddeaaa9ad52e4caf4152f53816ae50a9a6a49cf986a31c00fa]]]　　1.1 Machine learning classification and reinforcement learning

[[[00000000000000001960---7862acff7932409153c89019c4e726d22b69a76d7e1bfa7c382c4b58770a5535]]]　　　1.1.1 Supervised learning

[[[00000000000000001961---1c595d108ff42cd9af4f20d21490ce48d425f44d44fb42cb9bc5a3ba6de4ce72]]]　　　1.1.2 Unsupervised learning

[[[00000000000000001962---e6fc834c6d46243e603deba371e6ada67a9b4a2a64bb917a99f9e596e987a836]]]　　　1.1.3 Reinforcement learning

[[[00000000000000001963---22eefe9e10fd99808d981db6ee6d72507461fe9700505eca191fc0d701251806]]]　　1.2 The bandit problem

[[[00000000000000001964---151aacad69d731ae3d5fc427c444650882fee5bd85f4bf1443a259dd11c45f7a]]]　　　1.2.1 What is the bandit problem?

[[[00000000000000001965---79d6ac7aac2a29d458d05d8aa3e53d65984e486578889891f78845fe91223046]]]　　　1.2.2 What is a good slot machine

[[[00000000000000001966---fb289a3f0b041ac389e49067565fe2cc06f26edb419e31ffc316746e115cb25d]]]　　　1.2.3 Expression using formulas

[[[00000000000000001967---78b7ddb63457935f629e753ba383ac64a6c6dfc389759c4aa803a9c4f88b137e]]]　　1.3 Bandit Algorithm

[[[00000000000000001968---1b902630a3897cadef15a07573399d4cb5625ce3f74d37fda7e046c40badf4a4]]]　　　1.3.1 How to estimate value

[[[00000000000000001969---e7251dd7f2e263c7028b585f26051062c59d32b53f7ee4b5b25761e11406275a]]]　　　1.3.2 Implementation to find average value

[[[00000000000000001970---541989a4cc46de241f6098dd23db45c7fa4d1174d0ff215dac80a8e3ecb8dc37]]]　　　1.3.3 Player Strategy

[[[00000000000000001971---9b4efa6515f7fd791615936c3ec0cb4d8c56f88ab9790e361fafa64e94c67602]]]　　1.4 Implementation of Bandit Algorithm

[[[00000000000000001972---99af2a13e877a34fa1accfce52865e35e7c18103b67fe10bd85586cc61e79b92]]]　　　1.4.1 Implementation of slot machines

[[[00000000000000001973---e0d140b87c74331ac8a81a3f225ff7b88388948b7bfaa8e3d20e804646a0c4ae]]]　　　1.4.2 Agent Implementation

[[[00000000000000001974---86c224a1f6e57da4d9eae18325648d8616b6549660a5a0b2bb78a8a5d87bfcf3]]]　　　1.4.3 Try it out

[[[00000000000000001975---5267d43b7de935886ca7fe19f8263aded75a0a28e09ccebf89e88c41a927da65]]]　　　1.4.4 Average Properties of Algorithms

[[[00000000000000001976---7545522517971d269f386b883cb473e60d1183c323af12223fdc3092fe690cc9]]]　　1.5 Transient problems

[[[00000000000000001977---a2d2aad3a91eee217b8fb5107cc660fd5e3f3755df6e7e4f8278c6d158a829c3]]]　　　1.5.1 To solve transient problems

[[[00000000000000001978---b25d07eed60ceb50761a0a82f2769025dec014d4c672c4d0ddcbe926669f41e8]]]　　　1.5.2 Solving transient problems

[[[00000000000000001979---ff1910a9f792a0584b6de42a82186714e8f92cae4499c12ee7e07dc5d51419e5]]]　　1.6 Summary

[[[00000000000000001980---aabcf143802605ea538f6a9576b744f5342f01dc1f7cf1859e5dbe3031e8f16e]]]　Chapter 2 Markov Decision Process

[[[00000000000000001981---1b1ffcae5976f29add1ba1d5d5dfe27d7fddb6bc9e8aa5e785eb1b1d78ee0980]]]　　2.1 What is MDP

[[[00000000000000001982---364d7c76a34721816273c363bf877833c050a7bc00f6bd6438021e98203d736d]]]　　　2.1.1 Specific examples of MDP

[[[00000000000000001983---23d93cb0e69347d7a13cafd4f050512c22e990a48203a9756361fb9fd47d0ee0]]]　　　2.1.2 Agent-environment interaction

[[[00000000000000001984---b05dfc43b62d135b327408b6d5f258642bc0480dd76450209b971989beb5f632]]]　　2.2 Environment and agent formulation

[[[00000000000000001985---ae8b5fcf534e4acb16d5c8cdedf39e2e6cf7816fe47fefcff73e07ad19ea98b1]]]　　　2.2.1 State transition

[[[00000000000000001986---a2d7816d827456adaefaad5a0ee68995ee63c9a30aea93aba89d2d6f3ec3e187]]]　　　2.2.2 Reward function

[[[00000000000000001987---5d38be968021608ff7174512cce93179cbe34b481e3b12ce729350d05429c3c1]]]　　　2.2.3 Agent strategy

[[[00000000000000001988---e7fa3d37162982eb0e6700e31e45b6d56e5f67811adfb7ff29cb7e2915ff9cad]]]　　2.3 Goals of MDP

[[[00000000000000001989---bbc49facb38c993b2749f28260a455004ab0fb761bcdc59de44fab1f9b00584f]]]　　　2.3.1 Episodic and continuous tasks

[[[00000000000000001990---e429f500046cd64a21caa4392a5365c1b8d47d8f51f9e5341be9a3dd046cfad7]]]　　　2.3.2 Revenue

[[[00000000000000001991---fcf7a3674cc51070ccda2da37d640370d57bea7b9dd7dbf924e0da7813125225]]]　　　2.3.3 State value function

[[[00000000000000001992---a3b1cf69d8a55636111ad81285f46cf046d17ee6c07a61f553b0a060cf6f47ab]]]　　　2.3.4 Optimal Policy and Optimal Value Function

[[[00000000000000001993---0420d93573a37f25b760f4916d018b22fbb1d547509f430bb5a760364cf69c6a]]]　　2.4 Examples of MDPs

[[[00000000000000001994---25faddab1abe42e7798e0545799814d869f41ab71abcd47958aa3e07903128d7]]]　　　2.4.1 Backup diagram

[[[00000000000000001995---d833f7a69358e168c6d8a5028b5678e0770b221e61c4dd6974bf043d8bde486c]]]　　　2.4.2 Finding the optimal policy

[[[00000000000000001996---ae0ab6766354c7b59938512b9ae8835e923b25f90b8d194270eadf7cebadd72a]]]　　2.5 Summary

[[[00000000000000001997---f5a3643b0a99e44418f0cdae8aea47ddc3e345347b4b047698f7c225bd15a421]]]　Chapter 3 Bellman's Equation

[[[00000000000000001998---f4d971296a929cb985346220006c2641aa5439b1369be2ff70bbda6f1c2f355f]]]　　3.1 Derivation of the Bellman equation

[[[00000000000000001999---014f1c1fc49c558c184c5fed99e3625f4609cb3a2b5a197d66e060351701c967]]]　　　3.1.1 Probability and expected value (preliminary preparation for the Bellman equation)

[[[00000000000000002000---754f9e6e13ec00d812f6f9bcdac3f22f4201f8c919578e3a9551877d6da1818e]]]　　　3.1.2 Derivation of the Bellman equation

[[[00000000000000002001---e4f4b690fb4fc6b6637f38bf29e06ab2421d4dfde8600988758ff90bb57d6a7e]]]　　3.2 Examples of Bellman equations

[[[00000000000000002002---ef6c35a4baccd21d7e697f5181ebb709f2600af8e44365a3151f4076d6ba09bf]]]　　　3.2.1 2 grid world

[[[00000000000000002003---4e0b6ac225f2efc1c7f84772d3959b0f6543fa951edebfc9d0c0433fae07925f]]]　　　3.2.2 Significance of the Bellman equation

[[[00000000000000002004---0104b46f9136957400219f11c35e95806494151a5b400546b962e29c04aaa0d8]]]　　3.3 Action-value function and Bellman equation

[[[00000000000000002005---1d96ef601e466a4a3226e48076d09d3bd56832722c719ee8fae2fe06b09707fd]]]　　　3.3.1 Action-value function

[[[00000000000000002006---39c49bd7ef40d3ddf372b68da4cf618cce531cdf243574e98bbf5e6f0e5a4935]]]　　　3.3.2 Bellman equation using action-value function

[[[00000000000000002007---d5a50a05b6fbbea070cd92e310c11840adb3fdd8fb72d847ca7b450c860bb2a4]]]　　3.4 Bellman optimum equation

[[[00000000000000002008---c9a87a8e69343bdc4226b86e8c06abd982514085ebf1637a6de666be9fae8316]]]　　　3.4.1 Bellman optimum equation for state-value function

[[[00000000000000002009---cfba07d4ab90b8dc712d360976b5b39a60193cb1d5fd06ba1c657a56b6b20514]]]　　　3.4.2 Bellman optimum equation for Q function

[[[00000000000000002010---7e9eb85e6ef862005f5a7ed364dc7342ccc96d3587fd3b1985e7761d94e922c5]]]　　3.5 Examples of Bellman Optimal Equations

[[[00000000000000002011---4f2b5b29dd0f1bd4a2bc86428a13395ece3cdaa150d77a9d5ac9a9bb05b89f41]]]　　　3.5.1 Application of the Bellman optimum equation

[[[00000000000000002012---22b09ce3823914897456a0f2ae40ba8232cc8a22c5e8f94d6286af0e4ee06f70]]]　　　3.5.2 Obtaining the optimal policy

[[[00000000000000002013---eb1470deb618b8c50848fc501cd76cfaf2c022236de44babcc48446467413729]]]　　3.6 Summary

[[[00000000000000002014---d562531d95a550f68df26c94d51d9eb21e16eb44a9521803768eacbd65113eb6]]]　Chapter 4 Dynamic Programming

[[[00000000000000002015---c89e8935323406b7c2a7015f66cb06e5c75949b688ed3d6ac37879ca227c058c]]]　　4.1 Dynamic programming and policy evaluation

[[[00000000000000002016---be39c05733c8bdf8ab47e14e34c6caeddb0d946fb2bd10ae2b0116bb411cfc65]]]　　　4.1.1 Overview of dynamic programming

[[[00000000000000002017---56fd95502a6687a6ec92d9df8a06ae96ce6ea52eff1a51c0480891a7c32e12ba]]]　　　4.1.2 Try Iterative Policy Evaluation

[[[00000000000000002018---a4eb2137155891f6b85a3425bf1bdcc72a37ad65cf8379924f2382799b201f36]]]　　　4.1.3 Another way to implement iterative policy evaluation

[[[00000000000000002019---b3a5a20ec9f20554ccbea9d90fdd4c77ede9b89d7c2207d10075c44b2bd0c160]]]　　4.2 On to the bigger problem

[[[00000000000000002020---464fc7fd916c86f8c77f66a21d634d6fa1f98e9fdefc540727c66795387cb393]]]　　　4.2.1 Implementation of the GridWorld class

[[[00000000000000002021---b9ca49b55cdaf30c3d7df09496bbc3d24b4d1b6647b71c3944d900ccc9da9877]]]　　　4.2.2 Using defaultdict

[[[00000000000000002022---4c2ea24084344fc6f650ea9c4c400918ae5f44f650ed6489dce54d94fccbe6a9]]]　　　4.2.3 Implementation of Iterative Policy Evaluation

[[[00000000000000002023---9f09300cfaa8665e57544c72c256d71dce61923713a81b306614b938d6ad21eb]]]　　4.3 Policy Iteration

[[[00000000000000002024---c491d22e06a1520c27d3c663654717b04e783f5c63b4640f38a9712ab9b0971f]]]　　　4.3.1 Improvement of measures

[[[00000000000000002025---ffbb0415789d17092336c8515c5913afb0d2a84fe7bb62c961959a704e935fd4]]]　　　4.3.2 Repeat evaluation and improvement

[[[00000000000000002026---1c1490bd59ca719afbc64bee03705344cffc74cb43f1abe81e629fd08e67bb36]]]　　4.4 Implementation of Policy Iteration

[[[00000000000000002027---b3438afaf2d6f3299ff5e6634090232568ab39cbb21ad17214582a5ec59c7398]]]　　　4.4.1 Improved policy

[[[00000000000000002028---0d3aa7555a6231c9cf89dc6e6b8be85de26593784f4f0878b6455c1fae62c7f1]]]　　　4.4.2 Repeat evaluation and improvement

[[[00000000000000002029---99a520aff5bebac610cc3043ef7086a1d1529d7398fc0bea6222d79c86512dc9]]]　　4.5 Value Iteration

[[[00000000000000002030---4216f4686a8a91117f50e658e12dd81c3958d3d9221d0d9242de53ee64ba2d22]]]　　　4.5.1 Derivation of value iteration method

[[[00000000000000002031---2617ff77d48edc93eda3e991257edee272943036cd97edddd949ea03ab497e45]]]　　　4.5.2 Implementation of value iteration

[[[00000000000000002032---c3a9d8e0214478c3191967de476da1b6f3e40e01a013a8985301dc9e8faad53a]]]　　4.6 Summary

[[[00000000000000002033---b4ee8d41bb5aa23c5b7fdf6e058c9ac9277fc99a16cabf42577de7b3cbf61b63]]]　Chapter 5 Monte Carlo method

[[[00000000000000002034---32960c077dc2618e5f21241a8560063b4b1762fcbf037e2c7243738ef3bcebbf]]]　　5.1 Basics of the Monte Carlo method

[[[00000000000000002035---d04a7e6328e3e596fc2befebd1d8cb33b7a3017b8d6147714bed5864e112875d]]]　　　5.1.1 Sum of dice rolls

[[[00000000000000002036---e4da9d6f6e17fcdbcea18a296bef6c760fac110ec28704853d7fb4ae4a33914a]]]　　　5.1.2 Distribution model and sample model

[[[00000000000000002037---334d3e0abea07a462cc2f604d647dfe43e1af8e0bb12189a6e08f9001f93bf32]]]　　　5.1.3 Implementation of Monte Carlo method

[[[00000000000000002038---4a454545692df0c38f79b54d3fa39959c1c0fa3e9469d3d5d8df2f0828487668]]]　　5.2 Policy evaluation by Monte Carlo method

[[[00000000000000002039---31347fd53f8cfe738631a22995c6bab3dcd19c28fa5f9b6acf97aed140a73d41]]]　　　5.2.1 Determining the value function using the Monte Carlo method

[[[00000000000000002040---e71b5dda50c6edde2380d43b4be912ea0b63bcb892e5b4f0ec6604efa969b50f]]]　　　5.2.2 Find the value function of all states

[[[00000000000000002041---122baa3d29e2bf55c97bd4bd59529021ee6cfa67eed82c1b235299307a13f2a9]]]　　　5.2.3 Efficient implementation of Monte Carlo method

[[[00000000000000002042---67c51950b4415ec8ce4a73446c72f8e09a1eec8b9ea379a1e245eb325dd4b842]]]　　5.3 Implementation of Monte Carlo method

[[[00000000000000002043---747a838f1341e43d0aaa171ee3073f4a08c0c77b166f360336315077d888d76e]]]　　　5.3.1 step method

[[[00000000000000002044---b0c4eefce296de720935df8b72183970d3d6427fdf9b4ae37f155c6c0f51c13c]]]　　　5.3.2 Implementation of agent class

[[[00000000000000002045---51a7b99823cb3eb16fa82d0d00e3dae3e0c7693e790816c452ff4bbd855491c4]]]　　　5.3.3 Running the Monte Carlo method

[[[00000000000000002046---c7e86fc652852dde88a549565455f70dbd3ef0cc592fd0cc56dbd1bc86d3727e]]]　　5.4 Policy control by Monte Carlo method

[[[00000000000000002047---f7ce46a0faf139f97873c6c918d94e3a36c2dcc6e46cd2df444995facd94f710]]]　　　5.4.1 Evaluation and improvement

[[[00000000000000002048---05e2c934fb89845c2a4746af270747f4e44238b71b58153f62d06d81150e7891]]]　　　5.4.2 Implementation of policy control using Monte Carlo method

[[[00000000000000002049---4d06e5949b5f70e57fa4c0fa1b56421b774499a248ca2f52f3a01a361ab42eca]]]　　　5.4.3 ε-greedy method (first modification)

[[[00000000000000002050---3fa3a60f320ef1fb70edb104f888114f17aa3b50b46eeb4f9f7d7a071b7429cb]]]　　　5.4.4 To fixed value \alpha method (second fix)

[[[00000000000000002051---8262107185c4eab6ff93b0056a656892c77cf5d1daad9e66ca6a15210d5a3f03]]]　　　5.4.5 [Modified version] Implementation of policy iteration using Monte Carlo method

[[[00000000000000002052---3b733fa330332084940188011acad04efd266ba8900466ca53884ae8127cb156]]]　　5.5 Off-policy and weighted sampling

[[[00000000000000002053---86f791d0b2dc25e15c455caceb7e9b2ebca66a3fda1fbd3b79169630b2608a10]]]　　　5.5.1 Strategy on-type and off-type

[[[00000000000000002054---2bae902371bcf4c4b58161a810f49392c50ea9abd47ff0e72115340cad61c6cf]]]　　　5.5.2 Importance sampling

[[[00000000000000002055---72130da070dbde25e2940470b1c5b4b07effb9ff8a4df7a788b1b0de38a2cf12]]]　　　5.5.3 How to reduce variance

[[[00000000000000002056---8d969011b3f261a966e4292f30aa359130f937178406e5c028980bc020ca46e1]]]　　5.6 Summary

[[[00000000000000002057---1817900337c96f4f0e0ef4c0ff93f4be9289e37faa8d72edbc43e7d363a77888]]]　Chapter 6 TD method

[[[00000000000000002058---4225f4b99208992aefcbdb1dbda81682de6c839eb156410fb6022d0f86ad907b]]]　　6.1 Policy evaluation by TD method

[[[00000000000000002059---942d69e2c3a41dddefb25405c9f41626475a21cb927d9ddc11415999fe7b6504]]]　　　6.1.1 Derivation of the TD method

[[[00000000000000002060---bd2b4f7b062091a0231db327dc2f74075f682556688ed6517268deca158f6740]]]　　　6.1.2 Comparison between MC method and TD method

[[[00000000000000002061---7e24d102407126697e64860c2927cad7789ea9a57dee656ea3c467b2d626c0ef]]]　　　6.1.3 Implementation of the TD method

[[[00000000000000002062---bda9b9fce7caa8d7aa6678ccbcd69100d241a8001c337133da900c0395e0a5d5]]]　　　6.2.1 Policy-on SARSA

[[[00000000000000002063---0fa8a1264aadf79e6a25701c3e44f9258fe0458c635655765f8e2e2e10b88dee]]]　　　6.2.2 Implementation of SARSA

[[[00000000000000002064---94465e738080319158bcabd81f3a812742084b47a35d09a119d7c3fb802b26e3]]]　　6.3 Policy-off SARSA

[[[00000000000000002065---d6d94704b6b93ab07fef95a52ddf52b4e06185d2b9fa45126b9166f8f5732bab]]]　　　6.3.1 Off-policy and weighted sampling

[[[00000000000000002066---26902b09bb2f6bf268c559f26a646d96d3843bf1e1f228afd5edcf66df708c7b]]]　　　6.3.2 Implementation of policy-off SARSA

[[[00000000000000002067---1388294bfca3dbfee5b810e0824750f44539ed2a077d12b16b35dcf9b3608751]]]　　6.4 Q-learning

[[[00000000000000002068---3ba841a19e2ce0da51560e7334a7402cd5795d62c1f0e219d55d2db5d3b17ad1]]]　　　6.4.1 Bellman equation and SARSA

[[[00000000000000002069---ee1316d29c25507bb13663834d63d6c1f7921173a0e7e6952d113e462a560b23]]]　　　6.4.2 Bellman Optimal Equation and Q-learning

[[[00000000000000002070---9bdaf2946e16995adccc28986aa592e5f39b9f34ec3264fa438a0d5ddbe55072]]]　　　6.4.3 Implementation of Q-learning

[[[00000000000000002071---35616e20bb571bec4f02a6e48dacbff3a3e512b98ceb774d2eafe3771776ae64]]]　　6.5 Distribution model and sample model

[[[00000000000000002072---739ae1f18dfa219108edf723f1c94382fe3be09bc2dff3a996ce65516118aba7]]]　　　6.5.1 Distribution model and sample model

[[[00000000000000002073---234ece1794a3f255b8dd085896c195918e367058872be30f1a3053990e36a95c]]]　　　6.5.2 Sample model version of Q-learning

[[[00000000000000002074---bd6b7f11ef97f57139e54084fce81459a867cbfd6f4e912773a73333286fccc4]]]　　6.6 Summary

[[[00000000000000002075---792b3e55770e6d431abe67caf9e6cd1a36b779c42ffa421ff301d568204bf9fb]]]　Chapter 7 Neural Networks and Q-learning

[[[00000000000000002076---749e3529bd5f4e37f63a282452430e83a9aa2af5884495c6fd13ee3f166639b7]]]　　7.1 Basics of DeZero

[[[00000000000000002077---58e7c0e3085a0d30f6828d29511a10fd909fc9a057de735db48286de954a2156]]]　　　7.1.1 Using DeZero

[[[00000000000000002078---b29da2f8178c080aae55982ec2c28aa3fae2ef562be69f4c14528d2e209663b0]]]　　　7.1.2 Multidimensional arrays (tensors) and functions

[[[00000000000000002079---cd9d282a9550d0d79dd6c3c519c5b8091f15a424be91eeb4a852fa5481a345d0]]]　　　7.1.3 Optimization

[[[00000000000000002080---c90c1f8d543e9cfe211b9bbb73428e989e3f05c46c2f4efdaf77b8ccada8c79e]]]　　7.2 Linear regression

[[[00000000000000002081---bf84aff09ed141f43860952848d7dc4c0f47791b68b5c04e9019684843c031d1]]]　　　7.2.1 Toy dataset

[[[00000000000000002082---b70e3e67e98c61914064b94949ccebe2e5dcaed86bbd0ba3993506c09c28858f]]]　　　7.2.2 Theory of Linear Regression

[[[00000000000000002083---cedec346ca823449835a11d1ac8e94a135291aaea90cdbd363d70e97c7550c3d]]]　　　7.2.3 Implementing Linear Regression

[[[00000000000000002084---346aacdd1ae5e827243ea27e95b671da8eaab5786041c15751af25bc5f936fe7]]]　　7.3 Neural network

[[[00000000000000002085---a9c3c560ee52a6890fec896bfac3a5045b423451b065732f15c11fe72cf06d77]]]　　　7.3.1 Non-linear datasets

[[[00000000000000002086---e02b27bb40906066ceba8b8e8156024419e8da18d25ae39120617a1c731d3e5e]]]　　　7.3.2 Linear transformations and activation functions

[[[00000000000000002087---5f8f5e35ecc6f145d3375633cd86b78bf4490664b4859fc22648e23fa25fdd33]]]　　　7.3.3 Implementation of Neural Network

[[[00000000000000002088---7243886b64dc4485050dd4502f01fb598aa0122b3a7951c3cf58631a82f23edb]]]　　　7.3.4 Layers and models

[[[00000000000000002089---e5866030c2e48b107b1d468c973561a13eac758d3b545b69c79cae623eba6506]]]　　　7.3.5 Optimizer (optimization method)

[[[00000000000000002090---e871af2efc2da5ce2010af490892ec5cd18c26acb3324a999de37d0eb15a9395]]]　　7.4 Q-learning and neural networks

[[[00000000000000002091---5c45df38aab2fdf3afb72f4b97f0589faf65a2371574b4e5c3098e60b92b0560]]]　　　7.4.1 Neural network preprocessing

[[[00000000000000002092---00c21a49cb03682d0dcac2768ad9b21c6af9ed045b1e4846f0f0ca11084ba4ad]]]　　　7.4.2 Neural network representing the Q function

[[[00000000000000002093---d0c6e93356a2d7d6e10e525548e86aed717d647ec4104622f105cbdaeed2eb5b]]]　　　7.4.3 Neural networks and Q-learning

[[[00000000000000002094---68e4f9741cb151a837719caee670c44b876cc61f460234d054e00db94e3c8a2f]]]　　7.5 Summary

[[[00000000000000002095---c96efed75b169395348005481de4a134a12fb8a15f238f634dd8611003354653]]]　Chapter 8 DQNs

[[[00000000000000002096---293ab64c2a571a32b455ce427febc634389cf8ebaed6e0f5f7636e668f80f481]]]　　　8.1.1 Basic knowledge of OpenAI Gym

[[[00000000000000002097---83214a28f895dd0cc9dd5df0dfcc942a9529a6251e76588c52f4c52f9105ed23]]]　　　8.1.2 Random agents

[[[00000000000000002098---a3db499aa9f589867b4ca6ef859059b8d7e00206269bfbfc406a974e39f2c6d5]]]　　8.2 Core technology of DQN

[[[00000000000000002099---22d4d5a99a91ee2289b0442cea15f2d40a2cf2075be653489020ace97c34dd34]]]　　　8.2.1 Experience Replay

[[[00000000000000002100---067f79b9bd3658a84271ba260fd315d11fc369d7212e3601db320883d74e4fbe]]]　　　8.2.2 Implementing Experience Replay

[[[00000000000000002101---e2e7c6fa5e6a88779b3a8603a0b633ccb0d8e7d5aebb6719e755fca7dc45bbf1]]]　　　8.2.3 Target Network

[[[00000000000000002102---b9b58851a0332786e5a37d6bbf2bc84eb08006c9d8062063dfdecc842a06b1f1]]]　　　8.2.4 Implementation of the target network

[[[00000000000000002103---bd7140696257b122384bcadcd9dbd079468c36e5e2d324781d9601c8cf38afba]]]　　　8.2.5 Run DQN

[[[00000000000000002104---14593d38002f5f9db41ba8ef3c2675598f5ebf2592ff5fc0d621fea8a4abe783]]]　　8.3 DQN and Atari

[[[00000000000000002105---2fe1f138daba9fac6346c65aa80f4b16213c322c7a1c98290ab36778d3ac9988]]]　　　8.3.1 Atari game environment

[[[00000000000000002106---30fec05ca7b4163aa53929aaf1699f42da13bbae7798358aabea32051a872b62]]]　　　8.3.2 Pretreatment

[[[00000000000000002107---0edbdcf6bfc632a4d9a0b99d45cb004c916adc1a67d3e7b58e95a6173e9a1e5b]]]　　　8.3.4 Other ideas

[[[00000000000000002108---06e417e4b853ae75c5c21afeb5ba4f28011302d064ccfa60e582a82d51b9a3fa]]]　　8.4 Extending DQN

[[[00000000000000002109---cdd7f0afd8b7818d95e46d6a8159755323b53661e83cfa895fa83eb4713c26be]]]　　　8.4.2 Prioritized Experience Replay

[[[00000000000000002110---63837c2a9d29848f7b59ea06799ca72847a9a6b3584c277e3153ebceac232366]]]　　8.5 Summary

[[[00000000000000002111---533bb313135b90505cfa2f846ef4006159ba5a04e06bc8991a2621729ce4aab4]]]　Chapter 9 Policy Gradient Method

[[[00000000000000002112---9c9baf43b09a8dc0170af3eceb37d86c9a40d48df3de93ed6c24a73f765a0811]]]　　9.1 The Simplest Policy Gradient Method

[[[00000000000000002113---907b6337a83792e6a3d84177bcf97311b05417a8846e1a2da550c8726b51c273]]]　　　9.1.1 Derivation of Policy Gradient Method

[[[00000000000000002114---bad15272b266c0095c377cd41cab285447db8bf5c7fc3946991968c88b01904b]]]　　　9.1.2 Policy Gradient Algorithm

[[[00000000000000002115---45c4a63ab09bb704349747a992111bfd77bc9e533a2db3a70a8f9407915f8230]]]　　　9.1.3 Implementation of Policy Gradient Method

[[[00000000000000002116---747e5c98a8b6f801662565e078adf3046e5b7e25a37e88a54b8b94c2f1ade366]]]　　　9.2.1 REINFORCE Algorithm

[[[00000000000000002117---627a007ccc0c007b6dfe8bf9f6c18841ab3a801575dccc5707c44a5951577f0c]]]　　　9.2.2 Implementation of REINFORCE

[[[00000000000000002118---978732564b454b73b3cd902c79003f663ada0283dff6e72b2d844cc612595ad6]]]　　9.3 Baseline

[[[00000000000000002119---686cf3b5e38e68941c7378597b321abc7f8630c7d2bed973add6687e4a0fb1a6]]]　　　9.3.1 Baseline ideas

[[[00000000000000002120---54b0058ae6e7b4c147d7c5cf69a112f8f106090ae4e7817d9ca90d478ecd6e17]]]　　　9.3.2 Policy gradient method with baseline

[[[00000000000000002121---d3c5b9453a052b452ac47a0d911dbbe23678812733c3d9ee2d77dac1653d6977]]]　　　9.4.1 Actor-Critic Derivation

[[[00000000000000002122---84643f13b7670e502757e12a25642bf02a92e71e3b402e556e918433f8b7e14f]]]　　　9.4.2 Implementing Actor-Critics

[[[00000000000000002123---cb201df1d92b3965d0cdf12c53ad9bfbbf4ead932e0b3ff98be2cf75667fd04e]]]　　9.5 Advantages of the policy-based approach

[[[00000000000000002124---2e123c5e18c9d8a5a7bceae1da1359b7ef320770ae4ad9973a763d6dc01562a5]]]　　9.6 Summary

[[[00000000000000002125---f1bc16413290181ce09afa2864cd8d55cd80c70e156da870c9f9b4f51273ebd0]]]　Chapter 10 Further

[[[00000000000000002126---f1449cdf7a4f1786ef6a0793eb36e755bb773eb66afca22159f776e001b5f9c9]]]　　10.1 Classification of Deep Reinforcement Learning Algorithms

[[[00000000000000002127---124cc5f68f264d4463f76a2820548f328c7be46f1cfe01c358f169bb552f0fcb]]]　　10.2 Evolution Algorithms for Policy Gradient Series

[[[00000000000000002128---c9e225eff9cb25b17cb9f9f80be9223b101b980d71a4493eeab37ff6f7c10ba7]]]　　10.3 DQN sequence evolution algorithm

[[[00000000000000002129---d387fde1b1b5b1244b3e0756fe2539a3d761a395bfeeff2081c00db9dd5c1283]]]　　　10.3.1 Categorical DQN

[[[00000000000000002130---f0e298b7cf4d3a0d53b73d0bb3eb09b0ad25ede04bfbd42f2bcabd9246fd7266]]]　　　10.3.4 Advanced algorithms since Rainbow

[[[00000000000000002131---6415004aea65465ea0a58e6506c8ffddab17c88549630ebe57dfccc3e76e1b41]]]　　10.4 Case studies

[[[00000000000000002132---36b03a5915362b06ce869c746a1e04cefc7118bfb3180d6c4540ffcbba6b3602]]]　　　10.4.1 Board games

[[[00000000000000002133---d361fa6e53b5d789bc8364c5828ab977f8c9145897ef19269d885906135432ce]]]　　　10.4.2 Robot control

[[[00000000000000002134---9af019a7562a87262da95a40fe9c141cfb0be5060b17dcc16105ae6bdd999b6b]]]　　　10.4.4 Other examples

[[[00000000000000002135---290f00e104ad9ab86f60b1605f17b5e1eeea0a7149c3cebb754aa44cf7e73434]]]　　10.5 Challenges and Possibilities of Deep Reinforcement Learning

[[[00000000000000002136---de3feb135c8a3e738225c6f15372a5d51d48715ead91d1092339da5c71dc5b19]]]　　　10.5.1 Application to real system

[[[00000000000000002137---241ee05f0525548fbb965e92bb895aea4c9c9176d92f4fd23987a2f7eba2c12d]]]　　　10.5.2 Tips for formulating as MDP

[[[00000000000000002138---41a8118076b2db9aa4ce7c52dbd786ae498ffe76884ba9fc3e76e6e914edd1e7]]]　　　10.5.3 General Artificial Intelligence System

[[[00000000000000002139---900b11f452713da122d97a68852aec27c164a017f769b2f7454a2deea9dca472]]]　　10.6 Summary

[[[00000000000000002140---deceb11cec6b3e787022ff64f8fc007ff1fa681331e913db2e9750587eef2f14]]]　Appendix A Off-policy Monte Carlo method

[[[00000000000000002141---0bbe28e3afb5ea1ff9f953ffb79470928c08e8c4c79da6cf638529990ab0fcb3]]]　　A.1 Theory of off-policy Monte Carlo method

[[[00000000000000002142---4dc00a0335c41e736c1c05bd92917c2045569f86c61f6ab83afa3d593d37a45f]]]　　A.2 Implementation of off-policy Monte Carlo method

[[[00000000000000002143---49f40940917fcef300d39cfa780d9435d000393dd31156f021d2e2790b553a39]]]　Appendix B n-step TD method

[[[00000000000000002144---ca0dbf1271f21e099b1ba713d03ab48574a00491331ba8679802b41dce2f07e5]]]　Appendix C Understanding Double DQN

[[[00000000000000002145---9afcac9868fc66462587a4ff313eaaecbd2d4a810c20f956d26a4d5be135f9b7]]]　　C.1 What is overestimation?

[[[00000000000000002146---8649903767d0f6553f3ae52b8609196ceec102061822e0196701ac8bcc5206b9]]]　　C.2 How to resolve overestimation

[[[00000000000000002147---3b0913444d3c02563180b48b7b0305b648a27cb9531e92d2c08045ccae8d839c]]]　Appendix D Proof of Policy Gradient Method

[[[00000000000000002148---04362b1b07300f00efa2ddba30ff51f741dbe11286e42cb5d5296dce4676e782]]]　　D.1 Derivation of Policy Gradient Method

[[[00000000000000002149---02dfc4d5850238480c2ee7a477985ae69f9c0a1ab9706dbd17130744316fa4d5]]]　　D.2 Baseline Derivation

[[[00000000000000002150---011a6f34bca1b3aaaf230e390693587d87b64453210ac759a29c1b071eec532c]]]　in conclusion

[[[00000000000000002151---7d1b2ce73e1cc85cd8553303b8d164b9926a375fcfcc2b2f9c962f6f08260cd6]]]　References

[[[00000000000000002152---78c748b750c5565a6695bfce45017c02cb814212b08090570bb3e600dcbfc48f]]]　About the author

[[[00000000000000002153---cbd795697fe2923c1bfa79339686212ecf39db44832ae3daabbbe8854954d406]]]　colophon

[[[00000000000000002154---cd086c3657a4f65c196959f825480419f41cdd7994e885d30bca8d0c5d3a1fda]]]Appendix D

[[[00000000000000002155---c78d67cba13f3750250665c367a1197360dee17f56f3ada3a7a1e78c5ef3b560]]]Proof of Policy Gradient Method

[[[00000000000000002156---6e490a302bd861c0df97c1f61be2a2414fe19998f168e0cd8ea8115087010609]]]Here, we will prove the formula used in 'Chapter 9 Policy Gradient Method'.

[[[00000000000000002157---17f92cbd8c5116dc25e5b34850c7eb8c44ce9a56b41ff111ba59ccb1291541f1]]]Derivation of Policy Gradient Method

[[[00000000000000002158---7b4a26dcd053f92816ed3a3be9a294712dfce806291fbc4efb9234a2175c5544]]]As explained in '9.1 Simplest Policy Gradient Method', the gradient is given by the following equation (9.1).

[[[00000000000000002159---b97521c56a8c428e579de4b72b8770094f6850a11cf6945b5ead7daf9af7758b]]]Here, the derivation of formula (9.1) is performed.

[[[00000000000000002160---fde9df9e8c2b7e0b1a575649fd1f57a18f9bdb9a632c55eda18724f54252e4a3]]]First, check the symbols. Here, we express the probability of obtaining a trajectory when we take a policy. Then, we will expand in order by a formula.

[[[00000000000000002161---0f26815a191f6f7887532300f9ac1e2010f68a37938a47ed80c0e6e06d1c807f]]]Let's follow it step by step, referring to the comments to the right of the formula. If you have knowledge about differentiation, I don't think there is any particular difficulty. Here, I would like to make a supplement about one point, 'Tricks of Gradients'. It makes use of the following relationships:

[[[00000000000000002162---0a072eb47eba8db6b07ded070f67292d3af3c5681bb9204066ea59fd9c1886a2]]]The above formula only obtains the gradient of , but this formula shows that and can be 'replaced'. This is known by the name of the Log-Derivative Trick, and is a popular formula variant in the field of machine learning.

[[[00000000000000002163---6adb1656387a1b516dee1a7e531e744ef942ff1ef033e2d0c5d584b4fe589d4f]]]Then, to further develop equation (D.1), we make use of the following relations:

[[[00000000000000002164---62611ca6cb9f22ee7df9d0ebfe34535863870e5f9a5fa4b2bfef8c27f58f49b2]]]Here, we denote the probability of the initial state by . As above, the probability of obtaining a trajectory is expressed (decomposed) as the product of the initial state probability, the policy, and the next state transition probability. Also, can be expressed as:

[[[00000000000000002165---ecda88bd82e239bac625908f80ee9f4f3c6c82e10131d15f514f3cb57ae5a774]]]Therefore, it is expressed as a sum as shown in the above equation. From this, is asked for:

[[[00000000000000002166---cef7a483dd40c89efc6e4cea63cf97a5143528937fdaf9bae1744f4524396277]]]is the gradient with respect to The gradients of elements that are not related to are From the above we get the following formula:

[[[00000000000000002167---3c9e52b84d6fb3ec0571cca45c84f8f5018b8f75686235d645445a9cf494dcf4]]]The derivation is now complete.

[[[00000000000000002168---0e08f563b031927ec041f2cf95c96c2e171f3b58675d46e392a6713f22b0ed28]]]Baseline derivation

[[[00000000000000002169---da8b0583446d81a318ff388705c16f06574cb4f7be9d150f754d8694434b5e73]]]In 9.3 Baseline, we showed the following formula variant:

[[[00000000000000002170---1bd43190d2b77795f8ec0f0677d7876a564cf991e507d09995569c8957de9108]]]can be used instead of as in equation (9.3). is an arbitrary function, which we call the 'baseline'. Here, the derivation of equation (9.3) is performed.

[[[00000000000000002171---bc3c99eb4378454d392a83344cdf9129e57bf2ad6cec6629c97150bf4d5164f7]]]First, we prove that the following formula holds.

[[[00000000000000002172---b1e9043f3beda11ad246fca3bd824b2dcad31307b5e644bd23ee3645f73745c4]]]Here we assume that the random variable is generated from a probability distribution. changes the shape of the probability distribution depending on the parameters. Then the following formula holds.

[[[00000000000000002173---35e85da0ab7679fb78becf7a0e3931e2785e2279b11a2c976b3798a76291643d]]]is a probability distribution, so the sum over all is Next, find the slope of this expression.

[[[00000000000000002174---9c6af8e69ef6755f7bea6f31f673398bbb4e982d4212bec64bee9894f6b57456]]]Then use the 'log gradient trick' to expand

[[[00000000000000002175---bb691ae78fc128c382256ff62cda635cabe8a57fa13eb3323b4f2df586d3387f]]]Equation (D.2) has been proved from this.

[[[00000000000000002176---ec3b247eccb098587a3326fe69df43a2143bc0af18624e8fe9a51783ad7bd2a2]]]Then we apply equation (D.2) to our problem. Specifically, use instead of in formula (D.2). and use instead of . Then we get the following formula:

[[[00000000000000002177---ca80368a4b518b11c8068d5c34ee5d4cf610225f7763ee20dfdd68c65e296d85]]]Equation (D.3) is the expected value for Therefore, any function can be included in the expected value, as in the following expression.

[[[00000000000000002178---5f4dade518ac547112fe68363e6c2a27e14b99e35432241fac4755713ddf0176]]]is a function whose argument is always the same value even if is changed. Equation (D.4) is the expected value of , so the equality holds even if you include a function in the expected value.

[[[00000000000000002179---5a519788055c81b5f949146737f5d5d5893b6452fc7d44cf882add2b5de1c5cf]]]The following formula does not hold because earnings are action-dependent.

[[[00000000000000002180---041131f8eb22633d74280b25c02d0c704ef16a52e0d5ca8eb4e27fa158b2262f]]]Equation (D.4) holds for all This gives us the following formula:

[[[00000000000000002181---d7dd2d2504955694799d83df5130dba918ef9bf658b1424e396a7b31d387c722]]]From the above, we can see that equation (9.4) holds true.

[[[00000000000000002182---f38d5404a52e44f974500f76196c1d802178e13b6617ddbd703916329e976cb1]]]Chapter 4

[[[00000000000000002183---d5e8dae8d07207eeb2e42f9d925a9fdc1dbc8c94287cc86d0c89999a4305f87b]]]dynamic programming

[[[00000000000000002184---2cdd6e475b1e7cb0958a2b78906dcd548f4deafc138212e7ecd527ed40703c3a]]]In the previous chapter, we learned about the Bellman equation. Using the Bellman equation, we can obtain the system of equations. If you can solve it, you will find the value function. A diagram of this flow would look like this:

[[[00000000000000002185---b8dd1de7a501391d6ac1ecfb65644ae0337da456f4562438e8986b6c421141b4]]]
Figure 4-1 Flow of finding the value function using the Bellman equation


[[[00000000000000002186---f11875ff86bb96630068f4abffc19816024d90743de098c339ad085096113508]]]As shown above, the system of equations can be obtained using Bellman's equation from three pieces of information: the state transition probability, the reward function, and the policy. Then you can find the value function using a program that solves simultaneous equations (simultaneous equation solver). However, it is only for small problems that the method of explicitly stating such a system of equations and directly solving them is effective. In practice, even a small increase in the number of states and patterns of behavior can get out of hand. That's where Dynamic Programming comes in. With dynamic programming, the value function can be evaluated even if the number of states and actions grows to some extent. In this chapter you will learn about dynamic programming.

[[[00000000000000002187---0cec12cfa3d7307794623bbebdb99f7d5220b1ff2f6e5697966e194debfc3520]]]Dynamic programming and policy evaluation

[[[00000000000000002188---ae7da741c2515a29d648a2d660838b179a02220449df48e0cb18ed1493b9f1ee]]]Reinforcement learning problems often end up tackling two tasks. One is Policy Evaluation and the other is Policy Control. Policy evaluation is to find the value function of a given policy. On the other hand, another policy control is to control the policy and adjust it to the optimal policy.

[[[00000000000000002189---8e2a833399578af254d5788f0f28fae72402a5592df7f33ddbc159d2ec73b9b3]]]Of course, the ultimate goal of reinforcement learning is policy control. However, the first step in doing so is often addressing policy evaluation. This is because it is almost always difficult to obtain the optimal policy directly. Here, policy evaluation is performed using an algorithm called dynamic programming (hereinafter abbreviated as 'DP').

[[[00000000000000002190---23f22442df7b5af19fd28e6c55f7d97e2cce62d4860b2e90abf83360c15ec473]]]Dynamic programming overview

[[[00000000000000002191---510d962ac2a15af18c913d768033a674331fbdd8ce746110eb3ab688abaecb6c]]]Start by reviewing. We defined the value function as

[[[00000000000000002192---b8c2327188d1297f12ae73df9416c0cb172d311b4a1315f57050f9a5c3f2cac7]]]Calculating expectations involving this infinity is usually not computable. The Bellman equation below solves this infinity.

[[[00000000000000002193---03abd955628ca3645f8def90e66268ca1f54f23eea0792c26d410ccd3af13034]]]The Bellman equation expresses the relationship between the 'value function of the current state' and the 'value function of the next state' as shown in equation (4.1). The Bellman equation provides an important basis for many reinforcement learning algorithms. The method using DP is also derived from the Bellman equation. The idea is to transform the Bellman equation into an 'update formula'. Expressed as a formula, this looks like this:

[[[00000000000000002194---66c80b38fbc8d1b7c93000f8cd5b5efff23fb72a35498be46688bf8002b68e73]]]Here, we denote the th updated value function by the th updated value function. are 'guessed values' and they are different from the true value function. That's why it's written in capital letters.

[[[00000000000000002195---0496e61bdc3858081eb2f6dbbd66381a45cee44c05260ff2d95c12939260117f]]]The feature of equation (4.2) is that it uses the value function of the next possible state to update the value function of the current state. What we're doing here is using an 'estimate' to improve 'another estimate'. Such a process—the process of using estimates to improve them—is called bootstrapping.

[[[00000000000000002196---355de162c93ad6e61d596865ae4897cf051985721f0787d3e43d4161a4c489cd]]]A bootstrap is a loop on the back of the shoe that you hold with your fingers when you put the shoe on. Later, it came to refer to the process of self-improvement without the help of others.

[[[00000000000000002197---2851a43636e2b6cbd7a6a24b65ad786116ff2289a25a2c5b3331ee6e7bd54ae4]]]Now, I will explain a concrete algorithm using DP. First set the initial value (e.g. initialize like in all states). Then update from equation (4.2). Then update based on. By doing this repeatedly, we will get closer to the final goal. This algorithm is called Iterative Policy Evaluation.

[[[00000000000000002198---ed751ce8d810d27f5228d225f6284eba4e2816a10cdce624aed3b0f17ebfda2f]]]When using an iterative policy evaluation algorithm in a real problem, we need to stop the repeated updates at some point. To determine how many times to update, you can determine from the amount updated. I'll show you an example implementation of that shortly. It is proved that if the updating by Eq.(4.2) is repeated, it converges to . However, several conditions must be met for convergence. For details, see reference [5].

[[[00000000000000002199---8b3ccd6f2613d3056a0ef4caf1e2a1a9167a36617101dae927e88d67e456c5ee]]]Dynamic programming (DP) is a generic term for algorithms. DP refers to the general technique of dividing a target problem into smaller problems and seeking answers. The essence of DP is 'don't do the same calculation twice'. There are two ways to achieve this: the 'top-down method' and the 'bottom-up method' (the 'top-down method' is also called 'memoization'). The 'bottom-up method' is the method of updating the value function by advancing one by one, as explained earlier.

[[[00000000000000002200---96a09fe5cb5b2daa29959070bd5a4b004c6799893d456453f05f0fabd17d5307]]]try iterative policy evaluation

[[[00000000000000002201---4d982adc00b53fa7fd34bddf5de7849b8f28a82b2250f4076192704041f6d550]]]Let's take a '2-square grid world' as an example and see the flow of the iterative policy evaluation algorithm. Consider the problem in Figure 4-2.

[[[00000000000000002202---1addbe06c3a64ec37234a1fd365f864e47a10c44978e1b2b24ff7674e17c5817]]]
Figure 4-2 Grid world with 2 squares (reward for hitting a wall, reward for moving from one to another)


[[[00000000000000002203---a362b31852845550b0afc4ab5f2ebf81554a1466c531eecbc42d6651b9138826]]]As shown in Figure 4-2, the agent follows a random policy (0.5 probability to move left, 0.5 probability to move right). Note that state transitions are determined deterministically in this problem. In other words, if you act in one state, the next state is determined by one. In the formula, let's say that the next state is uniquely determined by the function . In that case, the value function update formula (4.2) can be simplified to

[[[00000000000000002204---97539809e384cf9d125c0e8a7669d239a85a32d98a6fa9ab348669bda3a5cc0f]]]
Figure 4-3 Update formula when state transition is deterministic


[[[00000000000000002205---bb51220f645c45db4d29ddd8afd6113d76c50ad132d47c6aca5d8a9c76ec541f]]]Here, we will divide and write equation (4.2) once. Second, the state transitions are deterministic, which further simplifies the expression. Specifically, in this problem, the next state is uniquely determined, so it is not necessary to calculate the sum for all states, as in , but only for one. Expression (4.3) in Figure 4-3 expresses this. In this problem, we will update the value function according to formula (4.3).

[[[00000000000000002206---e6487df91eb1bee8dcdaa984c90f0d7566b33719f8e49501494a1a93944d4296]]]Now, let's find the value function of the policy using the iterative policy evaluation algorithm. Let's start with as the initial value. Since there are two states in a two-square grid world,

[[[00000000000000002207---2e955e0b4e2991955f83f9521383938447e1163a2bbdec3d1871424722372b91]]]becomes. Then update according to equation (4.3). This is best understood by looking at the following backup diagram.

[[[00000000000000002208---651dd13e5d6e69a7a3cd94934873f22bf961db18bca16baa13212fc71be24fe0]]]
Figure 4-4 Backup diagram starting from state


[[[00000000000000002209---0eb7de9b4c7607f35b7e7150c320878b20b3a69617fcd27fbd62c8cd09363597]]]There are two branches, as shown in Figure 4-4. One possibility is if you choose the left action with a probability of 0.5, get a reward of , and stay in the state. Applying this to equation (4.3) with the discount rate as

[[[00000000000000002210---139ee487afde8bb7d86f5c7ebcee82ca692e45420d2aeda90e46e43fb4cdc567]]]becomes. Another possibility in Figure 4-4 is when the state chooses the right action. In that case, get a reward of 1 and move to the state. Applying this to equation (4.3), we get

[[[00000000000000002211---8772ebf6fe3a5bd65342f11d4ef702fac061725db2dfdbb162d511edbd2487b9]]]becomes. From the above, the following is obtained.

[[[00000000000000002212---020786691cb219d80a29557a657db25cf1411d73a1597fcb32d219ace71ed946]]]In the same way, we can also calculate The formula is:

[[[00000000000000002213---4d7ff526a3fd6a1a6feb21ebd6f5dfa70d08998edf2dc91ad3f291d0f61a94d9]]]We have now updated the value function for all states (there are only two states in our problem). The results are summarized in Figure 4-5.

[[[00000000000000002214---b1aa85e9726584970d7ff0119d9368d0c711f8114e0647520afe202feac9ac64]]]
Figure 4-5 First update of the value function


[[[00000000000000002215---8f2942e81383c5a1e28208f1d8215c9283d68baf1d35649fd07272798b88c198]]]Updated to as shown in Figure 4-5. After that, just repeat this. Specifically, it computes from, then computes from, and so on. Now let's implement this calculation in Python. The code looks like this:

[[[00000000000000002216---4376ca1e7f020f79a1ddc3137111c6fd2b0ebe373d5a5455048b800d726fa045]]]# a copy of V

[[[00000000000000002217---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000002218---b0167ccf1e4ee17aeb11f8d35d75cad57859b4b4aaf39495fa4ac42466cc83f3]]]Since our problem has two states, we store the value functions of the two states in a dictionary. Let V be the current value function and new_V be the value function to be newly updated. If you run the code above, you can see how V is updated. By the way, the true value function values are [-2.25, -2.75]. Looking at the results above, the values are almost the same after 100 updates.

[[[00000000000000002219---ceb6e811e85f222045b13a7e5dedc9d0caf2f610fd99fa04832d1766d12b574f]]]In the code above, we are copying the dictionary like new_V = V.copy() and V = new_V.copy(). This makes new_V and V different objects. Since they are separate objects, updating the elements of new_V will not affect the elements of V.

[[[00000000000000002220---7b86b805f16927ee88cdc8fcf48b70cd7e3d65e4a4c9cf6866295cf06fcbc07c]]]Now, in the code above, we did a fixed number of updates (specifically, 100 times). Next, let's set a threshold and automatically determine the number of updates. Here is the code:

[[[00000000000000002221---2c96ed06a81689d4a697a778a541204137dca7577abd5ea7cde6dbf7ca55b481]]]# record how many updates

[[[00000000000000002222---0db639c59bd641e792db9eeb6d55b85bbb694c83b416656d438e19b8ef4d6e88]]]# Max updated amount

[[[00000000000000002223---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000002224---26efe4c0885a38aa4814e69deea18b497f344174d8fbb5b7f2a9f3c18b974bf8]]]Here we set the threshold to 0.0001 and keep looping until the maximum updated amount is below the threshold. Specifically, find the absolute value of the difference between the corresponding elements of new_V and V. Since there are two elements this time, the larger absolute value of the difference between the two is selected. Its value is delta. If this delta becomes smaller than the threshold, we stop updating. By the way, looking at the above results, you can see that the value function is almost correct after 76 updates.

[[[00000000000000002225---3360f32234b32eea24c91c062eccb3c47603e32517a9461526ec72021d941a36]]]Another way to implement iterative policy evaluation

[[[00000000000000002226---d9a025c65e4c09d8defddb54d272f7e667080b65751a54c84a824680f44b10eb]]]In implementing the iterative policy evaluation algorithm in the previous problem, we used two dictionaries. One is V which holds the current value function and the other is a dictionary called new_V which is used when updating. We used these two dictionaries to update the value function as shown in Figure 4-6.

[[[00000000000000002227---f3a624a472c95a114ab989d73b434348ead79df0764354e439c0e00fdcc0ff43]]]
Figure 4-6 Previous Update Method


[[[00000000000000002228---f3ab8ae2ab1835fa4c7d565f738f30683b4653c4673112a8ec7d11e56eeb43a8]]]One thing to notice in Figure 4-6 is that it uses the value of the dictionary V to compute each element of new_V. There is another scheme for this implementation. It's a method of overwriting each element of the dictionary. This is shown in the figure below.

[[[00000000000000002229---9a1560dcde052a72a9aa6109fd1ecdc4ffa90e5981935058a7a26cdfc03d51db]]]
Figure 4-7 New update method


[[[00000000000000002230---f8de736ddfff9b8e871eb9d8fd0139063e701076edcca2843e4faf5f97553a44]]]The scheme in Figure 4-7 uses only V to overwrite each element. We will refer to this as the 'overwrite method'.

[[[00000000000000002231---1102408f10c2adec2e356f19767591c615509ca3102f265f0c66a59744360f6f]]]Both methods converge to the correct value after an infinite number of iterations. However, in most cases, the 'overwrite method' makes the update speed faster. This is because in the case of the 'overwrite method', the updated elements are immediately available. For example, in Figure 4-7, the values updated on the left are used to update the elements on the right.

[[[00000000000000002232---04409f2450d560ce4e294a353f947f273d82f7cea8d4c5430f14be628f7c779b]]]Now, let's implement DP in the 'overwrite method'. Here is the code:

[[[00000000000000002233---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000002234---1072c5434957ea5ca197dc469b5bb894934b9423af08cab892bb5a16ff092bf3]]]This time we'll just use a dictionary called V, so that each element will be overwritten immediately. Looking at the results above, we end up with 60 updates. Since the last time was 76 times, it is true that the number of updates has decreased. From now on, we will implement the iterative policy evaluation algorithm in the “override method”.

[[[00000000000000002235---c7d179a163e759866344347e27ad107759fb520555979ad48f4f0ac5a9ae89ad]]]to the bigger problem

[[[00000000000000002236---d448ddf24db29cb6416bd9fead1aad7b68691f024af081c7f2dcb4e70e10c7b9]]]By using the iterative policy evaluation algorithm, it is possible to solve quickly even if the number of patterns of states and actions is large. Here, we leave the '2-square grid world' so far and challenge a bigger problem (although it's still a small one). This time, we will challenge the '3x4 grid world' as shown in Figure 4-8.

[[[00000000000000002237---bc1bc9cf02b905556e7d635262f9dad30f6f9b79c560c59d2ee244279e878b5f]]]
Figure 4-8 3x4 Grid World


[[[00000000000000002238---165369f9ba13e1496dd837fd85c2155b5f44d2bbf805b7cc48958ed446eb83f1]]]Here are the settings for the '3x4 grid world' problem:

[[[00000000000000002239---6a6e7833dfeee541c91b6e9bb2bb82c7577d7d7397bb99fca52ce201bb856637]]]Agents can move in four directions: up, down, left, and right.

[[[00000000000000002240---b320292fed2d94e2de6a7bcea28d026f056bff05aa5bb3bd96fd5ddc9f4b0534]]]The gray squares in Figure 4-8 represent walls, which cannot be entered.

[[[00000000000000002241---c46408fea9da195d44c3b1c3504de7713bd5ebb116897fe7408e4bfbd25e965b]]]There is also a wall outside the grid, and you can't go further

[[[00000000000000002242---88dc5126c7d1feb9ca19cbb3ea137edcf21a5e10e2a220bf2ee3286ff47e93a1]]]The reward for hitting a wall is 0.

[[[00000000000000002243---4f63c09e341272e55f2d11f7bc58cc35316eb616595994cd544370084e8a907e]]]Apples are rewards, bombs are rewards, otherwise rewards are 0

[[[00000000000000002244---d4e32e4fedc8727f052115a30ba1dc5f019cd84af4b15dabdd76a90fc5ca8062]]]The state transition of the environment is uniquely determined. In other words, if the agent chooses to go right, it must go right (if there is no wall).

[[[00000000000000002245---d6ea31f5d09e7b81f04c871d4cde35e36670ccc99682b4cfdb640b51cdd2ce92]]]This task is an episodic task, and ends when the apple is taken.

[[[00000000000000002246---800e9c0eef30be63b12ba016805387b851b17a24c3f5f0e10dcbd67396cdc94e]]]Now, let's implement this '3x4 grid world' as the GridWorld class.

[[[00000000000000002247---6d16e2ebba136e9aa2b6d8526ddc7fb3d770bd9a071d2ce2aa4b5bbaedfc32a7]]]GridWorld class implementation

[[[00000000000000002248---0695d2cd77803b3c47c1f8e0e1ce2611a13e902b1ef5a1b995d30a08cbc8ef71]]]Here the GridWorld class is implemented in common/gridworld.py. First, here's just the initialization code:

[[[00000000000000002249---e45b2493d1c965a48f9c959cb1f5ed8e4d5ff69b680e31a20279b17db14a4675]]]The GridWorld class will have some instance variables. self.action_space represents the action space, or “candidate actions”. In the code above, actions are represented by four numbers [0, 1, 2, 3], and the meaning of each number is defined in self.action_meaning. For example, 0 means move up (UP), 1 means move down (DOWN).

[[[00000000000000002250---c2f3e946d34bc55a2be8e8edf4bea8e5b2df7528fefac07f78c2841c0402f5f4]]]self.reward_map is the reward map. This reward map represents the reward value you get when you move to each square. The reward map is prepared as a NumPy two-dimensional array (np.ndarray), and the coordinate system is shown in Figure 4-9.

[[[00000000000000002251---b3d4bd134bd1433156c659dbb1e7803ba854d8388dbc4ca0e19ddecba52e1ae5]]]
Figure 4-9 Reward Map Coordinate System


[[[00000000000000002252---9e6be656ccc31ae270d192bb37ce3c62648d4800d7671d1c4097f8e2b82a9e28]]]Also, this time the problem is an episodic task. In other words, when the agent reaches the goal, it returns to the starting point again and performs the same task. The goal state is (0, 3), as in the code above self.goal_state = (0, 3). Also, the wall is represented by self.wall_state = (1, 1) and the initial position of the agent by self.agent_state = (2, 0).

[[[00000000000000002253---cd5029405ffd976b302e20ef70f4e152d065d3264bdd4b31f5ba1cdb588e2d3a]]]Following are the methods of the GridWorld class.

[[[00000000000000002254---51b7234ff835276cfc74c8602d6b9cb64c78d80dc4ee26cb193af2da6b67c179]]]Here we use the @property decorator to implement some useful instance variables in the GridWorld class. By placing the @property decorator on the line before the method name, the target method can be used as an instance variable. In this case, we can know the size and shape of the grid world as follows.

[[[00000000000000002255---4d5ce32fbe9bc27fbae69618e5d17402c4a3e65e0a9ec4ad3b3c308fb7fff26e]]]# can be used as env.height instead of env.height()

[[[00000000000000002256---0a515856dfc382a05f1fcfca97d097e4a2ee81a810fa6ee9c0e005ecda01d901]]]The above code also implemented the actions and states methods in the GridWorld class. This method gives you access to all behaviors, all states in turn. An example usage is:

[[[00000000000000002257---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000002258---2e8743a8f0376d1a80b371ccf8712e4d48f979c0054f1e9f8ef8dd52160dba0d]]]As above, you can access all actions and all states using the for statement.

[[[00000000000000002259---fd6c544b7c0f3c3909987a51a58e21f0057fb66a9fd51795d9ef1b5532c17cb6]]]The states() method uses yield. Like return, yield can return the value of a function, but it can also pause the processing of the function and move on to another process (when the other process is finished, the function resume processing). Thanks to this feature, it can be used in combination with iterative processing such as the for statement as shown above.

[[[00000000000000002260---36cee3b21c17af057fda4ec826432111cfb3df495dc151b9adbf1b8cfadfbcb5]]]Next, implement the method next_state() representing the state transition of the environment and the method reward() of the reward function. Here is the code:

[[[00000000000000002261---61159f7518ec73c02c10eb213467dbe73ce650630130448abebe0e82027af8de]]]# ① Calculation of destination location

[[[00000000000000002262---e138f98095991a3463203ca91c9a88c3b11eafd10013dd5c2768fa62b2d31d35]]]# ② Is the destination outside the Grid World frame, or is the destination a wall?

[[[00000000000000002263---8ac81f7e2a9b3a93d54da9f7504e72a076078cbe61b7df301fe46f3b3ef82456]]]# ③Return next status

[[[00000000000000002264---f9c39919a5c3e1ce1c36de228d4e61c2dbc3746d9e64b60c154ac178ebcfe707]]]In code 1, walls and grid world frames are ignored and the destination is calculated. Then, in ②, it is judged whether it has moved outside the frame or on a wall, and if it cannot move, set next_state = state. Also, in this task, state transitions are deterministic, so code ③ returns the next state.

[[[00000000000000002265---3a4fca1f86d5aef540a3b24c4ef7a35501fa040855f2d0113d455f6f0aa09789]]]The reward function method reward(self, state, action, next_state) has such arguments to correspond to the arguments of the formula. However, for this problem, we will only use the following states to determine the value of the reward:

[[[00000000000000002266---999300803ecdba3b76eeb96506469c8ad841c7fd010f02d62dd880b26f72b103]]]The GridWorld class also has a method render_v(self, v=None, policy=None) for visualization. The code inside is not important, so I'll just explain how to use it here. It can be used like this:

[[[00000000000000002267---b7016bd8b0817583bf34d457d2a09356f5126e72aa43b69c99374e04d1fa395c]]]
Figure 4-10 3x4 grid world visualization


[[[00000000000000002268---2d8f161556d96e5fcd850973660ff2151ffaa19803b99928759ce23c63c61488]]]After running the code above, a grid world will be displayed. The wall is drawn in gray and the reward value is drawn in the lower left corner of the square. Locations with a reward of 0 are drawn blank. This render_v method can be given a state value function as an argument. As a trial, let's draw by giving a dummy state value function.

[[[00000000000000002269---82dfde4906bcd13adee5a58b3e32d906ed283cf2bb89a125f2410d809b5f6b74]]]# dummy state value function

[[[00000000000000002270---eae92418ac97ee7d5d3901c5991077dad8319f3867672372c59cb770a40005d5]]]
Figure 4-11 Visualization of 3x4 grid world (giving state value function)


[[[00000000000000002271---7f1f53c1c7b41e071dd0241ca8e1fc3b6e489d7e17795d5356c7351803cc23e7]]]Here we have prepared V as a dummy value function. If you give a value function to the render_v method, the value of the value function at each location will be displayed in the upper right corner of the square. It is also drawn in a heatmap according to its value—negative values are more red, positive values are more green.

[[[00000000000000002272---a34d72237101204ba53cf497b93ab71f1d2ec25f7fa19479991d9af4442c60f2]]]The render_v method can also give a policy. In that case, draw an arrow that indicates the action that is most likely to occur at each location. We also have a visualization function for the action value function (Q function) as the render_q method. How to use the render_q method will be explained in '5.4 Policy control by Monte Carlo method'.

[[[00000000000000002273---a65988335dab624500fff708fd75ed30be21d5328b23cf32cc1136674011fbad]]]The above is the description of the GridWorld class. The GridWorld class also implements reset() and step(action) methods. reset() is a method to return the game to its initial state, specifically to return the agent to the starting position. The step(action) method causes the agent to perform action and advance time by one step. These two methods are used when actually running the agent. The iterative policy evaluation algorithm doesn't use those two methods because it doesn't actually force the agent to act.

[[[00000000000000002274---66f9e780d93f26b7736f382027ba33dbc5399563e5dea1232f7e4487af8a6376]]]We then implement an iterative policy evaluation algorithm. Before that, I will explain how to use collections.defaultdict in the Python standard library. Defaultdict allows simple implementation of value functions and policies.

[[[00000000000000002275---bb944506f0d269811b26495eb0d47901a849da6de6199d2ff6dfdc437ad65050]]]Using defaultdict

[[[00000000000000002276---987c43084aa07d4516d0b98432d0c577ab0c21d8f99f9e8702e7c769e34cb184]]]In our examples so far, we have implemented our value function as a dictionary. For example, code like this:

[[[00000000000000002277---b4cc04f2dc66aca02d783d0afc432ce4d1a596c9333c2baf46e9aa5592b06a83]]]# Initialize the elements of the dictionary

[[[00000000000000002278---7794f6aef35ba960e14016ecbae2f67976fcb51aa8093122c8677b39585b4270]]]# print the value function of state (1,2)

[[[00000000000000002279---1ac98a3aed98216d1c3778859448918f57a05feb64e8cd16aba8054bb1c02f81]]]In the example above, we implement the value function as a dictionary with V = {}. Dictionaries are used like V[key], but it is an error if the key does not exist. So you have to initialize all the elements first like above. The defaultdict in the Python standard library eliminates this initialization effort. It can be used like this:

[[[00000000000000002280---06067ea9499fc1c79d111e84a4e094a6cb870afb85d1995b42868e9f8e49d1ba]]]# import defaultdict

[[[00000000000000002281---24db033f928894e6bcb72f3b2eddaacdf1d22674ac6066f3540241d760ce714f]]]As above, V can be used as a normal dictionary if V = defaultdict(lambda: 0). Additionally, when accessing a key that does not exist in the dictionary, a key with a default value is automatically created. In the above example, when accessing like V[state], there is no element with state as the key, so an element with a value of 0 is created at that timing. We will use defaultdict to implement the value function from now on. We also implement policies using defaultdict.

[[[00000000000000002282---ed4fc5e28c92aec3dec7e322ef2688d8ade7c0c452ec16f655f670d9c4ee59de]]]There are several ways to set the default value of defaultdict, but the simplest is to write it using lambdas. For example, defaultdict(lambda: 0) or defaultdict(lambda: 'A').

[[[00000000000000002283---cb315d317a0700f17f61d089fae357e74848ca433775e6c16fa6d8993cf181a1]]]Let's implement a random policy using defaultdict. In the '3x4 grid world' problem, there are 4 actions an agent can take, and each action is represented by [0, 1, 2, 3]. If that four actions are uniformly random, the probability of each action being taken is . So the probability distribution of actions can be expressed as {0:0.25, 1:0.25, 2:0.25, 3:0.25}. Given this, a random policy can be implemented as follows:

[[[00000000000000002284---30b86d21bbcf2419b3ecb9c760b765f28e4ca9e32cf066d37b2ac11096644d54]]]Here we implement the random policy as pi. Given a state, pi returns the probability distribution of actions in that state. In the example above, for example, we print the probability distribution of actions in state (0, 1). As you can see from the above result, the distribution is such that all actions are chosen with a probability of 0.25.

[[[00000000000000002285---696d932bf174512d5aff1f3067b46abcbcbe785440274aacc0339fb334e4b127]]]Implementing Iterative Policy Evaluation

[[[00000000000000002286---4036e676878c66382ea14eb7cbb04f6fbba8fb5c45f94277b21e94699f1e148d]]]Then we will implement the iterative policy evaluation algorithm. First, implement a method that does just one step update. The eval_onestep function implemented here takes four arguments:

[[[00000000000000002287---6b113f61db0d6e8d581e948fe77a3e54a0db5f465cb72d1b133da413b4c30c06]]]pi (defaultdict): Policy

[[[00000000000000002288---dfbcc6f11a346bbfc560e196a5fdb414642dbd4a76347199dd53c6641f5d5dc4]]]V(defaultdict): value function

[[[00000000000000002289---cac26bc4f7a3733c2705ee9296ea039e2761180c815f887584f93c08f9aabf72]]]env (GridWorld): environment

[[[00000000000000002290---9c79a5a12ffcfa2873ca429e88907d4fbe6cf5d3c778b4cb32dbd424df0e091a]]]gamma (float): discount rate

[[[00000000000000002291---4ba1f19875c017afd765bf71ee07a319e603a1e079d5581a48bfe83940c733b0]]]So here's the code:

[[[00000000000000002292---92406db9538766414ca3addd6b738639c61ce84b66586d64441208651252a177]]]# ①Access to each state

[[[00000000000000002293---c41220a027788a4aa37e818e414299d6f85041ecf77400efc8c8514ed3a4ec11]]]# ② Goal value function is always 0

[[[00000000000000002294---b5ce237dd4ac035efd6f2cdf4f0bff9d58c4403c88177db1cb1b3089491d6c44]]]# probs stands for probabilities

[[[00000000000000002295---d8ef40c3ca511e77bb384ceff9cfb364bb465915843ca0922e68ccfbbe289393]]]# ③ Access each action

[[[00000000000000002296---90653ee0613aedaf7ff9551c36bd71764f49bbdc1ab969ca7477ef92c5c7e19d]]]# ④ New value function

[[[00000000000000002297---a42b038f331d895e17a57e8bb2c4c373e009b2f8e6fa56d4dc4eefa5c6c62543]]]The for statement is used twice in the eval_onestep function. Code 1 traverses all states one by one. Graphically, this looks like this:

[[[00000000000000002298---4c6dad7504ba3a944bc18a2c8e8906e902a586481e072103e88414fe711b8f95]]]
Figure 4-12 Accessing Each State in Order


[[[00000000000000002299---a193647e994afc98301ead61524d73823c2bd3b4a77dafceadc530f6ff580320]]]Access all the states in order, as above. Code 2 sets the value function to 0 if the state state is a goal. The reason for this is that the agent at the goal is the end of the episode and there is nothing to do next. Therefore, the value of the value function at the goal will always be 0.

[[[00000000000000002300---a0b2cd69e1ddad7d6ea8010c0664213c6df56d47014bd4c5750a1da88529bb5d]]]In code 3, we retrieve the probability distribution of actions. Then, get the 'next state (next_state)' from the state transition function (env.next_state(state, action)). After that, it is applied to formula (4.3), which is the update formula of the iterative policy evaluation algorithm (repeated).

[[[00000000000000002301---10e1c7ca6365c38cb500e329085482e8154dc4651b24a75afa03afc92e36bd33]]]Comparing equation (4.3) with ④ in the code above will make the correspondence clear. This eval_onestep function finishes the one-step update of the value function. Then repeat this update. A method for that could be implemented as follows:

[[[00000000000000002302---f786787db5edb1cfd45f77f3a92b1aa601ab6ced89033f0ae4d4bfb7da326463]]]# Value function before update

[[[00000000000000002303---69a7b3a03b6cfab0f3b76aa5bbb9887d4493a497d2ad8b65282686110d819f54]]]# find the maximum value of the updated quantity

[[[00000000000000002304---99f0e3c200e9c8e55a5d9430cd8a4a28eafc84b739abdc2268f8404b568fe198]]]# Compare with threshold

[[[00000000000000002305---1163149382c59666e1e5344fb4fa8e16865bb55da5445a795d9029ea503b82ca]]]The argument threshold is the update threshold. As shown in the code above, we repeatedly call the eval_onestep function and stop updating when the maximum updated amount is less than the threshold.

[[[00000000000000002306---68dc3dcc546ca25b065aec5bedeaecf8ca3fb52586be5c1a4726fdcf0e6ac4cf]]]Now, let's perform policy evaluation using the GridWorld class and the policy_eval function that we have implemented so far. Here is the code:

[[[00000000000000002307---a10600e744143c8330f14897084db08283514bc377ecf153649ad9c7fd49b32d]]]Here we initialize the random policy as pi and the value function as V. Then evaluate that random policy to get a value function. Running the code above gives the following image.

[[[00000000000000002308---3ce7b30ea01d87ec1fd31f381e8999368875eaa781fb184149e03347028a5e6e]]]
Figure 4-13 Value function for random policies


[[[00000000000000002309---6788d5cf74d3f089d36771c5282e5b15aade424ffa39ebb5a1268621e76569da]]]Figure 4-13 shows the value function for random policies. For example, the value function at the bottom left starting point is This means that if you move randomly from the starting point in the lower left, the expected return will be . Agents move randomly and can (accidentally) get bombs. You can see that the probability of getting the bomb (reward) is slightly higher than the apple (reward). Also overall, the bottom and middle rows are all negative. You can see that the impact of the bomb is greater in those places.

[[[00000000000000002310---646596eaa6f2230f407bb7ff8bba59dd3427fe37b3c47caf0485432e9339e106]]]If you run the above code, you will get the result instantly. By using DP, you can efficiently evaluate policies. Now it's okay if the size of the gridworld grows (somewhat). However, we are still only evaluating policies. Next, we will look at how to find the optimal policy.

[[[00000000000000002311---d31b6401138c4558b4ce515da205690c9cc4fc1f4e78b7da6e602ab7efa2b399]]]policy iteration

[[[00000000000000002312---2a84ee4e5779a86f88b3e34eed5b501a122bf2ba8d29d258f2a9051bc377f6da]]]Our goal is to obtain the optimal policy. One way to do that was to solve a system of equations that satisfies the Bellman optimum equations. However, this method has a computational problem. Specifically, when the size of the state is the size of the action, the computational complexity of the order of magnitude is required to find the solution. This only slightly increases the size of the problem, and it cannot be solved in a realistic amount of time.

[[[00000000000000002313---5b10f245555e2ddbf270dd88edba4f699edbc19be98600f0a74b420b473b9707]]]Solving the Bellman optimum equation directly is often difficult. Therefore, the first step is to assess the current policy. This is because if we can correctly evaluate the current policy, we can 'improve' it.

[[[00000000000000002314---8a1792f38924b505258bf99a55ab911e8d5dacf22c8b77f5324196869fb4d3db]]]In the previous section, we used DP to evaluate policies. It was an algorithm called 'iterative policy evaluation'. We can 'evaluate' the policy. Next is the “improvement” of the policy. Here you will learn about techniques for improving policies.

[[[00000000000000002315---e2b3767044e0da130f5cf244576fc1e543d691cb7c61ebdb3e93302b4b1999d3]]]Policy improvement

[[[00000000000000002316---e2f17642a181e6854ef9508207b95bcf405266aa16d6a0d9da825b972246cea2]]]Hints for improving the policy can be found in 'Optimal policy'. Here we use the following symbols to explain how to improve the policy.

[[[00000000000000002317---1c2421ac41ea195d1b8492bce1de01f34fe7de5e9fc1104e95fcca4c41b20551]]]Optimal policy:

[[[00000000000000002318---1eea8245f2362b6e95d707da7cbd90d4b1c3c42314830735faf77484cebfbb88]]]State-value function for the optimal policy:

[[[00000000000000002319---85c223ab7f0b0dedfa115066e1da966bf0b86b65628ec3c129b5f24943514e55]]]Action-value function for the optimal policy:

[[[00000000000000002320---c1f69775d1ed8fb9e97b166d1afce724e446c5ac369596222c260083918f6584]]]Start by reviewing. As explained in '3.5.2 Obtaining the Optimal Policy', the optimal policy is expressed by the following formula.

[[[00000000000000002321---76e169fdb653176f9149cb9c8ac1bfdac61d9815144cf5248f7abda799926bad]]]According to the above formula, the optimal policy is determined by the action that takes the maximum value. This computation determines the best action among the local candidates. For this reason, the policies obtained by equations (4.4) and (4.5) are also called 'greedy policies'.

[[[00000000000000002322---2cca74568fc26f75920f1ffa2855a9e6273fb79e550b03d62e7b97de6227e0cc]]]From equation (4.5), if we know the optimal value function, we can find the optimal policy. But to know, you need an optimal policy. This is a 'chicken and egg' problem. In other words, to know , you need to know .

[[[00000000000000002323---c58ebb540c8b729fa9d8259f3b8a941667d85946e5351688f0bbce0682671be1]]]Formula (4.4) is for the optimal policy, but here we consider applying formula (4.4) to 'some deterministic policy.' That is, the formula

[[[00000000000000002324---ea4c29297f049b87da2f3fede801b1da41a39542e00d63cd450a1b775b70b292]]]here,

[[[00000000000000002325---2877110fbaeb6c4b62836d993c016053b31ffb278f286582b70c4fda18bff989]]]Current measures:

[[[00000000000000002326---ca36bf7fbc696f7fd0d33d63888eded735d6604bf180186737d5a1a6ee2884bc]]]State-value function in policy:

[[[00000000000000002327---84bb22c2d44772fd02b65f8a6d0a6539ba853afdf0b9af9603898ddcd29d681d]]]New measures:

[[[00000000000000002328---56c66ce4438a01ef1fd3cb262cf0d55c0b58fa47f082e4e7bd0d9c3b19fb979c]]]It is written in . Also, we call the updating of the policy by Eq. (4.6) or Eq. (4.7) 'greedy'.

[[[00000000000000002329---91182be1bab6d3b10f996cdfd950115312eded6756f8e2630b9ccb1a06648f56]]]So what are the properties of a greedy policy? One thing to say is that a policy is already an optimal policy if is the same in all states. Because if (4.6) does not change the policy, then

[[[00000000000000002330---b148ebcec83a6f52aefb70b3d4baf0e1ac7810655ed2bc8e3faad1737f2f4456]]]The above equation is exactly the equation that the optimal policy satisfies. So if greedying doesn't update in all states, it's already an optimal policy.

[[[00000000000000002331---da1ba6bad2adfecae5e43fffd5cd838a1fd604fd740a3c47104f106bf0e608a9]]]So what if greedying updates the policy? That is, when policy and strategy are different. In that case, we know that the policy is always better. More precisely, holds true in all conditions.

[[[00000000000000002332---a27240d4974abff9e9a8fe0fb6e4a54c675c2d55356b5a87a6859e64c1536735]]]Here, we only show the fact that the policy is improved by formula (4.6) or formula (4.7), and omit the proof. The mathematical basis for improving policies is given by the Policy Improvement Theorem. For the policy improvement theorem (and its proof), please refer to [5].

[[[00000000000000002333---0efc30a3195c61f55fb12f8e4b0f3d7b8d42558e803befb896a372811a06af4f]]]To summarize the above, by making the policy greedy

[[[00000000000000002334---087f78bef98aa7994d4a5cb68cbfeaf77a49df7aae86bc0562d3169358c5fe97]]]policy is always improved

[[[00000000000000002335---6380af37eac96417216d4b3b2c84a85cfffad4d3435582f33565908adeb1f965]]]If there is no improvement (renewal) of the policy, it is the optimal policy

[[[00000000000000002336---836eeea1e55ed06773054e72e15fdb97bcfbf8b9766ad1f11aa712cdee1a8469]]]That's what it means.

[[[00000000000000002337---db29135dcf4d91829645540d5dd59c9f198bcddd7365f00c26800e9c7d975caa]]]Repeat evaluation and improvement

[[[00000000000000002338---aa8e92a9b0d1e4ddf56c69ba58c3ac423dde2a7d5bcca9ccf99e8429357ae53e]]]It turns out that by greedying the policy—either by updating Eq. (4.6) or Eq. (4.7)—the policy is constantly improving. Here, we assume the formula (4.7) based on the state value function and proceed with the discussion. In equation (4.7) a state-value function is used. We implemented an algorithm to evaluate the state-value function in the previous section. Now we have all the key ideas for finding the optimal policy. That method is represented in Figure 4-14.

[[[00000000000000002339---d6e861a24cfc59d50eb36c419c9d2ea9c794e5c11ff9103af7e7fc72744f866d]]]
Figure 4-14 Policy improvement process


[[[00000000000000002340---c57105418e274f86e3440ec37e87d0fdb2bb9926d88efdf025effb5efd1cd591]]]If you write the flow of processing in Figure 4-14, it will be as follows.

[[[00000000000000002341---644915bebbbf1d586440b8a8a0cd6e5a1f818d5bcd550ad16ae1329974d813e2]]]Start with the first policy. Since the policy can also be a probabilistic policy, it is written as not

[[[00000000000000002342---fb016a0d88ea9ad319c3b44d9a1a44ee93ce908ab3bb0cfe231d832784d3520f]]]Next, evaluate the value function in the policy to obtain This can be done by an iterative policy evaluation algorithm

[[[00000000000000002343---fced2077a60bfdb73908c0336f95a1b7ab99f9ef74bdb9278534410960ea4e1b]]]Then, greedy using the value function (updating the policy by Eq. (4.7)). As a deterministic policy, we get

[[[00000000000000002344---d6707c065bd26a7e69ba7b443bf4c08c915c265f08daeefebd9837fb789c4df5]]]Then repeat this flow. Continuing this, we reach a point where greedying does not change the policy. That policy is the optimal policy (and the optimal value function). An algorithm that repeats this evaluation and improvement is called Policy Iteration.

[[[00000000000000002345---9dcd6df776fbe9168c9fa93b975d98d932d2b4cfb3caa160392e71753d47998c]]]The environment is represented by state transition probabilities and a reward function. In the field of reinforcement learning, the two are sometimes referred to as 'environmental models' or simply 'models.' If the model of the environment is known, the agent can evaluate the value function without taking any action. And we can also find the optimal policy using the policy iteration method. The problem of finding the optimal policy without the agent taking any actual action is called the Planning problem. The problem addressed in this chapter is a planning problem. On the other hand, reinforcement learning is often performed in a problem setting in which the environment model is not known.

[[[00000000000000002346---5a6ebb01cde85f35aabf2e8647c2f8db12d439669aeb1c317781f06ab200c73d]]]Policy iteration implementation

[[[00000000000000002347---0d4f3957ab86ef3e1807b44772ac6a3332b13091dd510aa4c6da935a0efe4ee7]]]Here we solve the same '3x4 grid world' (Fig. 4-15) as before. The goal is to obtain the optimal policy using policy iteration.

[[[00000000000000002348---5f4c9e654b15ac89b37b6b571ad609ba87b625aacc4a036122ab31c68ca5d629]]]
Figure 4-15 3x4 grid world


[[[00000000000000002349---dc1e6cae5b8d6a80c27b5f2d6d98aa0b0d24cf0a0187711fe74718b67dcc1bd6]]]We have already implemented code to evaluate policies. The rest is “policy improvement”. Here we will implement the code for that.

[[[00000000000000002350---e2b3767044e0da130f5cf244576fc1e543d691cb7c61ebdb3e93302b4b1999d3]]]Policy improvement

[[[00000000000000002351---77cb33b080cbd9a20896517f442ca2afaeb397ab1a2c6d5ca1e62f0bc361fd6c]]]To improve a policy, get a greedy policy for the current value function. This is represented mathematically as:

[[[00000000000000002352---be904f4ac87a901d27a84d437da84e85b5eb079634e153eab1e1d72b2a0d4393]]]Also, in this problem the state transitions are unique (deterministic). So greedying a policy can be simplified as follows:

[[[00000000000000002353---e46f36b89cf57b31f91e08158b3f900cf691d2973a403fbfbde55ce7ebf01d55]]]As per equation (4.8), there can be only one next state. Now let's implement a function that obtains a greedy policy based on equation (4.8). Here, as a preparation, we implement a function called argmax. It takes a dictionary as an argument and returns the key with the largest value in the dictionary. If you show how to use it first, it will be as follows.

[[[00000000000000002354---b46a5b855992421e64f656f33f63c85c4930334cb76b1dac68f12a66034b334b]]]# 9.9 is max (key is 2)

[[[00000000000000002355---b6d3f655e595303c5c0010d417bde1f315d4e9586abe96e4d648588c811037f4]]]Output result

[[[00000000000000002356---a2c266a727d3f14d0eb52eb4e6ebd6965d058a1f45caff6d05adfb83f1fe319a]]]Returns the key with the maximum value for the dictionary, as above. This function can be implemented like this:

[[[00000000000000002357---342f792ac4707794e19e3097491c7c2f72a06d746c9a013778ce6781edadbb88]]]The code is pretty simple. Extracts the maximum value for d in the given dictionary and returns its key. For simplicity here, if there are multiple maximum values, the last key is returned. Therefore, one key is always returned as the return value.

[[[00000000000000002358---ac0b8e8f349e353dbc22a7b7c885a6923e94dfe0ff6a4a773a70d76835c978a3]]]Now implement a function to greedy the value function using the argmax function. Here is the code:

[[[00000000000000002359---28717988d71b397e731d64aface75bf41f2436f4cb9ae72b4dcfb4895821e7f2]]]The greedy_policy(V, env, gamma) function takes as arguments a value function V, an environment env, and a discount rate gamma. It then returns a greedy policy using the value function V given as an argument.

[[[00000000000000002360---2b6aae3274e88c88cdf2d7d3ce1e8356bc02c82094d2bc11391a7067248791c8]]]In code 1, we compute (4.8) for each action. Then, in ②, use argmax to extract the action (max_action) with the maximum value function. Finally, generate a probability distribution so that the probability of max_action being chosen is 1.0. Then set it to the probability distribution of actions in the state state. The above is the function to greedy the policy.

[[[00000000000000002361---619e942f20f7812c7a827b89de0bfee0c404e3f171c148f15c15af9570633eff]]]Greedying a policy makes it deterministic. Therefore, you can specify only one action like pi[state] = max_action in ③ in the above code. However, the policy_eval(pi, ...) function we have already implemented for policy evaluation takes a probabilistic policy as an argument. Therefore, it is implemented here as a probabilistic policy.

[[[00000000000000002362---db29135dcf4d91829645540d5dd59c9f198bcddd7365f00c26800e9c7d975caa]]]Repeat evaluation and improvement

[[[00000000000000002363---e0916a2943580e0c1ccc55cdacef0cdb453a6fa53d4fa13f087f2638f6a4115d]]]Now we are ready to implement the “policy iteration method” that repeats evaluation and improvement. Here, we implement the policy iteration method with the function name policy_iter(env, gamma, threshold=0.001, is_render=False). A description of each argument follows.

[[[00000000000000002364---34765a0ca40bb3061069055a669fa18fffaade51c8f601c599307cdf8b82619f]]]env (Environment): environment

[[[00000000000000002365---9c79a5a12ffcfa2873ca429e88907d4fbe6cf5d3c778b4cb32dbd424df0e091a]]]gamma (float): discount rate

[[[00000000000000002366---41aa3640cd257314c5cb2052b359b8a9ff04210ef0da2fcfa7fea4c1ca260fbb]]]threshold (float): Threshold to stop updating when doing policy evaluation

[[[00000000000000002367---ab1994adc6705e5990deacce2296a04f0ba97c60af869d4b2941a9e0687c9836]]]is_render (bool): Flag for whether to render the process of evaluating and improving policies

[[[00000000000000002368---4ba1f19875c017afd765bf71ee07a319e603a1e079d5581a48bfe83940c733b0]]]So here's the code:

[[[00000000000000002369---ff27bcdfd115fa2580bb5dc31a7872a19d33c9af872e319c8b6137fcc3cb02f1]]]# ① Evaluation

[[[00000000000000002370---2767c802a982abc5a5be8c34e607c13992cadefe8f46dcebad02d977f48d341d]]]# ②Improvement

[[[00000000000000002371---ada03d66bc7186b44f9845e4bd5b81ee03180f7f11c2e56a4323b6118e61f395]]]# ③ Update check

[[[00000000000000002372---89631c6e92c2f228259a2db1ca651a367bf14a98b1060a02b9aaacdd4261aefd]]]First, we initialize the policy pi and the value function V. They use defaultdict to give initial values. For policy pi, the initial value is set so that each action is evenly selected.

[[[00000000000000002373---acb580deb55862c18fa24646d9266d46bd081ce2d081e1629b9d4de6eb8858b2]]]The main points in the above code are 1 and 2. In (1), the current policy is evaluated and V of the value function is obtained. Next, in ②, we obtain a greedy policy new_pi based on V. Step 3 checks whether the policy has been updated. If it is not updated, it means that the Bellman optimality equation is satisfied, and pi (and new_pi) at this time is the optimal policy. In that case, exit the while loop and return pi.

[[[00000000000000002374---b927d7b9ed555d3867884687f3d55dc2c6adcd0a1812cce32001708961704617]]]Now, let's actually use the policy_iter function to solve the problem.

[[[00000000000000002375---0087cd1898874304f85d6d1a6fc04ff8d0154554cd222fe8077abf3e8f7f1957]]]Running the code above visualizes the results of each step of the policy iteration. The result should look like Figure 4-16.

[[[00000000000000002376---53af1517f60c7390db116b95db3caa7abfc9fd626cb9c16da4729dee7db79bef]]]
Figure 4-16 Results of the first and last value functions and policies (value function values and policies are shown in each square)


[[[00000000000000002377---4d1c45bba9da4e692e3b2207aa8b200cec09fc9204ecd20c35e4b6615af4b108]]]As shown in Figure 4-16, we started with a random policy, where the value function of each square is predominantly negative (red). However, if you continue to update, after the 4th update, all squares except the goal point will be plus (green). Also, if you look at the direction of movement (arrow), you can avoid bombs in each square and go in the direction of picking apples. This is the optimal strategy.

[[[00000000000000002378---a293615e7ce2209fc14fced058a148a2638bb136de51dacbbdfa05bcfa928849]]]congratulations! We were able to arrive at the optimal policy using policy iteration. In other words, it means that '3 × 4 grid world' was completely cleared.

[[[00000000000000002379---3620cfc2eb6fea5c234c8a2ebda5941db50557e1ca7b525606e12f01b9879ec4]]]For the '3x4 grid world' problem, there are two deterministic optimal strategies. One is the strategy shown in Figure 4-16. The other is a policy that goes 'up' in the lower left square for the policy in Figure 4-16. From the lower left square, you can reach the goal in the shortest time by choosing either 'right' or 'up'.

[[[00000000000000002380---6fce3824ae8b81b8d4ad0b5f86f6345c6f12c8ef9a3aff6a77b25f47c890da0d]]]value iteration

[[[00000000000000002381---880441758a0471dc7c01dafceb15bc3b34af36f81d78f66fbc1b6dba11e5268d]]]We were able to obtain the optimal policy using policy iteration. As a refresher, the idea of policy iteration is to alternately repeat the two processes of 'evaluation' and 'improvement' as shown in Figure 4-17.

[[[00000000000000002382---9e3f464c6bbbb5be0173b618ee217931af9621e2eaec849b93606a9a97e755f7]]]
Figure 4-17 Flow of policy iteration method


[[[00000000000000002383---56ab0eb6fea52031070901f3df7b50213091fbcb1ab0414885fce404a3c8cf0e]]]As already mentioned, in the 'evaluation' phase, the policy is evaluated to obtain a value function. And in the 'improve' phase, we get an improved policy by greedying the value function. By repeating this alternately, we gradually approach the optimal policy and the optimal value function. If we were to represent this process graphically, we could draw it as follows:

[[[00000000000000002384---cf786c2b276b0d207d85bfc53eb53797a2eb81045bdba57c605aeeba1a66f3ad]]]
Figure 4-18 Transition of value function and policy by policy iteration method (image diagram)


[[[00000000000000002385---a4dc4203a7f17a8cf45bf219e5a3bcaf1e7ec2fa0c58681b199d139ad88d6a00]]]Figure 4-18 shows the two-dimensional space that value functions and policies can take. Originally, the space is multi-dimensional and complicated, but here it is illustrated as a two-dimensional space for intuitive understanding.

[[[00000000000000002386---7f40939fd035486e86adef98386d03865a5265299a6360277650f444d64e8cad]]]Two straight lines are drawn in Figure 4-18. One is a straight line. This straight line is where the arbitrary value function and the policy's true value function meet. And the other straight line is where the policy obtained by greedying the value function coincides.

[[[00000000000000002387---6bedaaeb0e4f27ce819b19b80c3055dfb910fc0e5f6703e46eeb9967f4d9975c]]]Policy iteration alternates between 'evaluate' and 'improve'. In the 'evaluation' phase, the policy is evaluated and obtained, which corresponds to moving along the straight line in Figure 4-18. On the other hand, in the 'improvement' phase, it is greedy. This corresponds to moving along the straight line shown in Figure 4-18. By repeating these two operations alternately, and are updated, and eventually the two straight lines overlap and arrive at .

[[[00000000000000002388---926949d627fa672f3d239f82eff8418482285699faa06f11f4a6cdf99a9640be]]]Here the policy is represented as deterministic rather than probabilistic. This is because by greedying the value function, one action is determined for each state.

[[[00000000000000002389---75ffef1b3ef0f35a78ed180668f6ba790bb5cc7de849461f65898709a2195257]]]Policy iteration zigzags back and forth between two straight lines to reach the goal, as shown in Figure 4-18. Of course, there are many possible variations on how to reach your goal. For example, a trajectory such as:

[[[00000000000000002390---eaf486720feefc4f4611b655f53601e37a0c3d9b340ad1542ffa18a39e518f8c]]]
Figure 4-19 Another transition example of value function and policy


[[[00000000000000002391---5076ffdb738da3d2a3d14a751f6172dccfb943dc5e6fc3a678e8dedebe29b316]]]Figure 4-19 shows a zigzag trajectory, but it turns around before reaching two straight lines. In fact, in an algorithm (framework) that alternately repeats the two tasks of 'evaluation' and 'improvement', it is necessary to go to the 'improvement' phase before completing 'evaluation' and By going to the 'evaluate' phase, a trajectory toward the goal is created, as shown in Figure 4-19. This is known by the concept of Generalized Policy Iteration. As the word 'generalized' implies, it is an idea that is general and has broad applicability.

[[[00000000000000002392---dbade4c71c1da53ac22de75da71e706139d148118c9de88078e1be35b5014b52]]]Algorithms that alternately repeat the two processes of policy evaluation and policy improvement have a degree of freedom in terms of the “granularity” of evaluation and improvement—the degree of accuracy of evaluation (or improvement). Specifically, the 'evaluate' phase doesn't have to be completely updated to , but only needs to get closer to . Similarly, in the 'improve' phase, you only need to move towards greedy (only some of them are greedy).

[[[00000000000000002393---0909353892b57260dc6e45beae2ac128d60dbe24415a40d78e557e69196c6c89]]]In the policy iterative method, we 'evaluate' and 'improve' perfectly (precisely speaking, the iterative policy evaluation algorithm stops updating at the threshold, so it is not a perfectly correct evaluation, but it is still an almost correct evaluation. ). This policy iteration method is one implementation of generalized policy iteration.

[[[00000000000000002394---b6938be3ce12e4280caa17815418f0a97c402906b80a454ac3774c98f91e5edb]]]In the policy iteration method, the phases are switched alternately, performing “maximum” each of “evaluation” and “improvement”. Then, what happens when 'evaluation' and 'improvement' are each performed 'minimally'? This is the idea of Value Iteration.

[[[00000000000000002395---eb78fdbf51e90e462aa453dd09f1316a67bfa8541c859f34db53b64f0563f495]]]Derivation of value iteration method

[[[00000000000000002396---296f323dcf0645ea70586d786797e9bb0770b454b813f71a25eadb3127a1b33d]]]Before we get into value iteration, let's review policy iteration again. In the 'evaluation' phase of policy iteration, we update the iteration value function, as shown in Figure 4-20.

[[[00000000000000002397---146f1908b10ea9488226ae76e0f0ca6b3485a0524c558ae970a48a9be9fcc923]]]
Figure 4-20 Update by Iterative Policy Evaluation Algorithm


[[[00000000000000002398---daac977ce6b91274cba15317acf7134691c9c02f651a109660128e107a3e6ba6]]]We update the value function for all states many times, as shown in Figure 4-20. Once the update work is completed, we move on to the 'improvement' (greedy) phase. At the other end of the spectrum is an update strategy that updates just one state once and immediately moves to the 'improve' phase. This is the core idea of value iteration. Graphically, this looks like this:

[[[00000000000000002399---7881488e85e79f6118bf12c3e9e0a91daae8483afa99698c26568ccca5ac7b25]]]
Figure 4-21 Flow of repeating improvement and evaluation in order in each state


[[[00000000000000002400---4664629128ddca8c79d9b1572814f1a63e250130003c81fad79e2e7a575ea059]]]In Figure 4-21, as soon as the improvement phase for one state is over, we move to the evaluation phase. In the evaluation phase, we update the value function in one state only once. After that, improve another place (the second place from the upper left in the figure) and proceed in the flow of evaluation.

[[[00000000000000002401---dfc605e0950cce376cf6d8395fdca67a860b706563da2d1f8fbdf3ea927af65a]]]In Figure 4-21, first improve the upper left location (state). In this example, we start with the refine phase first, but since the two phases alternate between refinement and evaluation, there is no loss of generality whichever comes first.

[[[00000000000000002402---81af5d14134889e48014db16ce06105d0e891784cfd6b6d85a000ddf8ce9535a]]]Let's organize the ideas in Figure 4-21 using formulas. Start with the improvement phase. The greedying performed in the improvement phase can be written as follows in the formula.

[[[00000000000000002403---fee24c3151c21b59be6377c67951cb1d5d594a4a0c8ab3a66beca71def1afe39]]]Here, we will express the current value function as Using the current state, the reward, and the next state, as in Eq. (4.8), it is calculated by Since the action is determined by , it can be expressed as a deterministic policy as

[[[00000000000000002404---78708e1001905f9b8143b7494052634ff423ac09017f6ec2fe1214fcd1ec07a0]]]Next is the evaluation phase. Assuming that the value function before update is the value function after update, the update formula by DP (iterative policy evaluation algorithm) is expressed as follows.

[[[00000000000000002405---be84f8207bd0728d126abdc9e86c4eb7ae95e6262a9bea71b94e578ea1265212]]]In equation (4.9) the policy is written as a probabilistic policy. However, once through the 'improve' phase, the policy can be expressed as a greedy policy. A greedy policy is a deterministic policy because only one action with the maximum value is chosen. (4.9) can be treated as a deterministic policy. Then equation (4.9) can be simplified to

[[[00000000000000002406---ed2057284f9789fb18fc5db31fb22b23bb5fd26920257e4974f5273af261e284]]]This is the update formula for the value function in the 'evaluation' phase. Now, let's write equations (4.8) and (4.10) in succession.

[[[00000000000000002407---18752aa68d076fc7dd92e121389edaec2813039b456fc010f2a08cc92bb5585e]]]
Figure 4-22 Calculations Made During the Refine and Evaluate Phase


[[[00000000000000002408---b812a4e2d71e42378be7c462c481c065618c8eafb3d40e126c49e082d4a9104e]]]Looking at Figure 4-22, we can see that the same calculations are duplicated during the Refine and Evaluate phases. More precisely, after doing a calculation to extract a greedy behavior in the improvement phase, we do the same calculation again using that greedy behavior. This duplicate calculation can of course be combined into one. Written as a formula, this looks like this:

[[[00000000000000002409---1e5644281bbb204ca51792bb7603cbea54d9c6e892b363df4b2449cbcebdb874]]]We directly update the value function using the max operator, as per equation (4.11). You can see that formula (4.11) eliminates the duplicated calculations in Figure 4-22. Also note that the policy does not appear in Eq. (4.11). In other words, we are updating the value function without using any policy. Therefore, the algorithm that obtains the optimal value function by Eq. (4.11) is called 'value iteration' (the word 'policy' is omitted because no policy is needed). The value iteration method performs 'evaluation' and 'improvement' at the same time with this one formula.

[[[00000000000000002410---a2f48afbba8541aef2388088fe3f9ded7578a52b249e2a7e1db32a267d120a3c]]]The Bellman optimum equation is given by

[[[00000000000000002411---0183b3f9cb64a640ecba56a0e6ac26fb6fe1845511ecc4d098875e6d4d00c229]]]Compare this Bellman optimality equation with Eq. (4.11). Then, we can see that formula (4.11) is the 'update formula' of the Bellman optimum equation.

[[[00000000000000002412---dd6a8e54254ca28394c19ec7789a3045379f410a4aac8862a80b05dc734fb5be]]]Note that the update formula of formula (4.11) can also be expressed as follows.

[[[00000000000000002413---9ac767cdb4b04ee170a83a1831d42f2dd9e65cbcfd33c083dac43b43daf751b8]]]As above, the value function updated times is represented by the value function updated times. Value iteration is an algorithm that updates the value function in order starting from . It satisfies the characteristic of DP, 'never do the same calculation twice'. That is, each of is computed only once (e.g., is not computed twice). Therefore, the value iteration method is an algorithm using DP (belongs to the generic algorithm DP).

[[[00000000000000002414---d81a5a43053fe9abc4d02553a34f65b5de130aa58a654d7023bccde0970f3480]]]The value iteration method can be updated an infinite number of times to obtain the optimal value function. But the reality is that you have to stop updating at some point. One way to do that is with a threshold. Set a threshold in advance, and stop updating when the amount of updates for all states falls below the threshold.

[[[00000000000000002415---19ff80540bf8ef574fea2b32c1834990ca74377075f27509466528a69e08e533]]]If it is still possible, then the optimal policy is obtained as follows.

[[[00000000000000002416---d87645d0cf665956faf93aef882e5ba70671f9b152585672efa2a1bdf53ac644]]]As shown in equation (4.12), if we find a greedy policy, it will be the optimal policy. This is value iteration.

[[[00000000000000002417---be06f90fa5931365d9267aeaadb902145ca879312335a768243c9eb9df2be70d]]]Value iteration implementation

[[[00000000000000002418---c971e182410254a527cfb5eae1543636602dfe0d7f52e05d2d1c3c329ab9c7f4]]]Next, we will implement the value iteration method. I will solve '3 × 4 grid world' again this time. Since the state transitions are deterministic in this problem, the value function update formula can be simplified as shown in Figure 4-23.

[[[00000000000000002419---4b4f1a059b8233d87b8d1d907a886b48c71a55887c7cb46a1e8a5a4d208f8ed2]]]
Figure 4-23 Update formula by value iteration method when state transition is deterministic


[[[00000000000000002420---f94790f0dc33a4489125aee8c74716045ae21fb5cff0fadd9ee18ef969200915]]]Now let's implement it. First, implement a function that updates (only once) according to equation (4.13). Here is the code:

[[[00000000000000002421---d7915cd52576a104cc1ba01ce778b88387a03226dca343ceb6777302d0930041]]]# ① Access all states

[[[00000000000000002422---22bc1d519cc5d4dfb12e4fb9906c3e3e8709ab12d1feae902e18dc10a8620183]]]# the goal value function is always 0

[[[00000000000000002423---5ae30e316b96242901df44a9ac06b97128b925f6bc509d696d23bbad5e90ba06]]]# ② Access all actions

[[[00000000000000002424---b31c1e678e510a9e22b5b78da6b280bdbd126b37e7da04224302f75faab98694]]]# ③ Number of new values

[[[00000000000000002425---fe944a0d457ce37dbd28b30cc566a81b829d8e5313e5658d40eeffb4c177c689]]]# ④ Get the maximum value

[[[00000000000000002426---625a5921fe5b01a717cf3d0f45de3489563d41ed1a56c022ae5cc7d801222d1a]]]The arguments of the value_iter_onestep function receive the env of the V environment of the value function and the gamma of the discount rate. In the code above, point 1 accesses all states in order, and point 2 accesses all actions in order. Code ③ calculates the contents of the operator of expression (4.13), and ④ extracts the maximum value and sets it to V[state].

[[[00000000000000002427---3b5d4d5d2b0a327fb57cf55b7595408cdbcbcbd42f764e80d7a68c6b2de15466]]]After that, keep calling this value_iter_onestep function repeatedly until the update converges. The function for that is implemented as value_iter like this:

[[[00000000000000002428---f786787db5edb1cfd45f77f3a92b1aa601ab6ced89033f0ae4d4bfb7da326463]]]# Value function before update

[[[00000000000000002429---69a7b3a03b6cfab0f3b76aa5bbb9887d4493a497d2ad8b65282686110d819f54]]]# find the maximum value of the updated quantity

[[[00000000000000002430---99f0e3c200e9c8e55a5d9430cd8a4a28eafc84b739abdc2268f8404b568fe198]]]# Compare with threshold

[[[00000000000000002431---c9ca10f9282ec10a939e337a53b3ed599db8d2bdf5cc2fa3d86778944a301d25]]]In the value_iter function, we repeatedly call the value_iter_onestep function we implemented earlier. The number of iterations to update is determined based on the amount of updates in the value function. Specifically, updating is continued until the maximum update amount of the value function is smaller than the threshold. Also, the value_iter function receives a boolean is_render as an argument. When is_render = True, render the values of the value function on every update in the while loop.

[[[00000000000000002432---ac9660b67106c8eeb74985f8ccd2ebf1be67bfc7e7f5e36221b9d2d8e547ac45]]]Now let's use the value_iter function. The code looks like this:

[[[00000000000000002433---2a9454df4768f6126d25687fdcc3284ba9c3f5902da76fde7a8049c68866b546]]]First, use the value_iter function to get the optimal value function. Once we know the optimal value function, we can obtain the optimal policy by greedying it (see equation (4.12)). Its implementation is already implemented in a function called greedy_policy. Running the code above gives the following result (image):

[[[00000000000000002434---3b8a1b3542ddba949b580f72c5cef63445ec3bec869771bf7f3e7939ec70b543]]]
Figure 4-24 Initial and Final Value Functions with Value Iteration


[[[00000000000000002435---2f61ff75511a2a3b67f9c2eae9481dc890af9d61ee37d20274bdcefbe1496d67]]]The value function initially starts with a dictionary with all zero elements. And as shown in Figure 4-24, the value function converges after 3 updates. That is the optimal state value function. Also, if we obtain a greedy policy based on the optimal state value function, we obtain the policy shown in Figure 4-25 below.

[[[00000000000000002436---6a3665bd9ea04b0f244209f4c7d9c6c0b7a0182d4d701efff0b2b49179daad35]]]
Figure 4-25 Optimal policy obtained from the optimal state value function


[[[00000000000000002437---6105cf18208c28244f8a03e808a36f10a7bc9048450c300345bddf3ad929b4cd]]]Figure 4-25 shows a greedy policy for the optimal state value function. And this is the best policy. Certainly, we have obtained a policy to move to the 'apple' with a reward of 1.0 in the upper right. We were able to efficiently obtain the optimal policy by using an algorithm called value iteration.

[[[00000000000000002438---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000002439---8f0e7ada43f23b5a8f33f400cb163ef612f08308c6e47f164339357c43c67d7a]]]In this chapter, we learned how to obtain the optimal policy using dynamic programming (DP). Specific algorithms are policy iteration and value iteration. Policy iteration alternates between two phases: evaluation and improvement. During the evaluation phase, the DP is used to evaluate the value function. Once the value function can be evaluated, the policy can be improved by greedying the value function. If there is no improvement, it is only in the case of the optimal policy.

[[[00000000000000002440---898cdfdba5f01dfa12d627f3a6e1ae76d70d72044cfb129f2306d43358320b1c]]]The value iteration method is a technique that combines evaluation and improvement. Specifically, we update the value function with just one formula: By repeating it, we can arrive at the optimal value function (if we know the optimal value function, we can also obtain the optimal policy).

[[[00000000000000002441---a12113bf12ca97a57e4e584efcf341b489f5fce0e2fd708c1bc3fee175f06c2c]]]In this chapter, we implemented two algorithms: policy iteration and value iteration. We then applied it to the 'grid world' problem and obtained the optimal policy.
