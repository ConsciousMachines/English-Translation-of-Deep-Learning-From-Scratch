

[[[00000000000000000000---e93fe08e7ccbc4c3536f1e40a309957727c2390e68e17ab4e1e7af83654bbc1a]]]step 1

[[[00000000000000000001---7ce077256a76aeb90bf0e8fbb4d401f942450245c84b7c59d9aedc2af2b9ac0d]]]variables as boxes

[[[00000000000000000002---19859225333f2eee886db7bfee6ac6755fa7f47247af4f157061d0a226679f9e]]]The first step in this book is to create the 'variables' that are DeZero's building blocks. This variable is the most important part of DeZero. In this step, we will consider the function of the variable and implement it to satisfy the function.

[[[00000000000000000003---d49c9e158e5cd64540745fd12bd6a82d8298d0b5e09832da3a6e31157c54f9ca]]]What is a variable

[[[00000000000000000004---03769fa63d1652dea19cc076859daaa71fe8f5e169ff12dc39aa929bba58c41a]]]Quick, what is a variable? If you open an introductory book on programming, etc., variables are explained roughly as shown in Figure 1-1.

[[[00000000000000000005---1bb74d8023de6c4dc3e715f7a0cb7a5fb0352280f067db30df514c0bab8ddbdb]]]
Figure 1-1 Example of explanation of variables


[[[00000000000000000006---1efe6dd1d601479f2e7ffcfc69dbf87ea74a82300e6a7565837aa6d40a63e4d7]]]As in Figure 1-1, there is a picture of putting data in a box, followed by an explanation that the box is a variable. This description of the variable as a 'box' illustrates the nature of the variable (to some extent) well. To summarize:

[[[00000000000000000007---9b96e9c3cfcfbab59ed7a85bfe89034f397dc7f4f54087473116cebbfe7c1755]]]Boxes and data are different things

[[[00000000000000000008---baf0401111ab0deaa4b1f086dc59f4724e9b6e71dc26319c86001325d417e689]]]Box contains data (= assignment)

[[[00000000000000000009---1a1358042f6de6c5b2cc4ca2e79475c4934d98a38bbece0fd49cda8e5e8cdac0]]]You can see the data by looking inside the box (see =)

[[[00000000000000000010---1e2ef3d39a2baf26cc0da4824335368b799bd7c4d3604b78b9f7a86ab13a4940]]]It can be said that Now, let's implement DeZero's variables to match this 'box' image.

[[[00000000000000000011---83176565d6b308f90120b5537d6acbe84a71c399620465059f3fca489bfcaec8]]]Implementation of the Variable class

[[[00000000000000000012---4e734cb9ad04891e8c6618945863c76dd1130ab42cf2fd84bc41e358f3a324ab]]]A variable is variable in English. Therefore, we will implement DeZero's variable as a Variable class. By the way, in Python, it is a common coding convention to capitalize class names. This is stated in the Python Coding Standards called PEP8.

[[[00000000000000000013---9ffe4916921b898f21940d1819478de2647778eb5c3482a8f96b76a0e841b157]]]Now, let's implement the Variable class as a 'box'. If we were to write that functionality in minimal code, it would look like this:

[[[00000000000000000014---fe227e64f8606569ab7d3801c2cec2b686242f082452712514ff1c045703099b]]]As you can see, it just sets the instance variable data to the arguments given in the initialization. It's a very simple code, but now the Variable class can be used as a 'box'. Because the actual data is held in the variable data. This will become clearer when we look at the following usage example.

[[[00000000000000000015---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000016---15df1c0c7622bb779cfa1741f6b15c78654fc7b301356f13e8f996f9e3aaf427]]]This example uses a 'NumPy multidimensional array' as the data to put in the box. Then x is a Variable instance and the actual data is in x. In other words, x is not data, but a being with data—a box into which data is packed.

[[[00000000000000000017---1576deb7b211a2d11c9d33fc9a9c26dc5ae67a7df75e2d8103c14f757ecc873f]]]In this document, when code is posted, the corresponding file is shown in the upper right corner. In the example above, steps/step01.py is written in the upper right corner of the code. This means that the code exists in steps/step01.py in the book's GitHub repository. Also, in this document, there may be cases where the file name is not described in the upper right corner of the code. In that case, it means that the corresponding file does not exist.

[[[00000000000000000018---a3db18eb16e37d8b6e09f6949c44e8d202d804b91257dda71ce52a7745dda5b3]]]Machine learning systems use 'multi-dimensional arrays' for their underlying data structures. Therefore, DeZero's Variable class is designed to handle only NumPy's multidimensional arrays. The class of multidimensional arrays in NumPy is numpy.ndarray (np.ndarray). This instance can be created by the np.array function as shown in the code above. In this document, numpy.ndarray instances are simply referred to as ndarray instances hereafter.

[[[00000000000000000019---13c652c8666738b5b501b24d44717fb91d4f3d8fb4b055fb24d30bfe2cf0dba5]]]Next, try substituting new data for x in the code above. It can be written as:

[[[00000000000000000020---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000021---d0e05fc94f35b5fed4bbe02d6d4666421685bc35d54b7c30b51765dde23a501d]]]As shown here, writing x.data = ... assigns new data. Now the Variable class can be used as a 'box'.

[[[00000000000000000022---471effd705c059dda6551b73ae2d9691cd42c7c4e3e240934a079577e992ea8e]]]The above is all the implementation performed in this step. Currently, the Variable class has only 3 lines of code, but this is the starting point for building DeZero into a modern framework.

[[[00000000000000000023---0d9c3de3d3295a0927f551be25253b8cc28235cddd6de0cdab467995a001080f]]][Supplement] NumPy multidimensional array

[[[00000000000000000024---f1a232a80ba83b6ae044e319c8d03079889d17c17edf4650e623cc5a1a0fab17]]]Finally, a quick side note about multidimensional arrays in NumPy. A multidimensional array is a data structure in which elements such as numbers are arranged in a regular pattern. The arrangement of elements has a 'direction', and that direction is called a 'dimension' or an 'axis'. Figure 1-2 shows an example of a multidimensional array.

[[[00000000000000000025---61df4a13b9c09263da07a70f589b8950c62cc8d5aa9363b9b8407ba27ba5dfc9]]]
Figure 1-2 Example of a Multidimensional Array


[[[00000000000000000026---a3e0d00c0c414270fa4057acd5438f158bc7ca1ada7add5d4b5030e1e0b39a8a]]]Figure 1-2 shows, from left to right, a 0-dimensional array, a 1-dimensional array, and a 2-dimensional array. They are called 'scalar', 'vector' and 'matrix'. A scalar simply represents a single number. Vectors have numbers along one axis, and matrices have numbers along two axes.

[[[00000000000000000027---24bb2543fcebd61ca9e125d9537ab8592d44d3711b5db510d89f095665662755]]]Multidimensional arrays are also called tensors. In that case, the example in Figure 1-2 is called a 0th-order tensor, a 1st-order tensor, and a 2nd-order tensor from left to right.

[[[00000000000000000028---3b485efd947091f697208d10d0265e98878d47dd3366de784075294b573b9825]]]NumPy's ndarray instance has an instance variable called ndim. ndim is an abbreviation for number of dimensions, meaning 'the number of dimensions' of a multidimensional array. In practice, it looks like this:

[[[00000000000000000029---b92d566d7d6d83d43a06300f5ae87507b8d43e377ccdcd76c7d5debf1edd76ee]]]Here we start and run it in the interactive mode of the Python interpreter (in the interpreter example we will write the >>> symbol). As above, the instance variable ndim gives the number of dimensions.

[[[00000000000000000030---8e5eefe0e0bedaebd9a2a26676013236125ccf727c76c97c6e5c711c8001aa0b]]]The word 'dimension' should be used with caution when dealing with vectors. For example, np.array([1,2,3]) is a vector, also known as a 'three-dimensional vector' because it has three elements. This 'vector dimension' refers to the number of elements in the vector. On the other hand, the 'dimensions of the array' when we say 'three-dimensional array' means that it has three axes (not elements).

[[[00000000000000000031---fa80867118f402d0b489e27b8845eece65ae7a6080d33a9c94cc1d3e5c80c6cf]]]As you can see, ndarray instances can be used to create scalars, vectors, matrices, and even higher-order tensors. However, in this book, for the time being, we will only use 'scalars' in our calculations. Later (from step 37), we plan to extend DeZero to handle vectors and matrices as well.

[[[00000000000000000032---61a067b63e6ec2617294e13c6fd058ed243902436abfc91cc221044444f4544e]]]step 56

[[[00000000000000000033---88f9cf4477466639cc4aaeb252445b1dfe9fb9f48d3a9c5aae42c1ea21551d97]]]CNN mechanism (2)

[[[00000000000000000034---7168c5c2b62bba4d055600f5f5a2b5aa6aebb3d71a1d7e8fcbcee51741f16e6e]]]In the previous step, we explained the convolution operation for two-dimensional data (second-order tensor) that are aligned vertically and horizontally. However, in the case of images, in addition to the vertical and horizontal directions, data is also arranged in the 'channel direction' like RGB, so it is necessary to handle such 3D data (3D tensor). Here we follow the same procedure as in the previous step and look at the convolution operation on a 3rd order tensor. Then we'll talk about pooling.

[[[00000000000000000035---3a37f51bc4b83f965ed8e0480da686924debf5699e24d258f03da9036308f4c9]]]3rd order tensor

[[[00000000000000000036---2b2df5322f7569d9e94d71a8bb7ffe880723fa7045b76138cb473d2aed63ed1d]]]A quick example of a convolution operation is shown in Figure 56-1. Here, we will show how the convolution operation is performed using data with 3 channels as an example.

[[[00000000000000000037---8b925ff33bac6c7162eeb74872fc171b7677635df1f1bad65c2e55b2cf572475]]]
Figure 56-1 An example of a convolution operation on a 3rd order tensor


[[[00000000000000000038---ae1925d5bd0691a5bc70c9036de4b35e3cb631749043deac950e22bbce2c5f74]]]The procedure for the convolution operation is the same as for a second-order tensor, as shown in Figure 56-1. There is no change in how the filter works or how it is calculated, except that there is more data in the depth direction. The point to be noted here is that the 'number of channels' of the input data and the filter should be the same. In Figure 56-1, both the input data and the filter have 3 channels. On the other hand, you can set the vertical and horizontal size of the filter to any number you like. In the example above the filter size is (3, 3), but you can set it to (1, 2), (2, 1), etc.

[[[00000000000000000039---e6617130a6008a7cbd8addde5ca72a49359fb91563649a4e297860bfdb52fe48]]]The filter in Figure 56-1 moves in two dimensions, horizontal and vertical, as in the previous step. Therefore, (although it is a convolution operation on a 3rd order tensor), it is classified as a '2D convolution layer'. Many deep learning frameworks implement it under the names Conv2d and Convolution2d.

[[[00000000000000000040---357664c22d02c3ca406ebabd52d2ba9632d6d384316153148ed5066a7ec098d8]]]think in blocks

[[[00000000000000000041---2afdec470929202dc27d13cc341dd214fd336e7e3293ae41c8bf481e718c3e97]]]A convolution operation on a 3D tensor is easier to understand if you think of it in terms of rectangular blocks. Therefore, we will use a three-dimensional rectangular parallelepiped to illustrate the 3D tensor, as shown in Figure 56-2.

[[[00000000000000000042---e1cf7b47de9ec37aa180f490d3b600d1f4a3e12e57d4e5fc6788f47deea664fc]]]
Figure 56-2 Thinking of Convolution Operations in Blocks


[[[00000000000000000043---52731e625566cfc51581fa487f12cfd88a5ec4a800dd002da6d28842e3c43553]]]In Figure 56-2, it is assumed that the order of data is (channel, height, width). For example, the data shape with C channels, H height, and W width is written as (C, H, W). Similarly, in the case of filters, write in the order of (channel, height, width). For example, if the number of channels is C, the filter height is KH†1, and the width is KW, write (C, KH, KW).

[[[00000000000000000044---679282c353d3fc1e5a3ce7ae83cf399243d0440f83226ae0045ea510461f70c3]]][†1] KH stands for Kernel Height and KW stands for Kernel Width.

[[[00000000000000000045---ec39ebd1d49bea303816f59263369400f01f389d1b0972d6cfbae236f9aea465]]]The output is called a 'feature map', and in the example in Figure 56-2 it will be a single feature map. Next, let's say we want to have multiple feature maps in the channel direction. It uses multiple filters (weights). Graphically, it looks like this:

[[[00000000000000000046---39a52c0805127419eff0697ecbd03ac884de68d1becebc1af06943005253ddb9]]]
Figure 56-3 Example of Convolution Operation with Multiple Filters


[[[00000000000000000047---2aee584ddabf1683b49e3ee69b72c0cb853229fd2a52c8e97b4b67f995127c58]]]Apply the OC†2 filters separately as shown in Figure 56-3. Then the output feature maps are also generated OC times. The OC maps are then put together into a block of shape (OC, OH, OW).

[[[00000000000000000048---ea1e756c3c50fc4eb255df19f8a7bc9f43c52f25a8cce7965369a345bc70f7da]]][†2] OC stands for Output Channel.

[[[00000000000000000049---4773a2c729bb72fb83fd2a6eab5c3ea2bae1f70227394cd3c5952afec69680af]]]As shown in Figure 56-3, the number of filters also needs to be considered when it comes to convolution filters. Therefore, the filter weight data will be a 4th order tensor and will be stored as a sequence of (output_channel, input_channel, height, width). For example, if there are 3 channels and 20 filters of size (5, 5), the shape will be (20, 3, 5, 5).

[[[00000000000000000050---2c88a518ac1b317970e517255e8125a47290387bebac6b6f7d4809773b9b72de]]]Bias is present in convolution operations (as in fully connected layers). Adding bias addition processing to the example in Figure 56-3 yields the following.

[[[00000000000000000051---55dccf32649308a8868adf500dc30087d3aee254ea1840f18a4479d94c332cad]]]
Figure 56-4 Processing Flow of Convolution Operation (Adding Bias Term)


[[[00000000000000000052---86e5f1e271aae2875bda45276a0fd6c367848b3d686b64922cbcc9ad636cab4d]]]Bias has only one value per channel, as shown in Figure 56-4. Now the bias has shape (OC, 1, 1) and the output after applying the filter has shape (OC, OH, OW). Because of the different shapes like that, the biases are broadcast and then added. The above is the convolution operation including the addition of the bias.

[[[00000000000000000053---67075417bc3111315d94494bd55ce569fe4a1b13fd0e0915b69af29b30014c8f]]]mini batch processing

[[[00000000000000000054---a2cd238095b6013c9a3570c7f546b259aa7e45a2bdbd719b19c46e49d8b5475b]]]In neural network training, the input data is processed in batches (this is called 'mini-batch processing'). Similarly for convolution operations, we use mini-batch processing. For that purpose, the data flowing through each layer is treated as a 'fourth order tensor'. For example, a convolution operation on a mini-batch of N data looks like Figure 56-5 below.

[[[00000000000000000055---cb879e5eb6570b37ad1cc32bd339c576a5849883c54695ac3018c52507877e13]]]
Figure 56-5 Processing flow of convolution operation (mini-batch processing)


[[[00000000000000000056---9f2596780e82f05248bcbd274f4d3a2056baa75f813b3d18dd0c72f3ccd1e9c9]]]In Figure 56-5, a batch dimension is added at the beginning of the data. The data format is then a sequence of (batch_size, channel, height, width). Mini-batch processing performs the same convolution operation (independently) on each sample data in this 4th order tensor.

[[[00000000000000000057---21b91f1667a25e40b7426b092dfe9d0f2e09380374dfcf8c3254da3895b1e620]]]The above is the calculation of the convolution layer in CNN. Next, I will explain “another layer” in CNN called the pooling layer.

[[[00000000000000000058---f309dbb24a54a67b7b375aad515a2bdb98b20562c5566997ce4863ff9519ad7a]]]pooling layer

[[[00000000000000000059---d9183ce6a18a89a7106fe935aa581fad387626469c9755c4939e98fc6b46ae5b]]]The pooling process is an operation that reduces the space in the vertical and horizontal directions. The example in Figure 56-6 shows the processing steps for max pooling with a stride of 2. Max pooling is an operation that takes the maximum value, where '' represents the size of the target region. Take the largest element from the area of , as shown. Also, the stride is set to 2 in this example, so the window moves every 2 elements. In general, set the pooling window size and stride to the same value, eg stride 3 for , stride 4 for .

[[[00000000000000000060---60e2c6783fc9cbde07a17792b64dadeee20f42c1ae175e5271a874f2572a017c]]]
Figure 56-6 Processing Procedure for Max Pooling


[[[00000000000000000061---936499dbb5c2b7e3b557f313feb26c7b5b485ae69a99333973b78f8bdba4db27]]]In addition to Max pooling, there is Average pooling (average pooling). Max pooling is an operation that takes the maximum value from the region of interest, whereas Average pooling calculates the average of the region of interest. In the field of image recognition, Max pooling is mainly used, so when we say 'pooling layer' in this book, we mean Max pooling.

[[[00000000000000000062---7320f04cd779a8c648430a439f676d4fedec508f9d68abe6fe4bedf8be4d077c]]]The above is the explanation of the pooling layer. This pooling layer has the following characteristics:

[[[00000000000000000063---14a71fd96c75082043222904651d09bfa7098d430fa37c274b35c3cc6a787307]]]no parameters to learn

[[[00000000000000000064---a28afb580ac4f87972fa46ad3bbe257bfc1bab6281973f57e7f347540029a103]]]Pooling layers, unlike convolutional layers, do not have parameters to learn. This is because pooling is just a process of taking the maximum value (or average) from the target area.

[[[00000000000000000065---0fe9b19267932b6a5525e9ae527494ff2d55bed1c49d899b359d5b8427e519b5]]]No change in number of channels

[[[00000000000000000066---d5619a602d0907ccbc1505b301d9069a5b3da4c7ad28c8f73581c22311e6272e]]]Pooling operations do not change the number of channels of input and output data. Each channel is independently calculated, as shown in Figure 56-7.

[[[00000000000000000067---3cd1135a8038c83bce3cf4ceff6a66b8cc80809cf4052e31b02913d3d443d3c8]]]
Figure 56-7 Number of Channels for Pooling


[[[00000000000000000068---d19f84a42556da02cc1dc62e5ed639011418cd8a08a5862b048176c2fe293304]]]Robust against minute position changes

[[[00000000000000000069---894cf97df59bfce205a1120d005469db0a70cfce1ea0c85c204b2963ab70b376]]]Pooling gives similar results for small deviations in the input data. Therefore, it is robust against minute deviations in input data. For example, in the case of pooling , pooling can absorb input data gaps, as shown in Figure 56-8. In the right figure, the input data is shifted by one element in the horizontal direction, but the output has the same result (it may not be the same depending on the data).

[[[00000000000000000070---c21ebe7e20f06d97bf94186169e4de50bd9699b70e1f4e03f2d2be7bae23b428]]]
Figure 56-8 Comparison when there is a small deviation in the input data


[[[00000000000000000071---9e17dc2ce479c3d402d1adc52309aa0de0bd5e4d68423b00b6b4af633f31218b]]]The above is the mechanism of CNN. So far, we have mainly discussed “convolutional layers” and “pooling layers”. From the next step onwards, we will implement these two processes as DeZero functions.

[[[00000000000000000072---86fb541d2313621abb2e411fc7563ed5522bd2c9f7dd470c88285aa56a723f48]]]step 37

[[[00000000000000000073---b2c0e54704943372f1fba431351f69dfc331ef4b167d9a7fc863f5a5ac85d834]]]deal with tensors

[[[00000000000000000074---875307646beb5609b6a26cdf77c216abfaccbea299a2eb400d02e5a5b25dbe23]]]So far, we have mainly dealt with 'scalars' as variables. However, in machine learning, 'tensors' such as vectors and matrices play a leading role. In this step, we will describe the precautions when using tensors and prepare for extending DeZero from now on. We will also explain that the DeZero functions we have implemented so far can be used as they are in tensors.

[[[00000000000000000075---299dd14bc6e89882a5881b93692cfba781801997f57f4b9afa3816699f7b4673]]]Computation by element

[[[00000000000000000076---89283186475d89834223b719fc647a835349f05761b95bc845b7476f6b75ebf2]]]We've implemented several functions for DeZero so far. For example functions like add, mul, div, sin. In implementing those functions, we have assumed that the inputs and outputs are all 'scalar'. For example, when I implemented the sin function, I assumed the following cases:

[[[00000000000000000077---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000078---2e4f8e1196303c4e8cedfbb2d5a3c862de41206939766191db87ea601504b20f]]]In the example above, x is a single-valued scalar (exactly a 0-dimensional ndarray instance). So far, we have implemented DeZero with such scalars in mind. Now what if x is a tensor, say a matrix? In that case, the sine function is applied elementwise. In practice, it looks like this:

[[[00000000000000000079---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000080---5fa045d0335f80d6ee95eee3cd771dacb571e0aea0357863464c45c9f1713e53]]]As above, the sine function is applied to each element of x. So the shape of the input and output tensors does not change. In fact, the input x has shape (2, 3) and the output y also has shape (2, 3). As you can see, the DeZero functions we've implemented so far are calculated element by element. For example, addition is calculated element by element as follows:

[[[00000000000000000081---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000082---fe79c536659411ebad4005ddbe88ec7dc9161e74ec84ceb32cfffb46f9026dd3]]]As above, the result of y is added element by element of x and c. So the output y has the same shape as x,c.

[[[00000000000000000083---640928358fe7b0b3411fa849dc4461b0e15ce3f3d1cafa20578088192188e369]]]In the code above, x and c should have the same shape. Doing so creates a one-to-one correspondence between the elements of the tensor. NumPy also has a feature called broadcasting. This is a function that automatically copies the data and converts it to a tensor with the same shape if the shapes of x and c are different. Broadcasting is covered in more detail in step 40.

[[[00000000000000000084---ceb3ef8f7216772fe4f2eba86697f9486a595c5e76d9cf56c588dccd821d7658]]]Backpropagation when using tensors

[[[00000000000000000085---4fd5f0634df71aa41d1f38a5200ff47fb93628e30920d88c9a1f6ea3cd710c98]]]This is the main point of this step. So far, we have focused on ``scalars'' when implementing backpropagation. What if we do backpropagation on the computation with the tensor here? In fact, for the functions we have implemented so far, the backpropagation code works correctly even if we perform calculations on 'tensors'. The reason is the following logic.

[[[00000000000000000086---4cbbc1285ac9b153469b3d00c7b518f206610485aec73b1c8ec9232d78c51940]]]So far, we have implemented backpropagation for 'scalar'

[[[00000000000000000087---c767de7a57932e050b17eef35cce99ba57129fa6c7bd3c6e69feaf02c6b91165]]]The DeZero functions implemented so far are calculated as a 'scalar' for each element of the tensor when a 'tensor' is given.

[[[00000000000000000088---8001c670581d77a661ca29e46c4d797e06c002eb1e36df9940f0e456f4b1b87c]]]If the calculation is performed as 'scalar' for each element of the tensor, the backpropagation implemented assuming the case of 'scalar' also holds for the calculation for each element of 'tensor'.

[[[00000000000000000089---44f1ba9e57cb4273a80811e597c5fad70a6f58dad5bfdbc87a5281f009223e24]]]From the above logic, it can be seen that for DeZero's functions that compute element by element, backpropagation works correctly even when using 'tensors'. Here's what it looks like when I actually try it:

[[[00000000000000000090---2470bd5c8e77d312ef8ccdd044fb3d1620f3e758c061aa84f49acc9f93acf303]]]Here we show a calculation using a function called sum that performs addition and finds the sum. The sum function will be implemented in step 39 and will be used here in advance. The sum function takes a given tensor, sums its elements, and outputs a single scalar. In the code above, x, c, t are all of shape (2, 3), but only the final output y is a scalar.

[[[00000000000000000091---5cf1832258b7e7249734470ae63721b8f17b91126dcefaf7d3bec5d405f2d9b5]]]In machine learning problems, it's common to set up a function (loss function) that inputs a tensor and outputs a scalar. In the code above, I assumed a machine learning problem and did a computation that outputs a scalar at the end.

[[[00000000000000000092---9145bc851e17691470bea096ca465a71728e69b67b08ab220dce004046bb8f52]]]By the way, the calculation performed by the above code is shown in Figure 37-1 as a calculation graph.

[[[00000000000000000093---b733647233b624ddeceedfdddd81bb0db3e21f4e1d010d221e5866b027cb58ba]]]
Figure 37-1 Computation graph with tensors


[[[00000000000000000094---5fb27a3e233e0bf72ed86bcc193058328fd32b8258f0078001775a091a34a2d6]]]Figure 37-1 specifically shows the data for each variable. Looking at it, the final output is a scalar. Here, backpropagation is performed on the 'computation graph whose final output is a scalar'. Now let's actually do backpropagation. After the code above, write the following:

[[[00000000000000000095---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000096---09534d5c1582ed027987f7ecd6b8f1adf0679ab26fa4316be34561bdd914227e]]]As above, calling y.backward(retain_grad=True) will find the derivative of each variable. Here, the argument is retain_grad=True, so all variable derivatives are preserved. And the output is correct. In this way, the DeZero functions we have implemented so far can perform backpropagation correctly even if they are calculated using tensors.

[[[00000000000000000097---2ab0e369d55d5693dfdb9d8a75eb67ea73eba0f23701f747ff91163dff0570e0]]]One important point here. That is, the shape of the gradient and the shape of the data (data in forward propagation) match. That is, x.shape == x.grad.shape, c.shape == c.grad.shape, and t.shape == t.grad.shape. This property makes the implementation of functions that are not elementwise computations, such as the sum and reshape functions, more predictable. We'll look at this in the next step when we implement the reshape function.

[[[00000000000000000098---d02c12ecc34677cd38ee69512dfdfb5ccd453ed6648ad9063eaed776c865c4ae]]]The derivative of a tensor is called the 'gradient' in machine learning. grad in the Variable class is also short for gradient. From now on, in this book, we will not say 'differentiation', but 'gradient'.

[[[00000000000000000099---784caf9415940ae89ec14b9e19c45d4f8c9392e94abc80b8067d2c7c8881a41c]]]This is the main content of this step. Finally, I'll add some math to backpropagation when using tensors. This addendum is somewhat advanced, but can be safely skipped as it does not affect subsequent steps.

[[[00000000000000000100---ca8d42458b1ea5bb205bc68ace2c1771c970b3bf4aaa188fc78eec9944ab5de0]]][Supplement] Backpropagation when using tensors

[[[00000000000000000101---e554161076a5f2da2b888db7d1713727013af47bfe4d49096fd55627d6fcd182]]]Here, we will use formulas to explain backpropagation when using tensors. First, as a preparation, consider a function called. Here, is a vector, and we assume that both vectors have number of elements.

[[[00000000000000000102---eb0a3387163ae795d283487af5a257f6baf858d4d26c8c788447b1926684aad8]]]Here we will limit ourselves to 'vectors'. However, the conclusions (theories) obtained in this section can be applied as they are to the case of 'tensors (rank tensors)'. This is because in the case of calculations using tensors, it is only necessary to add 'vectorization processing' (processing to format the elements into a vector so that they are arranged in a row) as preprocessing. Then the theory of 'vectors' obtained here can be applied directly.

[[[00000000000000000103---c49a7d219009d31c12e4d39d99a9801cf5b24ef517738da0e1ba11f29a997db2]]]Now let's look at the differentiation of The derivative with respect to is defined by

[[[00000000000000000104---562cc7f889557e61671bba3f15a972c9b90f8d7c3ccb6067d477fe035439b5a2]]]Since both and are vectors, their derivative is in the form of a 'matrix' as above. This matrix is also called the Jacobian matrix. Incidentally, if was a scalar rather than a vector, the derivative with respect to would be

[[[00000000000000000105---7aea94353c51b9bd7b2af526c429c9c43c001d43220d74f0d6e801dcec679680]]]This is the Jacobian matrix of , which can be viewed as a row vector (= side-by-side vector).

[[[00000000000000000106---20192a5f0a3bbee8ad06d0010d80201f4e6bd6abe06cbf9fc2a658684ce3c25d]]]Now let's consider composite functions. Here, we consider a composite function , which is composed of three functions , , and . At this time, the variable , is a vector with the number of elements. And consider the case where only the final output is scalar. Then the derivative with respect to is expressed by the chain rule as

[[[00000000000000000107---a9a3b1fad83ff1d467a06187793402aebb044b2607328434d3c430f2441f1c30]]]Equation (37.1) is the result obtained by the chain rule. where or represents the Jacobian matrix. Then compute them as 'matrix multiplication' (matrix multiplication is explained in step 41). This is what expression (37.1) represents.

[[[00000000000000000108---e1c711199f32ccda7d4c16640b5fdd705bae815ca5c5bf444a0001a5cf7ec0d4]]]Next, let us consider the 'order' of computing the matrix product of Eq. (37.1). There are two ways to do that. The first method is to calculate from the input side to the output side as shown in Figure 37-2.

[[[00000000000000000109---572f738a1a74f095005033a77f19609bc82c09e9eb14507a6fbffed99776176b]]]
Figure 37-2 Parentheses from input side to output side (forward mode)


[[[00000000000000000110---103d24f453aeaff4826a7e6bdab48b7fc8750373135e540d826a11f556070a6f]]]The calculation method that brackets from the input side to the output side as shown in Figure 37-2 is called 'automatic differentiation forward mode'. The point to note here is that the result of the intermediate matrix product is a matrix. For example, the result of is the matrix of

[[[00000000000000000111---877da2e011f90a4745de467d5d1bb5841ebc4ca64336f0b3a47d3a031c26f365]]]On the other hand, as a second method, a method of calculating from the output side to the input side as shown in Figure 37-3 can also be considered. This is reverse mode (correctly called 'reverse mode of automatic differentiation').

[[[00000000000000000112---77c8a7aa9ed81f4a63e6e44886d60865efcb7ea8ae316d73f947a04e5b6cac78]]]
Figure 37-3 Parenthesis from output side to input side (reverse mode)


[[[00000000000000000113---b8f2824457bea76017b593b20d6ad5dbe34ca6be9e063995a2353fb100b87c6f]]]In Figure 37-3, the calculation is done with brackets from the output side to the input side. Since this time is scalar, all intermediate matrix products result in vectors (row vectors). For example, the result of is a vector with elements.

[[[00000000000000000114---0f07b7f5b9722fa33faaf0302913caad789b78fb10707b516235a90da03ae82e]]]In forward mode, it propagates matrices of In reverse mode, on the other hand, we propagate vectors. Also, vector-matrix multiplication is less expensive than matrix-matrix multiplication. For these reasons, reverse mode -- backpropagation -- is more computationally efficient.

[[[00000000000000000115---f74c264c58ea4f5a0fb019c10465e9ad90e469f57aa70fdbbbed8d3b70cb813b]]]As shown in Figure 37-3, the reverse mode consists (mathematically) of multiplying a vector by a Jacobian matrix. In the example of Figure 37-3, we first multiply (vector) by (Jacobian matrix). Then multiply (vector) by (Jacobian matrix). In this way, backpropagation finds the product of the vector and the Jacobian matrix for each function.

[[[00000000000000000116---ac33d577c9e234b0c887b14db60b46b8cb3160d8befbf7ba132f720f61db6258]]]The important thing here is that you don't need to explicitly find the Jacobian matrix and perform the 'matrix multiplication' calculation, you just need to find the result and you can do backpropagation. For example, in Figure 37-3, suppose we have performed an element-wise operation (for example, ). If we want to find the Jacobian matrix for this function, we get:

[[[00000000000000000117---f2786e9ff290d592e1fb0a99bf8915adcad8feb8b5c6f649b1a1329c91e1536f]]]As above, for element-wise operations, the Jacobian matrix is diagonal (a diagonal matrix is a matrix whose entries are 0 except for its diagonal entries). The reason is that it only affects (here, integers between and). And if the Jacobian matrix is diagonal, then the product of the vector and the Jacobian matrix is

[[[00000000000000000118---959aea0db5fb6820df407bd2259987217e47ea0102dbb0f01f68028657d17771]]]Looking at the above formula, the final result can be obtained by taking the derivative element by element and multiplying the derivative element by element. In other words, for element-wise operations, backpropagation is also obtained by multiplying the derivatives element-wise.

[[[00000000000000000119---9326286ed59aa65e0f269af47b9d507219d89fe4e42d5ee83413fa7fc4f35d0d]]]The important point here is that you don't need to explicitly find the Jacobian matrix and compute the matrix product, just find the result. Therefore, if there is a more efficient calculation (implementation) method, that calculation method can be used.

[[[00000000000000000120---c8c8648ee5a47b125df751d9eafc4bd23e189f7389a2fe2c9e2f09a838051424]]]That's the mathematical explanation of the tensor version of backpropagation.

[[[00000000000000000121---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O'Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000000122---a58e1b965d1d846135ad73659461d3d53569400f8a3563ddded30e7e6d04bff6]]]Deep Learning from scratch ❸

[[[00000000000000000123---afe0c7c8215b9aabe8151bdd21f33b3c45204ae3d2fb29f971e496794e5f73d8]]]Framework

[[[00000000000000000124---dd292d9ddce5a340062dbd871a69d58f9691cc9b4a75b3aeb32ef9a76d60ccd3]]]April 16, 2020 First edition 1st edition issued

[[[00000000000000000125---f2f5b6f470310eaf5a90b614a718214b9b06a6adc028924fc6a0d8ba94857806]]]Author Yasutake Saito

[[[00000000000000000126---16f1fe5d091f36817279a9042f8e7f2e6ad26cd5721935ce9cf0857d2ba4fffb]]]Published by Tim O'Reilly

[[[00000000000000000127---e892f0b49ac7c195817b2cb6f935741d2a6d67d4e082b402c808644ac63fb9ef]]]Publisher　　　

[[[00000000000000000128---5120b30e23f0646f3a418cbb6d18df0283e91b171be2b9c3a910b3aff3d0337e]]]O'Reilly Japan Co., Ltd.

[[[00000000000000000129---173f76b1e0e52042ce8770cd7df500066657395900d568d00501179717b0fdf9]]]
　　　　　　12-22 Yotsuyazaka-cho, Shinjuku-ku, Tokyo 160-0002

[[[00000000000000000130---e670ad760848546aa0571bd75410cabcb7eaedab59ad888830f53973b3d582cc]]]
　　　　　　e-mail 

[[[00000000000000000131---f6191178ed3d59d9934e432efc352d4e7d09dc7cdbff312412f9a051ce5b5012]]]production cooperation　　

[[[00000000000000000132---525b80b0d33a191efc087976b152945e86bb5fa5894b81a40506dc24e8ae8f94]]]Top Studio Co., Ltd.

[[[00000000000000000133---30aa1d3390e3e60bb7682390d9a92c0cfc22c9564bb8f0481644044cf2f0e92c]]]step 40

[[[00000000000000000134---cc0503931ab20e748991fb0a90885199e52aba6a175efd3b70012fa61eef63ba]]]function to broadcast

[[[00000000000000000135---c5d4ac496636584c15eff802ae0e2aca6a8cee7f131ed75d054b746fcfe7dea7]]]In the previous step, we implemented DeZero's sum function. Backpropagation of that sum function used the broadcast_to function preemptively. In this step, implement the broadcast_to function. We will also modify some DeZero functions so that DeZero can broadcast like NumPy.

[[[00000000000000000136---2b375c7782bae5f3af0e566438dc51245a4518b4d689861a4bc5221682140f13]]]NumPy has a feature called broadcasting. Forward propagation in DeZero can cause that NumPy broadcast. However, the current DeZero fails to do correct backpropagation when a broadcast occurs. In this step, we will modify DeZero to handle broadcasts correctly.

[[[00000000000000000137---b9ee291cace63dc184dc0e89348b73bffc716d2cf6d0b1095bec44654c569cfb]]]Here, we will first explain using NumPy functions. After that, we will explain in the flow of implementing DeZero functions.

[[[00000000000000000138---367f55653a32bfa95c47cacb4367e3d2d874fa747b0bed825721a75ecff1fc4e]]]broadcast_to and sum_to functions

[[[00000000000000000139---d2064af4fee6f83d124d1c5ce331a269f71ea49d4afe943a84bf5b194745639d]]]First, let's take a look at NumPy's np.broadcast_to(x, shape). This function duplicates the elements of x (an ndarray instance) to have the shape of shape. For example, you can use it like this:

[[[00000000000000000140---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000141---40cd976d17de4648801282c28dcc184190d8ed1be3cde9a275029ef57af1d72c]]]As above, it was originally a 1D array of shape (3,) and the elements were copied to have shape (2, 3). So what happens to the backpropagation when we broadcast (or 'copy elements')?

[[[00000000000000000142---78401a6d5c5ba1d8f3a9fb498e5a6fe84e84decebbb00adc8a1d4b14d6f0bf00]]]DeZero allows you to calculate using the same variable (Variable instance) multiple times. For example, calculations like y = x + x. Here, x + x can be regarded as using a 'copy' of x. Then the backpropagation runs the gradient into x twice and 'sums' the gradients. According to this principle, we know that if we 'copy elements', we should 'sum' the gradients.

[[[00000000000000000143---f0d50d7936771cccc5a7c268c5b456c03b8b499d71f80fe617f7a864827d126d]]]If you do a 'copy of elements', backpropagation will 'sum' the gradients. A function like np.broadcast_to would look like Figure 40-1.

[[[00000000000000000144---1d21e8141ba2daf6de0b539c3185c2075695283e024bb67e5e10ccbf55342301]]]
Figure 40-1 Backpropagation of the broadcast_to Function


[[[00000000000000000145---14f40527c699dcfd67a1c06dc42ddf692375249d0ac336953f2ded1c09d73b6a]]]Backpropagation of the broadcast_to function sums the gradients to the shape of the input x, as shown in Figure 40-1. A simple solution is to have a function called sum_to(x, shape). The sum_to function sums the elements of x into the shape of shape. Given such a function, a forward-backpropagation relationship is created, as shown in Figure 40-1.

[[[00000000000000000146---8eda6ff86d7291fa213a48d6e479a397b4ea489c8286c53dd3a5d57f903d3fe0]]]The sum_to(x, shape) function is a function that obtains the sum of the elements of x and makes the shape of shape. But there is no such function in NumPy. Therefore, DeZero provides a NumPy version of the sum_to function in dezero/utils.py. With that function you can do the following calculations:

[[[00000000000000000147---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000148---cbbe5f03714d4801058bb99958eb9412358f628a145c37869ff64591d5a5fc70]]]As above, the sum_to(x, shape) function sums to the shape of shape. What it can do is the same as the np.sum function, but the way the arguments are given is different.

[[[00000000000000000149---49c7c373c9b88ede6c4519125e3120a8ef54382eab0063dabbd442966a68e358]]]The implementation of the sum_to function in dezero/utils.py is based on the following Chainer code. https://github.com/chainer/chainer/blob/v6.4.0/chainer/utils/array.py#L51-L65

[[[00000000000000000150---98810b1f45193cface3d2efe327e4a8c1d213cfe59759b166539c45574ccf9a6]]]Next, consider the backpropagation of the sum_to function. sum_to(x, shape) sums the elements of x to the shape of shape. For backpropagation, the broadcast_to function can be used as is, as shown in Figure 40-2.

[[[00000000000000000151---15eac49d12b87860ee3481971fc350886797763963fa613252c00166d3890229]]]
Figure 40-2 Backpropagation of sum_to function is broadcast_to function


[[[00000000000000000152---a496c38f6809638d63f61f07d6ee0de8ffce116ea739e37f1c235d0c253f9ee0]]]As shown in Figure 40-2, backpropagation of the sum_to function uses the broadcast_to function to replicate the elements of the gradient to the shape of the input x. That's about the NumPy version of the broadcast_to and sum_to functions. Next, we will implement the DeZero versions of the broadcast_to and sum_to functions.

[[[00000000000000000153---4b531a003f291d828d7b8c1b59bce4250d955ffc96821fb0d6aa44077ea8d528]]]DeZero's broadcast_to and sum_to functions

[[[00000000000000000154---f758f163fada6286986313b7c47ce3e02f583abc5820a8e9e2a02de3d40d74fc]]]First, here's DeZero's BroadcastTo class and broadcast_to function:

[[[00000000000000000155---7022e735b889f9ac9cc31a93d5a6b0bcd145f252e1f876f45eb3ff9cdb715bb0]]]For now, let's focus on the backpropagation code. Backpropagation makes use of DeZero's sum_to function to get the shape of the input x. That sum_to function is about to be implemented. Next, we show the SumTo class and the sum_to function.

[[[00000000000000000156---cea0293648f15245f289e35078f272beb77535c8b7a6950e7e59c67c7fde2746]]]Also noteworthy here is the backpropagation code. Backpropagation duplicates the elements of the gradient to have the shape of the input x. To do that, we'll use DeZero's broadcast_to function that we implemented earlier. As such, the broadcast_to and sum_to functions are dependent on each other. The DeZero version of the broadcast_to and sum_to functions is now complete.

[[[00000000000000000157---40276558f3d8bb966ab3993075ec6d2a7135c33b5a79fe7f9af8100fa046bec8]]]Broadcast support

[[[00000000000000000158---17989dfea768288c95d3573292a41f33b38f0800aee021ba9e99a0b9fac8d8bf]]]There is one reason why we implemented a function called sum_to in this step. This is to support NumPy broadcasting. Broadcasting is a NumPy feature that allows operations on multidimensional arrays of different shapes. As an example, let's look at the following code:

[[[00000000000000000159---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000160---b388252ea0431bb531ef85366f35378cf2a89035d0159d04f4cec4c4e30b2fda]]]where x0 and x1 have different shapes. When this calculation is done, the elements are copied so that x1 fits the shape of x0. The important point here is that NumPy's functionality of broadcasting happens behind the scenes. And this broadcast will also take place on our DeZero. For example, let's look at the following example.

[[[00000000000000000161---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000162---d8ae2625dba7445560e0ac29e15c8e97c018159540da922f593767911e590c62]]]As mentioned above, in forward propagation, broadcasting occurs because the implementation is for ndarray instances. Of course, if forward propagation broadcasts, then backpropagation must back-propagate broadcasts. However, DeZero as it stands does not do any broadcast backpropagation.

[[[00000000000000000163---71c877182dad1ea3585736394a806f030b9fd07f7afde824a5bebe16a76f5446]]]Broadcasting in NumPy is done with the broadcast_to function. And the backpropagation of the broadcast_to function corresponds to the sum_to function. With that in mind, DeZero's Add class can be modified as follows:

[[[00000000000000000164---2c13f7fd0e7867845f92c75b943c60bfeb16d2ecbf1b1f08ebe7fbfe2f52424c]]]If broadcasting occurs during forward propagation, the shapes of x0 and x1 at the input should be different. In that case, do the backpropagation computation for the broadcast. To do so, the gradient gx0 sums to the shape of x0. Similarly, the gradient gx1 sums to the shape of x1.

[[[00000000000000000165---4442dbe4f7761eac8302d8b5fb7d6308b1cb3b9829094fee513429a6613a82d4]]]Make the above modifications to the Add class in dezero/core.py. Similarly, all classes that support arithmetic operations, such as the Mul, Sub, and Div classes, have the same modifications. You are now ready to broadcast. With the above modifications, we can write the following code:

[[[00000000000000000166---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000167---6dcc92412feffa025a65df710fda5519d5610f9f7a2626f2fded2d13eaf2bbb5]]]In the example above, a broadcast occurs at x0 + x1. But this time, the backpropagation of that broadcast is done correctly as a function of DeZero. In fact, the slope of x1 is 3, which is the correct result. With that, DeZero was able to handle broadcasting as well.

[[[00000000000000000168---289f335afb58775e253c26400e52d5637a6b3e64baf274638e85251e6ed76238]]]step 14

[[[00000000000000000169---7819978fdf52627af29b95afa638da39c13810a5a2a5fb136bd1a8ab9ad9c4cc]]]use the same variable repeatedly

[[[00000000000000000170---f4ba32af9ff4bdbd1ed2f2665aae98d8a9884d753581a12d9111650cdd9fda90]]]DeZero as it stands has a problem. The problem arises when you use the same variable repeatedly. For example, consider the computation y = add(x, x), as shown in Figure 14-1.

[[[00000000000000000171---376db05fc2c0017ec46d4780f73e31ea534b82fa646bcafd00175de743310fea]]]
Figure 14-1 Computation graph for y = add(x, x)


[[[00000000000000000172---4a51aeac8073d0c5a97eac6bbb0742d6a6c20f97f162d23a0fb58200cff0c748]]]Our DeZero cannot obtain the correct differentiation when we perform addition using the same variables as shown in Figure 14-1. Let's try it out and see what the results look like.

[[[00000000000000000173---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000174---54eab7c2563a4eb64bca35f300ca9e7a72321637e96f1e4c66633b8fd141fe47]]]Here, addition was performed with x=3.0. This time the value of y is 6.0, which is the correct result. But the derivative of x (x.grad) is 1.0, which is incorrect. More precisely, when , its derivative is

[[[00000000000000000175---9072668c022c6f9b44d843d91ec1905eaa0f9a7e683f18449befdb81265ac1d5]]]Cause of a problem

[[[00000000000000000176---a9a805c70e604da49168a60bdfc9c6b34dfdc43a046721c4b83d0244daff2d57]]]What is the reason for the above result? The cause is in the following part of the Variable class.

[[[00000000000000000177---de20762519f1aff01121ae79313b5786d5731fd12e04d21f5560ca6482018a03]]]# Here's a mistake! !

[[[00000000000000000178---a7bcfc91c6d26bfde9fbc74b7f3931b21126469c6d6da625c300cd515187a507]]]As the above code shows, in the current implementation, the derivative transmitted from the output side is substituted as it is. Therefore, if the same variable is used repeatedly in calculations, the values of the propagated derivatives will be replaced. For example, in the addition example above, the derivative propagates as shown in Figure 14-2.

[[[00000000000000000179---4e95474d1db6cfe123225851fb463550e961051c9f03e8fd8f8eb5e1c311f14d]]]
Figure 14-2 Backpropagation of y = add(x, x) (showing derivative values propagating up and down the arrow)


[[[00000000000000000180---b2be2251a8e46a118163e22244f700279713cc10f406c9d7c73fc5d9c7ac7dad]]]Figure 14-2 shows the value of the derivative that propagates. Then the correct result is that the derivative of x is 1 + 1 = 2. That is, we need to find the 'sum' of the propagating derivatives. However, the current implementation overwrites that value.

[[[00000000000000000181---e83a7a0fbefea056dc68336b62b9e4a76195bf1628d8e445372533fbce51859c]]]solution

[[[00000000000000000182---91978d955bd50bd601df28b992916b1ee09ef756b950d98531c93984762b0c71]]]The solution is simple. Based on the above considerations, modify the Variable class as follows.

[[[00000000000000000183---eb72a603218b813b7bcb88c6e9f09550d6ebb79777a8c812927ff4221aa73ae0]]]As above, if you set the derivative (grad) for the first time, just substitute the derivative transmitted from the output side as before. And if the derivative is transmitted after that, modify it to 'add' the derivative.

[[[00000000000000000184---f8dcd37f61dc09e4ffbfb32be97efff82a2eebccb7a752c63543e830396cd620]]]In the code above, I wrote the code x.grad = x.grad + gx to add the differential. It looks like this can be written as x.grad += gx using the accumulation assignment operator (+=). However, in that case, there are cases where problems arise. Because the reasons and background are somewhat complicated--and to digress from the essential problem of deep learning--we omit them here. Further details are provided in Appendix A of this document. Please refer only to those who are interested.

[[[00000000000000000185---8c3a9673eedce51fff103396af056fb95f601e9b7761fc70ff375fe1e53b16fd]]]Now you can use the same variable repeatedly. As a test, let's try again the calculation that ended in failure earlier.

[[[00000000000000000186---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000187---0047545c0be597656345ca6846bcd19d2c27b7b881f9512f86200a04fcc754d6]]]Running the code above gives the correct result of 2.0. Now let's do the addition using x three times.

[[[00000000000000000188---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000189---f32b4b3255e76cf2d53bce2c1dfe581711126a5e40e9ab4ebefb94f5a2b72dc9]]]The above result is 3.0. Checking with the formula, , its derivative is , which agrees with the result above. This completes the implementation for using the same variable repeatedly.

[[[00000000000000000190---442a12c3b1f584bf41855bb3e73e6f6a50cb08c5814394e5acd48459a123bce9]]]reset the derivative

[[[00000000000000000191---ddb6642c3335957d1486bce3f84a273556b15d7a2b1543f65f4fe721c8f3e0ac]]]The only change made in this step is to add backpropagation derivatives. This change introduces new things to watch out for. That's when you do another calculation using the same variables. For example, let's look at the following code.

[[[00000000000000000192---5a0bbd172936eee9ca27becd48fb0bcdf7b5c1b03a5e5ffcc0ce746a374208f7]]]# first calculation

[[[00000000000000000193---00ff006288ac124fa983d0dda18fde5d73ce4a66a16634e67ee549c10b9928e0]]]# second calculation (using the same x, doing another calculation)

[[[00000000000000000194---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000195---2801c0f3870716e2ca475bd01746cd520557d5ee4f41ac0dc0b54a78791e39f7]]]The code above does two differential calculations. Now let's say you want to reuse the Variable instance x to save memory. But the second derivative of x is added on top of the first derivative. Therefore, the derivative of the second calculation is incorrectly 5.0 instead of 3.0.

[[[00000000000000000196---20ceeecde96a72877dbf415faa97017037e0127a4bd51250d39c29453921f414]]]To solve this problem, add a method called cleargrad to the Variable class to initialize the differentiation.

[[[00000000000000000197---7f4b9da1ec026bd7c28dc7c8becbf5828b31a8a5495b7401d2fe01df05df0634]]]The cleargrad method is a method for initializing differentiation. In it, simply set self.grad = None. Using this method, you can find the derivative of another computation using the same variables. Using the previous example, you could write code like this:

[[[00000000000000000198---5a0bbd172936eee9ca27becd48fb0bcdf7b5c1b03a5e5ffcc0ce746a374208f7]]]# first calculation

[[[00000000000000000199---00ff006288ac124fa983d0dda18fde5d73ce4a66a16634e67ee549c10b9928e0]]]# second calculation (using the same x, doing another calculation)

[[[00000000000000000200---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000201---6f51fd0d107f07f65c0d931f293778faf9b755b3658bf4bf221f6ee5293e67ec]]]This time, the derivative of the second calculation was also found correctly (3.0 in the second derivative is the correct result). Thus, by calling x.cleargrad() before calling x.backward() a second time, the derivatives accumulated in the variables are reset. Now you can do another calculation with the same variables.

[[[00000000000000000202---5c09f80583e40ccbfbd3db0762b605c60a451bcf466df80d9c45cf792d9b7393]]]DeZero's cleargrad method can actually be used when solving optimization problems. An optimization problem is the problem of finding the minimum or maximum value of a function. For example, step 28 involves minimizing the Rosenbrock function, which uses the cleargrad method.

[[[00000000000000000203---8f29ef7602f83e8d830d580afe43d4a5b7cc6c8246eb05da98c4bc17d055d32c]]]This is the end of this step. With the work of this step, the Variable class has made further progress. However, important issues still remain. The next step will address that issue. Once that implementation is finished, the Variable class is finally complete.

[[[00000000000000000204---f14065cb6f64ed593e44ce28e9676f95195699e399b8522a22da8c9d17722e7e]]]step 53

[[[00000000000000000205---8fb01b865091404547b14e14aa8e078b5053fb647a0f01d5c4f253d857c25a44]]]Saving and loading models

[[[00000000000000000206---3c729afcd7c15261a5f4fde4021548090ef65d2fa40a1c1159412dcffec05876]]]In this step, we will create a function to save the parameters of the model to an external file. We will also create a function to read the saved parameters. With such a feature, you can save a model in progress as a 'snapshot'. You can also load learned parameters and perform inference only.

[[[00000000000000000207---80492b4589f7ed55d40c31325d4d5abe862c5ec080d4d57d88f10e8084eb4d82]]]Now, DeZero's parameters are implemented as the Parameter class (which inherits from the Variable class). Parameter data is stored as an ndarray instance in the instance variable data. So what you should do here is save the ndarray instance to an external file. Fortunately, NumPy provides several functions to 'save' (and 'load') ndarrays. Let's first look at how to use those functions.

[[[00000000000000000208---64aad6e4cd60b88fc0aa04e1d6d67759f9e6a3ee90b7575f3e906ef97780885a]]]When running DeZero on a GPU, use CuPy's ndarray (cupy.ndarray) instead of NumPy's ndarray. In that case, we will convert the CuPy tensor to a NumPy tensor and then save it to an external file. Therefore, when saving to an external file, only the case of NumPy is considered.

[[[00000000000000000209---359613846b863b72b9db77ff4edbb091c8846070338aec3a1a16ce046fa835de]]]NumPy save and load functions

[[[00000000000000000210---44f5d889f085dacd5728bb4bb2cf11d8993711d07ca357e5ee6e3a156641d8a2]]]NumPy has functions np.save and np.load. This function allows you to save and load ndarray instances. Here's what it looks like when I actually try it:

[[[00000000000000000211---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000212---45a77da11e664c4cbb65d298ee90b92d1550235ec5f2e6b6a4de585c6c0f19b5]]]First is the np.save function. By using this function, you can save the ndarray instance to an external file. And to load data, use the np.load function. Now you can save and read one ndarray instance.

[[[00000000000000000213---e5572f1996be19d2f7ded43ba003f5e1a9d2a3385035cfb7b9f350eb331dad60]]]In the code above, I saved the file as test.npy. As this example shows, the extension should be .npy. If there is no .npy extension, .npy is automatically added to the end of the word.

[[[00000000000000000214---c37d1403b920ad8e67c51f554b1321ef60130447d1217743f248d4904c8ea329]]]Next is about saving and loading multiple ndarray instances. For that we use the np.savez and np.load functions. An example usage would be:

[[[00000000000000000215---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000216---bb9d0b789466cc7e5849a9999346134bb40a2b770f39cba23fe194be7595c5e3]]]As above, you can save multiple ndarray instances with np.savez('test.npz', x1=x1, x2=x2). At this time, you can give it as a 'keyword argument' like x1=x1 or x2=x2. In that case, read data out as arrays['x1'] and arrays['x2']. In the np.savez function, the extension of the file to be saved must be .npz.

[[[00000000000000000217---85b1507d7057dc2ffbd5f42cc7989278b32505425faa186a50af96154047335b]]]Next, let's do the same thing as the above code using a Python dictionary. Here's an example in action:

[[[00000000000000000218---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000219---4babd8d5ac14e9e5f5d3f39ac9aa914d35db93fb46e00fd7cf6e76f420eb402b]]]As above, you can save with the code np.savez('test.npz', **data). By adding **data and two asterisks when passing arguments, the dictionary will be expanded and passed as 'keyword arguments'.

[[[00000000000000000220---b7e7c8a33db5291984f48754a085dcb195de8da5cb05151214a16b7372243245]]]That's how you use NumPy's save and load functions. Next, using the functions described here, create a function that saves DeZero's parameters to an external file. The first thing to do is to “flat” out the Parameters contained in the Layer class.

[[[00000000000000000221---399be5c3b5e60466402be2d3de87666e272e21883788ed553d2a217ee053cf3e]]]There is a np.savez_compressed function that works the same as the np.savez function. The np.savez_compressed function has the same usage as the np.savez function and additionally compresses the file and saves it. Since the function is the same but the file size is smaller, I will use the np.savez_compressed function from now on.

[[[00000000000000000222---ae81a0d20fa0ddf2e6519118d42fc2c035f5f661e8a19510eeff9d2c77b02824]]]Flatten Layer class parameters

[[[00000000000000000223---a47a6041b386abd6abb8e0eee568805d4cbf48dabecc64dbd6547a39b73b47f3]]]As a reminder, the Layer class had a hierarchical structure. Hierarchical structure is a nested structure - a structure in which another layer is contained within a layer. As a concrete example, let's look at the following code:

[[[00000000000000000224---3ac5203c40fd38729eb8abd2bae7f84e4cd6ec0d305a7af179c574a5fdf8fa37]]]Here, layer contains another layer l1. Visualizing this hierarchical structure would look like Figure 53-1.

[[[00000000000000000225---0e875dadcc8e031c79dc9b7df6b7750028e6b0fb74f5822b0ed27d874c8ed64a]]]
Figure 53-1 The Layer Class Is Hierarchical


[[[00000000000000000226---d4d266872e50306d8ff435274ef930345812b91e32d31b494327bd852fc98b1e]]]Now, consider extracting the Parameters as 'one flat dictionary' -- as a non-nested dictionary -- from the hierarchical structure shown in Figure 53-1. For that, add a method called _flatten_params to the Layer class. If you show how to use this method first, it will look like this:

[[[00000000000000000227---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000228---4cdef6fdbaba03b16b74ae43dffe15d03cf7c8b46649fd528eb3d7807c5e8c5a]]]As above, prepare a dictionary params_dict = {} and give it as layer._flatten_params(params_dict). Then the parameters contained in the layer will be retrieved 'flat'. In fact, the parameter p1 in layer l1 is stored with the key l1/p1. So here is the code for the _flatten_params method.

[[[00000000000000000229---860eaebd913a1e72270a2ef24185995e5f7a4fe478531883b18591e1b7db85d2]]]This method takes as arguments a dictionary params_dict and a text parent_key. By the way, _params, which is an instance variable of the Layer class, holds 'Parameter instance variable name' or 'Layer instance variable name'. So the actual object should be retrieved as obj = self.__dict__[name]. And if the retrieved obj is a Layer, call the _flatten_params method of that obj. That (recursive) call allows you to retrieve the Parameter in a flat structure.

[[[00000000000000000230---c589031ab63eeb7de35bb96227dd07b4c1c044c521c5f8781b1a00080e35f31d]]]Layer class save and load functions

[[[00000000000000000231---8b0f2f1112567d394bdaf8100ae1daca3915bddbc172ac4e18e87b68a02088f7]]]Now we are ready to save the Layer class parameters to an external file. Here we add new methods named save_weights and load_weights.

[[[00000000000000000232---808794140dba056bdc5f5264a1469d324604899663338112205506daa4f75330]]]In the save_weights method, self.to_cpu() first ensures that the data is on the main memory (the data is a NumPy ndarray). Then create a dictionary (array_dict) that has ndarray instances as values. Then save the data as an external file using the np.savez_compressed function. Also, in the load_weights method, use the np.load function to load the data and set the data of the corresponding key to the parameter.

[[[00000000000000000233---aaaed0a08db9217f51bafdcf59142319249d0728c6aa7b5bc9b1e5d06f9d4a93]]]The code above uses a try statement to save the file. This assumes a keyboard interrupt by the user (such as pressing Ctrl + C). In that case (if there is an interrupt signal during saving), the file in the middle will be deleted. This prevents creating (and reading) files in an incomplete state.

[[[00000000000000000234---906fed52783ba063645bd20e3a88e78b52a8862c212d7d3fdc3100929c1df062]]]Let's use MNIST training as an example to save and load parameters. The code looks like this:

[[[00000000000000000235---7194ac38c5adb37ed338c4886d67a20e7b8e51bfb156682ed0cbd279f172b371]]]# read parameters

[[[00000000000000000236---2199666961b76540e279f82b42f363f9c0d7b5999d2e5a8413cc68e9ca83b4fd]]]When this code is run for the first time, the my_mlp.npz file does not exist, so the model parameters are randomly initialized and trained as usual. And finally save the model parameters as model.save_weights('my_mlp.npz').

[[[00000000000000000237---4e96886666aebac8351a8bdc1aaffa92851eb03597cb6c0f4690bc826066cc5e]]]The next time this code is run, the file my_mlp.npz will exist and will be loaded. That sets the previously learned parameters to the model. This completes the functions for saving and loading model parameters.

[[[00000000000000000238---a66fc8ffc5427c249ed59925da9020f42c09148dc5dbe0cbeeb81fd6d305193e]]]Appendix B

[[[00000000000000000239---3a2bdc1248b3ce37dba2b0b70e5ff3ca170153dd773bde2f87f4b8c3a6fb657b]]]Implementing the get_item function (supplementary to step 47)

[[[00000000000000000240---319ebdd422c26fcbcac92a7f14813a76c1207b58a1ba5633f1543a249ae9fe0c]]]In step 47, we only introduced the use of the get_item function as a DeZero function. This appendix describes its implementation. As a quick note, here's the code for the GetItem class and the get_item function.

[[[00000000000000000241---c89f5b12cf30689e1de9b72659150e39e541caa8d012848f590d5c74c369e33c]]]Here, we receive the arguments that we want to slice as slices during initialization. And the forward(x) method simply extracts the elements as x[self.slices].

[[[00000000000000000242---6f59561565328934bfddd3f22e594394c873312fe36a87c5fe66d2605d2a2ae7]]]x in DeZero's forward(x) is an ndarray instance, but gy in backward(gy) is a Variable instance. Backpropagation implementations need to perform computations using DeZero functions on Variable instances.

[[[00000000000000000243---a42fce221c3cb1a4bf3c98bd761b9c9380d21ae9c9986f4eea95a6355fa33c03]]]Also, the computation of backpropagation corresponding to the slicing operation does not exist in DeZero's functions. Therefore, we will prepare a new DeZero function class called GetItemGrad. In other words, implement the forward propagation of GetItemGrad to correspond to the backward propagation of GetItem.

[[[00000000000000000244---c3b88476dea1419ed555dcd67ccf8bfe01d6ada832be2f9efe9f68f0436ccdfb]]]Continuing on, here's the code for the GetItemGrad class:

[[[00000000000000000245---518bc893fd945a7add60c697bba0c843b2405c7c56cf2f8edf8a4186bee40956]]]First, during initialization, we receive the parameters for the slice operation (slices) and the shape of the input data (in_shape). And the main computation (forward) prepares a zero-element multidimensional array as the input gradient. Then do np.add.at(gx, self.slices, gy). This adds gy to gx at the location specified by self.slices. The use of this np.add.at function will become clear with the following example.

[[[00000000000000000246---87cc0c360197c31c74ea5978ff399d98d3e4694d6d8617b60d97e34454f17b19]]]If the multidimensional array slicing operation pulls out more than one element at a time, the backpropagation must add the corresponding gradients. Therefore, in the above implementation, addition is performed by the np.add.at function.

[[[00000000000000000247---23174140c189abbd939c7f6c29533487c94e8e485613d933778899835bb00bcf]]]Now we need to implement the backpropagation corresponding to the np.add.at function. Interestingly, it corresponds to the get_item function we're implementing right now. The get_item function is now complete.

[[[00000000000000000248---f758d1e995abb72282ad23bc04d91c7134af31d0f62680cd5d329070067a3027]]]step 12

[[[00000000000000000249---320d9799923648aa7a3a9e983b779f833540ff2255b7dc44ad2782b378724ad6]]]Variable-length arguments (improved version)

[[[00000000000000000250---6b3f3838739ee2b072e8faf3396a152f81c1c83e30a7afb2303d5e2b0b042d79]]]In the previous step, we extended DeZero to accommodate variable length arguments. But that code still has room for improvement. Here, we add two improvements to make DeZero in the previous step easier to use. The first is an improvement for the 'users' of the Add class (or a concrete function class). And the second is an improvement for the 'implementers' of the Add class. Now let's make the first improvement.

[[[00000000000000000251---eb0ffcbb9519969131b1f778dc58b735e49fd6681df0e2669aa9c619dc5c0b4d]]]First improvement: Easy to use functions

[[[00000000000000000252---9576dc2085ba47cd06e375b28305d4259ac68f6cc1b08e995c60bf20f1379644]]]In the previous step we used the Add class to do the calculation. There, I wrote the code for the left side of Figure 12-1.

[[[00000000000000000253---2711bebd9b89f844f2828825ed86b0f4aa3876b3322e4901754fd215a3413a25]]]
Figure 12-1 Current code (left) and improved code (right)


[[[00000000000000000254---7d05dde663257f798a8b49dc105d059c707e9817e23ad01a24a116b3b0291229]]]As the code on the left side of Figure 12-1 shows, currently the arguments given to the Add class are organized as a list and the result is returned as a tuple. However, as shown in the code on the right side of Figure 12-1, it would be more natural to pass the arguments of the Add class directly without grouping them into a list and return the result directly as a variable. The first improvement is to be able to write code like this.

[[[00000000000000000255---f0ff249a14a74b9386fa14a2a986b465bc009c3726cb7caefb6f3a9d4d13a7e5]]]Now let's tackle the first improvement. To do that, modify the Function class. The modified parts from the previous step are shaded as shown below.

[[[00000000000000000256---484f900f10ea7089bd8b729f892c178db3f6a1f32110b168c1383f7e2c0f957d]]]# ① Put an asterisk

[[[00000000000000000257---d015d288af11a9b2412701a3aa6542ae47f357e34745aefa5d7253868dd608bb]]]# ②If there is only one element in the list, return the first element

[[[00000000000000000258---665c90fd55fbe20f6e0ec7f0792c9c8880cebacec39cf8184e0152fd18e0d115]]]First, I will explain from point ②. There, if there is a single element in outputs, just that element should be returned, not a list. This ensures that if the function has a single return value, that single variable will be returned directly.

[[[00000000000000000259---bca5c2ec0e45212d1db0041b1e1aa135c6c5c5a596140dd5034da4aa96bd055e]]]Next is the 1st part of the code. There, I put an asterisk on the arguments when defining the function. This allows you to call a function with an arbitrary number of arguments -- called variadic arguments -- without using lists. The use of this variable length argument will become clear in the following example.

[[[00000000000000000260---6c2c7cbf3e882d156d831277a007f6b0ecffb54b6406c9c215ad78103d79c1c8]]]As shown in the code above, if you put an asterisk in the arguments when you 'define' the function, the arguments in the function call can be received together with the asterisked arguments. With the above changes, DeZero's function class (Add class) can be used as follows.

[[[00000000000000000261---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000262---7176ff32d1d3ac7064ab8c0d69149102bc55892afdd541c901ab1db039f007f6]]]This makes the usage more natural for people using the Add class. The first improvement is now complete. Now let's move on to the second improvement.

[[[00000000000000000263---66a84381caf511c36f30412b7b2f2eaed1c6a6f90d4d2ac0d973c994b96cd0f9]]]Second improvement: easier to implement functions

[[[00000000000000000264---b8274624a251f934680ea3849bf3ffaa14b6af730d6e1b6b47176cfaef38c76a]]]The second improvement is for the 'implementers' of the Add class. Currently, to implement the Add class, we need to write the code on the left side of Figure 12-2.

[[[00000000000000000265---b25e23d3707152afcecaddc4426d36c2140247307001a13c0f4782a53bddc070]]]
Figure 12-2 Current code (left) and improved code (right)


[[[00000000000000000266---b0b9f5fa068141c9cc947adc4fdab5d03ed415c36fdd99cc75f7f608ef37aebc]]]In the code on the left side of Figure 12-2, write concrete processing in the forward method of the Add class. At this time, the argument is passed as a list and the return value is a tuple. Of course, the code on the right side of Figure 12-2 would be more desirable. The arguments of the forward method receive variables directly and return the result variables directly as well. We will address this as the second improvement.

[[[00000000000000000267---88131286adad8ae215e605a7c614f7776af8d76285a9d96cf84a08dc8ff77131]]]For the second improvement, make the following modifications to the Function class.

[[[00000000000000000268---3be258fa2333794e7012c3625c2eddbdb6d72190df2f4ece034c4f5aabd95bb1]]]# (1) Unpacking with an asterisk

[[[00000000000000000269---7bacab9152833033b3715756e40830f8b14c567c46e34af88d479e8d6ab3c522]]]# ②Additional support for non-tuples

[[[00000000000000000270---d178ef0bff241f0d6699cce3d0e666e5386cb0908dced0788060a2a401299552]]]First is the self.forward(*xs) part in ①. There, I put an asterisk on the 'when called' function. This will unpack the list. Unpacking means expanding and passing the elements of the list. For example, if xs = [x0, x1] then self.forward(*xs) is the same as calling self.forward(x0, x1).

[[[00000000000000000271---25f9d1d93fd3c495f0b1e86a4f648cf299a33c0ba597a3f31edeefc520bc991c]]]Then at point ② in the code, if ys is not a tuple, change it to a tuple. Now the implementation of the forward method can return the element directly if there is one to return. With the above modifications, the Add class can be implemented with the following code.

[[[00000000000000000272---2aba405ac02231555d7f1274f9b6be0133a0d146b587bbd744c7d2d14e7f574f]]]As above, we can define def forward(self, x0, x1):. Additionally, the result can be written to return y and return only one element. Now DeZero is easier to write for those implementing the Add class. This completes the second improvement.

[[[00000000000000000273---f7ed3de50139d40d7f250c4133120d451f350adb4d6a727c65ab1b3f5a74e73c]]]Implementing the add function

[[[00000000000000000274---a0c49f2875dccce576438bb57a0ea052138cf49450c5427770dcdc31ba87efd4]]]Finally, add code so that the Add class can be used as a 'Python function'.

[[[00000000000000000275---b8a402e325d8d813bf8019bff4dc8d418a2b2eca7faee20310131e51f368744f]]]Using this add function, we can calculate as follows.

[[[00000000000000000276---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000277---4df3c5e996de4d63792143daff06777a8fbb443faf245e29f0c96c87e31b4a51]]]Now we have a more natural way of dealing with variable length arguments in functions. Here, we implemented only 'addition', but you can also implement 'multiplication' and 'division' using the same procedure. But still, only 'forward propagation' supports variable length. The next step is to implement a variable-length version of 'backpropagation'.

[[[00000000000000000278---edb969c45bf73613babafd9b70bb34d112876a1fed14cfab6ac2545e29c6466a]]]step 60

[[[00000000000000000279---b44894ef07cbca62483467721ab35dc6c8dd7e0a1aaea1c8af6138b1479f5e77]]]LSTM and data loader

[[[00000000000000000280---da4ed5b33fc4fcfef08eef3447e93a2ca3dd9d0b7ac5cfbf6f6bffde8770422f]]]In the previous step, we used RNN to learn time-series data (sine waves). Here we make two improvements to the code in the previous step.

[[[00000000000000000281---3fa85643917374d0e8c49456d41fb7467acf9b9283bd132b24207f410c9e162a]]]The first is to create a 'data loader' for time series data. In the previous step, we forward propagated the model on a single set of data (with a batch size of 1). Here, we use the data loader for time series data to perform forward propagation on a mini-batch consisting of multiple data.

[[[00000000000000000282---27839becc866c7fe03870c126b373fa2da4a87f80389d5b4687de42219f8bddc]]]A second improvement is to use LSTM layers instead of RNN layers. Using the LSTM layer, you can expect more accurate recognition. Finally, after making those two improvements, let's try learning the sine wave again.

[[[00000000000000000283---dbee2ca68fab2b7bfc43f02057fb1e4e730aad10eeaa54cc1c39518b019680c7]]]Data loader for time series data

[[[00000000000000000284---b3da415e8b2f8a1cd6ca64fc61ad1a3ccf61814b1bd8c19180f83d48bc4c6f74]]]In the previous step, we extracted the time series data one by one from the beginning (with a batch size of 1). Here, training is performed by collecting multiple data as a mini-batch. For that, we will create a dedicated data loader.

[[[00000000000000000285---d6ca4041aa4214696d674b5a7d61380e09af8926f727afa1c968de7719819aab]]]In order to collect time-series data as mini-batches, it is conceivable to 'shift' the start position of giving data in each batch. For example, suppose you have time series data with 1000 sequences and want to create mini-batches of size 2. In that case, the first sample data is extracted in order from the beginning (0th) of the time series data. Then, the second sample data is extracted sequentially from the 500th data (the start position is shifted by 500).

[[[00000000000000000286---3b3b29e23a5c36855aaf73815f581677762c8439e7a64061eddefd84a5200e9d]]]Based on the above points, we will implement a data loader for time series data. The code looks like this:

[[[00000000000000000287---0cf13cef16d5fe4db53d9509b358f6f37617f5570079940c177758863ee71c84]]]First, change the initialization part. For time series data, set shuffle=False because shuffling the data changes the order of the data.

[[[00000000000000000288---087fcb2dc6a2c993324ca76bc2a916bad5586ca1cac1df10a95a62714edc1f51]]]The __next__ method implements the fetching of the next mini-batch data. The important point is the shaded code. First, the amount of 'shift' is calculated as jump. And set the starting position of the index to retrieve each sample data to batch_index. Finally, we retrieve the data from the dataset's self.dataset.

[[[00000000000000000289---7d34964906b4a3319fc25cb8d4de88e0c82bc0cc8ae2f81b88ec3c5b79b5618b]]]The above is the implementation of the data loader for time series data. Now you can use the SeqDataLoader class like this:

[[[00000000000000000290---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000291---0bbb1b960a72db3556d3e3f135de40cdc990f88e55f9e80c086ef7eb7c55aef8]]]LSTM layer implementation

[[[00000000000000000292---9b2be7e31166c437f45111ac0e473ef19bb0eff8a2aeb2062411977f9e159709]]]Next is the second improvement. Here we implement an LSTM layer instead of the RNN layer. First, I will summarize the calculations performed by LSTM with formulas.

[[[00000000000000000293---0cec464e34fb82d0930978eee686004bc29ea682f887bc371aa0eebfe72a497c]]]The above formula is the calculation performed by LSTM. In addition to hidden states, LSTM also makes use of storage cells. Also, the notation in equations (60.2) and (60.3) is the Hadamard product, which stands for element-wise product. This is the end of the explanation about the formula, and we will implement the above formula with DeZero.

[[[00000000000000000294---5c37476a7ef58f5297c805fbb418f5a64cfe8d7948e0345273954936c7f160ee]]]In this book, we will only give a brief introduction to LSTMs. For a detailed explanation, please refer to 'Chapter 6 Gated RNN' in 'Deep Learning from Scratch ❷'.

[[[00000000000000000295---47a36925b18c6c7a19feba8c91f60b128f7854b9bd73093bedffdb7242931f38]]]Now, let's implement equations (60.1), (60.2), and (60.3) with DeZero. The code looks like this:

[[[00000000000000000296---b2a458bf54ceab2539337f128fb367d465e379e68351bdc6bdbb1e1d891c2f32]]]As above, the code is a bit more, but the main task is just to replace the LSTM formulas with the code. Our DeZero makes it easy to implement complex formulas like LSTMs. Finally, let's study the sine wave from the previous step again. That learning code looks like this:

[[[00000000000000000297---8e93b6aecdb1560b2f9aa2be381648056efb1588e05d22de21ba8529dc0af420]]]# ① Use data loader for time series

[[[00000000000000000298---df11dd7db36fd7f1591f7c46370c452f59025eb2f4a91517b6ec9af71cc251fa]]]# ② Use LSTM

[[[00000000000000000299---41e3e5526f6e2d7a89c8f48169628bb28829bb5654ac359e8818d74170db436a]]]# dezero.utils.plot_dot_graph(loss) # draw computation graph

[[[00000000000000000300---1e80c05226587fd782127aa274b4d0b1e764f3ad44ef6f24a3f31dda466d8642]]]There are only two changes from the previous step. The first is to create a data loader using the SeqDataLoader class, and the second is to design the model using LSTM layers. Training with this will progress faster than the previous step. Now let's use the trained model to make predictions on new data (noiseless cosine waves). The result should look like Figure 60-1.

[[[00000000000000000301---38726039047870145013769139b7e8324e3c47f5f7fe951630da09bca06eb521]]]
Figure 60-1 Prediction results of model with LSTM layer


[[[00000000000000000302---f75d0b2df1066f7212b7f9fc8c649612f4558a06324a85279ff9ca0f12d72cbd]]]As you can see in Figure 60-1, the predictions are very good. Comparing with the result of the previous step (Figure 59-7), we can see the improvement in accuracy. We successfully implemented a complex layer like LSTM and even performed a complex task of processing time series data! Finally, I would like to finish this step by 'appreciating' the computational graph created by the code above. The result should look like Figure 60-2.

[[[00000000000000000303---d4778dcf8df6d22542297ac713aefc944bbf3f92e3155df4a7963419d636f91b]]]
Figure 60-2 Computation graph created when learning time-series data with a model using LSTM


[[[00000000000000000304---44a8b814a5b5e2e52de7ba94080575c80a6c73b752abc29c2e5551e0045ff357]]]A fairly complex computational graph is created, as shown in Figure 60-2. That complex computational graph would be difficult to create without a framework like DeZero. DeZero's flexibility makes that complex task incredibly easy. From now on, DeZero should be able to solve any complicated calculations simply.

[[[00000000000000000305---da4ffbbd466a5c5e00090b8085f47dad8105caeba138f01b708e1825ba7c286f]]]That concludes the 60-step work in this book. This is the goal point of this book. The fact that we have reached this far means that we have achieved the “big goal” of creating our own deep learning framework. congratulations! And thank you for joining us on this long journey. As an author, I am glad that you took the time to read so many pages.

[[[00000000000000000306---fe5c77bc37abe71985c11abe164331cc76995a50cb4ffc12a8871c3cf07db833]]]Looking back, DeZero started out as a tiny box. From there, we expanded little by little, working on various problems and experiments as we progressed. Through such small accumulations, DeZero has grown into a respectable deep learning framework. In fact, DeZero now has many features that modern frameworks should have.

[[[00000000000000000307---9cc3cf9aa8dce93ed5e99e969bc336b758759d30966442400bda97ea35a5d464]]]This is the end of the work for this book, but of course there is still plenty of work to be done. Readers, go further - and feel free to keep building. You can create your own original framework based on the knowledge gained here. You can extend DeZero further. You can also switch to frameworks such as PyTorch or TensorFlow. Enjoy your new journey to your heart's content. In the next column, titled 'From now on,' I will discuss the future direction of DeZero. Please refer to it if you are interested.

[[[00000000000000000308---b6c6c31738f4528d53d50433e738864d9936ff4f16f1e59716cf1f5cec7dac21]]]Column: Moving forward

[[[00000000000000000309---931bd0094ff4b403e67733fb9205e7fe02c586f1544d5985f8aa358a6d32a2c7]]]In this column, I will talk about some of DeZero's 'Future Work.' In the future, I will summarize what I can think of, such as what kind of extensions can be considered for DeZero, and what kind of development and materials are necessary as OSS (open source software). In addition, I will list the stories of DeZero's production process (logo creation, etc.) that could not be mentioned in this book.

[[[00000000000000000310---154d8062f1bd266ae536502a92b7dc57ff86e5a655ce90fd2542e9a5cd28cbc8]]]Add functions and layers

[[[00000000000000000311---145fe023b3ad298f9cd623cea0953227ae6156ae378e74eef67c5b273fa36771]]]In this book, we have implemented many functions and layers of DeZero. Of course, there can be many functions and layers that we haven't implemented yet. For example, tensorDot function for tensor product and batchNorm function for batch normalization. You can also refer to other deep learning frameworks to see what other functions and layers you need. For example, the PyTorch documentation can help you identify missing functions for DeZero.

[[[00000000000000000312---90e67ecea65efd92c3d0d8cf088bb199ad634b537e4940ebeac529e0c861895a]]]Efficient memory usage

[[[00000000000000000313---ff82ad2b4013c7282fc50c8a89df36c0b95b64f4757b50ec0a78765551fef562]]]Improving memory efficiency is an important topic in deep learning frameworks. Especially when it comes to large-scale networks, it uses a lot of memory, and it often causes problems that it does not fit into the physical memory. DeZero has some improvements in terms of memory efficiency. The most important improvement is that all calculation results (data ndarray instances) during forward propagation are preserved. This is where DeZero holds all the intermediate results for use in computations during backpropagation. However, depending on the function, it may not be necessary to retain intermediate calculation results. For example, the tanh function can compute backpropagation without input for forwardpropagation. Therefore, the input data for forward propagation can be cleared immediately. Considering this, we can think of a mechanism that can determine the data to be retained for each function. In fact, such a mechanism is already implemented in Chainer and PyTorch. For those who are interested, Chainer's 'Aggressive Buffer Release' [24] mechanism is a good reference.

[[[00000000000000000314---331169423cf28d34be21e9d7d5ee14152f5329ce1cc629cb555ed93f411bc942]]]Static Computation Graph and ONNX

[[[00000000000000000315---897a7f359c2f8be3bd8102f052ee1b86a2585c2bbe28bd2f11fa5995524714e7]]]DeZero creates a computation graph using the Define-by-Run (dynamic computation graph) method. On the other hand, it does not provide a Define-and-Run (Static Computation Graph) method. Static computation graphs are useful when performance is a concern. Furthermore, compiling (translating) the static computation graph enables execution in environments without Python. With that in mind, you may want DeZero to have a mechanism that can run on a static computation graph.

[[[00000000000000000316---2740b32fa36d2a3eefcf9b1d68773c0cc3ff23b8701be4b82d5d0f77d0cec473]]]Also, in the field of deep learning, there is a data format called ONNX[40]. ONNX is a format used to represent models in deep learning and is supported by many frameworks. The advantage of ONNX is that trained models can be easily ported to another framework. If DeZero also supports ONNX, it will be possible to interact with various frameworks, and its versatility will increase.

[[[00000000000000000317---e395ffd1e1810fa50f8de6688bdd1c70f1b0df427263c01b4996ea003d00b28e]]]Publish to PyPI

[[[00000000000000000318---5db52a412e7a4d61a666e78660fede96fa976a43e664b006899f17a637005ce3]]]Once the development of the software is completed, we package it up and publish it for users to use. For Python, a package repository called PyPI (Python Package Index) is commonly used. Once registered on PyPI, it can be installed by the command pip install ... Now anyone in the world can use it easily.

[[[00000000000000000319---3e174d926637a3c8c2bd55b6c9d95e949a1b384a76f582ca226a2fefe357d3b2]]]DeZero is already registered on PyPI. There is a lot of information on the web about how to register with PyPI, so please refer to it as appropriate. Of course, you are also welcome to develop your own original framework based on DeZero created in this book and publish it to the world. Please try it.

[[[00000000000000000320---6913ddd0bbf1642b456d42a2d35c61ca4df0acc22011fa6d79c2baf604cb8936]]]prepare a document

[[[00000000000000000321---4989f558588841b4991a454bbc66ee0bd9cd7c7f153c60433d160df745403f49]]]If you publish a framework (or library), having documentation is useful for users. Many well-known frameworks provide documentation on how to use functions, classes, etc. (APIs).

[[[00000000000000000322---31c5a4528fefbd1adcd632ead25d8cff44205bc02ffe25fa43ab4c38d4802741]]]Python has docstrings (documentation strings). A docstring is a descriptive text (comment) given to a Python function or class. Leave comments in a fixed format on the code.

[[[00000000000000000323---83c4fb049037a4549568cc72438a6e3569f7b926b8d6d064b846b52f8de6f72b]]]The docstring is also added to the actual code of DeZero. For example, the code for the as_cupy function in dezero/cuda.py is written as:

[[[00000000000000000324---54e3b3a578caaef457cf7e9de4c373d922b366247178ecb559636eac227a7122]]]As above, the comments provide an overview of the function, argument types, return type, etc. There are several popular styles for this description format, such as 'NumPy style' and 'Google style'. DeZero adopts 'Google style'. A description like the one above would help the user know what the function is. Of course, this description can also be written in Japanese (DeZero wrote the docstrings in English with the expectation that the book will be translated into multiple languages).

[[[00000000000000000325---5c21fa1d6f4047a0cf349b2eac240c6fb86c494439a4ac0dec34ef9d944c0c0a]]]Also, if a docstring like the one above is prepared, you can use a tool such as Sphinx[39] to output it in a format such as HTML or PDF. Sphinx allows you to create dedicated pages without hassle.

[[[00000000000000000326---9e781b2dda72e79c770446c2d1893eebbb47a6f26671eb554a78ccc366694960]]]make a logo

[[[00000000000000000327---b86d5be6513b4a84471bcd0e87f4f975e8609ef0444a858b6ae596a041202af0]]]For OSS, it is possible to create a logo. Having an attractive logo will help users recognize you. Of course, the logo can also be created by the developer himself. However, if you want to make the design more attractive, it is one hand to ask a professional. Recently, there are many cases where logos are created using crowdsourcing. The DeZero logo was also created in a competition format using crowdsourcing. Thank you for your wonderful design.

[[[00000000000000000328---979bd7d284851123dc1843228a06dd616583c8348526c82f7674ed901b0fd16b]]]Add implementation examples

[[[00000000000000000329---00a7a171be5f74f8aa0d29cb8d73f8eb7f37bd18bbf75252c4428583beac8ec0]]]You've built DeZero so far, but the real fun is using DeZero to implement well-known research and new models of your own creation. As part of that activity, we can consider increasing implementation examples using DeZero. Implementing well-known studies such as GAN[41], VAE[42], and Style Transfer[43] with DeZero can demonstrate how to use DeZero. Also, through such work, we can see the missing functions of DeZero. There are some implementation examples using DeZero in dezero/examples (and we plan to add more). Please refer to it if you are interested.

[[[00000000000000000330---6df93974e63006f525f48de50c504d6905c7d8c1287f077809e1ef7a2baa20d8]]]step 16

[[[00000000000000000331---3a9e6c3890958204388a0f824824998c120214e7c1b3786dbad051ad9a0c20c7]]]Complex Computation Graph (Implementation)

[[[00000000000000000332---506674b294b1a19bb4604a87e018077a6773d5dc9101ad93ab22fb563cd475c2]]]This step is the implementation version of the previous step. First, we will implement the implementation that sets the “generation” during forward propagation. Then, during backpropagation, the functions are extracted in descending order of the number of 'generations'. It backpropagates in the correct order for any complex computational graph.

[[[00000000000000000333---080c35984652072449ab185aff21d7ef29d99165b0a8728921ffa17a9e2d2ebc]]]Add generation

[[[00000000000000000334---469a77a5b4978ca7a7f4d9f2075dc9135737bc957febfc7420125a8457f251e6]]]First, add instance variable generation to the Variable and Function classes. This generation indicates in which 'generation' the function (or variable) is. Let's start with the code of the Variable class. Here is the additional code for the Variable class:

[[[00000000000000000335---3a8d6a8ae21087ee7bc8a64320188b998edfd315038052b7eeaa85353724a2be]]]The Variable class initializes generation with 0. Then, when the set_creator method is called, set the generation to be one greater than the parent function. For example, a variable produced by a function with f.generation of 2 will have y.generation of 3, as shown in Figure 16-1. The above is the additional implementation of the Variable class.

[[[00000000000000000336---6bd8ea881311eb9011564f6f82081310452edb3d4c919f53950ff6c85d4e534f]]]
Figure 16-1 Variable generation relationship diagram (generation values shown above nodes)


[[[00000000000000000337---fa731f9e32282dea861fa87a5988282ad10d9ec51a6341c00945a3271e2c2372]]]Next is the Function class. Set the Function class to generation with the same value as the input variable. For example, if there is one input variable and its generation is 4, the function's generation is also 4, as shown in the left figure of Figure 16-2.

[[[00000000000000000338---dea06567c1745803b30e44d1827706e3bafb5f97204a6c4fbbf95b6e33530a08]]]
Figure 16-2 Function generation relationship diagram


[[[00000000000000000339---3dec703537c74aedcf7a5509c2c92ad1634f929aed6e08f14f8894055179203d]]]Also, if there are multiple input variables, the largest number of generations among them is adopted. For example, if there are two input variables and their generations are 3 and 4, as shown in the right image of Figure 16-2, set the function's generation to 4. To satisfy the above points, add the following code to the Function class.

[[[00000000000000000340---b995b091ee4c273ac25e793ea7bd0607dab902363edf93450af61dfe9ece6d9f]]]The shaded code above sets the generation of the Function.

[[[00000000000000000341---274a075c6a0136fcc7d1e1950aacd3cdd8dda5587a396a689cf04412660a9e84]]]Sort by generation

[[[00000000000000000342---37bf3e6492e62b96a3d769acb104b0e0165dac1316bbac8b68e199e0f8b4f438]]]With the above modifications, generation is set for variables and functions when normal computation (forward propagation) is performed. As a concrete example, let's look at a computation graph like Figure 16-3.

[[[00000000000000000343---f7c6756a55c632fbaff16b49e5e3a4e2329ee72f889475a3a1b3551cc7d6d387]]]
Figure 16-3 Example of 'Generations' in Computation Graph


[[[00000000000000000344---f42bbdfe6345ed319f0b9b6fbbddf43b7b97508becf374770ffe267ce530560b]]]Looking at Figure 16-3, function A has generation 0, functions B and C have generation 1, and function D has generation 2. If the generations are set in this way, the functions can be retrieved in the correct order during backpropagation. For example, functions B and C with large generations can be extracted before function A.

[[[00000000000000000345---041344a072f9b1479314745ba9af80f5e1f1ba73be46267a691df2582f9f3294]]]As mentioned in the previous step, in the backward method of the Variable class, candidates for functions to be processed are added to the funcs list. Then, when extracting from the list, by extracting the function with the largest generation first, the functions can be backpropagated in the correct order.

[[[00000000000000000346---c081a7be6dc5ad4b1644934aa5b3bb35e5e369f828bb994abefbc4099731e58b]]]Next, we will implement the implementation for retrieving the functions in order of generation. As a preparation for that, first, let's do a simple experiment using a dummy 'DeZero function'.

[[[00000000000000000347---81dac0c07d8ddb42d3708be194ed334ecef21b5745e6a7c18340d1d80e7fa986]]]# dummy function class

[[[00000000000000000348---460bea99d8daca9e167d3558c490c4f3551c41e21417004309d3909b5ae660ee]]]I prepared a dummy function as above and added it to the list called funcs. Now, let's take out the function with the largest generation from this list. It could have the following implementations:

[[[00000000000000000349---6407e4fb8883b035f5842090cd2132886702d65c22220b2d8a05e76a6035017b]]]As above, the sort method of the list sorts in ascending order of generation. Here, by specifying key=lambda x: x.generation in the argument of the sort method, when x is the element of the list, sorting is performed in ascending order of the value of x.generation. Then you can retrieve the largest function of the generation by retrieving the last element of the list with the pop method.

[[[00000000000000000350---007fe122f92a35f7067ef8d2aa1438f59f675c1d5f1fbf2edf8e2fe836f1da11]]]All we want to do here is take out the function with the largest generation. So you don't have to sort all the elements as above. A more efficient algorithm would be to use a 'priority queue'. This document does not implement it. If you're interested, go for it yourself (hint: Python has the heapq module).

[[[00000000000000000351---21e2b598a3b60dd54d849d25f94eeee548d135f5676eea8b120de205f9ae6508]]]Variable class backward

[[[00000000000000000352---35c2aa7f6bf5ab5a150aae967271287c94600b0d64fe3cecca558fdddbca1747]]]Now, let's implement the backward method of the Variable class, which is the main topic. Let's take a look at the changes from the current situation (shaded parts).

[[[00000000000000000353---172f812b95256147dc633f97349bf1be601de07cb3339a77a528b7edcfaf573e]]]Here we have added a function called add_func. We used to call funcs.append(f) when adding a 'DeZero's function' to the list, but we'll change that to call the add_func function. In this add_func function, the list of 'DeZero's functions' is sorted by generation value. Then, the next time you retrieve the 'DeZero function', funcs.pop() will retrieve the largest function of the generation.

[[[00000000000000000354---938f28053c1cf6d6aee5216190a6280a6d809865c55117bd07079fdb2ba4ead3]]]By the way, here the add_func function is defined in the backward method. This usage is appropriate when the following two requirements are met:

[[[00000000000000000355---ef25bdf24340f92b49accecec5cde89d5c5eda565276d1eead85ec587a60519f]]]Only used in the parent method (backward method)

[[[00000000000000000356---6c7f59af1ed948c848e2e4d6ae9a859351ddd5cadc19503806917df7df3e839a]]]It is necessary to access the variables (funcs and seen_set) used in the parent method (backward method)

[[[00000000000000000357---da40114588b861ff02a68cc6b17e14079ae766996db0fef24a30566b861e4027]]]The add_func function is defined in the method because it meets the above two requirements.

[[[00000000000000000358---52ae72dd898dd9215c255703beaeee66d636950666ed43caaf05248a332bc82e]]]The implementation above uses a set called seen_set. This is to avoid adding the same function to the funcs list more than once. This prevents the function's backward method from being accidentally called multiple times.

[[[00000000000000000359---5f867e41fe27e63b34fd8dfee8bfaf939a8849dd5e45f7bc8e60e094a27a65cc]]]operation check

[[[00000000000000000360---a3d4e266666321d983d0df446d4474911ad3bb5e740ffa6d3f2db049d310d342]]]With the above, functions can be extracted in descending order of generation. It should backpropagate in the correct order, no matter how complicated the computational graph. As a trial, let's find the derivative of the calculation in Figure 16-4. Here's the code for that as well:

[[[00000000000000000361---bbf81588281da7861f23f9edad57774aab261bbaa506af481808f812d9213688]]]
Figure 16-4 Example of a Computation Graph That Wasn't Handled Correctly Until Now


[[[00000000000000000362---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000363---e5303f3521d19ae46bbc959806494f31045240069db05b958e9b2606a72cefaf]]]Looking at the execution results, the derivative of x is 64.0. If you check it with a formula, you will find the derivative of . At this time, so, the derivative when is It certainly agrees with the results above.

[[[00000000000000000364---2df4cbe94d5d8b9da43b6e0976cbe0e47598611b841f65b8a05b8fe2430e971c]]]congratulations! We can finally handle complex computational graphs. Figure 16-4 was a somewhat simple computational graph, but in fact, DeZero now can obtain the differentiation correctly no matter how complicated the 'connection' is. For example, even with a computational graph like Figure 16-5, the derivative can be calculated correctly!

[[[00000000000000000365---cc2c64bc43e70d695459e56169b2788ab33bb8fa2dd3402444f808b1a4ccdedb]]]
Figure 16-5 An example of a more complicated 'connection' computational graph (computational graph of the fourth derivative of y=tanh(x) actually created in step 35)


[[[00000000000000000366---504809cd6695742a40224bd3d93ed18c5327fc0b4bb33f1faa7741171e515731]]]This is the end of this step. This step is one of the most difficult parts of the entire book. If you clear here, you will soon be able to know the power of our DeZero. The next step is to look at DeZero's performance, especially its memory usage.

[[[00000000000000000367---b17a1ed4deb4c7e1e046e4b0cf5ad9f7554b0f17785f63851afde493940de72f]]]step 39

[[[00000000000000000368---f6303892868fe1105b7aaf7ac5c3adbaae7d8a04d58e1ca0ea790a690dc068cb]]]summation function

[[[00000000000000000369---1135af0fb44a7838a2f980082ad79ba68576c0db82f81a720a322026a19dacad]]]In this step, we will implement a function to find the sum of DeZeros - the sum function. First, we will review the differentiation of 'addition' and apply it to derive the differentiation of the sum function. After that, we will proceed with the flow of implementing the sum function.

[[[00000000000000000370---639658cb82bfe1baf8aa8ff38cb2c858d7a8114bde85ab8f41ed48c18ecfedc2]]]Backpropagation of sum function

[[[00000000000000000371---f5f8c99593ba604818c58ca282192a5d8f1ec03a920824e03f36fb1e2aac0d4d]]]We have already implemented a function that does the 'addition'. The derivative of the addition is , when . Therefore, in backpropagation, the gradient transmitted from the output side was simply passed to the input side. Graphically, this looks like Figure 39-1.

[[[00000000000000000372---4f21a3f990905835f76d259304def61f827e46fef136b0af94bb445cccdad196]]]
Figure 39-1 Forward and Backpropagation of Addition


[[[00000000000000000373---3b648de98cfdf20b16e1d3ef9d01efee87f324f3301ec8a5307c46a3686aad68]]]The computation in Figure 39-1 backpropagates from the variable y after the addition. At this time, the gradient of 1 transmitted from the output side is 'copied' into two variables x0 and x1. This is the backpropagation of addition. This backpropagation of addition works just as well for vectors with two elements. Now let's look at Figure 39-2.

[[[00000000000000000374---371f831ae3e33db630e8ec5245a2ffb2494da515f0b15ded1cbca99bee03ff79]]]
Figure 39-2 Example 1 of calculation graph of the sum function (the function that performs backpropagation is represented by sum')


[[[00000000000000000375---f9dac4ecd74fb274323b5a06cb2474cb2b724ab3f17aadd0230b4004e0013a9e]]]The variable x in Figure 39-2 is a vector of two elements. Applying the sum function to that vector will output a scalar. The backpropagation spreads the value 1 transmitted from the output side to the vector [1, 1] (one-dimensional array) and propagates it.

[[[00000000000000000376---28a2fdb1838cc335cf651e4cdd65b2a3d424eafcea3350b32a9f70e88aeb4d3d]]]From the above, we can derive backpropagation for the sum of vectors with two or more elements. It 'copies' the gradient as many times as the number of elements in the vector. Graphically, it looks like Figure 39-3.

[[[00000000000000000377---f694a18510bf9b111a28c3ffd45ca94d7fd774440be64a6e6d8abcf6ff341884]]]
Figure 39-3 Calculation Graph Example 2 for the sum Function


[[[00000000000000000378---93b2fbc561f32f5185af621af2eb3e6de5aef5d4c5edef24f9ec7cd8c912759c]]]Copy the gradient to the shape of the input variable, as shown in Figure 39-3. This is the backpropagation of the sum function. This applies equally to arrays where the input variable has more than two dimensions. Then we move on to implementation.

[[[00000000000000000379---65b76b9125f86a12102fd608fa71842d09cf94f85d5029eed9a7dafae2770b6b]]]Implementation of the sum function

[[[00000000000000000380---7cefbb57728c512bcf9f4648347ede83cd9601324d5f40f31290e5698e182388]]]Backpropagation of DeZero's sum function copies the elements of the gradient to the shape of the input variables. However, backpropagation requires calculations on Variable instances. In other words, the copy operation must be performed as a function of DeZero.

[[[00000000000000000381---0bd8f6919d7544544e8d0179c3736e05987592bbcddcd7464e5a2ac68581a3ae]]]The operation of copying an element to a given shape is the same functionality as NumPy's broadcast. Such an operation will be implemented in the function broadcast_to(x, shape) in the next step. It copies the elements to the shape of x (Variable instance).

[[[00000000000000000382---af06540c7d956c465f1205ff9d3e1f93db9e8dbccb425da8a0a6f93d3ff566e2]]]Here, we will preemptively use the broadcast_to function. Then DeZero's Sum class and sum function could be implemented as follows:

[[[00000000000000000383---906da65b3dc693800fffed909a1f17b04f0666eb446ce55c4644eadeb64ba71f]]]As above, backpropagation uses the broadcast_to function, which we will implement in the next step. Use that function to copy the elements of the gradient gy to the shape of the input variable. This completes the implementation of the sum function. Now, let's use the sum function we just implemented.

[[[00000000000000000384---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000385---a6afe6f820ca716ac1a570a98edc1460996ae8f1b41a2858cea1805d79fa6587]]]As you can see above, we have successfully calculated the sum and the gradient. This works correctly even if the input variable is not a vector. For example, for a two-dimensional array (matrix), the result would be:

[[[00000000000000000386---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000387---346118b9483655c95ece5a7580673c234d84206f0edfa84940bae87089ab0012]]]As above, x.grad and x have the same shape. That value also gives correct results. That completes the 'basics' of the sum function. Next, we extend the current sum function to complete the 'real' sum function.

[[[00000000000000000388---9a836392fd7abd6f517121267708af3c27da51392cdbeba5594af42925bd312b]]]axis and keepdims

[[[00000000000000000389---3d14c2623adafb2996aafc376ff1078cb48aeb1bc5e2c13e2141534fc002fa5c]]]NumPy's np.sum function is more sophisticated. For one thing, you can specify an 'axis' when summing. For example:

[[[00000000000000000390---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000391---b1dd451cb09154d63a9830e9090a872c2670e1b5f17462d63e1fc36d8d0111d7]]]where x has shape (2, 3) and output y has shape (3,). At this time, I specified axis=0 like np.sum(x, axis=0). This axis means 'axis', which means the 'direction' of the alignment of the multidimensional array, as shown in Figure 39-4.

[[[00000000000000000392---b74f840d2b1b88f4a538a3760f055b86797bc77359e94cb80c8ed19e608b6ab6]]]
Figure 39-4 axis (number of axes) of ndarray instance


[[[00000000000000000393---fb1d66de2bf4185f8298156dd50b3ed1e62d8ad5fc6d5afb5002daf06810a9af]]]Figure 39-4 is an example of a two-dimensional array. At this time, the index of the axis is determined as shown in the figure. By specifying this axis, the np.sum function can sum along that direction (Figure 39-5).

[[[00000000000000000394---57588c5ef72236f179999522e3b9446567c871ac2c7899baabf3160bc425b456]]]
Figure 39-5 Calculation result of x.sum() for each axis


[[[00000000000000000395---eec585220dd59f5e445808aa3de16fa5925bf0d6b2b939c746a2b870bf887f9f]]]In addition to int, you can also specify None or a tuple for the argument axis. If None, sums all elements and outputs a single value (a scalar) (default argument is axis=None). If you specify a tuple such as (0, 2), the sum is taken with respect to the axis specified by that tuple (two axes in the case of (0, 2)).

[[[00000000000000000396---5e79d4766ece67f5df1082a118e0715867c9999c9aa4e0d28913f4ecf19bdff4]]]The np.sum function also has an argument called keepdims. This is a flag that specifies whether to keep the input and output in the same number of dimensions (number of axes). Here's an example in action:

[[[00000000000000000397---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000398---4b5d63db65d207647de3ca8774dda648bd9795a2294f89f5c181fa937416fc31]]]where the shape of y is (1, 1). If keepdims=False, the shape of y will be () (scalar). Thus, specifying keepdims=True preserves the number of axes.

[[[00000000000000000399---e6f465d5a8052dc6c69ea8fa31f4316ee77c0b81042b7aa5e0c39336248bcb9f]]]The two arguments we've seen so far -- axis and keepdims -- are commonly used in practice. Therefore, we will modify DeZero's sum function so that these two arguments can be specified. The theory of backpropagation for the sum function is the same, although the axis and keepdims make the calculation of the sum slightly more complicated. It just copies the elements of the gradient to have the shape of the input variable. So here's a modified version of the Sum class and sum function:

[[[00000000000000000400---769bf451921f64917147204523cf0a3a8e3baedc0b10b2371e58a7b0959e846e]]]Initialization of the Sum class takes the axis and keepdims and sets them as attributes. And forward propagation uses that attribute to find the sum. The implementation of backpropagation still uses the broadcast_to function. It copies the elements of the gradient to the shape of the input variable.

[[[00000000000000000401---51d5493092df1f7b8299a867d6ca3dd8ae8187b902a2780b67b97610f3a25243]]]The implementation of backpropagation uses a function called utils.reshape_sum_backward before the broadcast_to function. It 'tweaks' the shape of gy (because there are cases where the summation with axis and keepdims will reshape the shape of the gradient, so that's what I'm doing). This is a NumPy-related issue, not essential, so I won't go into detail.

[[[00000000000000000402---b8ce4783618bff2541fcbfbdb5f4689bece5db376966736664d91ccea695fcb7]]]This completes DeZero's sum function. Here, we will make this sum function available as a method of Variable. To do so, add the following code to your Variable class:

[[[00000000000000000403---8f017744c00b42107955d84105f1e0e66ac3f57b68406fdcf8bd737d7c6149c8]]]Now you can use DeZero's sum function like this:

[[[00000000000000000404---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000405---1cd71125f0330e68927d528eedab7d348281c321d58a41b25128aa4291500704]]]This is the end of this step.

[[[00000000000000000406---edc3efb149a25b9613ce36ac49d50fa08fbda9e7b7120a27b83d2c9a69b24a3f]]]References

[[[00000000000000000407---c3a7007c6c12300bdfed11cc1f0072ac8c4b9f897ac89c77fc08a5f170615014]]]1st stage: automatically find the derivative

[[[00000000000000000408---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000409---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000410---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000411---4c049531e0bdb5788087288867d9c48491e9e1954a74ebe29b7d492b1f183408]]] Python Document, “unittest --- unit test framework” 

[[[00000000000000000412---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000413---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000414---b8e78cadd069bf11ba13d10e73c9b796e16247472dccc3b5c12c4c3cb7f7b137]]]Second stage: express in natural code

[[[00000000000000000415---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000416---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000417---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000418---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000419---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000420---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000421---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000422---06ebb01f6050c2a2872f9c9b89b0df3a6d5844637abe97ad000c977f8a5eecc8]]]3rd Stage: Realizing Higher Order Differentiation

[[[00000000000000000423---11f66c3543a50d1554cabb7655b4926d19c4bf279d6c2ed78bd106e65a43dc01]]]) (November 2019)


[[[00000000000000000424---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000425---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000426---c47968755db109425d1c7b5f0a020c920a0672102f6e91d07d4fdfb33667ece8]]]Fourth stage: Build a neural network

[[[00000000000000000427---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000428---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000429---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000430---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000431---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000432---4893204ab92a4f1214648dfedb6cf2236e4fb64b86964397fd3fb32bb92a696c]]]Stage 5: Challenge with DeZero

[[[00000000000000000433---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000434---f112db132243d0310bcce20f1f5eb7b21066433e74982d11839a8268cbfd8300]]](November 2019)


[[[00000000000000000435---3201c4535a225da6767f4168a0dcc1e1993bb69c0dacf3cdf1cbbe8218953237]]]step 3

[[[00000000000000000436---6a372e9d47e467c7d04d34602224bb3f590022e6bcb5dd1a59b8d76292121933]]]Concatenation of functions

[[[00000000000000000437---411d4c42af91154383dcd6f609d37adaad46c53f556941a05c6770ce21a9d733]]]So far, we have created DeZero's 'variables' and 'functions'. And in the previous step, we implemented a function class called Square that performs square calculations. In this step, we implement another new function and combine multiple functions to perform the calculation.

[[[00000000000000000438---ac468a14f24169f0c4c010b09dbbe46cbdce73cfaa2b339b66bdcb0f3986c4bc]]]Exp function implementation

[[[00000000000000000439---b76682928d0c5949277d53ba3c8861ee8cb33c88b2ac1b29b30f51e05663821b]]]First, implement one new DeZero function. Here we implement the calculation ( is the Napier number, specifically the value of ). Let's implement that code now.

[[[00000000000000000440---11681eb6a3f63f173f43858766bd6a412f518ea5c5827339951956d89f41b334]]]As with the Square class, extend the Function class and implement the desired calculation in the forward method. The only difference from the Square class is that the content of the forward method has changed from x ** 2 to np.exp(x).

[[[00000000000000000441---dca43dc16370e54e5004e4577ba74ac41309a8134f5e6e97b98beb6c36caf03d]]]Concatenate functions

[[[00000000000000000442---e7281264bcf286bac4c17af499e2eba3e6087cec53afb03051413104e4a60fb8]]]Both the input and output of the __call__ method of the Function class are Variable instances. So it's natural to use DeZero's functions in succession. For example, consider the calculation In that case, you can write code like this:

[[[00000000000000000443---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000444---c8a3e052f69232bcd2a53cc71c537e01873d8830577742568c4afe544a9f599f]]]Here is code that applies three functions - A, B, and C - in succession. The important point is that the four intermediate variables -- x, a, b, y -- are all Variable instances. Since the input and output of the __call__ method of the Function class are unified with Variable instances, multiple functions can be applied in succession as shown above. By the way, the calculation performed here can be represented by a calculation graph in which functions and variables are arranged alternately, as shown in Figure 3-1.

[[[00000000000000000445---68c95c93869fc87d8229eb7b58140168b5c8839b932f8c9921957e67c76f1ffe]]]
Figure 3-1 Calculation graph with multiple functions (○ is a variable, □ is a function)


[[[00000000000000000446---5cbe0fde626fe77ddb2eacfeaace72799e0be8d1f5841fb9495178470805ca90]]]A transform created by applying multiple functions in sequence, as in Figure 3-1, can be viewed as one large function. A function composed of multiple functions is called a composite function. The important point here is that even if the functions that make up the composite function are simple computations, applying them in succession can lead to more complex computations.

[[[00000000000000000447---8f494576d6d8e6a9275219b5ae27661e0f56cbd49f1a0b11b8a94ce1231bc82f]]]By the way, why do we represent a set of computations as a 'computation graph'? The answer is that we can efficiently find the derivative of each variable (precisely, we are ready to do so). That algorithm is back propagation. From the next step, we will extend DeZero so that backpropagation can be realized.

[[[00000000000000000448---f9d6e3e08e0b1616b013b51145e90742024e5e562bdb95de30f10bafb702a8a6]]]step 25

[[[00000000000000000449---b57c2a490d875fd6d9df2cbffd9e72e24d74c13d3e5c72b4d3b7b03ef1492e61]]]Visualization of computational graphs (1)

[[[00000000000000000450---bab3441ff109bcbc1730d7d0164b45aefa09a8e037723e8f4668973e4479c005]]]Our DeZero can now easily translate even complex formulas into code. In step 24, we actually coded a rather complicated function called the Goldstein-Price function. By the way, what kind of 'computation graph' is created behind the scenes when such complex calculations are performed? Wouldn't you like to see the 'whole picture' with your own eyes? For those people, this step visualizes the computational graph.

[[[00000000000000000451---743e588ef11283b95fe8da9027acad791d2fff1d9bb298652d4e32e1d9558150]]]Visualizing the computational graph makes it easier to identify the cause when a problem occurs. Visualization of computational graphs can also lead to the discovery of better computational methods. Furthermore, it can also be used as a tool to visually convey the structure of a neural network to a third party.

[[[00000000000000000452---2b3500462edda633b39d7cba210dded5e918925f9732125754db650e2b023953]]]It's possible to build a visualization tool from scratch, but that digresses a bit from the subject of this book, deep learning. Therefore, we will take advantage of an external asset - a tool called Graphviz[18]. In this step, we will mainly explain how to use Graphviz, and in the next step, we will visualize the computational graph using Graphviz.

[[[00000000000000000453---73719bd87b3d8f78b863b5f6cf2f145bf933f3e5fcfa248e0f3b46274e46f93e]]]Install Graphviz

[[[00000000000000000454---2b4c09fee19f2edb5d690d05805eda3751f7a70fd76329cc1fdebd63a11be5da]]]Graphviz is a tool for visualizing graphs (where 'graph' refers to a data structure with nodes and arrows, like a computational graph). Graphviz makes it easy to draw beautiful diagrams. First, I will explain how to install Graphviz.

[[[00000000000000000455---734ad7abca1628dd9673fd2a06985f1ff48850f171a4e88dc619b742d8fde1b9]]]Graphviz is available for Windows/macOS/Linux. Here, we will introduce the installation method for macOS and Ubuntu (one of the Linux distributions). For other OS installation instructions, please refer to the Graphviz web page (https://graphviz.gitlab.io/download/).

[[[00000000000000000456---e573e316b69065330b75eea1332a4940127ee17653c494fd32d74d59fbf13e1f]]]On macOS, it can be installed using Homebrew or MacPorts. If you use Homebrew, open a terminal and run the following command:

[[[00000000000000000457---3e2e8b2cfc25c0a033e1ea634e53d6a516a7b6bbd2b8dadf513022e42f767652]]]For Ubuntu, you can install it by running the following command from your terminal:

[[[00000000000000000458---a3e0ad4656b54c865318fc450b4cdafe68fe6089866811c8c389c07218d3776b]]]Once installed, you will be able to use the command dot from the terminal. Now try running the following command:

[[[00000000000000000459---e0ce601f021cf2199dd0a62cd41230a483849b936ba292152c9626cf0cbe2df9]]]If the version of Graphviz is displayed as above, it means it was installed correctly. Next, I will explain how to use the dot command. Use this dot command like this:

[[[00000000000000000460---061221a877e142c7b79916ae68f62a9e559af7472fe37aba33183a535f3a5a45]]]This is the command to convert the file sample.dot to sample.png. After the -o option, specify the 'file name' to be output. And after the option -T, specify the 'extension' of the output file. Here, png is specified as the extension, but pdf, svg, etc. can also be specified.

[[[00000000000000000461---f44d94c85091e9ac9da5a4bd0a81cd37483b56b7b925a75f876f864a9b07c807]]]In the command above, we specified a file called sample.dot as the first argument. In this sample.dot file, write the contents of the graph you want to draw in 'DOT language'. The DOT language is a language for describing graphs, and can describe graphs with a simple grammar. Next, we will learn about this DOT language.

[[[00000000000000000462---c675df19dd21dac74197e1a27d4563fc55613135ccc62fbeb20e119922a15855]]]Draw a graph in DOT language

[[[00000000000000000463---97137fd6c8dc4dc82d3903de99292e4f33b23101e6a872a633c09ca57c4d569d]]]Let's draw a graph in DOT language. Open your favorite editor and enter the following text.

[[[00000000000000000464---008231fcdc0a4492759d16c0a3cf0bcf52e8011ddc70f537fed5e39ac5ddd06b]]]I will explain the grammar of the DOT language. First, write the sentence digraph g{...} as a 'rule'. Then, write the information of the graph you want to draw in this .... In the example above we wrote x and y, which would render two nodes. Note that each node must be separated by a 'line break'.

[[[00000000000000000465---a27bfb9c0fb11bfbeb448022be9880127de06defcd2ec40a504795b33ecfe3f7]]]After entering the above, save it as sample.dot. And then run the following command:

[[[00000000000000000466---2f36294d5c4e90bc86b31c4898c6545593873cbf0db3ec3e54a406aefa79ffd2]]]Executing the above command will output the image in Figure 25-1.

[[[00000000000000000467---223795fd313587c484ab8d3ab8bd74bf2316c7d67a810b70add90aa74326f551]]]
Figure 25-1 Image converted from dot file


[[[00000000000000000468---3eb387b38697b0f3fdc6125856781c8ace123c1b6e5feb90eb684056f726497b]]]This is the simplest use case of the DOT language.

[[[00000000000000000469---0f2a3ab707a4350603c549681cd6223ddea7c804236f0e99ec0cc043fe5856a6]]]Specify node attributes

[[[00000000000000000470---332f1a9cbf813ef6d7f0a73a8bdc3c07b4b7d0e83bba44dfee4c6eda164d7b60]]]It is possible to specify 'color' and 'shape' for the node. For example, let's modify the sample.dot file we used earlier as follows:

[[[00000000000000000471---1ef4d6fa42f27057bc9c06cd470430288ca5641c83d5ba9fe4fee743ee269226]]]As before, each line contains information for one node. But unlike before, here we write numbers like '1' and '2' at the beginning of the line. This represents the ID of the node. Then write the attributes of the node with that ID inside [...]. For example, writing label='x' will display the letter x on the node. You can also specify to draw in 'orange' with color=orange, and to 'fill' the node with style=filled.

[[[00000000000000000472---fd980772ace1daabbe2ee7d9e5277e630be8c58f7188c86cd4ed312817c9e33c]]]Node ID can be set to any value as long as it is an integer greater than or equal to 0. However, you must set an integer that does not overlap with other node IDs.

[[[00000000000000000473---22df60d225e3d71db696ae913551ca52d8f68cfdb1706cd7b1f904c247f8ad21]]]Now, as before, let's run the command dot sample.dot -T png -o sample.png from the terminal. Then you get Figure 25-2.

[[[00000000000000000474---0be13ae4e9f4b4adacddc8bbabaf80c5dc4ef2b0d513282abee5d2a9252e42d3]]]
Figure 25-2 Change Node Color to Orange


[[[00000000000000000475---387d0751f8e46cf6f2ca6db0f72d6af1925c61aa080af3e8b086aca903783b7e]]]Two orange nodes are drawn as shown in Figure 25-2. Following this, let's add a rectangular light blue node. Add the following to it:

[[[00000000000000000476---64d2d6c6386d214ae64cebbc7f0643d145ecef3f3265b0385f496604c91b5732]]]As shown above, add a new node and set its attribute to 'rectangle (box) light blue (light blue)'. This file yields Figure 25-3.

[[[00000000000000000477---94327495c4e5e04c56f669e9af0e5b4fb40948bab311f3663b6dae9279ceeff7]]]
Figure 25-3 Examples of Circle (Ellipse) and Square Nodes


[[[00000000000000000478---b09d1c852175db4f53e75d774b844c99864cb475dea4d75b112e32b2711795eb]]]A square node has been added, as shown in Figure 25-3. Now we are ready to draw DeZero's 'variables' and 'functions'. Then just connect them with arrows.

[[[00000000000000000479---130f4768ff54896929e5f173276e948dac998dd39dde32fc2646c01486de0a9b]]]In this book, when representing computational graphs, variables are represented by circles (ellipses) and functions by squares. In the visualization using the DOT language, we will draw variables with circles and functions with squares, as before.

[[[00000000000000000480---e127e5ad451cb95583cbcacef8fa13e7b12379d2aa3875f2bae4d8173c8e5482]]]connect the nodes

[[[00000000000000000481---c2f53420df00d050ca915c47858fcac78976600033a538ccca4ce83c26823446]]]To connect nodes, connect the IDs of the two nodes with '->'. For example, 1 -> 2 draws an arrow from the node with ID 1 to the node with ID 2. Let's write the following dot file.

[[[00000000000000000482---c1dd521583eff4e51385b3ece8f2be52354f48016b89970df58ab8030fd6651c]]]This dot file gives Figure 25-4.

[[[00000000000000000483---17be7cc6466c64c53420586394a4cc5183546aefba34fd47e064d3825f1d4501]]]
Figure 25-4 Nodes Connected by Arrows


[[[00000000000000000484---5cc0521fbc9f213c42f9534448746ef7f48c9d644f90699e8cb12c09c2996fda]]]You could connect the nodes with arrows, as shown in Figure 25-4. The DOT language has many other features as well. But for us, this knowledge is enough. Now we are ready to draw DeZero's computational graph. In the next step, we will add a function to output DeZero's computational graph in DOT language.

[[[00000000000000000485---3a0824baa1752fb7403fd9483c25c1f2909dbae9da80b9699f1c258f4cc88ea5]]]● Author introduction

[[[00000000000000000486---299ac4470df01946755c3411ec8c9fd68793965c598cb2725a530e9847394c99]]]Kouki Saito

[[[00000000000000000487---bcfeb45140b230147d0eb2e44169720fd7a2c90da966ed02604f4b17a3acaef6]]]Born 1984 in Tsushima, Nagasaki Prefecture. Graduated from the Faculty of Engineering, Tokyo Institute of Technology. Currently engaged in research and development related to artificial intelligence at a company. His books include 'Deep Learning from Scratch' and 'Deep Learning from Scratch ❷', and translated books include 'Practical Python 3', 'Computer System Theory and Implementation', and 'Practical Machine Learning Systems' (O'Reilly Japan). be.

[[[00000000000000000488---3d694018432ec6390871639c8b40ec94a95602e9286fe67071d94981216e15e3]]]step 54

[[[00000000000000000489---73cd85fbbf12eed29965b9b4aa99bf2c62d4009de56fdeff972ee8002d103ac8]]]Dropout and test mode

[[[00000000000000000490---26defc38fbe9dee8b81798533549fcd15810cf6221c9e41289bcf9c42df62cf0]]]Overfitting is a common problem in neural networks. The main cause of overfitting is

[[[00000000000000000491---2cf007fcb701165161faf552ffb698db098fbd508baafd7f61ad6c2fbab2bc21]]]lack of training data

[[[00000000000000000492---81fdecfd93fddf4795e6b86fc04763571b7e351f5ec27a8cb83945cf8f13dd36]]]The model is too expressive

[[[00000000000000000493---3d4147047a833de70f811738ef64c5ebcbcf43f1a02aaf5a0c41e74245d02629]]]And so on. Regarding the first cause, it is effective to increase the data and use data augmentation to artificially increase the data.

[[[00000000000000000494---a64ea0dbe1dc003aef437b35b9a22d7adae3c18a772b21bbd863bba2a4ef9f9e]]]The second cause is weight decay, dropout [34], batch normalization [35], etc. In particular, the method called Dropout is a simple yet effective method that is often used in practice. Therefore, in this step, we will add Dropout to DeZero. Dropout also needs to change processing during training and testing. Therefore, we will also create a mechanism that can distinguish between the learning and testing phases.

[[[00000000000000000495---a8faf8e16c93c7010dd235e904f7a249ebc945c519cf9152f8389a58d1066efd]]]What is dropout

[[[00000000000000000496---eb4c7d6c611c6e58bcc30e017effdad154ee05fa52e691594748b8deb0fda361]]]Dropout is a method of learning while randomly deleting (disabling) neurons. Randomly select neurons in the hidden layer during training and delete the selected neurons. Erased neurons cease to transmit signals, as shown in Figure 54-1.

[[[00000000000000000497---6c8020dcd0f3c505a1e935fe54587b840319fb0efbfe8b60675aae30b0548457]]]
Figure 54-1 Behavior during Dropout learning


[[[00000000000000000498---d86b5bc0b66a3d3f2de64413677f2b263d543db2b54db6db24f6e2189dc68315]]]Learning using Dropout randomly selects neurons to be deleted each time data is passed. For example, let's say you have a layer of 10 neurons, and then you want to randomly drop 60% of the neurons using a Dropout layer after that layer. This behavior is shown in code as follows:

[[[00000000000000000499---a203f596906add363bd0893710f8f38a1948c431daff776aac02fe8bf21ad37a]]]where mask is an array whose elements are True or False. To create this mask, first generate 10 random values between 0.0 and 1.0 using np.random.rand(10). And by comparing it with dropout_ratio (= 0.6), only elements greater than dropout_ratio are converted to True, otherwise to False. This example produces a mask with an average false percentage of 60%.

[[[00000000000000000500---ba963cb2bedc025e3fcb36799c467f172ef796075dfd9ba9b35854a45e81f999]]]Once you've created the mask, all you have to do is do y = x * mask. This causes the elements of x corresponding to False in mask to be 0 (that is, cleared). As a result, on average only 4 neuron outputs pass to the next layer each time. The Dropout layer performs the above processing each time data is passed during learning.

[[[00000000000000000501---18f1c1acb20cc7a4f40bcc9a3fa45720e825c7264b9c0bbbe4df633f9da53d82]]]Ensemble learning is often used in machine learning. Ensemble learning is a method that trains multiple models individually and averages the multiple outputs during inference. In the context of neural networks, for example, prepare 5 networks with the same (or similar) structure and train them independently. And when testing, the 'answer' is the average of those five outputs. By doing so, it is experimentally known that the recognition accuracy of the neural network can be improved by several percent. This ensemble learning is closely related to Dropout. Dropout randomly erases neurons when training, which can be interpreted as training a different model each time. In other words, Dropout can be considered to simulate the same effect as ensemble learning with a single network.

[[[00000000000000000502---9800151c7024f838facce4a43f8229e939d93516595f581ecc231ef816897fa4]]]Now, the code I showed earlier is the process when Dropout learns. When testing, we need to “mimic” the behavior of ensemble learning during training, using all neurons. We can do that by using every neuron to compute the output and 'weakening' that output. Weakening fraction is the percentage of neurons that survived during learning. Specifically, in code, it looks like this:

[[[00000000000000000503---9352e25177ae69832c547c4a2c6a65057c82e24af52520d4fc4468bb857ee988]]]# When learning

[[[00000000000000000504---44a90975a02a36985fa8241d3c928bc149c14d31ae6c0a84caeb1d41642e6b40]]]# when testing

[[[00000000000000000505---e4bcb95be74457a00b6e0fd2c0bfdc1ed102b868078480d1ed1912daaca8eb3d]]]As above, when testing, y = x * scale to do the scale conversion. To give you concrete numbers, here on average 40% of neurons survive during training. With that in mind, when testing, we compute using all neurons and multiply their outputs by 0.4. By doing so, you can match the scale of learning and testing.

[[[00000000000000000506---78b2bd7c2f52b23255cc24e4d1715a8d2c6996768c9c49e0ec0c1ce685e7fd5c]]]The above is 'normal Dropout'. I use the word 'usually' here because Dropout has a different implementation. It's called 'Inverted Dropout'. Next, I will explain Inverted Dropout. The 'regular Dropout' explained so far is hereinafter referred to as 'Direct Dropout' to distinguish it.

[[[00000000000000000507---a6c4ad93bd2a96d5e9d96afb208ce64f06aa9845f075d4189e3f86b8aeecb019]]]Inverted Dropout performs scaling processing during learning. As a refresher, we just multiplied the scale when testing to match the scale. Now, we preemptively multiply the neuron's value by 1/scale during training so that we don't do anything during testing. That way, when testing, you don't have to do any scale conversions. This Inverted Dropout looks like this in code:

[[[00000000000000000508---9352e25177ae69832c547c4a2c6a65057c82e24af52520d4fc4468bb857ee988]]]# When learning

[[[00000000000000000509---44a90975a02a36985fa8241d3c928bc149c14d31ae6c0a84caeb1d41642e6b40]]]# when testing

[[[00000000000000000510---cb03f88258d461267c8473f81ddcebee6da0e3a3a5be3a23f4c41d08a09646aa]]]This Inverted Dropout works in principle the same way as Direct Dropout. However, Inverted Dropout has its advantages. Specifically, it does not do anything during testing, so it speeds things up (only slightly) during testing. This is a desirable property if only inference processing is used.

[[[00000000000000000511---3999924b3384ace22d3e67423091f07671d33a5e342c6288eaadfc81bd3e25d3]]]Also, Inverted Dropout can dynamically change the dropout_ratio during learning. For example, you can set dropout_ratio = 0.43455 for the first data flow and dropout_ratio = 0.56245 for the next data flow. Direct Dropout, on the other hand, needs to learn with a fixed dropout_ratio. If you change the value in the middle, it will not be consistent with the behavior during testing. Due to the above advantages, many deep learning frameworks implement the Inverted Dropout method. DeZero also adopts the Inverted Dropout method.

[[[00000000000000000512---52254abfd2119148d6a5369fa25753dfeb03cd7d420d3e81e66f4a13f5aa9d8a]]]Add test mode

[[[00000000000000000513---965b55fd956c094c197c9a23c5c38b7044c7a28aa6079ebd74626a42cad4c0a1]]]To use Dropout, we need to distinguish between learning and testing phases. For that, we can use the backpropagation-free mode (with dezero.no_grad():) mechanism created in step 18. First, add the following shaded code around the Config class in dezero/core.py.

[[[00000000000000000514---335d98e5d8a8e1bcd512cc8c3d1b337562009727a0c88dde658edda30fc11b91]]]As above, add a class variable called train to the Config class. This variable defaults to True. Note that dezero/__init__.py has a sentence from dezero.core import Config, so other files can refer to it with the value dezero.Config.train.

[[[00000000000000000515---88300c322d8e6a441237095f194a1a16b326e15f2a4a08d4f64e16ce80417f4d]]]Then add a function called test_mode. By using this function with the with statement, you can switch Config.train to False only within that block. Since this function is also used by users, add the line from dezero.core import test_mode to dezero/__init__.py. Now you can import from dezero import test_mode from your code.

[[[00000000000000000516---5c18d47ca7bd687d16514b17aaf2f4786ae94f08b9c73c1496eb0a0b217a9019]]]Dropout implementation

[[[00000000000000000517---e38d6f25e1ca07af9aa6017bc39edd4809a5c76d4bc496c8f1b7321f757c937a]]]Finally, we will implement Dropout. This can be implemented as follows†1.

[[[00000000000000000518---6b9dc868217fc067c36316d0d6983a96574f28e4f191b8b450d2fea274308e8b]]][†1] DeZero's Dropout implementation can also be implemented by inheriting the Function class and defining the Dropout class. That implementation method has better processing efficiency, but Dropout's processing is simple and nothing is done during inference, so we will omit the implementation of that method.

[[[00000000000000000519---e240ba93daef317ffe14e55cd6474b257f91b85bf7f916a4506f0cd3e7a0029d]]]where x is either a Variable instance or an ndarray instance. Get the appropriate package with xp = cuda.get_array_module(x), even if it's a CuPy ndarray instance. The rest of the code is already explained. Now you can use the dropout function like this:

[[[00000000000000000520---9352e25177ae69832c547c4a2c6a65057c82e24af52520d4fc4468bb857ee988]]]# When learning

[[[00000000000000000521---44a90975a02a36985fa8241d3c928bc149c14d31ae6c0a84caeb1d41642e6b40]]]# when testing

[[[00000000000000000522---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000523---b7c414d647358b4a7ae8992d646453f0bdd83a76697e90340b28c7a4e192e566]]]As above, you can use the F.dropout function. We also have a mechanism to specify the 'learning/testing' phase. From now on, let's actively use Dropout when overfitting occurs.

[[[00000000000000000524---2a5a43314ecf3a3e2404d578290adb212694d549ca9f4962215d68b7f3e7381a]]]step 26

[[[00000000000000000525---bc9c94a140902e7c3df96f904da039e0153cb13b72b5c762598bf54675ff2a95]]]Visualization of computational graphs (2)

[[[00000000000000000526---f98ce525859c0876fc41698094a4d38258c7868fdaf95df3f1d99fa250422b0b]]]In the previous step, we learned how to write in the DOT language. Here, based on that knowledge, we convert DeZero's computational graph into the DOT language. Specifically, we will implement a function that converts the calculations performed by DeZero into the DOT language.

[[[00000000000000000527---b38b4b16573f3ded285fc38172dc5c53e844ca3a426579d92921a3edba3ef289]]]Example of using visualization code

[[[00000000000000000528---0cd6773fdd5c301303c9841f8dad65215aa292be90a111d40f8481daa0629687]]]We will implement a computation graph visualization function called get_dot_graph in dezero/utils.py. First, let's look at an example of how to use the function.

[[[00000000000000000529---5a0482fe00f532daeac658a4d38981f886b46422bac7f2e1a7ca2d20b864fffc]]]# some computation

[[[00000000000000000530---d89fca49c1a99f06b708a5f2e831b761c41b603942c3c0ae2ae3820b79d3ca15]]]# give the variable a name

[[[00000000000000000531---7b40f6b185aecbfc6fb41b369f6e4b6aec3af605fdbc702c60c4a1da09f1ba5e]]]# save as dot file

[[[00000000000000000532---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000533---d2af84ec49a7ce79a8af5c2430ea5ab63bbeb1c176dd0db02da192fa606b7c32]]]As shown here, the get_dot_graph function is given a variable y which is the final output. Then, starting from the output variable y, the calculation process is returned as a character string written in DOT language (the argument verbose will be explained later). Also, before calling the get_dot_graph function, name is added to the variable instance attributes like x0.name = 'x0' and x1.name = 'x1'. It is intended to draw label names to variable nodes during visualization of computational graphs.

[[[00000000000000000534---57cd24af2816e1bffa873a02b0f823905b3a91c1016286a70012dd3eb58a99fe]]]Once you have the string represented by the output above, write it to a file like sample.dot. Then you can convert it to an image with the command dot sample.dot -T png -o sample.png from the terminal. You should now have an image like Figure 26-1.

[[[00000000000000000535---7fe98b1e6cc88c317b74b62684ab5d61b6b68e8cd784b28cfc2916dc545d9868]]]
Figure 26-1 Example of Visualized Computation Graph


[[[00000000000000000536---920b12716af18c8f7022731834fa18c8f55b0e0b3c86e00bcdf76a9e83757343]]]The above is the flow of visualization of the computational graph. In summary, what we want to do is to use the DOT language to express the calculation process that the variable has traced, starting from the output variable. In fact, we already know how. The logic implemented by backpropagation can be used almost as is.

[[[00000000000000000537---ac321666193183cbb5015cbfcc697ea0904716493d58c70ef4c7610d63b08eeb]]]Backpropagation started from the output variable and went backward through all the nodes (variables and functions). By using that mechanism, the nodes of the computation graph can be converted into the DOT language.

[[[00000000000000000538---9d5dc38ee9fecb12f9778ba98a2e29b57fdfa3a7d39a5117ae14b6155db3d4d5]]]Conversion from computational graph to DOT language

[[[00000000000000000539---03a2fd1fe23655558169a4220bc04a44862748cf835087b63893c547a4a64c9a]]]With the above points in mind, let's move on to the implementation. Here, we implement a function called _dot_var as an auxiliary function before the get_dot_graph function, which is the main subject. The _ (underscore) at the beginning indicates that this function is meant to be used locally only - just for the get_dot_graph function. So here is the code for the _dot_var function and an example of its usage.

[[[00000000000000000540---9795dac0e755749821893089c531f798b98cc82d8c8b315777a1a9385227cd6c]]]# Example of use

[[[00000000000000000541---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000542---158f14c51eefb06e00e9777a701c97225301a3f513c6f3feb323f869bcd73299]]]As mentioned above, if you give a Variable instance to the _dot_var function, it will return that information as a string written in the DOT language. Here, we use Python's built-in id function to uniquely specify the ID of the variable node. You can use the id function to get the id of an object. An object's ID is unique to the object and unique to any other object. Therefore, it can be used as a node ID when describing in DOT language.

[[[00000000000000000543---639ecff26e0a5fbcded356bc481e3f3d165ddd81710be20d0941ba5596ae3133]]]In addition, in the code above, the string is manipulated using the format method. This format method replaces the characters {} included in the string with the object (string, integer, etc.) given to the argument of format.

[[[00000000000000000544---72cbb281d6949911a7c523b265a0f671a34ab6b08d1207f5779ca2f75e2be7a0]]]The _dot_var function has an argument called verbose. When this is True, the 'shape' and 'type' of the ndarray instance are also output as labels.

[[[00000000000000000545---18e6ca516c1ece86db62a856cc8fe4d57d767bdad429774fab4f34f0c75a6748]]]Next, implement a convenience function that converts the 'DeZero function' to the DOT language. Here, the function name is _dot_func and it is implemented as follows.

[[[00000000000000000546---660ab8ab2f26b47c6f40199ff797306dbc3956c63a7eac2dd3b544f923d15d93]]]# y is weakref

[[[00000000000000000547---9795dac0e755749821893089c531f798b98cc82d8c8b315777a1a9385227cd6c]]]# Example of use

[[[00000000000000000548---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000549---f1fb21a5896e9f8568703d073c4aebc7a080a03bfe7d1700cae48389d9e73fb2]]]The _dot_func function describes 'DeZero's function' in DOT language. In addition, 'connections between functions and input variables' and 'connections between functions and output variables' are described in DOT language. As a reminder, DeZero functions inherit from the Function class and have instance variables inputs and outputs as shown in Figure 26-2.

[[[00000000000000000550---5c4efc28dbfd6232dcf307ce3a6ad6c4e197e3f65f1f0f2b48fa5f4ad18fb57b]]]
Figure 26-2 Inputs and outputs of the Function class


[[[00000000000000000551---f6b184d52084dc8345544b9ac7057f43f1d4833fa58b92fc52c09f7b0ce708d6]]]That's it, you're ready to go. Now let's talk about the main topic, the get_dot_graph function. By referring to the backward method of the Variable class, it can be implemented as follows.

[[[00000000000000000552---fd62822848136dd34adc83d26f1bacce595a45efba1307f01add53b51a492c45]]]The logic in the code above is almost identical to the backward method of the Variable class (shading shows the changes from the implementation of the backward method). In the backward method, differentiation was propagated, but here, instead of differentiation, we will add a string written in DOT language to txt.

[[[00000000000000000553---d3d14b9e1bb64557659f212dc60b4f873da8bda4f37821bf4bdeb8594764049e]]]Also, in the actual backpropagation, the order in which the nodes are traced is important. So I gave the functions an integer value called generation and took out the functions in descending order of that value (see steps 15 and 16 for details). However, in the case of the get_dot_graph function, the order in which the nodes are traced does not matter, so the part that sorts by the value of generation is commented out.

[[[00000000000000000554---cf308e757545040ff5af777981e1a33f9227d67bbe7bfd0ae5bb179a258280bd]]]The questions here are 'what kind of nodes exist' and 'which nodes are connected to which nodes'. In other words, the 'order' of tracing the nodes does not matter, so there is no need to use the mechanism for preferential extraction using generation.

[[[00000000000000000555---af9f71b4161136587fc268dbf79395fc3f22beb3d7802ccc8dca5e53fea2c7db]]]This completes the code for visualizing the computational graph. Next, we will add a function to visualize the computational graph more easily.

[[[00000000000000000556---6442eeb2415a736eab0be7b67cf8be8e749d3939cfbe09136fc678512f2dbbf9]]]Conversion including imaging

[[[00000000000000000557---7c9a84a94ffba3f8a909e04568fd3739a0dd523e473dd1e8a0bcd2263d97a232]]]The get_dot_graph function converts the computation graph to DOT language. And to image that DOT language, we need to run the dot command (by our hands). But it's annoying to run such dot command every time. Therefore, prepare a function that includes the execution of the dot command. Here is the code:

[[[00000000000000000558---1bb229c94c5b771e96f3a23087d4df0e05268979e85adaf4b90f9cb35518a549]]]# ①Save dot data to file

[[[00000000000000000559---0cb804c5bf0caa80c03ea4e2acddbbdab2ea3b8e952774773d7b0f246fb7a8c8]]]# create ~/.dezero directory if it doesn't exist

[[[00000000000000000560---c3eac3e94c80b9b1dcc613b3fae611cb4f0e9fe5c5c17976b53ba84e938f6f0c]]]# ②Call dot command

[[[00000000000000000561---22e657abfc7942ff5217121b14b157485d8cd48bd564684285d8f2d5ff5523a1]]]# extension (png, pdf, etc.)

[[[00000000000000000562---f14f91d0ec7aa6afeadd0c7671101ebdc72a9855d961a411469d05bedcba6d0e]]]First, in (1), call the get_dot_graph function implemented earlier to convert the computational graph into DOT language (text). And save that text to a file. Set the save destination directory to ~/.dezero and the file name to tmp_graph.dot (since this file is used temporarily, it is named tmp). There is os.path.expanduser('~') in the code here, but this expands the path of '~' in the home directory.

[[[00000000000000000563---5c79c5558ac13e1e50232e23c69a26a3cad9cc04b4859e64808f244a1eb22ce8]]]Next, at point ②, specify the saved file name and call the dot command. At this time, to_file, which is an argument of the plot_dot_graph function, is the name of the save destination file. Here we use the subprocess.run function to call an external program from Python.

[[[00000000000000000564---53cfca6472d18b1ffdb968fcdd3d0fb7fb599c457920525577972f4456d7005b]]]In the actual plot_dot_graph function, a few lines of code are added in addition to the code shown above. It is intended for use with Jupyter notebooks. Specifically for displaying images directly in a Jupyter notebook cell when running in a Jupyter notebook.

[[[00000000000000000565---3ec753a3382830ae2f6d66a164e0486f826615686311e843b5a1db4fb1c4640a]]]The above is the implementation of the function that visualizes the computational graph. The functions implemented here will be added to dezero/utils.py as they will be used in various places later on. Then you can import it as from dezero.utils import plot_dot_graph.

[[[00000000000000000566---5f867e41fe27e63b34fd8dfee8bfaf939a8849dd5e45f7bc8e60e094a27a65cc]]]operation check

[[[00000000000000000567---1c859395abe3ebcfe078423853f7009b0793da542feec7116dec806b3543d190]]]Now let's visualize the Goldstein-Price function implemented in step 24. Here is the code:

[[[00000000000000000568---f3880fa237923711f3e0922e66bf06f4dcfc2d214808b02398963b230bbb6a9b]]]Running the above code will generate a file called goldstein.png. The result is a computational graph with a complex intertwining of various variables and functions, as shown in Figure 26-3. If you look closely, you can see that it starts with input variables x and y and finally outputs variable z. By the way, when implementing the Goldstein-Price function in DeZero, we were able to translate the formula almost verbatim into code. But behind the scenes, a complex intertwined computational graph was created, as shown in Figure 26-3!

[[[00000000000000000569---103b82be2b8a2e9d9156063693b3feb7084700628557d009cf1bf8e5219f71c0]]]
Figure 26-3 Computational Graph of the Goldstein-Price Function


[[[00000000000000000570---bb270d85d8331edc8c77adc1135adff4faff8b00985d821543f311260174313a]]]This completes the visualization of the computational graph. We will continue to use the visualization functions implemented here as needed.

[[[00000000000000000571---ef403c253a76ab8aac61bcf5eb47c5fd7e2020626bec268385434c78840aad32]]]step 30

[[[00000000000000000572---cf7a37f289a23d2c83e07cc453cc04663119327536d3d37184a1b0067b938cb2]]]Higher-Order Differentiation (Preparation)

[[[00000000000000000573---c1d98bb9f86f17678eb1ed4897cb3b21aa9ecde0a043bdcca465a4bc1a008ce1]]]The current DeZero can automatically calculate the derivative, but it is limited to the first derivative. Here, we will extend DeZero so that it can automatically find the second derivative -- and even the third derivative, the fourth derivative, ... and higher derivatives.

[[[00000000000000000574---86ecc186c283e1adb3d60b02d63616a9750492ec3247abf306fc5f1efc84abf8]]]Finding the second derivative using DeZero requires a complete overhaul of the current implementation of backpropagation. The basis of DeZero's backpropagation is done in the Variable and Function classes. So, first, let's briefly review the current implementation of the two classes, Variable and Function. Since the story from now on is a little long, I will divide it into three sections and perform the confirmation work.

[[[00000000000000000575---3436be8caa3fa790c9f75093be0ed0e7a08624549cb21c2ef353eeb9aac512df]]]Confirmation step 1: Instance variable of Variable

[[[00000000000000000576---e59d906881db60b490e36be8184a399fe3f0a0e168b43288f5ba8bdf22e136b3]]]First, let's review the instance variables of the Variable class. First, I will extract only the __init__ method that initializes the Variable class.

[[[00000000000000000577---f2907a2fb49a36ae7fc29c205ec1f091d094655facdb6d5b8e17f4db689f233c]]]As above, the Variable class has some instance variables. Here we focus on data and grad in it. This data and grad are used when calculating forward propagation and back propagation respectively. Note that both data and grad hold ndarray instances. Here, in order to emphasize this point, we will introduce a projection as shown in Figure 30-1.

[[[00000000000000000578---8041e1714ae7270d69e7e3e7ee276ed3b12db144494836fd3e3f3c5aa0940395]]]
Figure 30-1 New Projection for Variable


[[[00000000000000000579---581c1de64feb742c037bbf7afd2ed28ca8dec6f6307f73dd0cf9db6515f0d053]]]Let's draw data and grad as 'cubic containers', as shown in Figure 30-1. And when that data and grad refer to ndarray instances, we will draw them as shown in Figure 30-2.

[[[00000000000000000580---8c5d744113f778c724db9ec693baa09267ebe45054b474b8cd66f160833d7371]]]
Figure 30-2 New Projection for Variable (Browsing Data)


[[[00000000000000000581---2ba1057a7204fc0dfcdf070be2ba3db89cb977732c9bb6623fcb3d87a4bcdca8]]]The left figure in Figure 30-2 is, for example, the state when x = Variable(np.array(2.0)) is executed. After that, if you set x.backward() and x.grad = np.array(1.0), you will get the state shown in the right figure of Figure 30-2. I will use this diagram to explain.

[[[00000000000000000582---6ddce00f9b5859cd25d1f53fca26253a5ec2f05951c372806b76b425c4ffd022]]]Confirmation work ②: Function class

[[[00000000000000000583---6409de2f8e42ec7d97b5694b0de19913b0136ad96b02f456b0d20a55e6a01b98]]]Next is Function. Here is the code for the __call__ method of the Function class. Let's focus only on the shaded areas.

[[[00000000000000000584---24cf8f798bbbbe9c3cd6ddfe19b2706f3e52f142494df05588b905ffa1207c3c]]]# ① Calculation of forward propagation (main process)

[[[00000000000000000585---f569f68826e6093fa909885930d9f43c62a33ad0f65151e54a09b572a4fa90f9]]]# ② Make connections

[[[00000000000000000586---2a3a75d1b7115b72b6c84e78fb3cf7adc6555f7817ecb1dcd911866072ba38df]]]First is point ①. There, xs = [x.data for x in inputs] retrieves data, which is an instance variable of Variable, and collects it into a list. Let this list be xs and call forward(*xs) to perform concrete calculations.

[[[00000000000000000587---8c828fba6925adc475b1cc194b1f6482e776c2d6c2305052e385ab2dc264a43c]]]Next, let's look at the second part of the above code. There, a 'connection' is created between Variable and Function. A 'connection' from a variable to a function is made by the set_creator method. The principle is to make the newly created Variable remember its creator function (self). Also, by assigning the function's input and output variables to the inputs and outputs instance variables, we maintain the 'connection' from the function to the variables.

[[[00000000000000000588---564ce09e826e732bb9f580bcc9d10adfeb1d516bf0238a9a092cb19b911967f1]]]The reason for creating a 'connection' between variables and functions is to reverse the differentiation later. DeZero dynamically creates that 'connection' as the calculations are performed.

[[[00000000000000000589---25d9fbfbadbf636ca4c66ffa19bb45778165dec4dca736e857b4f9a0883c317a]]]All DeZero functions inherit from the Function class. Then, in the inherited class, implement the concrete computation in the forward method. For example, for a Sin class that does the calculation of the sine function, I wrote code like this:

[[[00000000000000000590---2a10ccb99f861c40b176044eba0e608f8bc501dcc22c5b557eb8d785637deb6e]]]Here, both the argument and return value of the forward method are ndarray instances. Similarly, the argument and return value of the backward method are both ndarray instances. Using this Sin class, we can compute:

[[[00000000000000000591---d13c336688057b67bedbb7704630036d6172f7e2bd079776bc4ff76a81141f94]]]Here we have only forward propagated the sine function. At this time, if you visualize the 'movement' of variables and functions, it will look like Figure 30-3.

[[[00000000000000000592---e54d3bbeda19c0431c37f05e5de03ed431d9d71beb877c4d3a104c677af71b54]]]
Figure 30-3 Computation graph for y=sin(x) (forward propagation only)


[[[00000000000000000593---b2e4107c7e77ef11cde07a293fe8eff23ea1ce9a26db12142dc0ae7dbac8f7c7]]]As shown in Figure 30-3, forward propagation computation causes the concrete computation to occur in the forward method of the Sin class. A 'connection' is then created between the variable and the function. Again, this 'connection' is made in the __call__ method of the Function class.

[[[00000000000000000594---1a1b41fc62188e78361f37d6b7b2ef19389c5128f74d836af50286cde3e4dd12]]]Confirmation work ③: Backpropagation of Variable class

[[[00000000000000000595---02fb98a6b697d955e703f1a58932a9ad334a45e8bb605045b8bcd07ad985a65c]]]Finally, we'll look at the backpropagation logic. Backpropagation is implemented in the backward method of the Variable class. So here is the backward method of the Variable class. Again, let's focus only on the shaded areas.

[[[00000000000000000596---4eba6ad4f64796e62cc734c7ed37f5ee69c4838e6996c665a1d3cf7f2e139416]]]# Calculate backpropagation (main process)

[[[00000000000000000597---41ab87738a668c85929cd8788e4cace0a01b6b36edafc039d8211f8c3169d692]]]First, at point ① in the code above, grad, which is an instance variable of Variable, is compiled into a list. Here the instance variable grad refers to the ndarray instance. Therefore, a list containing ndarray instances is passed to the backward method in ② of the code. Then, at point ③ in the code, the derivative (gxs) propagated from the output side is set to the grad of the input variable (f.inputs) of the function.

[[[00000000000000000598---e20a66899e2c28f9a42e5074624e9f1cbeeda234b35bf9834dad812ef7f77bba]]]So, with that knowledge, let's look at the following code:

[[[00000000000000000599---e64b9629104454f7205e83b7fc9c9c67f1a4b90e09b43d97efcaa1e6a67347d2]]]The above code is a code that calculates the sin function (forward propagation) and then backpropagation. Here, y.backward(retain_grad=True) keeps all variables derivatives (this feature was introduced in step 18 to improve performance). At this time, visualizing the 'movement' of variables and functions will look like Figure 30-4.

[[[00000000000000000600---df3f4188617d503e5e7497bbcfada864fd341d959368092095f5512a1448f948]]]
Figure 30-4 Forward and Backpropagation of Computation Graph for y=sin(x)


[[[00000000000000000601---d47e5f3e3e57ceaff6482ba5ab1aa13395d3a8bc3cee020ba5b3ccccb29ba718]]]As shown in Figure 30-4, when the normal computation y=sin(x) is executed, the computation graph is created and the instance variable data of Variable is filled. Backpropagation then calls the backward method of the Sin class and fills the instance variable grad of Variable.

[[[00000000000000000602---1b552c57fb7c20a5f19af01490230a89b7f1fbeddeb90ae6a7b2511f1d048214]]]The above is the implementation method of back propagation by DeZero at present. In the next step, we will modify the current DeZero so that we can obtain higher-order derivatives.

[[[00000000000000000603---83e04629aefdc037777c317b10722b76ea09ce71a685d45a1716ab774e4d06b9]]]step 51

[[[00000000000000000604---2e3e519a018a9fe6fa15bbf463b410b3fcf0582a7827c0c9250e799f3e7b77c0]]]Learning MNIST

[[[00000000000000000605---efdaa2d312d4b0e9cb115e8e7d91aaa77d71398c523302a16a7afa0e1680244e]]]So far, we have created a mechanism that makes it easy to work with datasets. As a quick review, the Dataset class standardized the handling of datasets (fixed interface). And I made it possible to set the 'preprocessing' of the dataset in the Dataset class. Finally, the DataLoader class allows you to create mini-batches from a Dataset. A graphical representation of these relationships is shown in Figure 51-1.

[[[00000000000000000606---c950355d9667044932b987cb45aef19445825ef1afc52d5fc558aaa1a9bd87dc]]]
Figure 51-1 DeZero Dataset Class Diagram


[[[00000000000000000607---594deefda3f85cfdb795ac1dccd1707931cfa8836edeecdaa961809a2b6e716c]]]Figure 51-1 illustrates the preprocessing object (callable object) as Callable. As shown in this diagram, the Callable is held by the Dataset, and the Dataset is held by the DataLoader. Then the user (the user's learning code) requests data from the DataLoader and gets a mini-batch.

[[[00000000000000000608---2a489762e4af3fac8de32e4a0246f0c420aa4a84cbb1d882916d3693470ff458]]]Here, we use the above dataset mechanism to train on another new dataset. That dataset is MNIST. Let's start with a quick look at the MNIST dataset.

[[[00000000000000000609---2922d37c96664d4822b69487561d8b903431ce1b63a0122a0b7d84529080cc80]]]MNIST dataset

[[[00000000000000000610---04a5a50b36e0f18c3b9c01a9da4cf95fb0d48fdf2049bed05aecec4c1a98ce41]]]DeZero provides MNIST classes in dezero/datasets.py. This MNIST class is implemented by inheriting from the Dataset class. In practice, it looks like this:

[[[00000000000000000611---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000612---96d248716f30dcd4ac6b33ea327d4fd012854203e89598dc98adb963bdd54069]]]Get training and test data as above. Here, we (explicitly) specify that no preprocessing is done on the data with transform=None. Then check the length of the training data (train_set) and the test data (test_set). The result is a train_set of 60000 and a test_set of 10000. That means we have 60,000 training data and 10,000 testing data. Now, let's run the following code after the code above.

[[[00000000000000000613---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000614---45da3690351f01a1081f469f5f6ccce4c4c281a51e62fb9c79ea5ea0921f17d5]]]Here, I took the 0th sample data of train_set. The MNIST dataset is also retrieved in the format (data, label) -- a tuple of 'data (image)' and 'label' pairs. Also, the shape of MNIST input data is (1, 28, 28). It represents image data in pixels with one channel (grayscale). Also, as a label, the numerical index (0 to 9) that is the correct answer is included. If you try to write a code to visualize the input data, it will be as follows.

[[[00000000000000000615---49b6f827b167cd37337ff33cb574a17e9a1169a50ded369f10326ca201e81be9]]]# sample data

[[[00000000000000000616---9d904300b3af38a86c63e118cbc1e2cf688aec396608f4dda09fc80ec8e8949c]]]# extract the 0th (data, label)

[[[00000000000000000617---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000618---522fc53bf45950bcdea424002645a7fd4e010036b207b9e2be47b7196d644f6d]]]After running the code above, you will see the image in Figure 51-2.

[[[00000000000000000619---2a9ac2f36621db35428fba0ef96a2e7fcb4b7151cf5816ed06e28cbc2253a0cb]]]
Figure 51-2 MNIST image example


[[[00000000000000000620---8e466af5e5ab6c7473fb9ad8e6bf6899186eeb6cdf7fb2e87d131851c7214d2a]]]This handwritten image data will be learned by a neural network. To do that, we need to do some preprocessing on the input data. Here, we will do the following preprocessing.

[[[00000000000000000621---3e48196596d01512e663a5fa853224822dc48b44532b954ee05deb16d6af100f]]]First, arrange the input data in one column. This transforms the shape of the input data from (1, 28, 28) to (784,). And convert the data type to np.float32 (a 32-bit floating point number). Finally, by dividing by 255.0, the input data will be between 0.0 and 1.0. Note that the above preprocessing is set as the default in the MNIST class. Therefore, even if you write dezero.datasets.MNIST(train=True), the above preprocessing is performed (however, dezero/datasets.py contains preprocessing using classes in dezero/transfroms.py code is written).

[[[00000000000000000622---2e3e519a018a9fe6fa15bbf463b410b3fcf0582a7827c0c9250e799f3e7b77c0]]]Learning MNIST

[[[00000000000000000623---38e62ea8b460c89ed1c02b09a33e89fe792cce58c5bced387a0248e345a62724]]]Now let's learn MNIST. The code looks like this (I've omitted the code for the import statements):

[[[00000000000000000624---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000625---79018f41105da584af76c09f806032810ea398912a42383e76ea9c126165a3c2]]]The changes from the previous step are to use the MNIST dataset and change the hyperparameter values. This is all you need to learn MNIST. The result was a recognition accuracy of around 86% on the test dataset. Increasing the number of epochs seems to improve accuracy, but there seems to be other room for fundamental improvement. So, finally, I would like to finish this step by creating a more accurate model.

[[[00000000000000000626---8e2bc9948b4ea03d30ad23817f9abbc6fae1f8cdf4ee4ef86c4f9e394256560d]]]Refinement of the model

[[[00000000000000000627---af51b915b21dadea69b3d36138b00b571a68b39bea60c58431d4bbf889cc1b26]]]The neural network activation function we used earlier was the sigmoid function. The sigmoid function is an activation function that has long been used in the history of neural networks. Recently, a function called ReLU (rectified linear unit) is often used instead. The ReLU function is a function that outputs the input as is if the input is greater than 0, and outputs 0 if the input is less than 0. Expressed as a formula, it looks like formula (51.1).

[[[00000000000000000628---ff508c07e14c25e187151b8dff41521fe1148f8d31b811572ad98313933d871c]]]As you can see, the ReLU function is a very simple function. DeZero also makes it easy to implement ReLU functions. Here's what that code looks like:

[[[00000000000000000629---b1282a611742f3112a655d990d67fdf82336c2b5d7d6176feb2deeedd7f8bd25]]]In forward propagation, np.maximum(x, 0.0) picks up the larger of the elements of x and 0.0. Backpropagation passes gradients where elements in input x are greater than 0, and sets them to 0 otherwise. Therefore, prepare a mask for 'passing/not passing' the gradient transmitted from the output side, and multiply it by the gradient.

[[[00000000000000000630---e2f40401dd11005d4ff97ae222d0f5ab1c275f16cfbeff4aa40cd6c8ee39e32c]]]What the ReLU function does is two processes of 'passing' and 'not passing' the signal. An element that passes a signal in forward propagation passes through the corresponding gradient in backpropagation. On the other hand, elements that pass the signal in forward propagation will not pass the gradient in backpropagation (become 0).

[[[00000000000000000631---298cb21708e4387f363defbb96c1cd2b8393f3c1be7b76c8f94bb7c730bee48f]]]Now let's create a new neural network using the ReLU function. In the learning code shown in this step, change the part that creates the model as follows.

[[[00000000000000000632---fa2f75f4739026b0cecf0864ba5e429e5fabc65a5535f40a4c0906fe44e32479]]]Here, we increase the number of layers from the previous section to make a 3-layer network. By increasing the number of layers, it becomes more expressive than before. And by using the ReLU function as the activation function, we can expect to be able to learn more efficiently. Using this neural network, change the optimization method from SGD to Adam and try learning. Then, the recognition accuracy is about 99% on the training data and about 98% on the test data. We were able to improve the accuracy greatly from the previous result!

[[[00000000000000000633---c1dce9499e27210335073e29a85f4837859ee79e9d05b4d992ec99029d80779d]]]This concludes the 4th stage. In this stage, we adapted DeZero for neural networks. With DeZero today, basic neural network problems can be easily implemented. DeZero could already be called a framework for neural networks -- and deep learning. More importantly, the knowledge you've learned so far also applies to popular frameworks such as PyTorch and Chainer.

[[[00000000000000000634---fdc071629aa1ced1fa783bff655dc85e8241efd7b2a858ffab44d9016879d7df]]]Let's take a look at the MNIST learning code [30] in Chainer's official examples. In fact, much of that code is in common with the code we implemented in this step. Also take a look at the MNIST code for PyTorch [31]. You should be able to figure it out quickly too. Although the class and module names are different, they are essentially the same as DeZero, just with different interfaces and names. We now have enough “living knowledge” to use even real frameworks like PyTorch and Chainer!

[[[00000000000000000635---95f3167f230205d7e6bf9799f61439f481b4c9f5c514f19a41188d0977185449]]]Column: Deep learning framework

[[[00000000000000000636---7150ed7a0e868a4184c07054cbe75d3cfa373e45ba42b44c5ad9d7a52fa227f5]]]In the early days of deep learning frameworks, there were big differences between them. However, modern frameworks are at a mature stage. In fact, popular frameworks like PyTorch, Chainer, and TensorFlow are all headed in pretty much the same place. They each have their own peculiarities and different interfaces, but they share many commonalities in their core design philosophy. Specifically:

[[[00000000000000000637---4f4fd9d705aea8630943adc5ac523f8e3bc9d854a9f1ec712e1b5bd1bc2afaa3]]]Computational graphs can be created in a Define-by-Run style

[[[00000000000000000638---367bc18feef9b1d043fe664b971bc2da7f60e4c68c3bd8c04ea842844ba0bc42]]]You have a collection of functions, layers, etc.

[[[00000000000000000639---52a5c34c42337d47af78412ff53f0173471de391328c17a46a96d72f09014b83]]]I have a collection of classes (optimizers) that update parameters

[[[00000000000000000640---4bbb96d827f7a7434bba5da4f18902894680973b7b18d1897df106ea3f17c413]]]Models can be subclassed and implemented

[[[00000000000000000641---997eda36208da2df1d8e26575ad7c7e01d72b59c53f4b63dee6906b73d109abd]]]There is a class that manages datasets

[[[00000000000000000642---4490f89b49b9f6cf86d1e9f4f3afd54ae5ec6a5a2fc238b1cfef3e003a22e834]]]In addition to CPU, it can be executed on GPU or own ASIC

[[[00000000000000000643---4de32a5a6362259cf8eee6122dc3fe76d86e42db471588dbb566d6aa630d9c4c]]]Has a mode that can be run as a static computational graph for better performance (and for production use)

[[[00000000000000000644---99b7d057b075ff3b428e281d65628c01a3a42f5394b3f9f365d3a62164e3007c]]]The above features are commonly seen in modern deep learning frameworks. Let's take a closer look at the first three points with specific examples.

[[[00000000000000000645---e698718446994be03eb19db3c36d25052a154160a874dab6dfa43738ec1d6a45]]]A deep learning “framework” is not, strictly speaking, a “tool” or a “library”. The difference between a library and a framework is 'who' controls the program. A library is a collection of useful functions and data structures. The user takes out what he needs from there and uses it. At this time, the user decides the control of the program -- in what order the code is to be executed. Frameworks, on the other hand, provide the foundation for the whole thing. A deep learning framework would provide the basis for automatic differentiation, on which the user builds the necessary computations. At this time, the framework takes full control. In this way, there is a difference between 'who' controls the program between libraries and frameworks.

[[[00000000000000000646---b1899a0752dc91441ee1425d7dedaf38c693a1915a88247f726ac36224ca404d]]]Define-by-Run style automatic differentiation

[[[00000000000000000647---adee8282397410e175867011b0647589bcffdb59546b60caa9fbcbf50e0c47a7]]]The most important feature in the deep learning framework is 'automatic differentiation'. Automatic differentiation allows us to find the derivative on the fly, without any hassle. Furthermore, modern frameworks create computational graphs in a Define-by-Run manner. The code is executed instantly, creating a computational graph behind the scenes. It makes it possible to create computational graphs using Python syntax. For example for PyTorch you can write code like this:

[[[00000000000000000648---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000649---fa086cca913d182ea0fbe10e2d17a576b346a228e4d1163fe356dcb1a61021c7]]]The class that handles tensors in PyTorch is Tensor (this class corresponds to Variable in DeZero). In the above example, the torch.randn method generates a tensor (torch.Tensor instance) initialized with random numbers. Then use the for statement to do the calculation. At this time, the calculation is executed immediately, and the 'connection' of the calculation is created behind our eyes. This is Define-by-Run. Then write y.backward() to find the derivative. This style is the same as our DeZero.

[[[00000000000000000650---ad77d62a897369c229d4e0885467e429491fe73264e485693fae88c39b080e4b]]]In addition to Define-by-Run, the PyTorch, Chainer, and TensorFlow frameworks also have modes that allow Define-and-Run (static computation graphs). Define-by-Run is suitable for the trial-and-error phase of research and development, while Define-and-Run is suitable for use in performance-demanding products and edge environments.

[[[00000000000000000651---bfbc9cf3969c54adf2372d6416429eebc9d0b34f5cbb647b06f18c760ffad54b]]]Up to version 1, TensorFlow had to use its own programming language (DSL: Domain Specific Language) to construct the computational graph. However, starting with version 2, we have added a mode that allows you to execute with a Define-by-Run method, called Eager Execution in TensorFlow, and made it the standard mode.

[[[00000000000000000652---4985c82330501c12df57d78cb3c90fbc1b2fe8aac6f148f4963bebe41504026b]]]collection of layers

[[[00000000000000000653---7f2b95465ff05134be994743fa991dee897e5eb543c27d6e310c10474bc99952]]]Models used in neural networks can be constructed by combining pre-existing layers such as Linear layers and Sigmoid layers. Therefore, deep learning implementations are often completed by simply combining existing layers like Lego blocks. Of course, this requires the framework to provide a useful collection of layers. For example, Chainer provides layers as shown in Figure D-1.

[[[00000000000000000654---667c41000aeea4bccf5e4a1ed509d190b44115e0476d8076a710ccc6ef94ced5]]]
Figure D-1 Examples of Layers Provided by Chainer: Chainer provides two modules, generally called layers, divided into links and functions. Here is a part of it from the link (image taken from reference [32])


[[[00000000000000000655---52cd7ae942876739ff1a84e6d7cb7ebeb7b85629c791bba8ff7e50a423244411]]]As shown in Figure D-1, Chainer provides various layers. From this, the user selects the desired layer and connects them to construct a neural network. As such, deep learning frameworks provide different layers. The important point here is that these collections of layers are built on top of the 'automatic differentiation' mechanism. A graphical representation of this structure looks like this:

[[[00000000000000000656---d8bd6121b18cd180437634cb2ab1ec45f307cb736335285077e1af513614433f]]]
Figure D-2 The various layers provided by the framework build on top of the automatic differentiation mechanism


[[[00000000000000000657---ff48279645173ed63cc5f85fd983f529d25c70ef48d1474067c66463a455765b]]]As shown in Figure D-2, deep learning frameworks are based on automatic differentiation mechanisms. Using its automatic differentiation mechanism, various layers are provided. This is the basic structure of the framework. By understanding this structure, you can look over the framework itself at a higher level without being confused by the details of the various frameworks that exist in the world.

[[[00000000000000000658---4256734eeed9a12d4c2ae10a8088ee84345ca4c45af55a66871ad855d57a9955]]]A collection of optimizers

[[[00000000000000000659---4a575539587f32c45e1926d0b043349292d362688910956c7ef8e14a75a3ad7c]]]Deep learning learning uses the parameter gradient to update the parameters one by one. There are various methods for updating the data, and new methods are being proposed even now. For this reason, parameter update work is generally provided as an independent module. For example, TensorFlow calls its modules 'optimizers' and provides them collectively, as shown in Figure D-3.

[[[00000000000000000660---4bb4fff73b3e7fa2ddfa75899ba7eb439d9e50a9957a630e5fefeafaac1a5b37]]]
Figure D-3 List of optimizers provided by TensorFlow (quoted from Reference [33])


[[[00000000000000000661---c85a24885a49d00b8c914e19364156e0d5f09e532335ec272d68e68ac822935c]]]As Figure D-3 shows, TensorFlow has various optimizers. Providing a collection of such optimizers allows the user to think at a high level of the work of updating parameters. In addition, it is easy to switch to another optimizer, which facilitates trial and error, such as experimentation and verification.

[[[00000000000000000662---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000000663---701089b3aecd64fc15fe3966f41fe243ce3035b0b874a318b20dd67f198e1dff]]]The features described here are the core features of deep learning frameworks. In summary, we have an auto-differentiation function, on top of which we have a collection of layers, and a collection of optimizers that update their parameters. A graphical representation of these is shown in Figure D-4.

[[[00000000000000000664---b7e0e43ce47087321fc939e1e9e8cecc92c15ddb189f2c7939d8be0f08ce72a5]]]
Figure D-4 Core functions of deep learning framework


[[[00000000000000000665---b78e55c8d83b62a64766d0302b1cfd9f1246880e6aa509e127ada7a800c47284]]]The three features in Figure D-4 are important features that almost all frameworks have. Among them, automatic differentiation is the “support” of the framework. The skeleton of the framework is completed by the presence of automatic differentiation functionality and the collection of various layers on top of it. On top of that, providing a collection of optimizers for updating parameters covers much of the work you do in deep learning.

[[[00000000000000000666---116ad5fd83f88e3ca1e2ca8cbd021120038ddcf16797a388f83097a0b32c0c5d]]]We can see the framework in greater perspective and in a simpler way by knowing the structure of the three core functions, as shown in Figure D-4. Understanding its structure will also help you when working with other frameworks such as PyTorch, Chainer, and TensorFlow.

[[[00000000000000000667---82eb3a9e9d118c9eaca1005909913961f89c58436eb41938ac7ec09cc47a6a43]]]step 7

[[[00000000000000000668---be8d5a37bb912245be47deb3df7e79dab34c98f803a45f13e0638044125e650e]]]Automated backpropagation

[[[00000000000000000669---41a98aabeda20a9e45931655fcd14d89519d47403b7aee7fdde6d9d5b04349bb]]]In the previous step, we succeeded in getting backpropagation to work. But there I had to hand-code the backpropagation calculations. This means that we have to write our own backpropagation code for each new computation. For example, if we consider a graph of computations like Figure 7-1, we have to manually write the code for backpropagation in each computation. It's error prone and, above all, tedious. Let Python do the boring stuff!

[[[00000000000000000670---bd11a69f2b642e4c7c4a4c79a4ef4aaf8f1b6342af5766b3174aa95aba6097c8]]]
Figure 7-1 Examples of various computational graphs (variable names are omitted and functions are indicated by class names)


[[[00000000000000000671---e91a82b124824d589b1d5c3e3f71fa4ce2da9a34bd74b791b24343cdb59cf2df]]]The next step is automated backpropagation. More precisely, if you do a normal calculation (forward propagation) - no matter what kind of calculation it is - create a mechanism that automatically performs backpropagation. Now is the time to get to the heart of Define-by-Run!

[[[00000000000000000672---192e43018770bba96a7c144fbb06c16646cf952440d35d4ebfe9564bcaab0708]]] Define-by-Run is a mechanism that creates a “connection” of calculations performed in deep learning at the timing of calculation. This is also called a 'dynamic computation graph'. Define-by-Run and its benefits are discussed in more detail in 'Column: Define-by-Run.'

[[[00000000000000000673---60f40f09588ca0fcf3efd071f15ba707cbee997cf15f08b5eb5c924239417ebb]]]By the way, the calculation graphs shown in Figure 7-1 are all straight-line calculations. Therefore, if you keep the order of functions in the form of a list, you can automatically perform backpropagation by following it backwards. However, for branched computational graphs or complex computational graphs where variables are used multiple times, the simple list approach cannot be used. Our future goal is to build a mechanism that can automatically backpropagate no matter how complicated the computational graph is.

[[[00000000000000000674---e31b93795fd8da6b2c78f2c39601978e4d5a0136851eb57b76d601763dbd06a7]]]In fact, if the data structure of the list is devised, it is possible to perform backpropagation correctly for any computational graph simply by adding the performed computations to the list. That data structure is called a Wengert list (or sometimes called a 'tape'). This document does not describe Wengert lists. If you are interested in the Wengert list, please refer to references [2][3]. For the advantages of Define-by-Run over Wengert lists, please refer to [4].

[[[00000000000000000675---59f7553144e2da05e9399663dbe23ea7a02bd513acfe406a8e03da12c3877c82]]]for automating backpropagation

[[[00000000000000000676---da49e760c067d3f3fb9b66a0e7291d80003e72b8b05afc70c9205dc9ae6c2299]]]In automating backpropagation, we start by thinking about the relationship between variables and functions. First, let's think about it from the point of view of the function, that is, 'what does the variable look like to the function?' From the point of view of a function, variables exist as 'inputs' and 'outputs'. As shown in the left diagram of Figure 7-2, variables exist as 'input variables (input)' and 'output variables (output)' for functions (dashed lines in the diagram indicate references).

[[[00000000000000000677---75a051b9556b3c79006e777e83410d8328aa159c17e405ac1ed7d351bce68849]]]
Figure 7-2 Relationships with variables seen from functions (left figure) and relationships with functions seen from variables (right figure)


[[[00000000000000000678---e35905af78d9a52d91a345b3b5f7d534745db69e56ae6c0d4109821632555460]]]Then what does the function look like from the variable's point of view? The point to note here is that variables are 'created' by functions. In other words, functions are the 'parents' of variables. In other words, it is a being that becomes a creator. If the parent function does not exist, it is considered a non-function-created variable, such as a user-supplied variable.

[[[00000000000000000679---9d7c78c216ea6512d8a90f496683a82cd42cbe471643d5ab2d2eb7a73248e45e]]]Now let's incorporate the function-variable 'connection' represented in Figure 7-2 into our code. Here, we will create that 'connection' at the exact timing when normal calculation (forward propagation) is performed. To do so, first add the following code to the Variable class.

[[[00000000000000000680---281c9c027217169236e55c6050f4b11c20a5f08068d31ee35b60cfa449e2c8be]]]Here we add an instance variable called creator. And add a method to set the creator as a set_creator method. Then add the following code to the Function class.

[[[00000000000000000681---950e0b1ad9fa68f32893cc7af6a9decf968a261a5ba20fadc048d6e78d4d635c]]]# Make output variables remember their origin

[[[00000000000000000682---6e62e5c2b9957e4a5a77785d1337b87c740694f57d1cbe73b30804fe223e3c16]]]# also remember the output

[[[00000000000000000683---040c078e03409622bb4d2e0169a4813fe52b223dbf004fa89ad4422ade774375]]]The Forward Propagation computation produces a Variable instance called output. At this time, the generated output is made to remember that 'I (myself as a function) is the creator.' This is the heart of the mechanism that dynamically creates connections. In addition, here, in anticipation of the next step, the output is set to the output of the instance variable.

[[[00000000000000000684---acfcdad77af4a726b00f3f1b5785c8710fe858d37af8dd9362943f818eff9f07]]]DeZero's dynamic computation graph works by recording the 'connection' in a 'box' called a variable when the actual computation is performed. A similar approach is also taken by Chainer and PyTorch.

[[[00000000000000000685---b68e37ee12e15d040873f2b9b6e9e74c4666a5ee89980025a8cfebe32a5b3806]]]In this way, if you have variables and functions that have 'connections', you can trace the computation graph backwards. Shown in concrete code, it looks like this:

[[[00000000000000000686---3dcc6da465518b5e04ca4128a8841b3fcd1a01aae614601908d05468fffeda00]]]# follow the nodes of the computational graph backwards

[[[00000000000000000687---ee63883337537806e593c6af2350369f19703a8ad92011ae5ef03bbd513a7682]]]First, let's talk about the assert statement. The assert statement is used like assert... where an exception is raised if ... is not True. Therefore, the assert statement can be used to check whether the condition is met. By the way, when I run the above code, it runs fine (without throwing an exception), so I know that all the conditions of the assert statement are met.

[[[00000000000000000688---5cd34324b2426aed4303db7193113122c31b1da97076fc55e28d0b44d038e6dd]]]As the above code shows, we go to the previous Function with the Variable instance variable creator. Then, it traces to the previous Variable with the instance variable input of Function. A graphical representation of this connection is shown in Figure 7-3.

[[[00000000000000000689---e0092229b9cb4ceebeff5f66fadd82cc0efba30638a8870c0ab1bc6b0c692252]]]
Figure 7-3 Traversing the computational graph starting from y in the opposite direction


[[[00000000000000000690---49e050ea664bfd50f172815af53fd2183f4a709643a82cc1cf1b342c4ef193dc]]]Our computational graph is built by 'connections' between functions and variables, as shown in Figure 7-3. And importantly, that 'connection' is made when the computation is actually performed (when the data is passed through forward propagation). This feature is Define-by-Run. In other words, 'connections' are created by the flow of data.

[[[00000000000000000691---9754eac309d8a0f1380f23036a2b1172b1aa2dcb4c1bcc95d6af49f8d8b41506]]]A data structure based on 'connections' as shown in Figure 7-3 is called a 'node with links'. Nodes are the elements that make up the graph, and links represent references to other nodes. In other words, we represent a computational graph with a data structure called 'nodes with links'.

[[[00000000000000000692---f9d57b5b7319d12a785c1cf249f1b96c9e44d1c67b2521b6cb2f787ba032602c]]]try backpropagation

[[[00000000000000000693---bdc6090df5b7afa8404ba1d66f994559e265837ef8be255abbb44de2fee52f0e]]]Now, let's perform backpropagation using the connection between variables and functions. First, we backpropagate from y to b. This can be implemented as follows (illustration is included for reference):

[[[00000000000000000694---6e4c5249cfe7903863ef056c8bb152a76589feee3b5a0cfe11457e4208dfa725]]]# 1. Get the function

[[[00000000000000000695---a4d6ffe6c4429730ae22a22defb181097ba9bb025189c328f65bbfe36f77fdc9]]]# 2. Get function input

[[[00000000000000000696---b1cc801f8caf46f140e1e0ab239d60111bfce52c5852824666f579becc4d9e8c]]]# 3. Call the function's backward method

[[[00000000000000000697---3d4917385bfaa4286f581bc80206923e93146c4692ce2e40d2dc74c166b58880]]]Here we get the function from y's instance variable creator and the input variable by that function's input. And then call the function's backward method. Following this, here is the diagram and code for backpropagation from variable b to a.

[[[00000000000000000698---6e4c5249cfe7903863ef056c8bb152a76589feee3b5a0cfe11457e4208dfa725]]]# 1. Get the function

[[[00000000000000000699---a4d6ffe6c4429730ae22a22defb181097ba9bb025189c328f65bbfe36f77fdc9]]]# 2. Get function inputs

[[[00000000000000000700---b1cc801f8caf46f140e1e0ab239d60111bfce52c5852824666f579becc4d9e8c]]]# 3. Call the function's backward method

[[[00000000000000000701---b87e5083b71a4cb81bbcb99f760adca6038672673f2cf4fff45c1de34d2fcef2]]]Again we do backpropagation with the same logic as before. in particular,

[[[00000000000000000702---15a79731f7229f3df61c8025df789d10dc9e4a3bac76220cff1009d4edc01a1e]]]get function

[[[00000000000000000703---84ca4b67a8d55f91e04102ead552a59de1cfa2613a5845e2e5c1e1c5726bc5a6]]]get function input

[[[00000000000000000704---54a874bcac70b2a06de608a0c3000a162b88c6a8960039ca72a67b4b2b5481b4]]]call a function's backward method

[[[00000000000000000705---5c92e85dacadbc2e3c34062d644336b557969b157269bbb14540f08b0069dd46]]]That's the flow. Finally, backpropagation from variable a to x.

[[[00000000000000000706---6e4c5249cfe7903863ef056c8bb152a76589feee3b5a0cfe11457e4208dfa725]]]# 1. Get the function

[[[00000000000000000707---a4d6ffe6c4429730ae22a22defb181097ba9bb025189c328f65bbfe36f77fdc9]]]# 2. Get function input

[[[00000000000000000708---b1cc801f8caf46f140e1e0ab239d60111bfce52c5852824666f579becc4d9e8c]]]# 3. Call the function's backward method

[[[00000000000000000709---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000710---a76ff32f4a13f37a939ec19e9150580de8fe502a649ac54a397008bd6262e81e]]]All backpropagation is now finished.

[[[00000000000000000711---2b2009a59172719c82840da2a5d625e6fa5fcb64b97686d8d309d4ca3727f6e7]]]Add backward method

[[[00000000000000000712---a5d569ca9c8952f8b6a66d1fc61638864549e18c6249beb4d40120c0ad29e561]]]As you might have guessed, the backpropagation code shown earlier repeats the same processing flow. More precisely, the logic of backpropagating from one variable back to the previous variable is the same for all. Therefore, we will add a new method called backward to the Variable class so that we can automate this repetitive process.

[[[00000000000000000713---6e4c5249cfe7903863ef056c8bb152a76589feee3b5a0cfe11457e4208dfa725]]]# 1. Get the function

[[[00000000000000000714---a4d6ffe6c4429730ae22a22defb181097ba9bb025189c328f65bbfe36f77fdc9]]]# 2. Get function input

[[[00000000000000000715---b1cc801f8caf46f140e1e0ab239d60111bfce52c5852824666f579becc4d9e8c]]]# 3. Call the function's backward method

[[[00000000000000000716---d34e227bbbe9f45ee309105b4741e653452ef67c8892ceda4bcd2ed9d2d15fe5]]]# Call the backward method of the previous variable (recursion)

[[[00000000000000000717---b76ef7a8f632531d938eef39ba3f7fedabe4fdd8cf75ca5ad5737c6b48d1418e]]]The backward method is almost identical to the processing flow that has appeared repeatedly so far. Specifically, it gets a function from the Variable's creator and takes out the input variables of that function. And then call the function's backward method. Finally, it calls the backward method of the variable one before itself. This will cause each variable's backward method to be called recursively.

[[[00000000000000000718---f4008cc1ba12aa830d5281cd273f13c8aa9ec8b70ebb56c007ca541b948e4b8c]]]If the variable instance's creator is None, backpropagation stops there. In that case, a Variable instance is meant to be created in some way other than a function - primarily a user-supplied variable.

[[[00000000000000000719---6647c9fc68ac26866ded7fcd921dc3fd811bb5f8918c191fff4bb6c8a834e1a7]]]Now let's do some automated backpropagation with this new Variable.

[[[00000000000000000720---5befead497495e202de214d4eaa7bc67fa13edfee96ae2a2a6a01cb0e532962e]]]# Backpropagation

[[[00000000000000000721---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000722---e4ba1608f909c4bfff9843f9404ac501fcac26d5e16aa01d6c5f941d309c2e33]]]As above, if you call the backward method of variable y, backpropagation will be performed automatically. The results obtained by executing are also the same as before. congratulations! This completes the base of DeZero's most important automatic differentiation.

[[[00000000000000000723---c884d0e580cc96b1d8a26959f3bb0edc3907b0d95b8c233d76b9763d34cb23cd]]]step 10

[[[00000000000000000724---640c848e34e6ceead6a7751a52d62a0450461663fb5d16a26e74764d85db062d]]]do the test

[[[00000000000000000725---854822c52970306fd48bcede81db6144e920129cbaf7673e693db25b8bb1683b]]]Testing is essential in software development. Testing makes it possible to notice mistakes (bugs), and by automating testing, software quality can be maintained continuously. The same applies to our DeZero. In this step, I'll explain how to test -- in particular, how to test deep learning frameworks.

[[[00000000000000000726---46202524fc0315505334337dd11952dd9275dc39af2808849037b9ccf46eb1f0]]]As the scale of software testing grows, it tends to have its own methods and detailed rules. But testing doesn't have to be hard, especially in the beginning. First of all, it is important to 'test'. In this step, we will do the simplest possible test, not the 'full-fledged' test.

[[[00000000000000000727---795ac3c5f51b360d9fbbfa8f9afacb3c278d7242fb098ededfb051850be1e3d0]]]Python unit test

[[[00000000000000000728---a576e4d392cd650e8d46ead139de1b3c3ad472dd2e06e8fe2ef4d37fa5c46c5a]]]For testing in Python, it is convenient to use unittest included in the standard library. Here, let's test the square function implemented in the previous step. Here is the code:

[[[00000000000000000729---b8cd311e1514cc6133cec7b20d294cfc3d590bc51649537b49663dd7b3c2fa01]]]As above, first import unittest and implement a SquareTest class that inherits from unittest.TestCase. The essential test is to create an arbitrary method whose name starts with test and write it in it. The test we write verifies that the output of the square function matches the expected value. Specifically, when the input is 2.0, we verify that the output is 4.0.

[[[00000000000000000730---b90ee91741c3a306a47cbbc3d092ab041141ee57751409fd1f29a07c95de6f8e]]]In the example above, the method self.assertEqual is used to verify that the output of the square function matches the 'expected value'. This method tests whether the two given objects are 'equal'. In addition to this method, unittest provides various methods such as self.assertGreater and self.assertTrue. See the unittest documentation [8] for other methods.

[[[00000000000000000731---108ef276a98fdc244b0aeea3353724d5586861dc7f17c61cd09f7e1725303b8a]]]Now let's run the above test. Let's assume the test code above is in steps/step10.py. In that case, run the following command from your terminal:

[[[00000000000000000732---89d20fe3b91a902818f4fb4a7d268e219c99d6e9496ca841993bf4a242ba4862]]]As above, you can run a python file in test mode by giving the argument -m unittest. Alternatively, you can add the following code to the end of the step10.py file to run the test only with python steps/step10.py.

[[[00000000000000000733---65903cf22a1ad2dc41a241dd3342f360562f3d736e315f4da573cb9b8d8882dd]]]Now let's check the output of the test. If you actually run the above command, you will get the following output:

[[[00000000000000000734---a394bfb80784413f49f30ce9f2249e09d77511b5ef69578149826cb8e4f80487]]]This output means 'I did one test and the result was OK'. So the test passed. If something goes wrong here, you'll see an output like 'FAIL: test_forward (step10.SquareTest)' that the test failed.

[[[00000000000000000735---c79bd6b8b8e7a3119ef3d493734aa16f83eee26c60c85756688ff0f61e8a8ed4]]]Testing backpropagation for the square function

[[[00000000000000000736---47babb4d891d22d36843ba2c8e7b056429be1284bed44888fb840836d44e1d74]]]Next, let's add a test for backpropagation of the square function. To do so, add the following code to the SquareTest class you implemented earlier.

[[[00000000000000000737---01c3a6811c61f75d87392e808af11e483bcf540eb961dfd3004e165f7c6ce04c]]]Here we add a method called test_backward. In it, we take the derivative by y.backward() and check if the value of the derivative matches the 'expected value'. By the way, the value of 6.0 is set as the 'expected value' here, but this is the value obtained by manual calculation.

[[[00000000000000000738---f06b2b1036ef1165b6df686a225bf5914a342f0df2ee2a697012148aa7f4d8e2]]]Let's test the above code again. The result is an output similar to the following:

[[[00000000000000000739---0cef0771f16c322ce2f5745318a86c2d3ef5e9e7416e45eb6030678ea1fdb7ac]]]Looking at the results, we can see that the two tests passed. You can then add other test cases (inputs and 'expected values') in the same way as before. And as the number of test cases increases, the reliability of the square function also increases. Also, by testing when the code is modified, the state of the square function can be repeatedly verified.

[[[00000000000000000740---84e3c8fc3ac2091a4cab631466b0dd72dee88ecc0036b4e2fd2caa26d4499ef8]]]Automatic test with slope confirmation

[[[00000000000000000741---7a3bdb6101fca443829f088972a28dbfcb822d9942939e51bc0f996f1e619a4b]]]Earlier we wrote a test for backpropagation. There, I gave the 'expected value' of the derivative by hand calculation. In fact, there is an automated test method that replaces it. It's a method called gradient checking. Gradient confirmation is performed by comparing the result obtained by numerical differentiation and the result obtained by backpropagation. If the difference is large, there may be a problem with the backpropagation implementation.

[[[00000000000000000742---8f74e7cf9edb35d77f9ddca12c7954275e92ca741418c4873fe49f79c6245775]]]We have implemented numerical differentiation in step 4. Numerical differentiation is easy to implement and gives approximately correct derivative values. So you can test the correctness of your backpropagation implementation by comparing it with the numerical differentiation result.

[[[00000000000000000743---ffec8ae23d8f8210a13260aaf01a7e57c74bd53168ed1bedd088090da76a00ce]]]Gradient confirmation can be efficiently tested because only input values need to be prepared. Now let's add a test with gradient checking. Here we use the numerical_diff function implemented in step 4. As a review, I will also post the code of the function.

[[[00000000000000000744---5740b8b26471203999fd2d7d230562fadedbdf86e42872103e0a02604ede310f]]]# Generate random input values

[[[00000000000000000745---b7e8dd6831b2ffeee58877a07267cf7c254c4515365c768aa87b69ce13d62621]]]In the test_gradient_check method that checks the gradient, we generate one random input value. Next, find the derivative by backpropagation, and then use the numerical_diff function to find the numerical derivative. Then, confirm that the values obtained by the two methods are approximately the same. For that, we use the NumPy function np.allclose here.

[[[00000000000000000746---ba12157987c34fc0a507614c7a92523afc7e9035805ebba6965de9a130276822]]]np.allclose(a, b) determines whether a and b of ndarray instances are close. The definition of how close values are can be specified by the arguments rtol and atol, like in the np.allclose function (a, b, rtol=1e-05, atol=1e-08). Then return True if all the elements of a and b satisfy the following conditions:

[[[00000000000000000747---bbac713da6e64e4812abb305c044ab1602ba58d6db65fa3334872ecbc23bd6f1]]][†1] The |.| notation represents an absolute value.

[[[00000000000000000748---b0510ee8ab482a337572a5695607ced89fd19af1935637acee5a1e4a61015e1e]]]Note that the atol and rtol values may need minor adjustments depending on the calculation (function) for which the gradient check is being performed. Reference [5] is a good reference for the criteria. Now let's add the gradient check above and then test it. This time I get the following result:

[[[00000000000000000749---ceeb44fee8057d1e42e35759d7a8991db2093f40fa4be563acf85c36c9926cb7]]]In this way, in the case of a deep learning framework that automatically obtains differentiation, it is possible to create a semi-automatic test mechanism by checking the gradient. It allows you to systematically (methodically) create test cases more broadly.

[[[00000000000000000750---0063bc81198be87878244299c9793fa2ea0a8de1d0c68ff8a6ec2d1bc587eef2]]]Summary about testing

[[[00000000000000000751---fff97ddb07932671d7cebc0ce16bf3d6baaa7aa5d8cf60d657357636c7635f77]]]The above knowledge about testing should be enough to develop DeZero. You can write test code for DeZero using the steps you learned here. However, in this book, we will omit the description about the test from now on. If you feel the need for test code, feel free to add it yourself.

[[[00000000000000000752---2faab8b7ec1b368574edbaf0aa5e78c7a312af14528c68101ab77ff5238dfc6b]]]Also, it is common to manage test files in one place. The test code in this book is also put together in the tests directory (In addition, convenience functions for testing are also implemented). If you're interested, take a look at the test code. There you will see a lot of code like the one you wrote in this step. The test files can be executed together with the following command.

[[[00000000000000000753---eadf89399dd12debf58a683b3c9469df5b8f50592e0cf5946ff6eba2d4d33e45]]]By using the discover subcommand as above, the directory specified after discover is searched for test files. And all files found are executed together. By default, it recognizes the pattern test*.py in the specified directory as files for testing (this can be changed). Now you can run all the tests contained in the tests directory at once.

[[[00000000000000000754---a35698811218543b60c102d4774dc645357eb7424909915a54504461f6f8392b]]]In DeZero's tests directory, we also test Chainer as the correct answer. For example, when testing the sigmoid function, we compute DeZero and Chainer respectively on the same input and compare whether the two outputs are almost the same value.

[[[00000000000000000755---abd1a097f55711a0ca9eab2ff0344f6556a79a0d4d110c023276ad275fbfaa66]]]DeZero's GitHub repository is also integrated with a service called Travis CI[9]. Travis CI is a service for continuous integration. DeZero's GitHub repository automatically runs tests when you push code or merge pull requests. And if there is a problem with the result, it will be reported by e-mail. In addition to that, the top page of DeZero's GitHub repository also displays a screen similar to Figure 10-1.

[[[00000000000000000756---a1c9d1350ffeb902406dd2414f9dc89843da94f26258f48820246fa805d572a2]]]
Figure 10-1 Top screen of DeZero's GitHub repository


[[[00000000000000000757---d49a6ca69c98eb7690e680a4359708b6ca9b5cfd00d9f9be7b7631671a792ab1]]]A 'build: passing' badge appears, as shown in Figure 10-1. This is proof that the tests passed (if they fail, you'll see a 'build: failed' badge). By working with CI tools like this, you can always test your source code. This keeps your code reliable.

[[[00000000000000000758---d31ecd844e10a30acf967665dea64b3429b506da8da403e14003c27a89a88ca0]]]DeZero is a small piece of software, but we will grow it into a bigger piece of software in the future. By introducing a testing mechanism like the one described here, you can expect to maintain the reliability of that code. This completes the first stage.

[[[00000000000000000759---395b0b6f562fb84821b76724a98ed1ecf01c59cb28be7dddc080ee69536ff288]]]So far we've been building DeZero bit by bit – and steadily. The first DeZero had only a 'small box (variable)', but now it has grown to perform a complex algorithm called backpropagation. However, for now, the application of backpropagation is limited to simple calculations. From the next stage, we will further extend DeZero so that it can be applied to more complex calculations.

[[[00000000000000000760---debad3f691f3abe711a9d576ea39c70a5ebf8a21ffff0a70afa167473d4a5eca]]]Column: Automatic Differentiation

[[[00000000000000000761---55ef7094268558b2485dfb6d79d71bbec468151276d85729cd8f4d5ab8bce134]]]A core technique in deep learning frameworks is backpropagation. Backpropagation is sometimes called 'automatic differentiation' in some literature. It should be noted that the term 'automatic differentiation'--especially in the academic field--refers to a more restrictive technique. Here is a supplementary note on the term automatic differentiation.

[[[00000000000000000762---ba284b43c997f702947a2602d0d9bdf9ea27893e672ed454017595352878fb22]]]If you interpret the automatic differentiation literally, it means 'a method (technique) for automatically obtaining differentiation'. 'Automatic differentiation' means that a computer (not a human) determines the differentiation. Specifically, it refers to a system in which a computer automatically obtains the derivative of a calculation (function) by coding it.

[[[00000000000000000763---39760b40c3ac9a53f67e18a7f90ece42b3ecb5da4b644546f9b09bc0fe3e8c3e]]]There are three main methods of obtaining differentiation in computer programs. The first method is numerical differentiation. This performs two normal calculations (forward propagation) with small differences on the variables, as implemented in step 4. Then, approximate differentiation is obtained from the output difference. Numerical differentiation is easy to implement, but the output tends to contain errors, and the computational cost is high when dealing with functions with many variables.

[[[00000000000000000764---cadc8ba8ba8dc917bfe71e51f83bc7c6ba224b962cce5f1564055a6592dec1f2]]]The second method is symbolic differentiation. This is a method of obtaining differentiation using the differentiation formula, as we learned in high school mathematics. Ultimately, the input is a 'mathematical formula' and the output is also a 'mathematical formula' (a formula can be expressed in a tree-structured data format). This method is used in Mathematica, MATLAB, etc.

[[[00000000000000000765---a0035ff796afd837793cca60961b4439febb87f30cecb1ba013af449bd6ef12e]]]The output of a mathematical derivative is the differentiated 'formula' -- that is, the derivative -- at which point no numerical calculations are performed. After getting the derivative, you can find the derivative at a concrete value (eg, ).

[[[00000000000000000766---f37e3ac057f53f49e6f2229e576f3d3ec5746c0575b01f7222f49df62664a672]]]The problem with mathematical differentiation is that the formula tends to bloat. In particular, implementations that don't take optimizations into account can quickly grow into huge formulas (yes, they 'explode'). In addition, in calculations such as those handled by deep learning, it is necessary to efficiently obtain the 'values' of derivatives (rather than formulas) for a large number of variables. This calls for a better method.

[[[00000000000000000767---fd85f5f95f430acde2db865bf5a2986da12f69d09840c5e7a8ed11b6e502165e]]]The third method is automatic differentiation. This is how to find the derivative using the chain rule. If you give some function as a program, you can find its derivative efficiently and accurately. Backpropagation is also included in one of the automatic differentiations. More precisely, automatic differentiation can be broadly divided into two types: 'forward mode' and 'reverse mode'. Backpropagation corresponds to the latter 'automatic differentiation in reverse mode'.

[[[00000000000000000768---3a5953e0e471c27c52abfa182af493fefd21efa6147545264b70f63ebbdc97eb]]]Backpropagation (automatic differentiation in reverse mode) propagated the differentiation from the output side to the input side. Forward mode automatic differentiation, on the contrary, propagates the differentiation from the input side to the output side. Both methods use the chain rule to find the derivative, but their “paths” are different. If you have a single output and want to find the derivative of that single output variable, the reverse mode of automatic differentiation is the way to go. In many machine learning problems, the output is a single variable, so the reverse mode of automatic differentiation is used. We will not discuss forward-mode automatic differentiation further in this book. Those who are interested in automatic differentiation in forward mode can refer to references [6][7].

[[[00000000000000000769---4af6d6a845536aa8d32922c435c2f2302406cc46fe283923ce9607834703ad5e]]]Summarizing the above, the 'method for obtaining differentiation using a computer program' is shown in Figure A-1.

[[[00000000000000000770---42273d7cc1987940d5f7000a222b529f0692193c452883d9840f8da1bbaf4dd4]]]
Figure A-1 Method of obtaining differentiation by computer program


[[[00000000000000000771---442dcb14f436bacef561473ca340772f4cf0c60ddfa14dc822b876129b85ebf5]]]As shown in Figure A-1, 'automatic differentiation' refers to one of the methods of obtaining differentiation on a computer. The deep learning framework is positioned to implement 'automatic differentiation in reverse mode' within it. However, some literature refers to backpropagation as 'automatic differentiation' without distinguishing between forward and reverse modes.

[[[00000000000000000772---a8ee8eabb8b3bf266711eb09da9b061fc7c7143ee0d8ed44a4aee246f1e714cd]]]Automatic differentiation has long been a field of academic research. It has a long history and has accumulated a lot of important knowledge. Unfortunately, I haven't had much interaction with the machine learning field so far. Recently, due to the boom of deep learning, the field of automatic differentiation has been attracting more attention, and new exchanges between fields such as machine learning and programming languages and the field of automatic differentiation have begun.

[[[00000000000000000773---b57daee1a15a41ba1ea385bba929d0d13a63c7742633e7119ca69e454a49ce36]]]step 47

[[[00000000000000000774---568279f8e28f52f7a2405750191130e8ea23bf619648f04ab7c46d558942e5bf]]]Softmax function and cross-entropy error

[[[00000000000000000775---96f632e283b99fa1d850eb26bcbdad9444043f7ab8d27e0e5adeda7ccbc83ba0]]]We have so far used neural networks to solve regression problems. From now on, we will challenge a new type of problem that will replace the regression problem. It is multi-class classification. Multi-value classification is, as the word suggests, the problem of 'classifying' into 'multi-values'. Given an unclassified object, guess which of several classes it belongs to. In this step, preparations for performing multi-value classification are performed. The next step is to implement multi-class classification using DeZero.

[[[00000000000000000776---0335badbd25333a750174d17ad3520aa83f03f78843d7cf4c45ce2f87e832bf6]]]Functions for slicing operations

[[[00000000000000000777---c72f02f6733f0abd6d9f078c02d95093309bd9472900910018eb2e731c5c97ce]]]First of all, as a preliminary preparation, add one useful function. It's a function called get_item. Here we only show how to use that function. If you are interested in how to implement it, please refer to 'Appendix B'. Now, here's an example of using the get_item function.

[[[00000000000000000778---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000779---3301315325e25231ca16b49097bf03c4ca9834bf68ab858aba46793455cafeec]]]As above, the get_item function extracts a part from the multidimensional array of Variable. Here, we extracted the first row element from x of shape (2, 3). This function is implemented as a DeZero function. This means that the backpropagation can also be done correctly. For example, following the code above, write the following code.

[[[00000000000000000780---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000781---5f6ad2917798772a8a257fa52c0202710f33a2b8d6c6e217b1333c52e84726f6]]]Here, y.backward() is called to perform backpropagation (at this time, y.grad = Variable(np.ones_like(y.data)) and the gradient are automatically completed). The 'computation' performed by slicing is passing through a portion of the data in the multidimensional array as-is. So its backpropagation will set its gradient where it was picked in the multidimensional array and set it to 0 otherwise. Graphically, this looks like Figure 47-1.

[[[00000000000000000782---997f5a537c1e79cf30ce67777e518bd5e059532759f9abc02fac2e29378f9177]]]
Figure 47-1 Example of Forward and Backpropagation for the get_item Function


[[[00000000000000000783---f860ea927dbbd90455f7a45158562b56db4e52bb736a9378839b9c62f6e35b6b]]]An operation that extracts a portion of a multidimensional array is called a 'slicing operation'. In Python, you can slice a list or tuple by writing x[1] or x[1:4].

[[[00000000000000000784---96dbbb9cc82eadb24e862a0d9563776c70a47a53676a0d495fade769e68ad646]]]You can also use the get_item function to extract the same element multiple times.

[[[00000000000000000785---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000786---b80853cf38bd74662ac8411ddcc0865fe8c1ccbd557a4c171fe85b520baf3aa4]]]The above is DeZero's get_item function. Next, set this get_item function so that it can also be used as a variable method. You can do that with the following code:

[[[00000000000000000787---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000788---d199d17108a44e59360d0eb547ea3a74d10c36691d3e42a28f06227e418c9deb]]]Set Variable.__getitem__ = get_item as above. By doing so, the get_item function will be called when you use notation such as x[1] or x[:,2]. And the backpropagation of that slicing operation is also done correctly. The setup of this special method should be called in the setup_variable function in dezero/core.py (the setup_variable function is a function that is called when DeZero is initialized). Now you can freely perform slicing operations on the Variable instance. Now let's move on to the main topic of this step.

[[[00000000000000000789---6b20766ff9848235a1057184e34ebf13a772cfab462668fe034c5555827d2e6e]]]softmax function

[[[00000000000000000790---bb460a41bd7eec69afe6013ce8ad01b2c269fe57fa0806a9681189dafd6badbc]]]When performing multivalue classification with a neural network, the neural network used for linear regression can be used as is. So far, we have implemented neural networks in a class called MLP, but it can be used as is. For example, if the number of dimensions of the input data is 2, and the problem is to classify into 3 classes, the following code can be written.

[[[00000000000000000791---743e9f877bb047e500767071a8958dfff3a8f207a0c83e702a52b95d0ae2b62b]]]As above, MLP((10, 3)) produces a fully connected network with two layers. The output size of the first fully connected layer is 10 and the output size of the second fully connected layer is 3. Now the model transforms the input data into a 3D vector (a vector with 3 elements). If you enter the correct data, it will look like this:

[[[00000000000000000792---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000793---456543525b25fe07eb4e5ee3fbfb7c85b9397c4b914fce953cd17b79000cfa81]]]where x has shape (1, 2). This shape means that there is one sample data and that data has two elements (= 2D vector). And the shape of the output of the neural network will be (1, 3). This means that one sample data has been transformed into 3 elements (= 3D vector). At this time, each element of the 3D vector corresponds to each class. The index with the highest value for that element will then be the classified class of the model. In the above example, the 2nd element (among the 0th, 1st, and 2nd elements) is 0.31733288, which is the largest, so it is classified into the 2nd class.

[[[00000000000000000794---a3b21244723e9cb3d3f30dfea272b7401e4833a0509be6366ecd36f06440d137]]]In the code above, there is only one input data, but you can process multiple data at once. For example, x = np.array([[0.2, -0.4], [0.3, 0.5], [1.3, -3.2], [2.1, 0.3]]) will combine four input data into one can be summarized. Then y = model(x) processes all four inputs at once. Specifically, y.shape is (4, 3), the ith input data is x[i], and the corresponding output is y[i] (i=0,1,2,3).

[[[00000000000000000795---18d00b250ebfdfe1a30302ee0ec9d787f9c31f7fc4bdc7c5d80c6edc2310572f]]]In the example code given here, the output of the neural network is just 'numbers'. You can also convert this number to a 'probability'. That's what the softmax function does. The softmax function is given by the following formula:

[[[00000000000000000796---b4373685e5ee7e43cda7b36191b25d48ca7ed5b95e55a124300abe885548847f]]]We assume here that there are a total number of inputs to the softmax function (this is the 'number of classes'). And formula (47.1) shows the calculation formula for the th output. The numerator of the softmax function is the input exponential and the denominator is the sum of all input exponentials. This will be and will be. That is, can be interpreted as a probability.

[[[00000000000000000797---b099267048ed2a74ed1c0906a9cc59a6d31bb3e982c3038d71676a43425fbb34]]]Now let's implement the softmax function in DeZero. First, let's try implementing the softmax function only when there is one input data (one sample data). Here is the code:

[[[00000000000000000798---cdee0c582063193fb2e4b42eeb6c03758ed9e2299fbd7ec3ba059ce7981b758a]]]The core implementation is just a matter of coding expression (47.1) using DeZero's functions (assuming DeZero's Exp class and exp function have been added to functions.py). The first x = as_variable(x) causes x to be converted to a Variable instance if it is an ndarray instance.

[[[00000000000000000799---1e2c7d6f4a1df67dbd528cbe6084a1e1bf24d77e565e5067eae98a380518dc71]]]In the calculation of y/sum_y in the code above, the shapes of y and sum_y are different, so the broadcasts are made to match the shapes. We already have DeZero ready for broadcast. So the correct backpropagation occurs even if a broadcast occurs.

[[[00000000000000000800---1d5e447b84a1ef79cf64c3b1ca3a0ce1ac34a43cb98d3529edab2e1fd6ede366]]]Now let's use the softmax1d function.

[[[00000000000000000801---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000802---6f899869037f7ff329b088385ae86dca43dc97a1ba1185c89b3a0bc20907008c]]]where each element of p is a value between 0 and 1 inclusive. And the sum will be 1. We have now converted the output of our neural network into probabilities.

[[[00000000000000000803---2af219c571cf352976136919ba16c373fffa6ae3a3e8d8b7f4202147839071d5]]]The calculation of the softmax function is an exponential calculation, so its value can easily become large (or small). Therefore, it is common to implement measures against overflow in softmax function implementations. This document omits explanations of countermeasures. For a better implementation of the softmax function, please refer to '3.5.2 Notes on implementing the softmax function' in 'Deep Learning from Scratch'.

[[[00000000000000000804---413ab3f08b3f3ae9e0dba72e2310c8770caf6d37af60a2f34ce20ef0bd55ac28]]]We then extend it to apply the softmax function to batched data. For example, consider applying a softmax function to each sample data, as shown in Figure 47-2.

[[[00000000000000000805---00f513c3a52b498a787ae0dfcf134c38e6eb3e9266349a2973809e257efb8903]]]
Figure 47-2 Example of applying the softmax function to two-dimensional data


[[[00000000000000000806---774e42b43307d04b0db847b5aa31fb960214ab9f89f71e1894eb6545e74de707]]]As above, we implement the softmax function so that we can apply the softmax function on the batch data. It can be implemented like this: †1

[[[00000000000000000807---05aeb062518e4745221a0bab9bf994e0401cef672287de4bf1109edd7b741a28]]][†1] This code is defined in dezero/functions.py. The exp and sum functions are also defined in dezero/functions.py, so calling DeZero's functions is simply exp(), not F.exp().

[[[00000000000000000808---9702b7eae5dbc954fdea6ea255d770a799182652ea9655a7d6bd08a33fb2ba5b]]]Argument x assumes two-dimensional data. And the argument axis specifies along which axis the softmax function should be applied. When axis=1, the softmax function is applied as shown in Figure 47-2. Since keepdims=True in the sum calculation, the division of formula (47.1) will be performed for each row.

[[[00000000000000000809---35d80d7f7798ac9626fbb658b8d2c9b3885ece3a10e93ec2501a405085faf91d]]]The softmax_simple function implemented here is a simple implementation using DeZero functions. It gives correct results, but there are improvements. A better implementation scheme is (again) to extend the Function class, implement the Softmax class, and implement softmax as a function. We omit the explanation here. It is implemented in dezero/functions.py, so please refer to it if you are interested.

[[[00000000000000000810---313be33e29a31ffa86262a7f3fb5b35b0674097c44d83298b4c24e4c0970dd2d]]]cross entropy error

[[[00000000000000000811---ff9a00a4d9898e4e0e5b1d21c1eb19a4813d50202df5042a09cf975635d88fe0]]]In linear regression, we used the mean squared error as the loss function. On the other hand, a dedicated loss function is used to perform multi-value classification. A commonly used one is the cross entropy error. The cross-entropy error is defined by the formula:

[[[00000000000000000812---ec42ac19a4ae4bc3bfd1bd60cb27150de7267bb5b18e138940ad20d11048fb7d]]]Here, it represents the value of the dimension of the training data. Elements of this training data are recorded so that the class that is the correct answer is , and the others are . This representation method is called one-hot vector. Also, is the output after applying the neural network's softmax function.

[[[00000000000000000813---dae670ad122b6a33048b51750f9228670d4db555d1e557903c1ff9e67618ff47]]]The cross-entropy error equation (47.2) can also be expressed more simplistically. For example, in the case of , substituting it into formula (47.2) yields In other words, the cross-entropy error can also be calculated by extracting the probability of the correct class number. Therefore, if the correct class number is given as teacher data, it can also be calculated as follows.

[[[00000000000000000814---a6ab8272d0a4cb1a46fadde46f8cde60b0fbd771d800e9709def2f3283054adf]]]Here it means extracting only the th element from the vector. This slicing operation is a function provided in DeZero at the beginning of this step.

[[[00000000000000000815---5e3c549c8cb7c6474217a86ebf12bbcb7181ad874d634eadb282693c3f6b8861]]]The discussion of cross-entropy error here is for the single data case. If you have data, find the cross-entropy error for each data, sum them, and divide by. It gives the averaged cross-entropy error.

[[[00000000000000000816---9f4fcc3809f0f0f21040c79825177223731b4ee2ebd652c1ebfcc85178e0ab16]]]Now let's implement the cross-entropy error. Here, the function that combines the 'softmax function' and the 'cross entropy error' is called softmax_cross_entropy_simple(x, t) and implemented as follows.

[[[00000000000000000817---092cb38d4eec3e575788220b447afddd96d84d533b66c71587ad66388f462306]]]# or softmax_simple(x)

[[[00000000000000000818---fd3864c4c5ee182fd248579894f63c0a0cef11c591977c0169154ff6f4320bf3]]]# Set p to 1e-15 or more to prevent log(0)

[[[00000000000000000819---239db4fbf86f8c3f36870e13753d506b614db73676e3087b37e59682170c7609]]]# this log is DeZero's function

[[[00000000000000000820---6d927d4c0a4c6e8bfb07f389bcc85e291533e3d2c596f76ac083a01231d0bca6]]]where the argument input x is the output before applying the softmax function of the neural network. And t is the training data. It is assumed that this training data is given the correct class number (label) (not a one-hot vector).

[[[00000000000000000821---8169e39143fcef9426bab879b809b93a6c8669254122d3d5d98a61b186cba067]]]In the code above, the elements of p in p = softmax(x) are between 0 and 1 inclusive. Then we calculate the log, but giving 0 to the log function causes an error (or more precisely, a warning). To prevent that, if it's 0, replace it with a small value like 1e-15. A function called clip does that. This clip function is used like clip(x, x_min, x_max). When doing so, any element of x (Variable instance) less than or equal to x_min is converted to x_min, and greater than or equal to x_max is converted to x_max. Note that the implementation of the clip function is omitted (the code is in dezero/functions.py).

[[[00000000000000000822---4af046c2d6e8859fb5da6f166bd120744703709b0c376ed933cfab2aa959800c]]]Also, np.arange(N) in the code above produces an ndarray instance of [0, 1, ... ,N-1]. By setting log_p[np.arange(N), t.data], log_p[0, t.data[0]], log_p[1, t.data[1]], ... and so on to the training data The output of the corresponding model is retrieved and collected into a one-dimensional array.

[[[00000000000000000823---031ed03e7e2052ed3a72207d5f51d14474c0d209830c1acc0fc83f65bdc3b6e1]]]The softmax_cross_entropy_simple function above is a simple implementation. A better implementation is in dezero/functions.py as the softmax_cross_entropy function. In the explanation of this step, a simple implementation method is adopted to give priority to clarity.

[[[00000000000000000824---9f15e5714202060eb8cc6fa1ec1706f5b9aa9bb2e2c7b91f811e96c38435f5d2]]]Now, let's calculate the cross-entropy error using concrete data for a neural network that performs multilevel classification.

[[[00000000000000000825---09a7751ab61d501ffb07ed6b592b5b60fd6e13d91c7984754f333f55d2bcb2ed]]]# or F.softmax_cross_entropy(y, t)

[[[00000000000000000826---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000827---58d80daa97e48cc8e368fbee943353d1d16aa9e2460587a80100e34f802acfa1]]]First, prepare x as input data and t as teacher data. In the teacher data, the number of the correct class is recorded. Then transform with y = model(x) and calculate the loss function with F.softmax_cross_entropy_simple(y, t). Now you are ready to perform multi-class classification. The next step is to actually do the multi-class classification.

[[[00000000000000000828---9a7b2d2c1692034d5f380fb23b2941267dae84c7ae21114c59ae23dec896eb00]]]step 31

[[[00000000000000000829---2e49b9dd356efa4a1eb19a4620c110df6b9b1c154d7feb399a1800c672cb8dad]]]Higher Differentiation (Theory)

[[[00000000000000000830---38a2549960bf37c20c8d48539b09bf3be50014f63254ec8fca3ce9ab0858a849]]]In the previous step, we reviewed the current DeZero implementation. The gist of it is as follows.

[[[00000000000000000831---2832d9918db81deb22fb3c2599df4c297ba6682825a47167a9f7d299b4db635a]]]Computational 'chains' are made in the __call__ method of the Function class

[[[00000000000000000832---80990d941803e967341e6f30720fbb45f9892ce5602998705878dacc9dc0501f]]]Concrete calculation of forward propagation and back propagation is performed by the forward method and backward method of the class that inherits the Function class.

[[[00000000000000000833---225fa89b8991587ba3112c1935c15a6db50823b4ecd921b54a2e8158fc7dfbde]]]The point I would like to focus on here is the timing at which the “connection” of the computational graph is created. It is created when forward propagation calculations are performed. On the other hand, for calculations performed by backpropagation, no 'connection' of calculations is created. Here lies the heart of the matter.

[[[00000000000000000834---43dd91b9fc21ff0411c9467f4bcf426ed427d3542768ca708a5b308402ea46da]]]Calculations performed by backpropagation

[[[00000000000000000835---c080851c175f259a10de35304e001746d969b7efeb60b58eea61e6dd266f27b4]]]Similar to forward propagation, concrete calculations are performed in back propagation. For example, for the Sin class that appeared in the previous step, the backward method is implemented as follows.

[[[00000000000000000836---fb42eeb632fc8877901d7024cfa1e04503087d3c1429e2811561a678ff49b866]]]As above, the concrete calculation gx = gy * np.cos(x) is done. However, in the current DeZero, no computational graph is created for that computation. Because that calculation is being done on an ndarray instance. What would happen if a 'connection' was created for the calculations performed by backpropagation? In fact, higher derivatives can be obtained automatically! To explain the principle, let's first look at the computation graph in Figure 31-1.

[[[00000000000000000837---bdfef3e9c82469264e56ccbad00bcfce3757b95edf766d18a10799cd251b1ee0]]]
Figure 31-1 Computation graph for y=sin(x)


[[[00000000000000000838---fa81c6ac7b9b5a910feb8e93cf5c299d01d0530297c0f1917e8461617ba2305f]]]Figure 31-1 is a computational graph of y=sin(x). At this time, calling y.backward() will find the derivative of y with respect to x. This is the example I've seen so far. Next, let's look at the computation graph in Figure 31-2.

[[[00000000000000000839---1f5ea55a6f4361d3999c93fce1c64256c9308e1b3167a07f29955c11f9b0ab9d]]]
Figure 31-2 Calculation graph for obtaining the derivative of y=sin(x) (gx is the derivative of y with respect to x)


[[[00000000000000000840---a5059b896cde9d843f11d765fc393b035adf070ae4a66ef036274a58bb3c56f2]]]Figure 31-2 is a computational graph for obtaining the 'differential of the sine function'. By the way, this computational graph is a computational graph of the Sin class backpropagation code (gx = gy * np.cos(x)) shown earlier. If you have a computation graph like Figure 31-2, you can find the derivative of gx with respect to x by calling gx.backward(). Originally, gx is the derivative of y=sin(x), so by calling gx.backward(), the derivative with respect to x is obtained. So that corresponds to the second derivative of x.

[[[00000000000000000841---c62b6bb5d4b33e22c937f56a34fe7eb0eb92c173e09605d717e4dec303e75651]]]If the above explanation doesn't make sense to you, try replacing x with 'time' and gx with 'velocity' in Figure 31-2. In that case, Figure 31-2 represents a computational graph that outputs the 'velocity' at that time when a certain 'time' is input. At this time, backpropagation gives the derivative (change) of 'velocity' with respect to 'time', which corresponds to 'acceleration'.

[[[00000000000000000842---f419cab837f19ff75ea1b911f262eb04f1cf32880cdd143a2aae0c3b24fbe4fa]]]My future goal is to create a computational graph of 'calculation for obtaining differentiation' as shown in Figure 31-2. 'Computation for obtaining differentiation' is, in other words, computation performed by backpropagation. Therefore, the problem will be solved if 'connections' can be created for calculations performed by backpropagation. From now on, we will think about how to realize it.

[[[00000000000000000843---d61d3692e2492ac77078a527882dd79e5e45c91875a6714da42ab2a708110ca4]]]How to create a computational graph with backpropagation

[[[00000000000000000844---9f3829538a17f459c7b6e3a4af911f04111f2072a3fe67bcacc94d6674d21923]]]In DeZero, 'connections' are created in forward propagation calculations. More precisely, 'connections' are created at the timing of normal computation (forward propagation) using Variable instances. In other words, even in the backward method of a function, if you perform calculations using Variable instances instead of ndarray instances, the 'connection' of the calculation will be created.

[[[00000000000000000845---68d937496ed8784ab00af4d3dcbbf2337a9bf0b51802c1b8a6a1abcb4776bb33]]]To prepare for that, we first need to store the derivative (gradient) as a Variable instance. Specifically, change the Variable as shown in Figure 31-3.

[[[00000000000000000846---8cd6fdaff145ad79d832415bd8246099a7a74b88b7d2f39f4fe1dca51e458624]]]
Figure 31-3 Old Variable class (right) and new Variable class (left)


[[[00000000000000000847---a35b63fa933cc3162a9b25a9d3eff22d59b45b64b36a79947b46db4c60c66603]]]As shown in Figure 31-3, Variable class grad used to refer to ndarray instance. Change it to refer to a Variable instance. With this change, for example, in the previous example of y=sin(x), we can create a computation graph like Figure 31-4.

[[[00000000000000000848---41ddeec9d021813c0e7ecd2041541485a7d124337aa59c9719324682dc0c3513]]]
Figure 31-4 Calculation graph of forward and backpropagation of Sin class (contents of backpropagation calculation are omitted)


[[[00000000000000000849---5ed4c6a5c123aa6a2493112f91c90f531b1a31d1b4927a9ba94cdb0bf5d1d0aa]]]Figure 31-4 is the computational graph after forward and backward propagation of the Sin class. The important point is that a new computational graph is created for the backpropagation computation as well. Thanks to the fact that gy, which represents differentiation, is now a Variable instance, there is a 'connection' in the calculations performed using gy.

[[[00000000000000000850---feeac372378196fa21b90335dd4e4152f4291a4700e2c45e63d76aca98e1a953]]]In the computation y = sin(x), if you do y.backward(), only the 'tail variables' like x keep the derivative. y is a function-generated variable, so it does not hold the derivative. Also in Figure 31-4 there is no reference from y.grad to gy.

[[[00000000000000000851---46ee740e03182b0a6ccce08ff68991a93d00da746b38ff78d4148efd30becfb6]]]Now, in Figure 31-4, we have omitted the backpropagation computation for the Sin class. When I implemented the backward method of the Sin class, the code for its differentiation was written as gx = gy * cos(x). Now consider all of its variables changed to Variable instances. Then, the backpropagation calculation omitted in Figure 31-4 actually creates a computation graph like Figure 31-5.

[[[00000000000000000852---8593efd760a8b626fe202cb87f5ba3c3c1aca913d0f3f03263dc913dfce11a89]]]
Figure 31-5 Calculation graph actually created


[[[00000000000000000853---2d6ead662103318f5d1297a16a7ba2bf0663cdcdc682dc645e285d9a8068cd8f]]]The computation graph in Figure 31-5 is a computation graph created 'newly' by calling y.backward(). In other words, 'a new computational graph is created by backpropagation'. Once you have a computation graph like Figure 31-5, calling gx.backward() on the variable gx will give you the second derivative of y with respect to x.

[[[00000000000000000854---87d3376fce4821eba65c03abc98729cd9b3ce0c90dcc4360680fc9ddfb35fdcf]]]The above is the strategy for obtaining higher-order derivatives. The next step is to implement it.

[[[00000000000000000855---c14f21e2c9fcf2e61164759417301bb30c1969294e3a74cba12bc95a53ba2f53]]]step 17

[[[00000000000000000856---ac27f638422e45c0b8133a20179c5ca527667b6a9bd9e7f9be14270303801247]]]Memory management and circular references

[[[00000000000000000857---5aa1fb7587e6ffcd428da39503c3ae506a9b86d72f804bca1201119db09a0449]]]DeZero is educational and prioritizes clarity. So you're sacrificing some performance. In fact, up until now, we haven't paid attention to processing speed or memory usage. In this step and the next step, we introduce measures (techniques) into DeZero that can improve performance. First, learn about memory management in Python.

[[[00000000000000000858---aacda9ec20bf278daca2173c2b207635d75344a0cf59a900ea476ac8fc32dbdb]]]Python is a programming language, but in reality it is a program that executes code called Python. That program is usually called a 'Python implementation' or 'Python interpreter'. The standard Python processing system is CPython, which is implemented in the C language. The description of Python's memory management here follows that of CPython.

[[[00000000000000000859---685e1270ade7f13938da5b2d9c3effdc1d2ed23d89aa90c41befc36df7908795]]]memory management

[[[00000000000000000860---ee1acb4aa8953d0381c4c7488100d65d66e561561e3417a0d767a7f153312281]]]Python automatically cleans up objects from memory when they are no longer needed. This welcome feature greatly reduces the number of cases where we ourselves are conscious of memory management. Objects we don't need are freed by the Python implementation (behind the scenes for us) so we can focus on the more important tasks of programming. But if you don't write proper code, you will sometimes run into problems such as memory leaks and out of memory. Neural networks, in particular, often handle large amounts of data. Therefore, if you do not manage memory properly, it is easy to run out of memory and take a long time to run (in the case of GPU, you can't run it due to lack of memory).

[[[00000000000000000861---e24feed7e1f453c7f921ca27b5af7e488b356b8f1feb2a01d25945dfb758990a]]]Let's take a quick look at how Python manages memory. Memory management in Python (more precisely CPython) is done in two ways. One is a method using reference counting, and the other is a method called generational garbage collection. Here we call the former 'reference counting' and the latter 'GC (Garbage Collection)'. First, let's talk about reference counting.

[[[00000000000000000862---050e159073c0e8586b66eadd5e023615699b4b7709096e8accd3454fc816ef40]]]Some literature refers to reference counting memory management as garbage collection (GC). Here, we explicitly call reference counting memory management 'reference counting' and do not include it in the GC.

[[[00000000000000000863---a7c8a412dc763e2ff3b4cf4e608df3d79a4d1f7194ac1cb9fca7748860270205]]]Reference counting memory management

[[[00000000000000000864---d80ad6b5e2f197387c2d0d7f9e4ed2f3f9343b4d819f5519462d4ce7edf2f778]]]The basis of memory management in Python is reference counting. The reference counting mechanism is simple (and therefore fast!). All objects are created with a reference count of 0, and are incremented by 1 when referenced by other objects. And when there are no more references to the object, it decrements the reference count by 1. Eventually, when the reference count reaches 0, it will be cleared by the Python interpreter. This reference counting ensures that objects are purged from memory as soon as they are no longer needed. This is reference counting memory management.

[[[00000000000000000865---df6a2eb3a94f2dbd85750852b46c7d443f0a674a45ac0f78855436f82c5937dd]]]By the way, the cases where the reference count increases are, for example, the following cases.

[[[00000000000000000866---d058f60d2a5667859fb1bf9c99720770ad0053d65e90b8fb2ec379aac5d37d99]]]when using the assignment operator

[[[00000000000000000867---6511bc8204048ae6629458125c193eeb0a0aaa163373556add00a8e51438dd0f]]]When passing function arguments

[[[00000000000000000868---ce083f7f1aec38af452b4fdc1b8760046b9269a672086b1669ade473299f979e]]]when added to a container type object (list, tuple, class, etc.)

[[[00000000000000000869---68bf3bb9075a796cfe990b05beb4191523e02f7e3a51f14bf1ad409e456ecb59]]]The object's reference count is incremented in the above cases. In actual code, it looks like this:

[[[00000000000000000870---7bed6969c3e926b830c0bb4a86c4ec8838d82e4d7971a22c4eab45f344b433a6]]]# reference count 1

[[[00000000000000000871---2b5d2a1153196031109aaf10f75a63bb42b18d8e71935d220cc58c74d1387e18]]]# reference count 2 in functions

[[[00000000000000000872---77df3cb2d0d64314aa6f1386d9c5ebd69fac3daf6e0058986364b59b4c120344]]]# the reference count is 1 when exiting the function

[[[00000000000000000873---a7226b3fa68d53391be6f3bcc41a8c08ae809fe9e5e8cd66e3c4a6b0a91de7cc]]]# reference count 0

[[[00000000000000000874---345ae9d2ad496cae90acdc74dc415b62a65d5e51faa54e78b8f8e66d0fb026fb]]]Here, the object†1 created by obj() is referenced by a. At this time, the reference count of the object is 1. Next, the function f(a) is called, but at this time a is passed as an argument of the function, so the reference count within the function scope is incremented by 1 (total of 2). And when you exit the scope of that function, the reference count is decremented by one. Finally, with a = None, the object has a reference count of 0 (no one can refer to it). At this point, the object is immediately freed from memory.

[[[00000000000000000875---c6a2a5c4912acb8e28acd1f76707e1037e65e36a098d0de40a6258a186cc5081]]][†1] In Python, all 'things' are objects. Both classes and functions are objects. An 'instance' created from a class is also an object. In this step, instances are called 'objects'.

[[[00000000000000000876---1a7696f5feb9b1042d5be4a6dd36b53daafd67f6f40a418be1296e5263fe1203]]]As you can see, the way reference counting works is simple. However, its simple mechanism solves many problems with memory usage. For example, let's look at the following code.

[[[00000000000000000877---d5afca80741ab01cf5b75e0a4d90fbf4b7bd3329eada6622b1e7a7a21b435e2a]]]Here we generate three objects a, b and c. Then a references b and b references c. At this time, the object relationships are as shown in the left diagram of Figure 17-1.

[[[00000000000000000878---409999486089adbe601947330f2b67f8b6ee7dfb51a72b0f3d72662d45fce7a8]]]
Figure 17-1 Object Relationship Diagram (references are dashed lines, numbers indicate reference counts)


[[[00000000000000000879---20a26ae86936e74c9246ea85ce17dee8e158eff343e415e288fcb2411f765a59]]]Then, if a = b = c = None, the object relationships change as shown on the right in Figure 17-1. Then a has a reference count of 0 (b and c have a reference count of 1). So a is immediately erased. In addition, b's reference count goes from 1 to 0 along with its erasure. This causes b to be erased in succession. In the same way, when b is deleted, the reference count of c becomes 0 and c is also deleted. By this domino-like principle, objects that are not referenced by users will be deleted en masse.

[[[00000000000000000880---2b28de43c47083819e32995933c94122495bd71c9a38b9502a7c24f641de5e54]]]The above is memory management by Python's reference counting method. This solves many memory management issues. However, there is a problem that reference counting cannot solve. It's a circular reference.

[[[00000000000000000881---63eb62a815f331d7134107d7ef615858c80f10f84860bc42a39714c576877022]]]circular reference

[[[00000000000000000882---def21f21e2609ae60670bc5cb1e9430d29314878f79912e5a7d971a3eb588eab]]]To discuss circular references, let's first look at the following code:

[[[00000000000000000883---523bdd97dc0705f69802f8402f7011c3de9d02a715b64a4d5f27faf885eeb481]]]This is almost identical to the code shown earlier, but now with the addition of a reference from c to a. At this time, the three objects refer to each other in a loop. This situation is a circular reference. The relationship between a, b, and c is shown in Figure 17-2.

[[[00000000000000000884---ef293d9c116f67026690642424e7fc19b48c741ba32980ba48ada91825774166]]]
Figure 17-2 Object relationship diagram for circular references (dashed lines indicate references)


[[[00000000000000000885---8ef0ceacc1b11ab95d5eebed371719191805c1d8f66c7aa3de01513a9f3e7ead]]]In the right image of Figure 17-2, a, b, and c all have a reference count of 1. At this time, the three objects are inaccessible to the user (i.e. they are unnecessary objects). But if you just set a = b = c = None, the reference count will not reach 0 due to circular references, and the memory will not be freed. That's where the second method comes into play. That's GC (or more precisely, 'generational garbage collection').

[[[00000000000000000886---2f04c234cdf36c94a3cf4c77570338c22efbb533f8dc15eb44f52824f48d9848]]]GC is smarter than reference counting and can determine obsolete objects. Unlike reference counting, GC is automatically called by the Python interpreter when it runs out of memory. GC can also be called explicitly. You can do that by importing the gc module and running gc.collect().

[[[00000000000000000887---1c53a1a5c59360037f475ce4812ee58dd28e45d78fc5be46916e2e8b512e9f4a]]]GC handles circular references correctly. Therefore, in normal Python programming, there is no particular need to be aware of circular references. However, postponing memory deallocation by GC causes the overall memory usage of the program to increase (compared to when there were no circular references) (see reference [10] for details). Memory is also an important resource in machine learning, especially in neural networks. Therefore, it is good practice not to create circular references in DeZero development.

[[[00000000000000000888---f41713af2707e8a6ef2509d478d54e4b402e4dbe7cc9ad1d7c7d2d958bfd84ba]]]Now that you have all the knowledge about memory management in Python. Now let's turn our attention to DeZero. Actually, the current DeZero has a circular reference. It's the 'variables' and 'functions' in Figure 17-3.

[[[00000000000000000889---3f6aee0ed7cfa835a70bd3eff97321dfe728c8764640b3cddd72da0a20100b94]]]
Figure 17-3 Circular Reference Between Variable and Function


[[[00000000000000000890---fbfacfdbc432b10299424f0eafc8b16c3cd1d246528b8eb7a7e94fe2e78b1c63]]]A Function instance references input and output Variable instances, as shown in Figure 17-3. A Variable instance then references the Function instance that created it. At this time, the Function instance and the Variable instance have a circular reference relationship. This circular reference can be resolved with weakref, a standard Python module.

[[[00000000000000000891---4e5463061a1a9012b6896dab566a910098a1b17d518fbc66f025204b67eeb6dd]]]weakref module

[[[00000000000000000892---b03786d8d846b1e7f04459780fbb156b91d5ba9e9eead81813034d2303b0d7ae]]]In Python, weak references can be created using the weakref.ref function. A weak reference is the ability to refer to another object without increasing the reference count. If you try using the weakref.ref function, it will look like this:

[[[00000000000000000893---ea5973d7a6fbd85e1ace76f530682d30d9d2382f1b7682f357d361038818deed]]]Here we target ndarray instances as objects. Reference it as a and also have a weak reference as b. At this time, printing b shows that it is a weak reference to ndarray. In fact, we write b() to access the referenced data.

[[[00000000000000000894---b56ed0c8770b674543366cb7da731d851a911b94ffdd8a95971510431732286f]]]Now let's run the above code followed by a = None. The result looks like this:

[[[00000000000000000895---a3bf50b09339730f7a21910f5e7031c606af4f8dc149f6898e573b0a90871142]]]As above, ndarray instances are erased using reference counting memory management. b has a reference, but it's a weak reference, so it doesn't affect the reference count. If you look at the output of b at this time, you will see the word dead there, indicating that the ndarray instance has been deleted.

[[[00000000000000000896---03e8709b205c74be13866ce18638af286281b247f2ea9d3155a1127a5dd8953a]]]The 'weak reference' experimental code shown here assumes that it is executed in a Python processor. If you run it in an implementation such as IPython or Jupyter notebook, the b in the code above will not be dead because those implementations keep additional references behind the user's back.

[[[00000000000000000897---26c9544f59dbd316db21dcd2f3c9a442f8cce4d0b14d7f9603142d357e567c89]]]We will incorporate this weakref mechanism into our DeZero. The code to be added specifically is the shaded area below in the Function class.

[[[00000000000000000898---94c39ee5c5def4b882fdd79c17b138cefb918aecf939d0dc19a2da37963a96c1]]]As above, where you set the instance variable self.outputs, change it to have a weak reference. The output variable of the function is then a weak reference. Also, due to this change, it is necessary to modify the part where the outputs of the Function class are referenced from other classes. At the moment I have to modify the backward method of the Variable class as follows:

[[[00000000000000000899---2d0661e8572eb50057dab747d4115daffb5e4b4e637cf1641c52ac84d06f0cb9]]]As above, change [output.grad for ...] to [output().grad for ...]. DeZero's circular reference is now gone.

[[[00000000000000000900---5f867e41fe27e63b34fd8dfee8bfaf939a8849dd5e45f7bc8e60e094a27a65cc]]]operation check

[[[00000000000000000901---239ddf32013be15c143895a9eb2df6e052060dd6a83fd06442e8a49ace44d1b6]]]Now let's run the following code in a new DeZero that doesn't have circular references.

[[[00000000000000000902---188649b06a7fa4a9d59b1d5f27fca962048609b42168f0e3a2f1c60f777e8fc2]]]# huge data

[[[00000000000000000903---187d33f4606bd2ea0dacfede0b59dc7cfae7a5b3dd2d429e03a840c8e3e4a09d]]]# do complex calculations

[[[00000000000000000904---73d78f385801990962b9c226b2a03b510463bb04608864a7d61fe9438bf247a8]]]Here we use a loop to perform repeated calculations. At this time, references are created in the loop as shown in Figure 17-4.

[[[00000000000000000905---50a8ab651f6aca6ad14c7b9044c6521d2f1eae14c97f689893dd1d9285165fcd]]]
Figure 17-4 Relationship diagram when the user is referencing x and y


[[[00000000000000000906---f04a3790cbca5a2e7b3f1d4a74333504800b185c6dffc6af421f609bf63758d5]]]And from Figure 17-4, the next calculation (the second calculation in the for statement) overwrites x and y. At this time, the previous computational graph is no longer accessible by the user. Since the reference count becomes 0, all the memory used for the calculation graph is immediately deleted at that timing. This solved DeZero's circular reference problem.

[[[00000000000000000907---130395331d8193c373dc758966fc11697027d2748c1219f670a904e2e21d10fe]]]To measure memory usage in Python, it is convenient to use an external library such as memory profiler [11]. Even with the above code, if you actually measure it, you can confirm that the amount of memory used has not increased.

[[[00000000000000000908---e10431be132081286d2165936547b23f249cee69f149e3a833605e6e7587b754]]]step 13

[[[00000000000000000909---4e847d997d8c7406fc80e17b79505b38db66e0ff91ce9e30770d750418c5540f]]]Variable-length arguments (backpropagation)

[[[00000000000000000910---1fe3eb1aa36268221b9b8ed39f7c3187c8ec4e2e6b2f79138eac2e18760b48c0]]]Due to the changes up to the previous step, the function can now have multiple inputs and outputs. There, I modified the implementation of forward propagation and confirmed that the computation was correct. Next to forward propagation is, of course, “back propagation”. In this step, we implement backpropagation.

[[[00000000000000000911---b46d2a5f324ac879ca1ae559bb262021981d860304bd170ebae04039541c51e7]]]Backpropagation of Add class that supports variable length arguments

[[[00000000000000000912---99e8e8bacca9ffea5915e302bcd2dcaafb49fa90f07ed2f26529c80578e291d4]]]To implement backpropagation, let's first look at the computational graph of addition in Figure 13-1.

[[[00000000000000000913---582cb90e32554b8bb03efd746a438ab306d30c0a151a630bbb3cf5c8a99cd1a1]]]
Figure 13-1 Forward propagation and back propagation of a computational graph that performs addition (the function that obtains the differentiation of y = x0 + x1 is represented by +')


[[[00000000000000000914---7e02de441feb76a8015ef43a35b671911f46c24a7f939989a94ee72b859308e0]]]Forward propagation of addition has two inputs and one output, as shown in Figure 13-1. During backpropagation, it is reversed, with one input and two outputs. If you check it with a formula, when , it is obtained from the differential formula.

[[[00000000000000000915---41d747d12908cf3324983b4f56c7b6b6a3a37a52509e7e8bd1329adcf0e293fd]]]The calculation (function) has two input variables. Such functions with multiple input variables are called multivariable functions. In a multivariable function, differentiation focusing on only one input variable (other variables are considered constant) is called partial differentiation. In partial differentiation, we use as the symbol for differentiation. In this case, for example, means that the other variables are considered constants and the differentiation is performed only by focusing on . In this book, we will simply use the word 'differential' even if it is a partial differential. Also, even in the case of one variable, we will use the symbol in formulas.

[[[00000000000000000916---702631b36538ab98074b2e06666f1a328d851234441943970cd377b4855b5331]]]In backpropagation of addition, the value obtained by multiplying the differential transmitted from the output side by 1 is the differential of the input variable (,). In other words, backpropagation of addition is to 'flow as it is' the derivative of the upstream. Considering the above points, the Add class can be implemented as follows.

[[[00000000000000000917---5053f35d191e99f55b7ae805f6ff7009254496044ecd54d50e00412c320d3e3e]]]As above, the backward method has one input and two outputs. Of course, the core implementation of backpropagation would need to be changed to accommodate such multiple return values. For DeZero it is done in the backward method of the Variable class.

[[[00000000000000000918---2990fc8e80c28d0c1066d8cd8a6bbd18e4ef3913258d0a0e34f90c9d3746296c]]]Modifying the Variable class

[[[00000000000000000919---65a5d42e8f7a5e041c2f1bdab389740b144bd79ec45da1bc3ce27e4aee2958e0]]]Now let's take a look at the backward methods of the Variable class. As a review, the current Variable class is implemented as follows.

[[[00000000000000000920---303be5b8dc1cb9e3cfab3a972e230659de86618fc6249c2352d1dda7de7319c5]]]# (1) Get the input/output of the function

[[[00000000000000000921---10b0a04bc996749915d396b70d7d2c5fa02921a596967723410e7cd76ef92473]]]# ② Call the backward method

[[[00000000000000000922---46cd24f75618395415d212a397e089e57bc7f0867c80424aed57953231c78432]]]What I want you to pay attention to here is the shaded part. First, the input/output variables of the function are extracted at point ① in the while loop. Then, in ②, the function's backward method is called. So far, we've limited the function to only one input and output in code ①. Modify this to accommodate multiple variables. Here is the code:

[[[00000000000000000923---38a60bf520c0ca9e24a17da0197f58d03a6df3f771175178cff1e4924aa650c5]]]I made four fixes here. First, at point ①, the derivatives of outputs, which are the output variables, are put together in a list. And in ②, we call the backpropagation of the function f. Now unpack the list by calling f.backward(*gys) and the function with an asterisk. Also, at point ③, if gxs is not a tuple, it is converted to a tuple.

[[[00000000000000000924---3be1f26100fef7c7e278edee26edf458a31a01808416dac4ee04dd0ba05a99cf]]]② and ③ in the above code are the same as the method adopted in the previous step to improve forward propagation. With ②, the argument is unpacked and passed when calling the backward method of the Add class. With ③, the backward method of the Add class can simply return its elements instead of a tuple.

[[[00000000000000000925---b7b9a0fdf29049a9480cd9ace771552131b48be8af7b6418edfe633e2f8d227a]]]At point ④ in the code above, the derivative propagated by backpropagation is set to the instance variable grad of Variable. There, each element of gxs and f.inputs has a corresponding relationship. More precisely, the value of the derivative of f.inputs[i] corresponds to gxs[i] if the i-th element exists. Therefore, we use a zip function and a for loop to set the differentiation for each pair. The above is the new backward method of the Variable class.

[[[00000000000000000926---07a266a51223ff55ffdab6626a4a6564be5a4bfd7320ec16e72742985cf8d9c5]]]Square class implementation

[[[00000000000000000927---02099fdeee625a676526122321e03211bd1416bfff77161cd79aafee817762a4]]]So far, the Variable and Function classes support variable-length input and output. And as a concrete function, we also implemented the Add class. Finally, we'll adapt the Square class we've been using so far to our new Variable and Function classes. There is only one fix for that. The code for the shaded part is as follows.

[[[00000000000000000928---62383918a04daddbb2d5638fbc9c6e0fc3f07d5e0853f4e019f433aead722737]]]# before fix x = self.input.data

[[[00000000000000000929---6b9b553e341d8c6a3edd3e9ebe0c8a1d8f30618b8ef7f42c2078b82ab31ec369]]]As you can see above, the instance variable of the Function class has changed from input (singular) to inputs (plural), so rewrite the code that extracts the input variable x from it. That's it, our new Square class is complete. Now, let's actually perform calculations using the add and square functions.

[[[00000000000000000930---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000931---13f9016e596ba1fa90e91893235353f8faaef35b1a747713f0c4f11c49ae9df1]]]Here we have calculated as above. Using DeZero, this calculation can be written as z = add(square(x), square(y)). And then just call z.backward() and the derivative will be calculated automatically!

[[[00000000000000000932---8437b09f6a76095a2336c91f830526cdd7612ba7473a3357d3f09a49f037c71c]]]With the above, we have a mechanism for automatic differentiation that supports multiple inputs and outputs. After that, if you implement the necessary functions as appropriate, it seems that you can perform more complicated calculations. But actually, there are some problems lurking in DeZero now. The next step will solve that problem.

[[[00000000000000000933---fdfcf7a4c07d7eb9d887e448b97032f3cf83514bf371d971bfcd0408cf922d7b]]]step 36

[[[00000000000000000934---ed1d3aefa27371e6c8e2f0dce815108b5c96a8429944bf45cb9c3e1bc1b8f503]]]Uses other than higher derivatives

[[[00000000000000000935---ea633c8d67936c23cbb6723f70f26a789899686ebc7454cbec32e7937d60dba9]]]So far we have used DeZero to find higher derivatives. That's essentially one thing we did. That is to create a 'connection' for the calculations performed by backpropagation. The important point here is that computational graphing of backpropagation is a new feature of DeZero. Higher derivatives are just one application of this. Now let's take a look at the uses of the new DeZero other than higher derivatives.

[[[00000000000000000936---4e2b11152fc78829ccf8d4893ac5069deec95817765d7da44823f48fd650c6de]]]The new DeZero can do more backpropagation on the calculations done in backpropagation. That feature is called double backpropagation (hereafter referred to as double backprop). Double backprop is supported by most modern deep learning frameworks.

[[[00000000000000000937---cb0e018a36ecc94c353f9a9e898d99e5918a26ecaae35e996f084418afb496a6]]]Uses of double backprop

[[[00000000000000000938---e412475ebf3d7ab0a75676bd39437a5657da973e57f6e5176533415e25bff56b]]]Now let's look at the uses of double backprop other than higher derivatives. First, consider the following problem.

[[[00000000000000000939---503fe252004291470571cf08929f9966b8fee9c0dba35d06da34244eff0c8e09]]]Q: Given the following two equations, find the derivative at (with respect to).

[[[00000000000000000940---d3d2154ce03282f27760c501745e29eb42bf4fd0adc5b317fe11bfc794b0c6f1]]]This is the differentiation problem we've seen before. What is different from before is that Eq. (36.2) includes differentiation. In other words, we need to find further derivatives for expressions that contain derivatives. This problem can also be computed by double backprop. Before explaining that, let's first calculate by hand. To do so, expand the expression as follows:

[[[00000000000000000941---80aa8e9e203b801335da3899ee75df3e06b48230bac293ca75f196deaa31aeaa]]]After expanding the expression as above, you can get the answer by substituting .

[[[00000000000000000942---121db4e6684eacea24478427248638ab425eaa65039279bebf852c4ae3780733]]]In the above formula is an expression, not a value. If you now find the value of and substitute it into , you will not get the correct result.

[[[00000000000000000943---13e3b9d329fdc25698e403b47aeaae6f22e247bcd6cd2d23a62c26fb4a77d1fb]]]Based on the above points, let's try to solve the above problem using DeZero. The code looks like this:

[[[00000000000000000944---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000945---f0c77c687c9534429b843f2942638c8593c9a13e9a4bac5090c7353f137089b9]]]The key point in this code is y.backward(create_graph=True). It uses backpropagation to find the derivative. This creates a new computational graph (in this case, a 2*x computational graph is created out of sight of the user). Then, using the computation graph created by the backpropagation, a new computation is performed, and then backpropagation is performed. This will give you the correct derivative.

[[[00000000000000000946---4e5b5d69f683cbba0d1f0a2728214aa221dba207b486b8b9bf5140a048c9ee5e]]]gx = x.grad in the above code is not just a variable (value) but a computational graph (expression). So we can do further backpropagation on the computational graph of x.grad.

[[[00000000000000000947---990880e76445bd0236419efd319e026e9a94a948ae21e758a7ed21259659c8f2]]]Problems like this--finding a derivative formula, doing a computation with that formula, and then finding the derivative--can be solved using double backprop. Such applications are also found in deep learning research. Following are some examples.

[[[00000000000000000948---3be6f16ede6885d4d55040f9e6807f19a6b80259c6838fef23300434b0377789]]]Use cases in deep learning research

[[[00000000000000000949---9dae85d8c2193b7c61102103654fa81467d7058b75ed56a244d578d07a90ad71]]]There are a number of studies using double backprop for applications related to deep learning. For example, the paper WGAN-GP[21] optimizes the formula represented in Figure 36-1.

[[[00000000000000000950---ce7363e20df4e6fae68d64c70431a03af408cea61d3edf09520325bc589225e9]]]
Figure 36-1 Optimized function of WGAN-GP (Formula is quoted from Reference [21])


[[[00000000000000000951---557d8172ec294e7cdfd2284aa80b04cafc22e1bce8524760e39b2ee44c4e0ae9]]]The interesting thing about Figure 36-1 is that the optimization formula has a gradient (the gradient is the derivative with respect to each element of the tensor). Its gradient can be found in the first round of backpropagation. Then we use the gradient to compute a function and do a second round of backpropagation to optimize the function.

[[[00000000000000000952---a1d21a527236a2b748d51e49e60c5dc57991f35346b7861b9615f0b4764ed476]]]In this way, double backprop is used even in the latest research. In addition to WGAN-GP, the double backprop function is actually used in other famous studies such as MAML[22] and TRPO[23].

[[[00000000000000000953---c56d701ae80cc07d8592c240a3671844d6636bf64da00c0b94baf5852d33b585]]]TRPO uses double backprop when multiplying a Hessian by a vector. The double backprop can be used to perform that calculation efficiently. The product of a Hessian matrix and a vector is discussed in the next column, 'Column: Supplementary Newton's method and double backprop.'

[[[00000000000000000954---cce86e558b1dffefff429bebf302b55a53a4d38e2fd9c4825855cd0c801dfc84]]]This concludes the third stage. At this stage, double backprop was made possible by reworking DeZero's backpropagation. That allowed me to find higher derivatives and implement Newton's method. From the next step, we will prepare the current DeZero for neural networks.

[[[00000000000000000955---2ea2ad743d6f1fe7e14af1ccabccab289e40255fd6cef5765b42b09afdc5cf77]]]Column: Supplementary Newton method and double backprop

[[[00000000000000000956---3db59b8abd6b4c086cbb1f209ee1eaf21c82bba1973bcb9ab60ff096d0d00d53]]]This column will supplement the third stage. First, I will explain Newton's method when the input is a vector. Next, we introduce another alternative to Newton's method. And finally, I'll show you a practical use of double backprop. In addition, this column deals with somewhat advanced content and uses a lot of formulas. If you find it difficult, skip ahead (the content of this column is not closely related to the content below).

[[[00000000000000000957---28f11631e78556f59d78988adc611d8087f13a8cf794b94b6074cedf8adb8b1d]]]Newton's method for functions of many variables

[[[00000000000000000958---1b825f30b1c45b31754e746452083b55bd2460619e7868ea96d78ab38731ea2f]]]We implemented Newton's method in the third stage. There, the minimum value was obtained using Newton's method for the formula. As you can see, there is only one input variable for this expression. Therefore, what we have done is exactly 'implemented Newton's method when the input variable is 1 variable (scalar)'.

[[[00000000000000000959---272b23f7ff68bf65a8f24592ff8a31633353efc31cc71c2d717393106e152cbc]]]Now let's look at Newton's method when the input is a multidimensional array. Here we consider the case of a function, with the input variables as vectors. Then let is a vector with and elements.

[[[00000000000000000960---67b2577a7d104f96783b20ffeb8a2ff2f1bb6dfaa2b7b7c939470aeade233b35]]]In mathematical notation in this book, if a variable is not scalar, it is shown in bold. If the variable is scalar, it is written in normal weight, such as .

[[[00000000000000000961---681a286d39be841bf9c846542b47c02f1dd3adebaabc48981861670145568851]]]So here's Newton's method for

[[[00000000000000000962---9674f4534c289a10500d6bb1612d2e61ef5c27f67aeb7a15bc442b8523260102]]]Let's start by explaining the symbols. (C.1) represents the gradient. The gradient is the derivative with respect to each element of . In fact, writing that element would look like this:

[[[00000000000000000963---55f381648021870e860ee7f119c90205ea4036754ed5263bd279005423122c1b]]]Also, is the Hessian matrix. The Hessian matrix is given by

[[[00000000000000000964---daf2bcf7bcb5b432728b8be9b272a4ad6479bbe5380b2b0bea11a59349d5354a]]]As shown in equation (C.3), the Hessian matrix is the derivative with respect to the two elements of . Since it is a combination of two elements, it is defined in the form of a matrix.

[[[00000000000000000965---1a1589c5b751930c9a3b7da71443ee5602844dc1c92828d1cc605ff33d2db9c2]]]The slope can also be written as The Hessian matrix can also be written as

[[[00000000000000000966---2f636a6af9fda507e81acafa75759ea1e40d7638ccaa115ef5d28910309e3aab]]](C.1) uses the gradient and the Hessian matrix to update Then, update in the direction of the gradient and adjust the distance traveled using the inverse of the Hessian matrix. By using the second-order differentiation information called the Hessian matrix, you can proceed more aggressively, and you can expect to reach your destination faster. Unfortunately, Newton's method is rarely used in machine learning, especially neural networks.

[[[00000000000000000967---78eabac0ef8dffab9669c618609d08e14b239144404fd48ccd700666a5c207b9]]]Problems with Newton's method

[[[00000000000000000968---3397e70f42f2a3dcd76a9a43f9c8d32e35f30dabe244a113147dc7dca6819524]]]For problems such as machine learning, Newton's method has major problems. The problem is that when the number of parameters increases, the computational complexity of the Hessian matrix of Newton's method, or more precisely, the inverse matrix of the Hessian matrix, becomes too large. Specifically, if the number of parameters is , memory space of the order of is required. Also, computing the inverse matrix of requires a computational complexity of the order of .

[[[00000000000000000969---812f5c159090cd18adc6048c8b775dd4a6d33e08f3557f0e21f04506d43bc0ec]]]For neural networks, it is common for the number of parameters to exceed one million. If 1 million parameters are to be updated by Newton's method, a Hessian matrix of size 1 million × 1 million is required. However, memory that accommodates such a huge size is not realistic.

[[[00000000000000000970---49e970470d19cc5764b86fcbf2351473c5f687ef25fe22dde6b1383b77f3a85c]]]Since Newton's method is often not a viable solution, alternative approaches have been proposed. A typical example is the quasi-Newton method. The quasi-Newton method is a general term for methods that approximate the 'inverse of the Hessian matrix' in the Newton method (there is no specific method called the quasi-Newton method).

[[[00000000000000000971---17f31090e91625f481f6a4eee0e504f42383706f91b44d2700abc90dee18fd3e]]]Several quasi-Newton methods have been proposed so far. Among them, the most famous method is L-BFGS. L-BFGS approximates the Hessian from the gradients only. It saves computational cost and memory space. In PyTorch, L-BFGS[20] is implemented and you can feel free to try it. However, in the field of deep learning, gradient-only optimizations – SGD, Momentum, Adam, etc. – are currently dominant. There are not many examples where quasi-Newton methods such as L-BFGS are used.

[[[00000000000000000972---89b446bf4b3026120785d359b111935c40d9ae3fb129374af193a54d75b334ad]]]Uses of double backprop: product of Hessian matrix and vector

[[[00000000000000000973---82ec64689a5a3e7311f8335924be93fdb93c40bbc9ea0eb18b5a2e3d8ea662db]]]Finally, a note about double backprop. One use of double backprop is to compute the 'Hessian-vector product'. As mentioned earlier, the computational cost of obtaining the Hessian matrix becomes enormous when the number of elements increases. However, if you just want the 'result' of the Hessian-vector product, you can use double backprop to get it efficiently.

[[[00000000000000000974---e932b5ebdcade37e9a45a79fd8261f7ba29bc8fa57cc064f60daa8f5c04100da]]]For example, consider and denote the Hessian by . Now suppose we want to find -- the product of a Hessian matrix and a vector. To do so, use the following formula conversion:

[[[00000000000000000975---264c7dfcc6c156fa823ecc9a092c76db6c17528be95750bfba444031a2383b7d]]]We can see that this transformation holds by writing down the elements on the right and left sides. In fact, expanding the expression only for vectors with two elements yields:

[[[00000000000000000976---6f250013b214d744cbdc55576a39d0c3383005e7d1544f204a2fd90291dfd09f]]]Here we have limited ourselves to the case of vectors with 2 elements, but this can easily be extended to the case of 2 elements. From this, we can see that equation (C.4) holds true.

[[[00000000000000000977---f8ccee6bf6a77fe956e0a291c63af5d1155e7af2bf876539b7c120a799c3da88]]]Let's look again at equation (C.4). What the right-hand side of equation (C.4) means is that we first find the product of the vector and the gradient -- the inner product of the vectors -- and then find the gradient on that result. This is computationally efficient because you don't have to create a Hessian matrix.

[[[00000000000000000978---0d267ba771fca74558494aded69b1c1164684374c2eedc3316b2a9fded09f86d]]]Now let's use DeZero to find the product of a Hessian matrix and a vector. Here is an example of calculation using a vector with two elements (here, we preemptively use the F.matmul function, which calculates the product of matrices).

[[[00000000000000000979---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000000980---cdff73efe4070ff6da3a387bfce8331f571f53755ae05808489009389da508a5]]]The above code in mathematical form corresponds to . corresponds to z = F.matmul(v, gx). Then z.backward() finds more gradients for z. This gives the product of the Hessian matrix and the vector. By the way, the output above is the correct result. This is the end of this column.

[[[00000000000000000981---6763aa62211b44a20c71505eed18421683192c2f2f37668012e8aaa008367120]]]table of contents

[[[00000000000000000982---8436e0b0ef902ebd6da9f6f0ec21e99f841c4c2565715de6ee9ef693060fba69]]]　large door

[[[00000000000000000983---c4cf23daec95a26831302f10c2c2ac50a27c1900d0b8edb5d828fb23984bdae2]]]　credit

[[[00000000000000000984---a714161224255075961929dbba0d0997d0781ec435a517f55ecf5a7aee11f0e9]]]　Foreword

[[[00000000000000000985---b97bdbacef58ace0f9fc4defd9905a71f3290e5058a1bbcd638f39e238b0fd9a]]]1st stage automatically find the derivative

[[[00000000000000000986---f1bde261529e78d9c3135c54996fb04ae119276c5970139c0204b31a4ce5dbab]]]　Step 1 Variables as boxes

[[[00000000000000000987---a3f596d8f51d20a5c0002ddd528729c2fc86b8dfd55bdcd774ba5dba75bd259e]]]　Step 2 Functions that produce variables

[[[00000000000000000988---6fc74905d9adc9ef1c6a6019e1afa4a1e2b360dc34a1b4818159d360d718c077]]]　Step 3 Concatenate Functions

[[[00000000000000000989---3d74c770d19f6e387d43786b1bb33456ae51b009f737b4ecfcc138efce92354c]]]　Step 4 Numerical differentiation

[[[00000000000000000990---1360743b6e96bff0b006484d9ede2526eeff5258acda92290b48b242adc39a43]]]　Step 5 Backpropagation Theory

[[[00000000000000000991---4726e955cdb92ee5bd7a3509165cd76db51241c74eae5c6928daf625f681abd1]]]　Step 6 Manual backpropagation

[[[00000000000000000992---356923b426c6ba27b9c31aa78c2a6baae77d81f29ae0e3961ba6eef5176a47cf]]]　Step 7 Automating backpropagation

[[[00000000000000000993---dd9e07408dfef18c362829a988cb7ee4ec5f25bd09862e2e5b0c4220878d4277]]]　Step 8 From recursion to loop

[[[00000000000000000994---13d3b8e5378208bde871ce0ec1bfcfeb02ecf1d21839977b20cfa94fe72413c3]]]　Step 9 Make your functions more useful

[[[00000000000000000995---35a4aec1f1186c1f24c54b999e4c0918a9ec818b5fa0fc90dcf116588da425ab]]]　Step 10 Test

[[[00000000000000000996---22290155e24e13b7af3c71168c6564ce1330736e1e971ace0e9545d44f9d2231]]]2nd stage Express with natural code

[[[00000000000000000997---c5aa475c7ef28ff01af6fc296daa7eb81dc533ccf663fc2a5cffebb2b39849d0]]]　Step 11 Variable Length Arguments (Forward Propagation)

[[[00000000000000000998---207c59e7ad74e4240c502eb6e3fc2da8c2d7e2149842269ca35d4b829e0c5a70]]]　Step 12 Variable Length Arguments (Improvement)

[[[00000000000000000999---08ef8ccc7254f8dbc43af29c2b508ea8022a833bc66fda277e18f3bd07d94674]]]　Step 13 Variable Length Arguments (Back Propagation Edition)

[[[00000000000000001000---089ed492250b3f484e2d7be930b753d3a507226447ea7ca2dbbf7eafdcf1fb5e]]]　Step 14 Use the same variable repeatedly

[[[00000000000000001001---c76b28b72bc3c5b39df7d3a7164a79a957f88f7cc4fa3a66328d9cc5cd6cc455]]]　Step 15 Complex Computational Graphs (Theory)

[[[00000000000000001002---5b43cbaa9035e483da904906ce099c0a0d41c90765966d5b6b2dbeada0756e58]]]　Step 16 Complex Computational Graph (Implementation)

[[[00000000000000001003---72b3aead77316b955ebfaf69e12b7b2ecae0911771f9a686ea3e3564e009c526]]]　Step 17 Memory Management and Circular References

[[[00000000000000001004---04923aed4b5446d3abbb26f1dfad979f9d5b0ac7a1a89ce271218162e3c643c8]]]　Step 18 Modes that use less memory

[[[00000000000000001005---4bca12ab9203f23780b2466489583e433d20dbd5b56306d711fe696f906f52f9]]]　Step 19 Easy to use variables

[[[00000000000000001006---05fb5e45b2600bf941755a44ce22238accd61cb9e66c7cf118afbfd8e9fd6882]]]　Step 20 Operator overloading (1)

[[[00000000000000001007---c7779e8ae766442f7157de542c2ba2fb6d319e29c0816fe4a48e7bb42015252b]]]　Step 21 Operator overloading (2)

[[[00000000000000001008---0c5f0803628fbad376ea15035f69ce6e414dead3bec9cabc6bf33c575509d01c]]]　Step 22 Operator overloading (3)

[[[00000000000000001009---4d657c174f218b9f42d74887f892407864950bb939fcec4e1dd92e782549d0bb]]]　Step 23 Put together as a package

[[[00000000000000001010---ec40917ab948e82a0687a2d7af7ca7c9023a933a2dff744b8f3832d4050743ed]]]　Step 24 Differentiate Complex Functions

[[[00000000000000001011---56f4709a932ca8cce6f5e9d2d17024d0a63ace2e9952819ce92e7d086ab90a35]]]Stage 3 Realize higher-order differentiation

[[[00000000000000001012---cf4e57d26e50f9c4ad4c791287ff4c2273eb6ee022890d4fba4a743484e311e1]]]　Step 25 Visualization of Computational Graph (1)

[[[00000000000000001013---9ebc495ea4d368f87134ed365f2e682c994e869ff83ec5fa973051531bd2b751]]]　Step 26 Visualization of Computational Graph (2)

[[[00000000000000001014---8c59450cb3122a046a227ebfd2086507e1c8640893f7d4b550cd66022f9d4314]]]　Step 27 Differentiation of Taylor Expansion

[[[00000000000000001015---a1c234a9d66190e7e58de1866cb9b0818af73ec23463e3bf7fe076cc9dc6fed6]]]　Step 28 Optimize Function

[[[00000000000000001016---26e92fe6b1c2a79ea2d495fe37c2e4685463370eb36abfae86ef02d9195056b6]]]　Step 29 Optimization using Newton's method (manual calculation)

[[[00000000000000001017---d8735535c88706a58f5473223fd5b36968c41a9d7c64f444494f95a1a1f6b683]]]　Step 30 Higher Differentiation (Preparation)

[[[00000000000000001018---c7524516ec1cc4ef49b97b583769d42caefe72882ac8281783015eda240aa48d]]]　Step 31 Higher Differentiation (Theory)

[[[00000000000000001019---38fb039d6c9eb741d2957464fd7d1eb45cca6e9ff1c5c2a0dcf19de41765fc44]]]　Step 32 Higher Differentiation (Implementation)

[[[00000000000000001020---2406c0090fb04363de7270f9f2d762de842a164e64915e88a27f96cc0733572e]]]　Step 33 Optimization using Newton's method (automatic calculation)

[[[00000000000000001021---f184ea3d709c1762810245136ba5a5d28cc26baa3a28aadad30c4551cf30b28c]]]　Step 34 Higher Derivatives of the Sin Function

[[[00000000000000001022---72d14e032414f3b93fa52e479e38f9d7930cc20d092fee7db230fe0803be308c]]]　Step 35 Computational Graphs of Higher Derivatives

[[[00000000000000001023---540da9f86ecdca116d1eac52c89874017373fd74a574261cf8dd6e238f1aab78]]]　Step 36 Uses other than higher derivatives

[[[00000000000000001024---7af4bf38605c64ee99ffd4df7997472341744028ff37b47b5328420247a53051]]]4th stage Create a neural network

[[[00000000000000001025---a516f9acf7ae845345145ecc30f2521bac47a1b3a6b8e2ba335a48767118ff95]]]　Step 37 Work with Tensors

[[[00000000000000001026---167f0c5dd3bed5d82b9dbd03409a05064c3e055dc2653d97485b70a909fc0053]]]　Step 38 Function to change the shape

[[[00000000000000001027---a5c7da6fad32c2859eb03d27a65dc583c4b2198c49673a0dd2da87e1e0d2d09f]]]　Step 39 Sum function

[[[00000000000000001028---5ee4635da79ac5fec54744d3e9eebf331f2c9ad3485d07b5785e7e8c72b13b0f]]]　Step 40 Function to broadcast

[[[00000000000000001029---fec7e3c1944ba9ea810170ea3c25a02bec48a0f98129e087af5bd4dfc7ac91cc]]]　Step 41 Matrix Product

[[[00000000000000001030---ab07ef58ec392cda169bee2abe88db1905717beabc90dda62dc661d0360f53d2]]]　Step 42 Linear Regression

[[[00000000000000001031---49762c64a2510c49d00567a144b55aefd2c1530bb7217ad66d6bee0458036c32]]]　Step 43 Neural Network

[[[00000000000000001032---9f93b4c6d6a450992709b10f9bd91ef6b5b2b272ee724fadee8c5961481786dc]]]　Step 44 Layers to organize parameters

[[[00000000000000001033---02dd7006eaa2d03e99125eba8e14b851e6f17f8c080322f57ca89b095c3cd75c]]]　Step 45 Group Layers Layers

[[[00000000000000001034---562c921df9bb9d36c9e10eff8f6ae3586a3dcd93c2c0bff26e868adb1fa6a903]]]　Step 46 Parameter update by Optimizer

[[[00000000000000001035---83149a9321be5e83024e3edabe3031b0dd6071ba1b349f17bd3cce14f61f92a1]]]　Step 47 Softmax Function and Cross Entropy Error

[[[00000000000000001036---863b7714b837c690d39eaed8a71963b1fa2e7b33394dac3d72ff7f2ae5c89e35]]]　Step 48 Multiclass Classification

[[[00000000000000001037---af8c52588fbcc82eee3d8d7b2e07a388cdcf6134427f5eb192307df03087f000]]]　Step 49 Dataset class and preprocessing

[[[00000000000000001038---8455496e0fb335f619a8541186a7b8cac4bc656ac84dc104664ce45e7183616d]]]　Step 50 DataLoader Fetching Mini-Batch

[[[00000000000000001039---410e4cf49a3d3781f7e20583442d7320079d5e3693925e4a1f5647a7b5908a5f]]]　Step 51 Learning MNIST

[[[00000000000000001040---6bd62cd6ec7945e6a74ff528aa643f86034df1fa5e570e8e54cb821dac85ec57]]]Challenge the 5th stage with DeZero

[[[00000000000000001041---415fec678e994021f598d43a4cd49ba6f4da3cbc5a7dff4268ab67616d294059]]]　Step 52 GPU Enabled

[[[00000000000000001042---2139b11fb9515af8ff858bfa663e6106b20fbaeadcc7c1fb2447d2ec10e108e5]]]　Step 53 Save and Load Models

[[[00000000000000001043---060e9010615cdfaf55466be6fac2aa4029550b46f5bf4a9f8b3d0c2abf5aecc3]]]　Step 54 Dropout and test mode

[[[00000000000000001044---c00ba37f772fc282a223ad517173fa1e026f7cb5b07a779f76a95f28c74c77cc]]]　Step 55 CNN mechanism (1)

[[[00000000000000001045---38d249d2c0b0aebb84eb01494ee0a60130b9f4cdf723e0b581900ba1ad0089b3]]]　Step 56 CNN mechanism (2)

[[[00000000000000001046---55896ae4d217f9f8b6c24c950cfc891251ed1878a34d29bbb63425ee22ef2937]]]　Step 57 conv2d and pooling functions

[[[00000000000000001047---56542aa59332110818568a3fd43338bdacfd800031b8be5c4abe2fbe25bdb920]]]　Step 58 Representative CNN (VGG16)

[[[00000000000000001048---76bb8fb030c4d8434cfd69a37da80a716c6a7fd37184b22cd3f519a26d74348a]]]　Step 59 Time series data processing by RNN

[[[00000000000000001049---a9041e9af5e4d1accb342f57be8ea706abb88d041eb3a0f88385d22af3dc8966]]]　Step 60 LSTM and Data Loader

[[[00000000000000001050---e7a05823ab910fadd90160af8176f890027a671c92c5c7fb2153ff873ab4142e]]]　Appendix A In-Place Operations (Additional to Step 14)

[[[00000000000000001051---838593cbac092ad56da5d780404fc52cecc3391ee3bda6b0b2e747bd7c58ac1b]]]　Appendix B Implementing the get_item Function (Additional to Step 47)

[[[00000000000000001052---7bdaed23fdc0ca92c52d6a9b014c32f324b0af290567f8b557d83798f95b0fec]]]　Appendix C Run with Google Colaboratory

[[[00000000000000001053---011a6f34bca1b3aaaf230e390693587d87b64453210ac759a29c1b071eec532c]]]　in conclusion

[[[00000000000000001054---7d1b2ce73e1cc85cd8553303b8d164b9926a375fcfcc2b2f9c962f6f08260cd6]]]　References

[[[00000000000000001055---78c748b750c5565a6695bfce45017c02cb814212b08090570bb3e600dcbfc48f]]]　About the author

[[[00000000000000001056---cbd795697fe2923c1bfa79339686212ecf39db44832ae3daabbbe8854954d406]]]　colophon

[[[00000000000000001057---85fe5b5d8139b443b13334a911741494486381e6bdb71380e9f5e3c7dd27a46b]]]step 52

[[[00000000000000001058---ce9761c6818596c51f41992e61b6c8196003b95be063f1d1599dcf970cb3e7f0]]]GPU-enabled

[[[00000000000000001059---d0eccfd376b19649f1038f86a281c91c3e8ff63c2e2d8679e0a14ae493a82b19]]]Calculations performed in deep learning are mostly “matrix multiplication”. Matrix multiplication consists of multiplication and addition and can be computed in parallel. And GPUs are better at such calculations than CPUs. Here, we will create a mechanism to run DeZero on the GPU.

[[[00000000000000001060---9c52ded512b454e0e55d991ad7b60ebca29e44f467772d4e8f1836d87600d79a]]]To run on GPU, NVIDIA GPU is required as hardware. Also, CuPy is required as a Python library. If your environment does not meet the requirements, you can also use Google Colaboratory to run it on a GPU in the cloud (as of February 2020, it is available for free). Google Colaboratory is explained in Appendix C, so please refer to it if you are interested.

[[[00000000000000001061---e52d5d40c9381ffa79b789b5219ec59c82cc72d27109b42ca3d2ca495ada755e]]]Installing and using CuPy

[[[00000000000000001062---f0516e63ea27f0bc59b45cc8a30ec6da673debe7199d7ae08a931fe2bc4f042b]]]CuPy is a library for parallel computing on GPUs. Installing CuPy can be done using pip as follows:

[[[00000000000000001063---b90ce1dd7eca6d7b630c09845007f1eac2626c306d1ebe9a0f9715475612c9c4]]]Now let's use CuPy. The nice thing about CuPy is that it has a common API with NumPy. Therefore, the knowledge of NumPy is also applicable to CuPy as it is. For example, using CuPy you can write code like this:

[[[00000000000000001064---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001065---6c360d9c016af7f4b1ecca5832654571c3bf0a46bcc2fb98f302d313f6b444ea]]]Here, CuPy is imported and the calculation to find the sum is performed. As you can see, pretty much the same code as NumPy—actually just using cp instead of np—does what you want. And behind the scenes the GPU is used.

[[[00000000000000001066---8ade337441d730f28c5d47329fbc2d0484249f533debdbd07abcbbf990328238]]]This way, converting NumPy code to 'GPU version' is easy. Because if you have NumPy code, just replace np(numpy) with cp(cupy) in that code.

[[[00000000000000001067---d359db98793ba2644dbf14fe5ca932197bc643a97f81f177113d428cb2850404]]]CuPy shares many APIs with NumPy, but they are not fully compatible.

[[[00000000000000001068---fbc4c244a6df76c1aa630acb690e915414e47e9351df21a513ab3beb34df47dc]]]Now let's make DeZero compatible with the GPU. What we need to do is to change the places where NumPy is used in DeZero to CuPy—more precisely, create a mechanism to switch. In doing so, there are two things you should know about CuPy. The first is how to convert multidimensional arrays in NumPy and CuPy. This can be converted to

[[[00000000000000001069---3feb56e855dec6180141e87bd607877eaab475049645d1ed8ecd4c1cb6f95c90]]]As above, convert from NumPy to CuPy using the cp.asarray function, and from CuPy to NumPy using the cp.asnumpy function.

[[[00000000000000001070---e4a349db71139f22378a435ce0df6558612ca488812515f0a1558072aebc9bee]]]The cp.asarray and cp.asnumpy functions transfer data from memory on the PC to memory on the GPU (and vice versa). This transfer process tends to be a bottleneck in deep learning calculations. Therefore, it is good practice to keep data transfers to a minimum.

[[[00000000000000001071---ed5d72d4853a370046e40c886f761b2849b9d3255278ba519dc214c9a5991312]]]The second thing to know about CuPy is a function called cp.get_array_module. This function will return the corresponding module depending on the data. Specifically, you can use it as follows.

[[[00000000000000001072---c638e775dd3965ea645456fe769563877aef553fa86754804d44297c0149bf63]]]As above, if x is a multidimensional array, either NumPy or CuPy, then xp = cp.get_array_module(x) will return the array's module. By using this function, even if you don't know whether x is NumPy or CuPy data, you can get the corresponding module. By using this, you can write code that supports both CuPy and NumPy. For example, if xp = cp.get_array_module(x) and y = xp.sin(x), both CuPy/NumPy are supported.

[[[00000000000000001073---3c4162377332e7d36d95b17c02d82cc55ca246eb04fde5d03838d8af8cc988aa]]]For CuPy, the above knowledge is enough. From now on, we will create a mechanism to switch CuPy and NumPy with DeZero.

[[[00000000000000001074---00babd998c9eb076db87786f13c408e8465e047d8c20fa649c79ec3c9b0ac5ed]]]cuda module

[[[00000000000000001075---9ee81d10613e625cc0744e97f13ed3374739bbce044d833c070ec1a2c86a4ee6]]]In DeZero, CuPy-related functions are grouped into a module (file) called dezero/cuda.py. CUDA is a development environment for GPUs provided by NVIDIA. First, here is the import part of dezero/cuda.py:

[[[00000000000000001076---a5253433edd76e3dbf5abb0cdd351eae081ddd2bfc85d2a556c2c351f543bda4]]]Here we import NumPy and CuPy. CuPy is an optional library, so even if it isn't installed it will be considered. So use a try statement to import and if you get an ImportError set gpu_enable = False. Now it works without errors even in an environment where CuPy is not installed. Next, add the following three functions to dezero/cuda.py.

[[[00000000000000001077---2e063acc654f99c6ce7ab170fd808c33f3a00b1ab97f72cc82dd230227c8041a]]]The first get_array_module(x) returns the module corresponding to the argument x. Let x be Variable or ndarray (numpy.ndarray or cupy.ndarray). It's basically a wrapper around the cp.get_array_module function, but it also handles the case where cupy is not imported. Specifically, it always returns np(numpy) when gpu_enable is False.

[[[00000000000000001078---9be044bb27496fe5a406f9b101f857fedb6b65333ca81df99f28ad4e1dcc696a]]]The remaining two functions are functions that convert to NumPy/CuPy multidimensional arrays. The function to convert to NumPy's ndarray is implemented as_numpy, and the function to convert to CuPy's ndarray is implemented as_cupy. That's all the code in dezero/cuda.py. We will use the three functions in this module (file) in other DeZero classes.

[[[00000000000000001079---2ead0b23e0986189c19abad61a2d6054ce131a108993159509317b47d07e2906]]]Additional implementation of Variable/Layer/DataLoader classes

[[[00000000000000001080---66ad6106e8685b1aaf2c7d4e2cd99b204bfcd5e531332b02703a07dd237e2586]]]Now, let's make other classes of DeZero compatible with GPU. Here, we will add code to GPU support for the three classes Variable, Layer, and DataLoader.

[[[00000000000000001081---9a0096af3417a50660a6c59982168e12f64ef64cdfb305e2b451eb5a4213d27a]]]First, in the Variable class, modify the existing __init__ and backward methods as follows.

[[[00000000000000001082---ad6045a80edd2852ada95c50f2dc87eb2cb966e89eb75ee5529853ea91e0f75f]]]Modify the __init__ method so that it can handle cases where the argument data is cupy.ndarray. For that I dynamically change the type to check as array_types = (np.ndarray, cupy.ndarray) when cupy is imported successfully.

[[[00000000000000001083---82adcd59b3def43bfc8ee071bc6aca4adc727dad6492abfeed5c9db8a1b2e3fb]]]In the backward method, we fix the automatic completion of the gradient (self.grad). There I want it to generate either a numpy or cupy multidimensional array depending on the type of the data (self.data).

[[[00000000000000001084---e4afde194a1523e5e775def9d65c5c1be98b8e862c3ac916ab7d4c799eefc2d3]]]So far, the Variable class has held a NumPy multidimensional array in the instance variable data. Here we implement a function called to_cpu to move that data to the CPU. It also implements a function called to_gpu that moves data to the GPU. Here is the code:

[[[00000000000000001085---c2c2b9e4a3555b09cacb164b778dd4080254099cccefd53cca6049df2f682abc]]]As you can see, just use the as_numpy or as_cupy functions. Now you can transfer the data of the Variable 'GPU to CPU' or 'CPU to GPU'.

[[[00000000000000001086---6df45123380ece702ee3bd019272e5e8aad6a08ba6a3e505246962fd509d8eb1]]]Next is the Layer class. A Layer class is a class that holds parameters. That parameter is a Parameter class that inherits from Variable. Here we add a function to transfer the parameters of the Layer class to the CPU or GPU. The code to add for that is:

[[[00000000000000001087---1541e736b929a65f2dc070e95975b316dec97f939b1e66c69fd19724bd853acc]]]Finally, add the following shaded code to the DataLoader class.

[[[00000000000000001088---4fc2860aa01c4a82891dbb744ff1c0397f9c34d620e93e16260fb43c87c29222]]]The DataLoader class's job is to pack datasets into mini-batches. That mini-batch is created in the __next__ method. Until now, it was created as a NumPy multidimensional array. Here, CuPy and NumPy are switched according to the instance variable gpu flag to create a multidimensional array.

[[[00000000000000001089---70458aefbde7041f326725f3e3b0518597e5fe7237ccf8b1f95e349bf6f56caa]]]With that, the three DeZero classes -- Variable, Layer, and DataLoader -- have been 'GPUified'.

[[[00000000000000001090---c65e043eb94913aa1f7c58d12560d766b8dd5af2984cc1038d81dd47a16ae709]]]Additional implementation of functions

[[[00000000000000001091---213ec1b8faf60fb30bfd028db5cd58109456d10f9d7cf765092735453bf4f07a]]]The main task remaining for GPU adaptation is to fix DeZero's functions. DeZero's concrete functions do the actual computation in the forward method. For example, the Sin class is currently implemented as follows:

[[[00000000000000001092---103d11f930d9684537be673b29fdb799b3f4791d43332b674fbf9fa94beb1c5e]]]Here, x in the argument of def forward(self, x): is assumed to be a NumPy ndarray instance. Therefore, calculations using NumPy functions such as np.sin(x) can be performed. Here, if you run on GPU, CuPy's ndarray instance is passed to the argument x. Then you need to switch to cp.sin(x) instead of np.sin(x). That is, you must use np.sin when x is NumPy and cp.sin when x is CuPy. With that in mind, the Sin class could be modified like this:

[[[00000000000000001093---cd01f3746353e23a4372d350cd68bb0da12b4c1bca9401dcab781a2d20a08667]]]As above, xp = cuda.get_array_module(x) retrieves the module corresponding to x. where xp is either cp or np. Then use that xp to do the calculation. Now the Sin class can run correctly in both CPU/GPU (NumPy/CuPy) cases.

[[[00000000000000001094---050b2f13eff9f8119227213b30396466a4f7bfc77a334d92e17f1a211f2c5e31]]]I've only shown the code for the Sin class here, but the above modifications should be made wherever applicable in dezero/functions.py. The corresponding part is the code starting with np., such as np.xxx(). In all its places, make the same modifications as above. I also do this same modification in dezero/optimizers.py and dezero/layers.py.

[[[00000000000000001095---7390e5e3c82838473bb447c71cc834d32718a7f01bf7eae6645bb1c5fbe5f728]]]Finally, modify the code for DeZero's four arithmetic operations. Specifically, modify dezero/core.py as follows:

[[[00000000000000001096---685e69dbe15bced712b2ed758d8c0e760fb3b32ede28564e35d950ed08d282e4]]]# do the same fix for sub, rsub, div and rdiv

[[[00000000000000001097---43018f4793d2de273a5c430965c740fc87c11051a55f80acfac012b2c084c051]]]First, add a new argument array_module to the as_array function. This array_module takes either numpy or cupy and converts it to ndarray with the module specified by that array_module. And use this new as_array function to modify arithmetic operations such as add function and mul function as above. By the way, the as_array function is used within the add function to enable execution of code such as x + 1 (where x is a Variable instance). Now code like x + 1 will execute correctly even if x.data is CuPy data.

[[[00000000000000001098---bd6053ca8bdffe56d281594917d9fa85668404810c0e822492bff85c86d1784c]]]Train MNIST on GPU

[[[00000000000000001099---bf6b246b00297ad2a645a7410b74974da46555a1b8109cff876fe08a1556ecf5]]]Now you can run DeZero on your GPU. Here, let's run the MNIST training code on the GPU. Here's what that code looks like:

[[[00000000000000001100---890f7251d583e81168a65e66d0b331f1925f56f6badfaf17d15b0616f1852e2b]]]This code transfers the data loader and model data to the GPU if the GPU is available. After that, the CuPy functions will be used.

[[[00000000000000001101---9a4cc1b6c98f7cb74d71606d7be63f0821bcd89604c947e33c914204e9225977]]]Let's actually run the above code in the GPU environment. As a result, the loss will continue to decrease steadily. In addition, you can see that its speed is also faster than the CPU. For reference, the result of running the above code in Google Colaboratory is shown in Figure 52-1.

[[[00000000000000001102---42eb01f6c45cd66ae2d0760b5757635e12b4d655c765aae289cb75e03b8f2b26]]]
Figure 52-1 Execution result in Google Colaboratory


[[[00000000000000001103---db435a3da78895edad71692deb29285c96eb6faee7d52c86d7455691efa3edcf]]]As shown in Figure 52-1, by using GPU, one epoch can be calculated in about 1.5 seconds. This result varies depending on the execution environment of Google Colaboratory (such as the allocated GPU). By the way, in the case of CPU, it takes about 8 seconds per epoch. Therefore, the GPU result in Figure 52-1 is about 5 times faster than the CPU. DeZero's GPU support is now complete.

[[[00000000000000001104---bb21644c2e1bfc0a2a05b13ac42c085dc01953ac15ef29b3953f2914d9da8f23]]]step 5

[[[00000000000000001105---f1e930dce7e82ca62316dd37c8cdf61bc6acc7e6a013a6fad06bbf9524b281bf]]]Theory of backpropagation

[[[00000000000000001106---47290ef092b765c57cea5b737061adb0ac5db52fc3e6ea12560c52a73e16a9d2]]]We were able to find the derivative by numerical differentiation. However, numerical differentiation has problems in terms of computational cost and accuracy. Backpropagation solves those two problems! In other words, the derivative can be obtained efficiently, and a value with a smaller error can be obtained. This step will only explain the theory without implementing backpropagation. The next step is to implement backpropagation.

[[[00000000000000001107---bd82cdebcfa93a457fb74ae3ebbd8f636dc84bdee4a181dc36b2919cd9239e04]]]chain rule

[[[00000000000000001108---640a0f6f4c898d14deb7a3351e2d1f9d21056ab0202de09b3ccaa75019b6888b]]]The key to understanding backpropagation is the chain rule. Chain means 'chain' and represents how multiple functions are connected and used. A chain rule expresses that the derivative of its linked functions (the composite function) can be 'decomposed' into the product of the derivatives of its constituent functions.

[[[00000000000000001109---913e380403df927b3a17eba253da998718d30b921b28c76b7dca7b638cdb5f81]]]Let's explain the chain rule with a concrete example. For example, let's say you have a function called And let's say this function consists of three functions: By the way, if this function is written as a computational graph, it will look like Figure 5-1.

[[[00000000000000001110---dcf269aa0e75e4945a1adb2d57b34f9999053fdb49ab0bccc9a98e95b4d7cf3a]]]
Figure 5-1 Composite Function Example


[[[00000000000000001111---2f17669282655cd90d31b99b1460f3dc4f2c89cde3b573c5e2b1b55d58adb6d8]]]At this time, the derivative of can be expressed by equation (5.1).

[[[00000000000000001112---d27a28632a3ac2be824ada8cb0b9d96005fa224145433beeb6de478852c402a4]]]As equation (5.1) shows, the derivative of is given by the product of the derivatives of each function. This means that the derivative of a composite function can be decomposed into the local derivatives of each function. This is the chain rule. Alternatively, the chain rule represented by equation (5.1) can be written with an explicit inclusion of

[[[00000000000000001113---9cc50c61316aaa537e0be0da860ffa10900040cd118f34b808422f00e89c1799]]]is the derivative with respect to ``self'' and is always 1. Therefore, it is common to omit the product of differentiation with respect to 'self' such as , but here we include it in anticipation of implementing backpropagation.

[[[00000000000000001114---bb2c83784166b330bf1fa91cd05a40457890c40290c32b1550431e6f8705c925]]]is the derivative with respect to At this time, if it changes by a small amount, it changes itself by the same amount. So the rate of change is always the same for any function.

[[[00000000000000001115---fb9d829a16dd1fe34bf530d1f4f22f927544429daff40a46ed3d8a96174fb196]]]Derivation of backpropagation

[[[00000000000000001116---de3a5b3fde8068d2bd7ea9cac49a1431ce9f341dda5b337abe1a64e3c64b39e0]]]Now let us take a closer look at equation (5.2). Equation (5.2) means that the derivative of the composite function can be decomposed into the product of the derivatives of each function. However, it doesn't say 'in what order to multiply'. Of course, you are free to decide on that point. Therefore, as shown in equation (5.3), consider calculating in order from the output to the input direction†1.

[[[00000000000000001117---53e007a146a79f425012bdad4c91448e996d4c587d87f716c31899df9107f2ab]]][†1] It is also possible to use parentheses to calculate in order from input to output. The technique based on it is called 'forward-mode automatic differentiation'. Forward mode automatic differentiation is described in the 'Column: Automatic Differentiation' section of this book.

[[[00000000000000001118---d198b2b6a460de078a99289ebbf98086f0cdbe5bcb890dad83966d261d181b85]]]As shown in equation (5.3), the derivative is calculated from the output to the input direction, that is, in the opposite direction to the normal calculation. At this time, the calculation flow of formula (5.3) is shown in Figure 5-2.

[[[00000000000000001119---103674f0609cf34eed4801cc3cbf89e14f38826aea8e9108dfa04dfc63da1881]]]
Figure 5-2 Flow of calculation in order from differentiation on the output side


[[[00000000000000001120---ba21fdbb35975c9d43f336260e43237ec9d1f433b226476b9bd05dd4b2d77f74]]]As shown in Figure 5-2, derivatives are calculated in order while multiplying from output to input. By doing so, we finally get If this calculation is represented by a 'calculation graph', it will look like Figure 5-3.

[[[00000000000000001121---c71db9f73d5bfdaa94a6fa873477441e5850b8bbeec6c05418cceab2a17e42f1]]]
Computation graph for Figure 5-3


[[[00000000000000001122---52e3f7a7b38053407c4ff744026e5ab0329d65eb0c057d54753a43ce8d3b0a5a]]]Let's take a closer look at the computational graph in Figure 5-3. First, start with and calculate the product of . where is the derivative of the function Therefore, if we express the derivative of a function as Similarly, Considering the above points, the computational graph in Figure 5-3 can be simplified as follows.

[[[00000000000000001123---c715a0d2cde40023214c677451b7bf4251f2cef5f1313d28734362ed08768673]]]
Figure 5-4 Simplified computational graph of backpropagation (multiplication of is represented simply by the node '')


[[[00000000000000001124---23fa78807b4aca24b05bd226f682c1ecef19841474992a7b623b005f650b4980]]]Let us represent the product with the derivative with a single function node, as shown in Figure 5-4. Now the differential flow is clear. Looking at Figure 5-4, we can see that the 'derivative with respect to each variable of' propagates from right to left. This is backpropagation. The important point here is that the propagated data are all 'derivatives of'. If you write it concretely, you can see that all 'derivatives of 's with respect to '' are propagated.

[[[00000000000000001125---c689666a7a624d907942e0528faed0c367d7538c5a6fd8d981d6009987c306a7]]]The reason for specifying the order of calculation from the output to the input direction as in equation (5.3) is to propagate the differentiation of . In other words, to treat as a 'key person'. If the calculation is performed in order from the input to the output direction, the input will be the 'important person'. In that case, the derivative to propagate is → → → and the derivative with respect to is propagated.

[[[00000000000000001126---9cb5867e0cf11fa7b78f7e842d57a947efcbd35262a7c6cf9a94ed9dae35102a]]]Many problems in machine learning can be formulated with a large number of parameters as inputs and a 'loss function' as the final output. The output of this loss function is (often) a single scalar value, which becomes the 'key person'. In other words, we need to find the derivative with respect to each parameter of the loss function. In such cases, we can obtain the derivative with respect to all parameters in a single propagation by propagating the derivative from the output to the input. Because of this computational efficiency, the method of backward propagation of differentiation is used.

[[[00000000000000001127---cbbca730450de156156f9d49e400b1f10ee9979beb05cd339c404ec9f03f3dea]]]represented by a computational graph

[[[00000000000000001128---d471846ed1c9e15298bd621fc8ffd16f3a99a7832c011fb66109291a53ca7266]]]Now, let's draw a graph of forward propagation (Fig. 5-1), which is a normal calculation, and a graph of back propagation (Fig. 5-4) for obtaining differentiation, side by side.

[[[00000000000000001129---aaa76f9f5d5f2dbcd612cbd5848e08b47de6bc06eab979fc44d02b9bd5eca0ea]]]
Figure 5-5 Forward propagation (upper figure) and back propagation (lower figure)


[[[00000000000000001130---39d09b842f4ac438e92fa4d73e6838b770c02f01d753e3da043d5f3cfae3c8e2]]]Looking at Figure 5-5, we can see that there is a clear correspondence between forward and backward propagation. For example, variables during forward propagation correspond to derivatives during back propagation. Similarly, and correspond to and correspond to. Also, you can see that there is correspondence between functions. For example, function backpropagation corresponds to . In this way, variables can be thought of as having 'normal values' and 'differential values', and functions can be thought of as having 'normal calculations (forward propagation)' and 'calculations for obtaining differentiation (back propagation)'. increase. That way you have a good perspective on backpropagation.

[[[00000000000000001131---558fe236146fa23373a34f766f80967fbba7eaeeff017742b635d3688472dd8a]]]Finally, let's focus on the function node in Figure 5-5. This is the derivative of the computation of , but the point to note here is that in order to compute , we need the value of . Similarly, to find , we need the values of the inputs. What this means is that backpropagation requires the data used in forwardpropagation prior to it. Therefore, the implementation of backpropagation must first perform forward propagation and remember the values of the variables into which each function was input—in the example above. Then it is possible to compute the backpropagation of each function.

[[[00000000000000001132---75d441e8993e34d4b19cff1980ba11ed407c631df9524f313bb6691062feb6b2]]]This is the explanation of backpropagation. This may look a bit complicated, but this is one of the most difficult parts of the book. There may still be some things that don't make sense, but you'll understand better by actually trying it out. The next step is to implement backpropagation and test it in action.

[[[00000000000000001133---d1abe78080fcdc108815aee62779030977acf06d66be714f05bb4e4046b852ca]]]step 23

[[[00000000000000001134---d23ca27c5e11d664554a6dfa98c0eaed121d07537540db46e1f01e3f78587f8b]]]put together as a package

[[[00000000000000001135---e678d7f8710059e985d921a00053da31018f06baf70a38239f4c119a90bc3269]]]So far, we've grouped (or 'stuffed') the contents of each step into a single file. Starting from step01.py and reaching step22.py in the previous step. Our DeZero has grown a lot. In this step, we will organize the results so far into a package so that they can be reused.

[[[00000000000000001136---c5a1c695fc657b2a55810287bfeaf13d82d4ff888169fea27d0a3ec49ff84707]]]In Python, the terms 'module', 'package' and 'library' are used. A quick way to organize those words would be:

[[[00000000000000001137---fe6dad4d2c1f23ddd1d1a6feb2829cc95bc865bfe2d5746633de5e3d692a8009]]]module

[[[00000000000000001138---0dca33b1f05506c96f86859a156f16ee30502fa85ca47a1decf2bedda2137e39]]]A module is a Python file. In particular, a Python file created on the assumption that it will be imported and used from another Python program is called a 'module'.

[[[00000000000000001139---faca15b19ede225d87be09331901998f6e30ec37d068e83428c509bc847e306b]]]package

[[[00000000000000001140---610e66a73c55f51d6bed4f11044b3a100780fffee69a53e1756b5602be64d451]]]A package is a collection of modules. To create a package, create a directory and add modules (Python files) in it.

[[[00000000000000001141---a2c566e9ccd4d17e151310b34b6782a583cd93feb0ef2f24589aacf1d855e8c2]]]Library

[[[00000000000000001142---a3bd5f70fb5ef7cfacc2c4371c74e6e21ffb650bcb45011d92689c912456b0e2]]]A library is a collection of packages. In terms of file structure, it consists of one or more directories. However, the term 'library' is sometimes used to refer to a package.

[[[00000000000000001143---d12a19b19659b3ff9208f54a97d99f893c8828a1ffb345747152286c5ce22b79]]]file organization

[[[00000000000000001144---2fa93a022b3c9349a884c7f3a6a21120d3431ce92b217f4d5898dad574b49619]]]Let's start by checking the file structure. So far we've written code in step01.py, step02.py, ... and each step file. Create a directory called dezero to use DeZero from those step files. The final file structure looks like this:

[[[00000000000000001145---ed40d2e1e605493831224ebe50febc9bb16ed7ac21a0d7ebadc19baa86a72e3e]]]Add modules to the dezero directory with the above configuration. This will create a package called dezero. This package is the framework that we build. From now on, we will mainly add code to files in this dezero directory.

[[[00000000000000001146---2c658fa181a81399dc2648496bfa8371277bfa81ac8b55f1561b6257021bd030]]]Migrate to core class

[[[00000000000000001147---2862db31d3c6613cc97f751e0ed78d39f6d23577280fb3b0b36be0a94a9d116d]]]Now let's add files to the dezero directory. The goal here is to move the code from step22.py from the previous step into dezero/core_simple.py (the core file). The file name here is core because the functions we have implemented so far can be regarded as the core functions of DeZero. Also, since it will be replaced with the final form of core.py later, I named it core_simple.py in anticipation of that.

[[[00000000000000001148---e3bcbef720835b7c7f55e13a5d7b50d8d86fc2ebb8e933a6f2fca853add6f8b2]]]Now copy the following classes from step22.py into the core file.

[[[00000000000000001149---492612b2506146e549572cd703c83c8918adb6098155babbe3e5c11433f5c707]]]Here (Function) in Add(Function) means that it inherits the Function class. As above, there are Config, Variable, and Function classes, and there are 6 functions that inherit the Function class. Next is the Python function in step22.py. Now migrate the following functions into a core file:

[[[00000000000000001150---f960db942594c3dc13022fbdd7c7f9504e99fbeab1084ab7f627a5881e8a8805]]]The first two functions above are DeZero's configuration functions, used to enable or disable backpropagation. The next as_array and as_variable are functions for converting the object given as an argument to ndarray or Variable. The rest are functions that DeZero uses. Now copy the classes and functions in step22.py as-is into the core file.

[[[00000000000000001151---036d1f85c13225d5b5b5beb2814df2a1b204eeed3bc90a3ff55f290c0bd06d2b]]]We've also implemented concrete functions for use in DeZero, such as the Exp class, the Square class, the exp function, and the square function. However, we will not include that code in the core files. We'll add them to dezero/functions.py later.

[[[00000000000000001152---21e646ebb0dd96fd94ca635c31ffdac90640c10f890f00439a63e974d47b1296]]]Now you can import dezero from an external python file like this:

[[[00000000000000001153---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001154---8bac3f95e4037d415fc0af44616cb7d1368cf91091bc24c95a03e3ba27cfaac4]]]As above, we can import the Variable class by writing from dezero.core_simple import Variable. Note that we write dezero.core_simple here. Immediately after, we introduce a mechanism that allows you to omit core_simple and write from dezero import Variable.

[[[00000000000000001155---d77d319c7b55c7b2d08cb7a007638eb04924b4a74400db4b120833aff6f95881]]]The from ... import ... syntax allows you to directly import classes, functions, etc. in modules. Also, if you write import XXX as A, you can import the module XXX with the name A. For example, import dezero.core_simple as dz will import the dezero.core_simple module as dz. In that case, to access the Variable class, write dz.Variable.

[[[00000000000000001156---4be983176b6d39d78a75c429bfc921b1457e03b4745f8f8565ff7d59b00c16b0]]]operator overloading

[[[00000000000000001157---914825fcca8ca227c857e2e63b52c0ba7844012e6f26ccb29e377b45b26e7ac6]]]Most of the code in step22.py has now been transferred. After that, we will move the operator overloading to dezero. To do this, add the following function to dezero/core_simple.py (core file):

[[[00000000000000001158---94bbaf4072d1d5455d6b56e3398467f23319507ad3d5c2f415ec541febd0b76e]]]The setup_variable function is a function for overloading the operator of Variable. Calling this function will set the operator for the Variable. So where would be a good place to call this function? A suitable file is dezero/__init__.py.

[[[00000000000000001159---6d102f458cc1e4bcb1172287f7ac085ea67f034ea76c97eff3717f222542bdde]]]The __init__.py file is the first file executed when importing a module. In our case, when we import a module from the dezero package, the code in dezero/__init__.py is called first. So I write the following code in dezero/__init__.py.

[[[00000000000000001160---95d4e169d6aaf3db18be1dcabf2861fb1d9f1508e4045a191c6c9642dd377756]]]As above, import the setup_variable function and have it call it. By doing so, users of the dezero package can always use Variable with operator overloading.

[[[00000000000000001161---ca02860542d2a0d216be7ce5150ca33813b5cacbac17bcce08702c9239886055]]]Also, at the top of __init__.py is code like from dezero.core_simple import Variable. By executing this statement, you can directly use the Variable class from the dezero package. In code it looks like this:

[[[00000000000000001162---f28fbee3bea82946978e4eb943c8cb95b57face030a7ae8d3d19067440829383]]]# user code using dezero

[[[00000000000000001163---41d05377948095b14b2ff86c701b6c95b6d766cc00fa6afe1a5128dca001c3ab]]]As above, where we used to write from dezero.core_simple import Variable, we can now write from dezero import Variable. The rest of the Functions, using_config, etc. can also be imported by the user in 'shorthand' by importing dezero/__init__.py.

[[[00000000000000001164---1a03e8948ab6713a0cee336c912a5aa32668874dcde2ea4451eee3fef9c946c3]]]Actual __init__.py file

[[[00000000000000001165---05e3bf237a44661c4398ff1f4795c42a611da606f161e45edc5c7d49b8ae963c]]]From now on in this book - between steps 23 and 32 - we will use dezero/core_simple.py as the core file for DeZero. And from step 33 onwards, I plan to use dezero/core.py instead (replacing the simple version). Therefore, in the actual dezero/__init__.py, we switch between importing core_simple.py and core.py. The code is actually written like this:

[[[00000000000000001166---3804b600aba4b2a07f0f61857827d41fc6d2d69b474e4294528a54f8c9483ddd]]]As shown above, the is_simple_core flag switches between import statements. Imports from core_simple.py when is_simple_core is True, and from core.py when is_simple_core is False.

[[[00000000000000001167---0964a48ae18473ee9258ce124e84c4909e92ac4960e32d1057c9c3853b706a2c]]]Readers should correct the is_simple_core flag accordingly and read on. As a flow, from now until step 32, switch to is_simple_core = True, and after step 33, switch to is_simple_core = False.

[[[00000000000000001168---12963028e14869ad47b86cb174b793d1f9398f5c99e442109395f528196af66a]]]import dezero

[[[00000000000000001169---1754186cb86b936c08b7b95c7e42aa91319fc719650e902c53900e9ccd1a8bfc]]]Now we have a package called dezero. Now you can write the following code from step23.py of this step.

[[[00000000000000001170---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001171---a05735ffadb360636ee320abca5633eba57cd4b15061ef72ffe7c9f0fbe5aae7]]]First, if '__file__' in globals(): checks to see if a variable named __file__ is defined as a global variable. As in python steps23.py, when executing as a python command, the variable __file__ is defined. In that case, get the path of the directory of the current file (step23.py) and add its parent directory to the module search path. That way files in the dezero directory will be imported correctly no matter where you run the Python command from. For example, from the command line you can either run python steps/step23.py or cd steps; python step23.py. In the rest of this document, the code for adding the module search path is omitted.

[[[00000000000000001172---2dfe0b66353fbc1873b62db5560385ed2983ba1ee5cbbb0dfc63ce2de92b34a7]]]The code that adds the search path is a temporary use to import the dezero directory currently under development. If DeZero was installed as a package (e.g. by pip install dezero), the DeZero package will be located wherever Python's search path passes. This eliminates the need to manually add paths like above. Also, the variable named __file__ is undefined when running in an environment such as the interactive mode of the Python interpreter or Google Colaboratory. Considering that point (to make the step file work on Google Colaboratory as it is), when adding the parent directory to the search path, I put the sentence if '__file__' in globals():.

[[[00000000000000001173---fb2f8cc75016fb75a5d58597c27765df53dde966c183f6df9f3188736a5f7985]]]Now, the code above is all the code in step23.py (no code omitted). Now we have the prototype of the framework called DeZero. From now on, we will extend the files (modules) in the dezero directory.

[[[00000000000000001174---8f30cbf54194ba35bea3691db6534d478ee3f58bdb4a5cc742ab382a7c916967]]]step 19

[[[00000000000000001175---ec4a60463b94cdb88ea2d8367100ccbd66bf85bdf74ffea46c709bbbbe1bbcb3]]]easy to use variables

[[[00000000000000001176---823808d4254abffc619a9650ea07e541a3649fe6be3f4f11ba0816fd97cef58b]]]The foundation for DeZero is already complete. Now you can create a computational graph and find the derivative automatically. Our next task is to make DeZero easier to use. Here, the first step is to make the Variable class easier to use.

[[[00000000000000001177---9336d2b8eb887175ae9a54ed6a5a039d33ad53009ab0b03c0e65f19635c71415]]]name the variable

[[[00000000000000001178---bcec089c8b713f60deada94a3aac11025a3127693b74796f6e1f28913f9efd60]]]From now on we will deal with many variables. It's nice to have something to distinguish between those large numbers of variables. So let's make it possible to set a 'name' to a variable. To do that, add an instance variable called name to the Variable class like this:

[[[00000000000000001179---c49e460b10bd7bfd64628fcb4126a1f4b1f8a8c0825dab15cd558ee7ff37eddb]]]As above, add name=None to the initialization arguments and set it to the instance variable name. Now, for example, you can name it input_x by writing x = Variable(np.array(1.0), 'input_x'). If you don't give it a name, the name will be None, an unnamed variable.

[[[00000000000000001180---92a9010dcf8a77951dd43d1fb9afc22ba37f45d45785ea71877b223fe9828e80]]]By being able to set a name for a variable, for example, when visualizing a computational graph, the name of the variable can be drawn on the graph. Visualization of the computational graph is done in steps 25 and 26.

[[[00000000000000001181---2e71c7fcd2d066bef598c689c77cad2ca5142950f40904e828011ee40614f430]]]ndarray instance variable

[[[00000000000000001182---8eafa42fdcb46a57a2ad878c7ae86404c33721a35112516ec3bd8c6229763cf5]]]Variables act as 'boxes' that contain data. However, for those who use Variable, the important thing is not the 'box' but the 'data' inside it. Therefore, we create a device that makes Variable look like data, a device that turns it into a “transparent box”.

[[[00000000000000001183---fc306925e9f72725b7d0d8a01d31e13ae4e99e98bdffbd1012945cd5aaf88396]]]As mentioned in Step 1, numerical computing and machine learning systems use multidimensional arrays (tensors) as their underlying data structure. Therefore, the Variable class is used as a dedicated box for ndarray. The goal here is to make the Variable instance look like an ndarray instance.

[[[00000000000000001184---71ab912d09cb2f99fdd8bcbafa27934da9a7bb45e7c58965749ce058d1be9d68]]]Inside the variable is an ndarray instance. This ndarray instance has some instance variables for multidimensional arrays. For example, there are instance variables such as shape shown below.

[[[00000000000000001185---e818ed6d5c7fbfdd745ffa2263396a0c59df695a1ce88d9b0042bb9fea46f0fd]]]As you can see above, the shape of the multidimensional array can be obtained by the instance variable shape. By the way, the result above is (2, 3), which corresponds to a matrix in mathematics. Now, let's extend the above operations to work on Variable instances as well. This can be done with the following implementation:

[[[00000000000000001186---cf4da6ddb9dbbe8a695f3d14d343f4a43ecc5c385699e42c45e29c18ee598991]]]As above, we implement a method called shape, in which we retrieve the actual shape of the data. The important point here is to put a single line @property before def shape(self):. The shape method can then be accessed as an instance variable. When I try to use it, it looks like this:

[[[00000000000000001187---ec5c92922cfbcf51c774e5813bc891fade032c3553ed64e50b2e2780b3b6ee97]]]# can be accessed with x.shape instead of x.shape()

[[[00000000000000001188---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001189---f3e8acc881cebef9ebe7570790961aff65f83e095ca24792c75949d1973b5586]]]As shown above, the shape of the data was obtained with an instance variable called shape. After that, you can add the ndarray instance variable to Variable using the same procedure. Here, we will add the following three new instance variables.

[[[00000000000000001190---68bc070ee8ed48c2bd79746098c5540684b62ecacad0ad1104d5f3c70c1eb6dc]]]Add three instance variables as above: ndim, size and dtype. ndim is the number of dimensions, size is the number of elements, and dtype is the data type. This completes the addition of instance variables to Variable. In addition to this, ndarray has many instance variables. Of course, it is possible to accommodate all of them. However, these tasks are tedious and will not be covered in this book. Readers, go ahead if you want.

[[[00000000000000001191---4e22d3bff0daac086e741bfad3fe69df558dad347b4f94c6839b9925d911a70b]]]So far in this book, we have proceeded without paying particular attention to the dtype, which is the data type of the ndarray instance. If you don't specify a dtype, the ndarray instance will be initialized with float64 or int64 (this may vary depending on your environment). On the other hand, neural networks often use float32.

[[[00000000000000001192---fb99662a09649f41e8652836b046113728bcefcd76410cc64521afdfe059ad74]]]len and print functions

[[[00000000000000001193---9634f9fbe606c262526517244076a7b616328c7199e0115f1f7cc12c63290e09]]]Next, extend the Variable class so that you can use Python's len function. The len function is a function for counting the number of objects, and is provided as standard in Python. An example usage is:

[[[00000000000000001194---724031b2ce8ed4159b6b436c8d2f9e450a42743dd0b92f846c566f122a86fd2b]]]As above, using the len function on a list or similar will return the number of elements it contains. For ndarray instances, it returns the number of elements in the first dimension. Here, we make this len function available to Variable as well. To do that, implement it like this:

[[[00000000000000001195---7588b0e8e0fba03110cf0d919f16e5275c67c962d7ee0959d2e7c9cce56a8e72]]]As above, if you implement a special method called __len__, you can use the len function for Variable instances as well. Now you can write code like this:

[[[00000000000000001196---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001197---cb3da1998442cefb2778e625d55fbeb74092f6c2d4a15c4d6a2012d8f26eafb5]]]In Python, methods with special meaning, such as __init__ and __len__, are named with double underscores.

[[[00000000000000001198---7da8c6ba28d321281fb37d11c1f44692f85c175f8250002848cdf994c31eb75d]]]Finally, we will add a function that allows you to easily check the contents of the Variable. It is a function to output data in Variable using the print function. An example of its use first would look like this:

[[[00000000000000001199---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001200---f2e0d32034fdacfa4de1c71003a3f84538a9aa23a65e5f06e797b216452a1715]]]As above, if you give a Variable instance to the print function, it will output the contents of the ndarray instance in it. Then, the output at this time is enclosed in the string variable(...) to tell the user that it is a Variable instance. In addition, it corresponds to the case of None and the case of outputting over multiple lines. For multiple lines, use spaces (blanks) on the second and subsequent lines to adjust the starting position of the characters to improve the appearance. To satisfy the above points, implement the __repr__ method of Variable as follows.

[[[00000000000000001201---3ea2ed3694e551021f9b2f6b04789fea36f29f1766426fee8d520662092d55ec]]]Override the __repr__ method to customize the string printed by the print function. The return value is the string you want to print. In the code above, str(self.data) converts the ndarray instance to a string. Behind the scenes, the __str__ function of the ndarray instance is called to convert the number to a string. After that, enclose the converted string in a string named variable. Also, if there is a newline code (\n), insert 9 blank characters after it. Now, when outputting over multiple lines, the starting positions of the numbers will be aligned.

[[[00000000000000001202---b422733f9648c237d83afb90fe6d66140175d4239baf1e238ef708db2cb470a7]]]That concludes part of the work of turning the Variable class into a 'transparent box'. We'll continue to do that in the next step.

[[[00000000000000001203---ea118e2b85656838fb84bf47e9eb039c7c030ae22698125108d7a775c8ee06f6]]]step 8

[[[00000000000000001204---fc399bd63fc767c25d895b2b004a652ab85675760cf6d9dad5a17c472d790bb0]]]From recursion to loop

[[[00000000000000001205---c328a367726d6ff00aec87793edfa80ba6479c62189cacb2ceb6a134b40bb01e]]]In the previous step we added a backward method to the Variable class. Here, we will change the backward method to another implementation method in anticipation of improving processing efficiency and future expansion.

[[[00000000000000001206---879a1a29966eb125b6abba782387de052a3709748f79a99136dc2b2a56fccca7]]]Current Variable class

[[[00000000000000001207---d3963d61c3abee4271e72fdfa5c81b156465edf44a33baa01ad2f4a3f5b8a43c]]]Again, we implemented the backward method of the Variable class as follows.

[[[00000000000000001208---733513c886fa6f3036b9e94f3e8d95b3909cad7b0b76ced753504b99a220c7bf]]]# omit the code

[[[00000000000000001209---77aa233ee93ba04b897bb6e5fc5f177b4de348cfb8555c73654eda3d6ca0d4dd]]]What I want to pay attention to here is that the backward method of the previous variable (to the input side) is called in the backward method. As a result, the process of 'a backward method is called within a backward method, a backward method is called by the backward method that was called, and so on' continues (the function self.creator becomes None). continues until is found). This is a recursive construct.

[[[00000000000000001210---4c52bc0a4d2212aed1ab1f455e7558daf7429d4667bead9b96b18a1a301d5994]]]In this document, some code may be omitted to save pages. In that case, I will mark the omitted part with '...' (the meaning is different from the '...' displayed at line breaks in the Python interpreter).

[[[00000000000000001211---7a74ee4dee19ef20b8dea8232c15ad2cfc379ff7b36acb2a4f11c42084dbc26d]]]Implementation using loops

[[[00000000000000001212---2c8d5ad68acc5a8700e5f47cda7bd3249fbd7597707c2ff16ad4f87a849d30c2]]]Here, the above 'implementation using recursion' is rewritten as 'implementation using loop'. Here's what that code looks like:

[[[00000000000000001213---d9ddde22d03a7d67fc4c53186f53d8f3afe12811a97e2e80abf21cec8a127350]]]# get function

[[[00000000000000001214---9e067480c243890be82a57a7ea9e043ef94d5565324458ff8281526e2e455f89]]]# get input/output of function

[[[00000000000000001215---425406f95736ab93eee89b97b803115b25b4620a81eb2394e9dee2ecab54f707]]]# call the backward method

[[[00000000000000001216---e8cda4ec94a46d402789c475f4911df48cbe34ae354765253af90c3504bde229]]]# add the previous function to the list

[[[00000000000000001217---332ffea23116dbd292c3c156d451d46247fa6866543fbf7142d065e43581e368]]]Here's an implementation using a loop. The important point is to add the functions to be processed to the funcs list in order. In the while loop, funcs.pop() takes out the function to be processed as f, and the backward method of that function f is called. At this time, the argument and return value of f.backward() are correctly set by obtaining the input/output variables of function f with f.input and f.output.

[[[00000000000000001218---b0a22e567994914dc251181e66bb029594f1a78218414848bed5555b979e98ca]]]The list's pop method removes the end of the list and retrieves its elements. For example, if funcs = [1, 2, 3] then x = funcs.pop() will pop out 3 and funcs will be [1, 2].

[[[00000000000000001219---5f867e41fe27e63b34fd8dfee8bfaf939a8849dd5e45f7bc8e60e094a27a65cc]]]operation check

[[[00000000000000001220---898f2e8ae5328722fc593229940518645f2dc2af1071ea7c4b3beb504d02e37c]]]Now, let's actually find the derivative using the Variable class above. Again, run the same code as in the previous step.

[[[00000000000000001221---5befead497495e202de214d4eaa7bc67fa13edfee96ae2a2a6a01cb0e532962e]]]# Backpropagation

[[[00000000000000001222---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001223---626c0de26bdef57dc4c66caa52704f3a8d143b42f3c1b60d6b41932556555b55]]]The result is the same as before. You have now switched the implementation method from 'recursion' to 'loop'. The benefits of this 'loop' implementation can be seen in step 15. It deals with complex computational graphs, but the current 'loop' implementation can be extended smoothly. Also, the 'loop' is slightly more efficient.

[[[00000000000000001224---3847784de4c3a6e17b3c916004710a5d00e208aad64446fd4b23b338c1eddafa]]]Recursion continues processing while leaving intermediate results in memory (piling up on the stack) each time a function is called recursively. Therefore, the loop method is generally more efficient. However, some memory usage is not a problem on modern computers. In some cases, a process called 'tail recursion' allows recursion to behave like a loop.

[[[00000000000000001225---6cd97d7901876d2eeec1e1ec58fc4b7442b76cac4df73d099241e551b99cbd4e]]]The base for implementing backpropagation is now complete. From now on, we will extend the current DeZero so that more complex calculations can be performed. In the next step, I would like to improve DeZero's 'ease of use'.

[[[00000000000000001226---59f564c49cccf82ceec9ea707d8f5a440e8749b8710c7eecaefd92dc74a21185]]]2nd stage

[[[00000000000000001227---edaf2166ae13d2c6d00d3a8d4e21794b11c19597ffaab50885d75640cf9612d7]]]expressed in natural code

[[[00000000000000001228---97a268e11e24ad7680c6529e9ad2db6039db501963a65284952c240f24168c21]]]Our DeZero has completed the first stage. Now, if you limit yourself to certain calculations, you can find the derivative automatically. For example, if a computational graph (straight line computational graph) consists of function classes such as squares and exponential functions, then the differentiation can be automatically obtained by calling the backward method.

[[[00000000000000001229---ec70d986ed0d4307577ff82cb8e41a1b6de7ffe99f4944361afce1fc3c8b76b3]]]Now let's move on to the second stage. The main purpose of this stage is to extend the current DeZero to do more complex calculations. Specifically, we will modify the foundation of DeZero so that it can handle functions that receive multiple inputs and functions that return multiple outputs. It also extends DeZero so that it can be expressed in natural code - for example, operators such as + and * can be used.

[[[00000000000000001230---509ad895a68046b18a7f155bf19831133b20fcc37b713ca0a4beb675ba1a772c]]]At the end of the second stage, DeZero is packaged as a Python package. Now we are ready to let third parties (other than us) use DeZero. Let's move on to the second stage!

[[[00000000000000001231---dba6f0d126a635bb9165e613e13503eec4ebe84a53f470a1bd360d8a7fbea171]]]System names and product names used in this document are trademarks or registered trademarks of their respective companies. Note that the ™, ®, and © marks may be omitted in the text.

[[[00000000000000001232---97edc33d5cbc5407b531f0ac3f29f91262da2bd25163647785cdb772074f3800]]]O'Reilly Japan, Inc. has made every effort to ensure the accuracy of the contents of this document, but please understand that we are not responsible for the results of operations based on the contents of this document.

[[[00000000000000001233---c65bce8e7d55534aacbcdb45e78588f484039a69d225ba54f3afc2d6c8a74be3]]]step 9

[[[00000000000000001234---ff5f9bf7bd55ad23871ee54f9e5e0054793509c95c4bafb62cb1368249ba43e2]]]Make functions more useful

[[[00000000000000001235---248d851c8033a28fe0283774909a5c997a70ef693962361276ae2c439d3aa422]]]Our DeZero can now compute with backpropagation. Furthermore, it has a feature called Define-by-Run, which creates a 'connection' of calculations at runtime. Here are three improvements to DeZero's functions to make DeZero more usable today.

[[[00000000000000001236---164e85639df7b50f5c762b6b15331df7573a3e60a7db77aa84ee30549a45c3ef]]]Use as a Python function

[[[00000000000000001237---d660454afa13ebe4abb59470c50672849ecbed29904fae511f7b9f1213303664]]]So far, we have implemented functions used in DeZero as 'Python classes'. So, for example, to do a calculation with the Square class, I had to write code like this:

[[[00000000000000001238---e3e4d9c025ddb18540a4e1874f750b01a238e72d3f2d2fa58c405c697eb2130b]]]As you can see above, doing the square calculation is a two-step process: creating an instance of the Square class and calling that instance. However, from the user's point of view, this two-step process is a bit cumbersome (you could write y = Square()(x), but that's also clumsy). A better option would be to use it as a 'Python function'. So add the following implementation:

[[[00000000000000001239---aae38821396338bd057dda4e5e4475d9157e66b4d97b814d31bf2ca0d5ebe873]]]As above, I implemented two functions: square and exp. Now you can use 'DeZero functions' as 'Python functions'. The above code can also be written in one line like this:

[[[00000000000000001240---52fa2c0112ab3b2d5a22de23a274fa1f88bf7b087ba7948d637e9865152b60c8]]]# write all in one line

[[[00000000000000001241---b42f3bf69682d3c48649477ff6ba5fb1265ec42a94c78d28243f3f9e697a6711]]]You can also write Square()(x) directly without referring to the variable name f as in f = Square(). Now let's use the two functions we implemented here.

[[[00000000000000001242---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001243---4989ab0468770a009845933aea42a3d6905c9f644423518e0ac3534e0f5cf55d]]]As you can see, if you wrap the first np.array(0.5) in a Variable, you can code it like you would do a normal numerical calculation -- just like using NumPy. Note that the above code can also apply functions consecutively. In that case you can write:

[[[00000000000000001244---3fdebe3edad34a95ff7007e702164ecd41cf756f1429418b90a27e3e1ceaa405]]]# applied consecutively

[[[00000000000000001245---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001246---17dd12d561f64f85c1f8091f26869c3b9531643e157609b4b1fe70aab05e827c]]]Now you can do calculations with more natural code. This is the first improvement.

[[[00000000000000001247---3ce3e51cb7cb974e8d76d88c37cf21f837986a930db58c4c2177a8383d6b2c4a]]]Simplification of backward method

[[[00000000000000001248---c8a579a61e8b5a4ff22337abb9e2cbc289704328b8ab2a78954bd4f7ef7c7c91]]]The second improvement is to reduce user effort in backpropagation. Specifically, omit y.grad = np.array(1.0) in the code I just wrote. Because we write y.grad = np.array(1.0) each time we do backpropagation. Add the following two lines in the backward method of Variable so that you can skip that work.

[[[00000000000000001249---58d99eab377098816eaae58ee0b7e852d3d7d997912942219aa33a4385411825]]]As above, if a variable's grad is None, it will automatically generate the derivative. Here, np.ones_like(self.data) creates an ndarray instance that has the same shape and data type as self.data and whose element is 1. If self.data is scalar, self.grad is also scalar.

[[[00000000000000001250---8288500ab145b7f994353864da95e6f4be7a53a5ce2cc37a8cd1705b5c904bd3]]]So far, I used np.array(1.0) as the derivative of the output, but in the code above I used np.ones_like(). The reason is to make the data type of Variable data and grad the same. For example, if the type of data is a 32-bit float, then the type of grad will also be a 32-bit float. By the way, if you write np.array(1.0), that data type will be a 64-bit floating point number.

[[[00000000000000001251---ee12e5ee95479ead97827e52748a729581c782fe0d5889590b0632f7d6f98ab4]]]With this, if you do some calculation, you can find the differentiation just by calling the backward method on the final output variable. Here's what it looks like when I actually try it:

[[[00000000000000001252---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001253---b124386ca65926733713c19694101e2ea3c18d37f5fe556d2ed869b64983f99a]]]handle only ndarray

[[[00000000000000001254---5c730f74db9ce713744de9da95492b5a213ce343544e4bcbb09ca1075a0812af]]]DeZero's Variable is a specification that handles only ndarray instances as data. However, it is quite conceivable that some users may mistakenly use data types such as float and int, such as Variable(1.0) and Variable(3). In anticipation of that, here we will add a trick so that Variable becomes a 'box' of ndarray instances only. Specifically, when data other than ndarray instance is put in Variable, an error will be immediately issued (However, None can be retained). By doing so, early detection of problems can be expected. Now add the following code to the initialization part of the Variable class.

[[[00000000000000001255---d225e27fc583def12f94a0b7c577b1918c35536895f3ba14ccf434b99806eb53]]]As mentioned above, if the data given as an argument is neither None nor an ndarray instance, a TypeError exception is raised. At this time, prepare the character string to be output as an error as shown above. Now you can use Variable like this:

[[[00000000000000001256---8ead1eec4e120c58439f126062490eb9f04df01cb5d2de548784d31c807c4320]]]# NG: An error occurred!

[[[00000000000000001257---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001258---283f3c92288260efc6343bc26be17b9ca1541fa7e14a0801efd2b91afdbcf537]]]As above, Variable can be generated without any problem if it is ndarray or None. But any other data type—float in the example above—throws an exception. That way you'll know right away that you're using the wrong data type.

[[[00000000000000001259---189f9ced787d7393a7652353c2110005a39977302564f62c9c3cd8134772cbf2]]]Now, with this change comes one more caveat. This is due to NumPy's unique way of doing things. To explain it, let's first look at the following NumPy code.

[[[00000000000000001260---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001261---37ca5ac00b4c14665931f54c43582993c6a084eca1b8b9efc97df6247d412d43]]]Here x is a 1-dimensional ndarray. Then the data type of y, which is the result of x**2 (square calculation), will be ndarray. This is the expected result. At issue is the following case:

[[[00000000000000001262---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001263---3a5bb8edc3b694978f86ab45e56e422583e467b27b804762d834dcdf8e60ad6f]]]where x is a 0-dimensional ndarray. Then the result of x**2 will be np.float64. This is by design of NumPy†1. In other words, if you calculate using a 0-dimensional ndarray instance, the result will be a data type other than the ndarray instance, such as numpy.float64 or numpy.float32. As a result, the variable that is the output of the DeZero function may have numpy.float64 or numpy.float32 type data. But Variable's data is a specification with only ndarray instances. To deal with this, first prepare the following function as a convenience function.

[[[00000000000000001264---e1b55e69cdf2f2442c03bb40e9ba2076a722cc838147a1414f4620619dbcb11e]]][†1] CuPy behaves in almost the same way as NumPy, but this 'behavior of 0-dimensional ndarray' is different. For CuPy, the calculation result using cupy.ndarray is always cupy.ndarray regardless of the number of dimensions.

[[[00000000000000001265---be193cfba06f3aa9646be5c0db2dba13c433c8d44eba22598082a8543330a1d8]]]Here, we use the np.isscalar function to determine scalar types such as numpy.float64 (this can also determine Python's int and float). Actually, when I use the np.isscalar function, it looks like this:

[[[00000000000000001266---330437bb5c5ff8c5387f407ab26bfc457cfde3d2ebf37b1f2ef30158653fe432]]]This way, you can determine if x is an ndarray instance with np.isscalar(x). The as_array function uses this and converts to an ndarray instance if it is not an ndarray instance. Once you have this convenient function called as_array, add the following shaded code to the Function class.

[[[00000000000000001267---2327fea6c89af800e5ea1cd8387dfb6f77240c47de2a45ef312eb9a6b4301928]]]As above, when wrapping y, which is the result of forward propagation, in Variable, we use as_array(y). By doing so, you can guarantee that the output of the output result is an ndarray instance. With this, all data will be ndarray instances even in calculations using 0-dimensional ndarray instances.

[[[00000000000000001268---ec79b389e3f034c8a1a770293be8962880e5bb8de07dd9d165a5156331a7a6cb]]]This completes the work of this step. In the next step, we'll talk about DeZero's 'testing'.

[[[00000000000000001269---a58e1b965d1d846135ad73659461d3d53569400f8a3563ddded30e7e6d04bff6]]]Deep Learning from scratch ❸

[[[00000000000000001270---afe0c7c8215b9aabe8151bdd21f33b3c45204ae3d2fb29f971e496794e5f73d8]]]Framework

[[[00000000000000001271---f18f6f9783a1c8de9a24cf9c9210b59e4d725b724e6c50464d4714e65306977a]]]Written by Yasuki Saito

[[[00000000000000001272---8e20d1d4524eac18dd0049c998d71cb63f6799bea0da7f697df3e314c6eb3e8f]]]step 2

[[[00000000000000001273---cbc9b2104ff7e40a89d7f91a2a9256b90ff5db8bc1455ce8f582bb124fa91fef]]]functions that produce variables

[[[00000000000000001274---c12fdb740ab60490c683a493a2faa325adae5eedb93ee5740b73dd848871d18c]]]In the previous step, the Variable class can now be used as a 'box'. However, as it is now, it is 'just a box'. It is necessary to have a mechanism to turn that 'ordinary box' into a 'magic box'. The key to achieving this is 'functions'. In this step we will consider functions.

[[[00000000000000001275---90ff1790f2b1f6a6104bf2dc64bd08b61dba5cc3a9ece2fa91dbb34a2d1d510a]]]What is a function

[[[00000000000000001276---4dc59b3d8212f9fa83affe81962a2189eaeaf3747f041b7fcfd006d6fdc87f61]]]What are functions? A function can be said to be 'a thing that establishes a correspondence relationship from one variable to another', if we use a slightly stiffer wording. As a concrete example, consider a function that performs a square calculation. If so, then the function determines the relationship to the variable. That is, the relation 'is squared' is determined by the function.

[[[00000000000000001277---d25d585904b18db9dcb98bd41024cbad507a78e8a2155b52b9ad03ef6d5d21f9]]]In this way, functions have the role of determining the correspondence between variables. At this time, the relationship between variables and functions can be represented visually as shown in Figure 2-1.

[[[00000000000000001278---63cc21cee7f433c550e90a268742872ce6962c42754e2a28feda35ff38ef4f6a]]]
Figure 2-1 Graph showing the relationship between variables and functions


[[[00000000000000001279---2158fd106096bd172156a80046c95a1577c2e6b4d0f1e40578124fab3386ebcb]]]Figure 2-1 visualizes the relationship between variables and functions. In this way, a diagram that represents a computation with nodes and arrows represented by circles and squares is called a “computation graph”. In this book, variables are represented by orange circles and functions by light blue squares.

[[[00000000000000001280---2ed8006faf9e8ec0221d19f18f4918a14e4a0a79713390724737348b5d45337f]]]When you hear the word “graph,” you might think of diagrams such as bar charts and pie charts. However, in computer science, a 'graph' is a data structure (and a diagram representing that structure) made up of nodes and edges.

[[[00000000000000001281---0515fbcd40309195c14aa73faa6885e9bd6183ff26dcb2add42473875251a2b4]]]Function class implementation

[[[00000000000000001282---0f92d1c570983c0bd8e75021682a2d9028c535a655ea1dfcb7bfd2256af6900c]]]Now let's consider the function represented in Figure 2-1 from a programming perspective. Specifically, we assume that variables are the Variable instances we implemented earlier, and implement functions that can process them as Function classes. Two things to note here:

[[[00000000000000001283---b7877b7ef50cb39c9a2e0fc0fca72fc51a95d97889cd466df3c6a2071a881f63]]]A method implemented in the Function class should have a Variable instance as an input and a Variable instance as an output.

[[[00000000000000001284---88171bbf01e6c6cf6c17c1016d8f9cacdab1f5b2f409e7d5575f4a908a8114d4]]]The actual data of the Variable instance must reside in the instance variable data

[[[00000000000000001285---de2e72be78287ae549cabe8c771b88cfc08348cc668d1f1489ace55ff6782c51]]]Paying attention to these two points, the Function class can be implemented as follows.

[[[00000000000000001286---89a1c1d5dd3899466194bba175170b1b8201fa1ab77ef7f81341635d36fdf6c6]]]# extract data

[[[00000000000000001287---9e322410cdb1de1ac74219c89ea2e2db96d9271bb4e0b58519397cd44e7493e1]]]# the actual calculation

[[[00000000000000001288---7a66585fec8312c34bc1624e0abae4833aef1ae2c333e7b5742e5ce7efcbd63c]]]# Return as Variable

[[[00000000000000001289---391cb86df09e628044330f19eccae3304026ba1d6f4e10bbaf26d0bcf3da7689]]]As above, here we implement the __call__ method. The __call__ method takes an input as an argument, which expects to be given a Variable instance. So the actual data resides in input.data. After retrieving the data, it performs the desired computation - in this case, the squared computation - and returns the result in a Variable 'box'.

[[[00000000000000001290---2f6cf28f6d9d958e66e03915b64fc2e2936991bce008b13830890cee79c5ddb9]]]The __call__ method is a special method in Python. With this method defined, when f = Function(), you can call the __call__ method by writing f(...).

[[[00000000000000001291---47c2ccbe11f0ef34ab4fd463991a2bd49d815d9e81f503e250a03b231d4ae2e2]]]Use Function class

[[[00000000000000001292---aa5b7779a6408b3bf15f995e4cc9c03457e0de5df085b9743608e0ca20e2495f]]]Now let's actually use the Function class. Here, let's input x of Variable instance to f of Function instance.

[[[00000000000000001293---3e16bdbafa94f5cb149a9d3d8058e2832269b853310e679498c785a4c7fb2f8d]]]# use type() to get the type of the object

[[[00000000000000001294---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001295---105dd04a9e2dc061bbe2f97573a8508d1cd18a8f1deda30c9ad71646ada72fc9]]]As mentioned above, we were able to use Variable and Function in conjunction. Looking at the execution result, we can see that the type of y is Variable and its data is stored in y.data.

[[[00000000000000001296---3ddb5c02b5c23430a527bb18254b94f931fb1e3de008295c9bfb54e4f9ea8cec]]]Now, the Function class implemented here is a specific function that 'squares the input value'. So a more specific name like Square would be more appropriate. In the future, various functions (Sin function, Exp function, etc.) will be added. With that in mind, it would be better to implement the Function class as a base class and have the functionality common to all DeZero functions. Therefore, we will modify the DeZero function so that it satisfies the following two points.

[[[00000000000000001297---4c5dfc5e42c25cf85fabe96a27309323994cb985ea4e80377e62454ffd680570]]]The Function class is a base class that implements functionality common to all functions.

[[[00000000000000001298---ea0bff5282891db02ad18ee668137969df1fb623d64cc31479b816888fb20af9]]]Concrete functions are implemented in classes that inherit the Function class

[[[00000000000000001299---29fb38b9913b0e4574027b9f20a73d0c2c1f1d6055c98c63cc6f6bba56cf0d34]]]Considering the above points, the Function class can be implemented as follows.

[[[00000000000000001300---149ca7c57ef53551985545773fb5f75dbad5e6a7bdd04b180db6df7fc13c801a]]]# Use the forward method for specific calculations

[[[00000000000000001301---84bfdffc5c5c0475d48b2ec48985a99b328d21dd769170e988ea04b55a65d556]]]Here we implement two methods: __call__ and forward. The __call__ method performs two tasks: ``extracting data from Variable'' and ``packing the calculation result into Variable''. And the concrete calculation in between is done by calling the forward method. The implementation of the forward method will be done in the inherited class.

[[[00000000000000001302---2478db1c6bbdfad0cbb662b590de4a5f52970b0ceb9933d86882e4461700ba08]]]The forward method of the Function class raises an exception. By doing so, it appeals to the person who used the forward method of the Function class (the person who used it) that the method should be inherited and implemented.

[[[00000000000000001303---3d208e50daba1b2beba8f772d2a649564bebe72ee5979fe94e012d334ef8b50e]]]Now, let's inherit this Function class and implement a class that squares the input value. Here we implement it with the class name Square as follows:

[[[00000000000000001304---6018741a607c687935d844f5b671282c216daaec7b5de60f8b87c62ac8652c81]]]The Square class inherits from the Function class, so the __call__ method is inherited. Therefore, the implementation of the Square class is complete just by writing the concrete calculations in the forward method. With this Square class, you can handle Variable like this:

[[[00000000000000001305---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001306---ef06d7c083d70ee55b735b8f7f5639b19e3114e6ac3eddc764a00fa866a60a56]]]As you can see, we got the same result as before. This completes the work of step 2. The basics of the Variable and Function classes are now complete!

[[[00000000000000001307---3e9c2daeed47464409f6bd00b278915159bbdb6a91f31ff12b654ac78f42498b]]]For the time being, the input and output of a Function will be limited to 'one variable'. From step 11, we will extend DeZero to handle multiple variables.

[[[00000000000000001308---f1346ad5781c9c4ee349e1ac33c0d22499e89abbeffaf60a56caff66fe5e8c77]]]Foreword

[[[00000000000000001309---56289fc36d138a5001e7d32b5557bea3ecd5e697585b9c957c26cf81a1981127]]]A voyage of discovery is not about looking for new landscapes.

[[[00000000000000001310---a9f250badce021162265683afccea17d0edbda552938dbe7a0e847e0e69fc7cb]]]To have new eyes.

[[[00000000000000001311---1e1eebae1b1cd45ff695dd4666e6af46c0659b3476fae3d454cd56befdb5e299]]]--Marcel Proust (French writer: 1871-1922)

[[[00000000000000001312---18273d841eb1ccaedef0c9ac743b29583b1b3fdc7e7d92884bdb68effdcc8d1a]]]Deep learning is currently revolutionizing technology in every field. Autonomous driving of cars, automatic diagnosis of diseases, high-precision machine translation, advanced robot control. There are many examples of stories that used to be depicted in the world of fiction, but recently they have become more realistic and are moving in society. Amazingly, many of these fictional technologies were only made possible (or are becoming possible) by deep learning. It can be said that we are truly living in an era where deep learning is changing the world in real time.

[[[00000000000000001313---7937169d6cd6ce031bb0127e7f474b4579acd243d4a550415a4c8f2a1a651320]]]With deep learning booming in this way, many deep learning frameworks have been created to date. PyTorch, Chainer, TensorFlow, Caffe... There are various frameworks in this field, and development is being carried out in a fierce competition every day. Thanks to them, researchers and engineers around the world can use them to efficiently solve problems. Deep learning frameworks are, so to speak, essential to underpinning and advancing state-of-the-art technology.

[[[00000000000000001314---517d87c05c5d1e36b93c3413ac1a414b1b056ea153ddcee6a2c35b1bd078c73f]]]Readers who pick up this book may have actually used a deep learning framework. Recently, a wealth of information has been prepared, and the execution environment has also been improved. Therefore, writing code for deep learning itself is very easy. It is thanks to such frameworks that advanced technologies can be realized with just a few dozen lines (or a few lines) of code.

[[[00000000000000001315---1f4e4b43663036bc1fc3c0a55f2f172396d5c7892e35ff103f29090559561683]]]So how does such a framework -- a 'real' framework that is used by many people and runs in many places -- work? What kind of technology is used and what kind of thinking is flowing at the bottom? By asking such questions, a new journey begins!

[[[00000000000000001316---36e8e1aa7785a15725a6b637a614a9d42fa34a8333fc335e55e173995b7c667b]]]Things that can be seen only by making them

[[[00000000000000001317---ba82a0780fe3731ce3fc2e36dba718c6a786c0dfca48108a26d46e918fdb8408]]]Deep learning frameworks are full of amazing technologies and interesting tricks. This book is a book for unraveling them and getting them to understand the technology correctly. And it is there to give you a taste of the technical 'interest'. For that purpose, this book will proceed with the policy of 'making from scratch'. Start from nothing, think while creating, and deepen your understanding while moving. Through such experiences, we will approach the essence of deep learning frameworks.

[[[00000000000000001318---de9f5c96cc59887bfa835685df20c902190f5540b63a8befcb34773798d5d46a]]]There will be a lot of learning in the process of creating a framework. 'I see, can such a technology be used?', 'Can such an idea be implemented in such a way?'—Such discoveries cannot be made by simply using existing tools. There are things you can understand because you make them. And there are things that can be seen precisely because we make them.

[[[00000000000000001319---6f3405a78f8376bb2bddaac2759ed9ebd3b253345a5aac1a3fe1f8f8e5be6adc]]]Let's take one example. Some readers may think that a deep learning framework is simply a collection of 'layers' and 'functions' like a library. In fact, deep learning frameworks are much bigger than that. It's a kind of programming language. More to the point, it is a programming language with differential computation capabilities (nowadays it is also called a 'differentiable programming language'). That's what you should be able to see through the 'process of making from scratch' as you read this book.

[[[00000000000000001320---18a31aa66a9b48e081e8f8f245b651b690959f42ef4b576f7c0af09a1e573fdb]]]Original framework for this book

[[[00000000000000001321---2b194b015410fbde26e85194fafe54227c1911f78b3e3442bd4ac77d18c6886b]]]Deep learning frameworks differed greatly in their early days. However, modern frameworks have reached maturity. In fact, popular frameworks such as PyTorch, Chainer, and TensorFlow are all headed in the same direction (of course, each has its own peculiarities and superficial interfaces, but their design philosophy is becoming common). In this book, we have designed a minimal framework that emphasizes the educational aspect, being conscious of the common denominator. I will call this framework 'DeZero' after the title of this book. The logo was also created as follows.

[[[00000000000000001322---6e98155511730409a76afa406d49400da7ddba668a7ada5d7f9784296506963f]]]DeZero is the original framework of this book. Its implementation is based on Chainer and also incorporates the design of PyTorch. In detail, the features are as follows.

[[[00000000000000001323---f95e1b5e3d6db36e5544f0f51988257fffd3d02d528dc450c32d2df6921b6602]]]1. Minimum

[[[00000000000000001324---7a64d9896898ee5dd8b4900ac80f0d0b58c8a2a46b48cbca4c6a1df8ec033412]]]DeZero is a framework designed primarily for simplicity. Minimal use of external libraries and minimal code inside. So you won't need a lot of time to understand the whole DeZero code.

[[[00000000000000001325---62e595b0fd31a4796187935d05d7776fccb568159faad4178a2f0aae308cdc7e]]]2. Pure Python

[[[00000000000000001326---f742eff9303670ddf7574f728be86b931ba6735387e866b655a33da5db5dcabf]]]Deep learning frameworks are often implemented using multiple languages (e.g. Python and C++). DeZero, on the other hand, is implemented entirely in Python. Therefore, those who have knowledge about Python can read the contents of DeZero without stress. And because it's pure Python, it's easy to run DeZero on your smartphone or in the cloud with services like Google Colaboratory.

[[[00000000000000001327---19bc84f96cb321dd7d7afba315d1ad8de473767474bf74e292ceb1a617e8adaa]]]3. Modern functionality

[[[00000000000000001328---298618760b491d86735a4d27db48e7ce4e4c0ad5e8970c30f600828b74aec812]]]Modern frameworks such as PyTorch, Chainer, and TensorFlow have many features in common. For example, one of the most important common denominators is Define-by-Run. Define-by-Run is a mechanism that creates a “connection” of calculations performed in deep learning at the timing of calculation (this part will be explained in detail in the main text). DeZero created in this book is a Define-by-Run style framework. And it's designed to have a lot in common with modern frameworks.

[[[00000000000000001329---a2d4d56572f27ae66800eee9b5ead80f222254b92cacb3ea1b412f3dd5fff414]]]In the previous works 'Deep Learning from Scratch' and 'Deep Learning from Scratch ❷', we implemented deep learning from scratch and learned how it works. However, I gave priority to simplicity, and set the 'connection' of the calculations 'manually'. A real framework automates that. One such approach is called Define-by-Run. In this book, you learn the mechanism by building DeZero from scratch. In addition, when reading this book, knowledge of the previous work 'Deep Learning from scratch' series is not assumed.

[[[00000000000000001330---2ff2868430efca0b72b7484d214647d0e0beaad56417bef4ee100c51fa424aa6]]]make incremental

[[[00000000000000001331---92d921e9acbc9cfc2a179916de2cd6a4cce75ca54b890414cfd0d1775ea018dc]]]DeZero is a small framework, but it's complex enough under the hood. To deal with that complexity, this book divides the process of 'making DeZero' into smaller parts. Specifically, by accumulating a total of 60 steps, we have a curriculum structure that builds up DeZero little by little.

[[[00000000000000001332---09f633aecf2149cc9782ab88ba9eccd7e5aaaf1fbef9b09539313d06fdc103bd]]]For example, the first step in this book is to create a DeZero 'variable'. The code to implement there is only 3 lines. And the next step is to add the code for the 'function'. The content of each step is complete at that point, so you can actually move it. In such a way, we will build DeZero incrementally (step by step) and deepen our understanding while moving it.

[[[00000000000000001333---0a2ba842b3ef6722fe0556f2ba220a9323b999457366f60c0f46a4ecfe3d846f]]]The experiences in this book are also good practice for software development. It's a re-experience of building a complex system from scratch, and it's the perfect material for learning software development. With that in mind, this book also devotes a page to software development practices.

[[[00000000000000001334---1f40276025faee4d39b9c66062a3917da5f8b3ef623dae681c4c52a457e696a3]]]Roadmap for this book

[[[00000000000000001335---aee51216476a3bfd268e64cc9c85994a306a63cdd27829f96b382385c6cb9969]]]As mentioned above, this book consists of 60 steps in total. The 60 steps are divided into 5 stages as shown in the figure below. Here's a quick rundown of what each stage does.

[[[00000000000000001336---bb8b039f341c9e34a6248f7289829f986e477c1d6eab7779adc6d0387aae2c04]]]The first stage creates the foundation for DeZero. Only simple problems are dealt with there, and in the shortest amount of time, we will create a mechanism for automatically obtaining the derivative ('automatically obtaining the derivative' will become clear as you read the text).

[[[00000000000000001337---4831c700a5472af0cb76e0ab4f619e9c9c8046bb4a6b4dabe66d023b1d0f855d]]]In the second stage, we extend DeZero to allow more natural code. By the end of the second stage, you should be able to write code using the usual Python constructs -- if statements, for statements, etc.

[[[00000000000000001338---713ed0fa059d5d48aa52b2dd603c92b5e828b93dd94a99b390995eb99cc69303]]]In the third stage, we extend DeZero to find the second derivative. To do this, we need to extend DeZero to do 'backpropagation of backpropagation'. By understanding how it works, you will discover new possibilities for DeZero - and for modern frameworks.

[[[00000000000000001339---4e178193444ee432f8f13ac9f8fbe2bbae55d715930107c707bff86b4a39a85a]]]In the fourth stage, we prepare DeZero for neural networks. As a result, you will be able to easily build neural networks using DeZero.

[[[00000000000000001340---bd87a33767f98df789192ad427f8bc0127074e2e1099c1e86a5ab9fdc093aaaf]]]Finally, in the fifth stage, we add features essential for deep learning, such as GPU support and model saving and restoring. We will also work on advanced models such as CNN and RNN. Although they are important as application problems of deep learning, they are not straightforward problems. However, DeZero (which is Define-by-Run) turns out to solve such problems with simple code.

[[[00000000000000001341---c0350e2114018d175664c791a669eb22bff56c6c91a6c43b19a2e764852240ae]]]DeZero, which will be completed at the end of this book, has been registered in PyPI (Python Package Index). PyPI is Python's package repository. Now you can install DeZero via pip install dezero on the command line. Of course, it is also possible to develop a reader's original framework based on DeZero created in this book and publish it to the world.

[[[00000000000000001342---34307fcfe796c91def745e44c0cd556e870260795a12bba92c75a1820b1fc6f9]]]On a journey to create DeZero

[[[00000000000000001343---510f8464bb9919a85f763ce851d31e54f2f96fb07a2d1217816b004d3d834a6a]]]That's the overview of this book. In summary, this book is for creating an original framework called DeZero from scratch. A small -- but powerful enough -- framework called DeZero in 60 steps in total. This will deepen your knowledge of modern frameworks such as PyTorch, Chainer and TensorFlow.

[[[00000000000000001344---b6b3cc4b684c7b540775db6b375c8d9904f0f3c86a27208355c28c6ea131305d]]]Finally, to reiterate, the purpose of this book is not only to create an original framework called DeZero. More importantly, the 'journey' of creating DeZero gives us 'new eyes' on modern deep learning frameworks. And with those “new eyes,” we will look broader and deeper into the field of deep learning. There should be true value in the journey of discovering that “new eye”.

[[[00000000000000001345---1eaf5fc5b482bbee087261addc1f32440b63f5eee1e15200913483e1903e1c41]]]Everything is ready. So, let's start our journey to create DeZero!

[[[00000000000000001346---0b2fc3b0c68e4b1e3dc1259a7ff3aec346cfd967d69e45cd6f856f5379615779]]]Required software

[[[00000000000000001347---ace444718be9030bba56299690d4337b9b590b44b1491aa6f5a52a4d580926d7]]]The Python versions and external libraries used in this book are:

[[[00000000000000001348---d52e1eac81f63d8c293342d29c36363a1999314e1ba7b05bf8285f2c98f0095c]]]Python 3 series

[[[00000000000000001349---facfb8689f171af2fc985281c34f86687bfb96a115ab9ee68213c60d0d5ab2c9]]]CuPy (optional)

[[[00000000000000001350---9d7fbe51eb0ec03a959a93d7eed2d3160f3d35fcc1fb0f3866f66b47974d632b]]]Pillow (optional)

[[[00000000000000001351---4a096683b8263d75c23c07c4b9e08ba3983b0811467f4a027c52192d14b2e674]]]DeZero also optionally offers the ability to run on NVIDIA GPUs. In that case, you'll need an external library called CuPy. Similarly, the image processing library Pillow is optionally required. In addition to Python, the following software is used.

[[[00000000000000001352---98abb1c0bf650b4ef7cdde1a661d290e84b2edf33f0368cc04c39256e3c9da04]]]Step 25 explains how to install Graphviz.

[[[00000000000000001353---d12a19b19659b3ff9208f54a97d99f893c8828a1ffb345747152286c5ce22b79]]]file organization

[[[00000000000000001354---233ae953a99efdfbc807581cdab7f2f8245d0c8ce1f92f1b5cb1bfc1e6817a73]]]The code used in this book is available from the following GitHub repositories:

[[[00000000000000001355---b6815672e3d60149d427f1980237a46b1a9962e18d0e86579b5d36f54e05e64c]]]The folder structure of this repository is shown in Table 1.

[[[00000000000000001356---1916adfede72bcd6157ae96291a211671d2aa2b881dac17af022b8b65db665a7]]]Table 1 Folder structure

[[[00000000000000001357---a073ec543ffd1f48dfad06dd3586661b74bac7346e5d7dc97b425b6357fda716]]]Folder name

[[[00000000000000001358---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000001359---cd0fe0c0a84beed5f0ca3f6e082b5224f7003c719bc93dd7824f47b14f401bfa]]]DeZero source code

[[[00000000000000001360---0f63ca26a4d931dc700ff6da05f5cff2b780da5458b125716cb6210967ef45bb]]]Implementation example using DeZero

[[[00000000000000001361---1295f7060c5bb3bae9c779d756dd59e5b134313c39f555ad8f6abcfaddf7c188]]]Each step file (

[[[00000000000000001362---adf0cc5d0a825432c03c6312e239c627680f1e73405682589bba506cbc57ee74]]]DeZero unit tests

[[[00000000000000001363---07d123c43d46f47eda4922ef40d759f8a07dba1e46cb8fc3f90a832503e36f20]]]The files step01.py, step02.py, ... in the steps folder correspond to the files created in each step of this book. To run those step files, run the following Python commands (you can run Python commands from any directory):

[[[00000000000000001364---705ed29b9f863d401055e3be3c18b106978653741bb8771f869e808d588d1872]]]step 43

[[[00000000000000001365---0b0739891461f74a49a5d6b095aca75ec4b1959eec0fe5da0dd51a1bacb59217]]]neural network

[[[00000000000000001366---24ce6bc44b85ac1fb82378bf7deece27eb5dd24464a1d8aa86a707151a52df30]]]In the previous step, we implemented linear regression and successfully got it working. Once you have a linear regression implementation, extending it to neural networks is straightforward. Here, we'll modify the code from the previous step to 'evolve' it into a neural network. We'll start by implementing the transformation done in the previous step as a DeZero linear function.

[[[00000000000000001367---dbf4850438873c49cd252a936e74fa76224a610f05ea4897a1229fcacc5efccc]]]DeZero's linear function

[[[00000000000000001368---154d4c71dfc8af714b5cb9e4725bf853239dee15d56e856ac5d756eb58b930ef]]]In the previous step, we implemented a linear regression on a simple dataset. The only calculations I did in that linear regression were 'matrix multiplication' and 'addition' (apart from the loss function). An excerpt from that code looks like this:

[[[00000000000000001369---83dc6ee1904d00bc944bb2edbc567fca21da67b17be9823cbe2e420696ed6046]]]As above, we do the matrix multiplication between the input x and the parameter W, and add b to it. This transformation is called a linear transformation or an affine transformation.

[[[00000000000000001370---93de48a88f2375361493bfa2d6f60b50e8812fe1dc74936dd61ddd341a0e15d6]]]A linear transformation is strictly y = F.matmul(x, W), excluding b. However, in the field of neural networks, operations including addition of b are generally called linear transformations (this book also follows that). Linear transformations also correspond to fully connected layers in neural networks. The parameter W is called weight and the parameter b is called bias.

[[[00000000000000001371---83f08f31a8717edcef11bee373642cc9e6d1e1ea5a767019244209391390e66e]]]Here, we will implement the above linear transformation as a linear function. As explained in the previous step, there are two ways to implement it. One is to use the DeZero functions that have been implemented so far, as described above. Another method is to inherit the Function class and implement a new function class called Linear. As already pointed out, the latter is more memory efficient. This can be seen by looking at Figure 43-1.

[[[00000000000000001372---f1aa4b5b099e313fb720ad42042a6f73adc447986ea329626e499537b9225ee9]]]
Figure 43-1 Two Implementations of Linear Transformation


[[[00000000000000001373---5606cc48ce8a70022d9eae418c1bdabe1311d0120a563b4ea92f9bcdce5f4099]]]The left figure in Figure 43-1 is an implementation method using DeZero's matmul function and + (add function). In that case, the output of the matmul function is recorded in the computation graph as a Variable instance. In other words, the Variable instance and the data inside (ndarray instance) will exist in memory as long as the computation graph exists.

[[[00000000000000001374---f323b7c4260730d4a01c1c246735e2bad6dd0590dc8cb4068ac991b69530ebdc]]]On the other hand, the diagram on the right in Figure 43-1 shows how to inherit the Function class and implement the Linear class. In this case, the intermediate result is not stored as a Variable instance, so the intermediate data used in the forward propagation will be deleted as soon as the forward propagation ends. Therefore, if DeZero is used by a third party, the latter implementation method should be used from the viewpoint of memory efficiency. However, there is a 'trick' that can improve memory efficiency while adopting the former implementation method. Here's the trick.

[[[00000000000000001375---fd4571595e3298c38da4f04b52124640dfd527d5c7657f797f7d8c8cd4ad0cd8]]]Now let's look at the left diagram in Figure 43-1 again. I have t in the output variable of the matmul function. This variable t is the output of the matmul function and the input of the +(add function). Now consider the backpropagation of those two functions. The first is +backpropagation, which simply passes the gradient on the output side. That is, the data in t are not needed for backpropagation of +. Also, matmul's backprop only needs the inputs x, W, and b. So matmul's backpropagation likewise doesn't need the data in t.

[[[00000000000000001376---70ca2f10db8fff9e9e9c841ee20b583cf721d2114b8242d51bf74c3e2046101f]]]From the above, we know that the data in variable t is not needed by anyone in backpropagation. In other words, we need the variable t as a computational graph because we need to flow the gradient, but we can quickly erase the data. Considering the above points, the linear_simple function can be implemented as follows.

[[[00000000000000001377---394b165ebc96576d6d06aa319972876db21417b382dab9cdfded953a76465803]]]# delete data in t

[[[00000000000000001378---2cefa292394b834ade31481acd6a8d000eed3b1065f3cef15db9a80b4c5a7f18]]]Here, the as_variable function is used so that the arguments x and W are ndarray instances. Also, the bias b can be omitted. If you set b=None, it simply computes the matrix product and returns the result.

[[[00000000000000001379---481c7435d20cc3af7095a7a335c73d315b6b9d589c54d7d2028b04ff651634a3]]]If the argument gives a bias, add it. At this time, the data of t, which is the intermediate result, is not required by anyone in the backpropagation. So it can be erased after the calculation y = t + b. So let t.data = None and clear the data in t from memory (it will have a reference count of 0 and will be cleared by the Python interpreter).

[[[00000000000000001380---1a5eb7719833c27f21a0473cf489080e6ba11b721f23aad47287815232093caf]]]In a neural network, most of the memory is occupied by tensors (ndarray instances), which are intermediate calculation results. Especially when dealing with large tensors, the size of the ndarray instance will grow. Therefore, it is desirable to delete unnecessary ndarray instances immediately. Here, we deleted unnecessary ndarray instances manually (like t.data = None), but there is also a mechanism to automate this. For example, Chainer has a mechanism called Aggressive Buffer Release[24].

[[[00000000000000001381---13a4891e50ad30b4be6fc72616708046cae24fdd6cd027e0c6b0c5f9368efc08]]]These are the 'tricks' that improve memory usage. The linear_simple function implemented here will be added to dezero/functions.py. In addition, the Linear class and linear function inheriting the Function class are also implemented in dezero/functions.py. It's a simple code, so please refer to it if you are interested.

[[[00000000000000001382---05965b4f9c4527f894b941547753fef59e30327480eb679435ef050294064adc]]]non-linear dataset

[[[00000000000000001383---f33cbbd26fb1a3bb1540b1721a0665a58b0a8a40c7927651dfa8433c72367f86]]]Now, in the previous step, we used a linear data set. Here, a more complex dataset is generated by the following code.

[[[00000000000000001384---31c483e9db789ebb7810241b3256b8eac7f76b238b8c2e09e134bb95a62d7745]]]Here we use the sin function to generate the data. Plotting this (x, y) point results in Figure 43-2.

[[[00000000000000001385---64afc0d60458a5b5c2756119fdc6bd9692b705e3ed5f47c6f91210e4a6cb0f55]]]
Figure 43-2 Dataset used in this step


[[[00000000000000001386---bd4732764ea381642f443deaf1812e1c29d40cb52ff049fd6bf3370e4511635e]]]This x and y relationship is not linear, as shown in Figure 43-2. Of course, such a nonlinear data set cannot be accommodated by linear regression. That's where neural networks come in.

[[[00000000000000001387---eb324699e22c3646f29bd513073177214da9f9c9d0d987c85ecfee0a82f6e750]]]Activation functions and neural networks

[[[00000000000000001388---a3c2cdc4c8ec02563294763ac9881a31011351fc9fdcd95dee731fc3614366f9]]]A linear transformation performs a linear transformation on the input data. On the other hand, neural networks perform nonlinear transformations on the output of linear transformations. The nonlinear transformation is called an activation function, and typical examples include ReLU and sigmoid functions.

[[[00000000000000001389---761c0ff4011c3c9c16857b8b0da7816e581c89a9560c8d5a155cff4908b9f069]]]Here we use the sigmoid function as the activation function. The sigmoid function is expressed by Equation (43.1) and its graph is shown in Figure 43-3.

[[[00000000000000001390---69f8eadc4ee26aa5e0bb65e2fd0dec7d291e53bf594f152cecac209b5bf4cb9c]]]
Figure 43-3 Graph of Sigmoid Function


[[[00000000000000001391---3a3a59e6117beae5bfb7a7fd26edbf3cd821d4e71ebb6229bd0883736d8ea517]]]The sigmoid function is a nonlinear function, as shown in Figure 43-3. This non-linear transformation is applied element-wise to the tensor. Now let's use DeZero to implement the sigmoid function. This can be implemented like this:

[[[00000000000000001392---4606209f46dfd7a224c2082d9694f8d1bd5c1137bd04e214edcc4059ce5d3c48]]]As you can see above, it can be implemented by simply coding the formula as is. Other than using DeZero's exp function as the exponential function, there shouldn't be any particular difficulties. Next, let's implement a neural network using this sigmoid_simple function.

[[[00000000000000001393---905a6e4898632328105168a35c87441a6e148bd7571c4ca008489d41911516d8]]]The sigmoid function code shown here is (again) memory-efficient. A better implementation method is to implement a Sigmoid class that inherits from the Function class. In the case of the sigmoid function, using one unit as a class makes the calculation of the gradient more efficient. The sigmoid class and sigmoid function are implemented in dezero/functions.py, so please refer to them if you are interested. Also, the derivation of the derivative of the sigmoid function is explained in '5.5.2 Sigmoid Layer' in 'Deep Learning from Scratch'.

[[[00000000000000001394---812f3b86916e5fdd7cc6f6c51241d8f3609917a16f540aa67a024403ab43a9e6]]]Neural network implementation

[[[00000000000000001395---4c4d51b935f0901674918db33d10ad7e5b2e306b518e6d0a470793727fe467d8]]]A general neural network performs transformations continuously in the form of 'linear transformation → activation function → linear transformation → activation function → linear transformation → ...'. For example, a two-layer neural network can be implemented like this (I've omitted the code to generate the parameters here):

[[[00000000000000001396---82d592bc28cde0cc692801375c9522cd4f7cb18bde695c80d508715af6cd2052]]]# or F.linear_simple(...)

[[[00000000000000001397---1de9c4fe3dc4cb539ef88c01d056f0a0a80c1a77890958cb944ed01ff1f3d7e7]]]# or F.sigmoid_simple(y)

[[[00000000000000001398---c4ed99b19999e0018ef78a786845d939379ca55fb6bbbefdd6c2e07d6c881361]]]As above, apply a 'linear transformation' and an 'activation function' in sequence. Here is the code for neural network inference (predict). Of course, it takes 'learning' to get this reasoning right. In neural network training, we add a loss function after the inference process. Then find the parameters that minimize the output of that loss function. That is neural network learning.

[[[00000000000000001399---369658e09c51fb86a2cc433b7c21cf268c10992eb0862d7ef4e738a1b849f3b4]]]In neural networks, transformations such as linear transformations and activation functions are called 'layers' or 'layers.' Also, like linear transformation, when a layer with parameters performs N consecutive transformations, it is called an 'N-layer neural network'.

[[[00000000000000001400---31a1e42be5007d8ca49dc7f77af277c156fdc6d59da296416f15f2edffa9c875]]]Now, let's try training the neural network using the actual dataset. Then I'll post the code together.

[[[00000000000000001401---e4381c42345ad5ca7e53cabd6ee7e91af8a0f019aede9388d49a6e12d3d003ee]]]# data set

[[[00000000000000001402---8c56a8edcd258fcd6e925d3894a06bd2eceda76521f8c45b3fb838b412926311]]]# ① Initialize weights

[[[00000000000000001403---fe3bc972ed11437dd29d7b0b1c3e829fde31b03118ff298bc292cf9cbab22aa8]]]# ② Neural network inference

[[[00000000000000001404---18417f0ad72992cf011bf9223b2bf5c6b7b2432ff8b3f02860db4c693db437ec]]]# ③ Neural network training

[[[00000000000000001405---dd643402f31d298af1671f2b55390bfa9e8cf25c9f65e963e199a1fa131b9599]]]# output every 1000 times

[[[00000000000000001406---8955bbc2d352c884a63dec8e4231e4b862667845c354c570923c899b7999f0d1]]]First, at point ①, the parameters are initialized. where I (=1) corresponds to the dimensionality of the input layer, H (=10) corresponds to the dimensionality of the hidden layer, and O (=1) corresponds to the dimensionality of the output layer. The I and O values should be set to 1 from our problem setup. H, on the other hand, is a hyperparameter and can be set to any integer greater than or equal to 1. We also initialize the biases with a 0 vector (np.zeros(...)) and the weights with small random values (0.01 * np.random.randn(...)).

[[[00000000000000001407---0d02527d5a83b587cb99d7328af9f4811bc34d06bb5c2a5c40424da817a3b420]]]Neural networks need to randomly set the initial weights. For the reason for this, see '6.2.1 Set the initial value of the weight to 0?' in 'Deep Learning from Scratch'.

[[[00000000000000001408---2be9e985bfde765c998d40cbc2c9303dd8a01c064c2b434af4ffe4dd617382ce]]]Next, in the point ②, the inference of the neural network is performed. Then, at point ③, update the parameters. This code in ③ is exactly the same as the code in the previous step, except for the addition of parameters.

[[[00000000000000001409---a45cfaadc7994371e27d956b29941fb6f019eb53927fc3dd8f0771d1b388b6e8]]]Running the above code will start training the neural network. And the trained neural network predicts the curve in Figure 43-4.

[[[00000000000000001410---781b9ed75be612bba3dd5e51c5b5ec5af51de678c2b34a91aa7d4314243bddb1]]]
Figure 43-4 Neural network after training


[[[00000000000000001411---af278a83b0ded3a3bf910a1f5a7b5be9f2668eb9525577ed775de92a47aca9fc]]]As shown in Figure 43-4, it gives a good representation of the curve of the sine function. Overlapping the activation function and the linear transformation for the linear regression implementation successfully learned the nonlinear relationship.

[[[00000000000000001412---9a5e2cf19ad521f855e3e1f3e8f37361cdcabba98e79d3f7d406aaf34f58eadd]]]Of course, it is also possible to implement a neural network consisting of deeper layers with this implementation policy. However, as the number of layers increases, parameter management—the work of resetting parameter gradients and updating parameters—becomes cumbersome. The next step is to create a mechanism to simplify parameter management.

[[[00000000000000001413---5cff2d25c23444f112a955ef9afc57593cc489f093621cc18c91251384aba37f]]]step 27

[[[00000000000000001414---433d6883ad4fcd46ea9b7a8ea31881670fb55f059c789e27e52e7cfff827294b]]]Differentiation of Taylor expansion

[[[00000000000000001415---1ab15d5db230579b8dd03344dc874e000563905e9aed4fd35e45eb028c82e910]]]We will now use DeZero to solve some specific problems. Now consider the derivative of the sine function. Of course, the derivative can be solved analytically. So, first of all, let's implement the sin function with DeZero. Next, let's find the derivative of the sine function using the Taylor expansion.

[[[00000000000000001416---fd2694943bac0698577365c6bea6f872cfb3692c78e8eed36ae428878f304170]]]Implementation of sin function

[[[00000000000000001417---617e3073d30dc657acafc9edde1886beabfe6494120314d745fa70faccfd980f]]]The derivative of the sin function can be solved analytically. When , its derivative is So the Sin class and the sin function can be implemented like this:

[[[00000000000000001418---84306717c77acc2924ba512dd44fd612bc61cfd3503f396ee21e56304703c715]]]As mentioned above, it can be easily implemented using the np.sin and np.cos functions provided by NumPy. Now our DeZero can also be calculated using the sine function. As a trial, if we try to find the derivative of , we get the following.

[[[00000000000000001419---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001420---721709c4cc044e9c78b1ae9b00715ec197f011d14898f4063f53489d7667abe0]]]The result is 0.7071067811865476 for both y value and x derivative. This value approximately matches 1/np.sqrt(2) (in math). Indeed, the above result turns out to be correct because .

[[[00000000000000001421---f27d42cff9bf3d21028f38e1488a428c486931d1b44270071b14fad4407ea47d]]]Theory of Taylor Expansion

[[[00000000000000001422---a56546e6a0313c4867ffa680d844a3aaa03214d7b06519a3c59ad793cbf53a90]]]Now let's get down to business. The next problem we want to consider is finding another way to find the sine function. Another way is to use the Taylor expansion. Taylor expansion is a technique for approximating arbitrary functions with polynomials. Expressed as a formula, it looks like this:

[[[00000000000000001423---758852356b77a761844459b7f81a74ec263a6e67a2243148925c70d56e2901f5]]]This is the Taylor expansion at a point. can be any value, and will be the value of at the point. Also, represents the first derivative, the second derivative, and the third derivative. And the symbol stands for 'factorial', where (factorial) is the product of all integers from to. For example:

[[[00000000000000001424---abc8836a444dcc4e768567fbc4fa046b42b73a1633ae436ba85f5f573428faf3]]]The second derivative is a further derivative of the normal derivative. In physics, the differentiation (change) of position is velocity, and the differentiation (change) of velocity is acceleration. At this time, the velocity corresponds to the first derivative, and the acceleration corresponds to the second derivative.

[[[00000000000000001425---6e7a54652115edb0fcec7ebe911b2c38ff3e88be6104502197fe2249e8337736]]]By Taylor expansion, can be expressed by Eq. (27.1) starting at a point. Equation (27.1) has an infinite number of 1st, 2nd, and 3rd derivative terms, but if we truncate them at some point, we can approximate the value of . Then, as the number of terms increases, the accuracy of the approximation also improves.

[[[00000000000000001426---6c1bdd7bcf179cda36cf1b9770acd3320173ff94a87d50ae2fcd58ea327e812c]]]The Taylor expansion when is also called the Maclaurin expansion. In fact, substituting into equation (27.1) gives:

[[[00000000000000001427---a784e19a90b592daf7ef33473f1f088f7be2a04a332b8a68bda3fec75c28bb92]]]As shown in Eq. (27.2), we have a simpler expression by restricting to . Now, let's apply it to formula (27.2). At this time, , , , , and so on. In addition, the following formula can be derived from the fact that

[[[00000000000000001428---1d5a1c952e4d1dc046447a2d09ce0237a0cf1b883e0c0f8e24a35977f5a751d9]]]As shown in Equation (27.3), the sine function is expressed as a polynomial whose terms continue infinitely. The important point here is that the accuracy of the approximation improves as . Also, since the absolute value decreases as is increased, you can use that value as a reference to determine the value of (the number of repetitions).

[[[00000000000000001429---aae845a2043d6a56111570d42dda1c27f7fafa017daa40c10a43816e0ba20c25]]]Taylor expansion implementation

[[[00000000000000001430---4cfa1579aae56b2c675218da961a88af524577c614f3d350558b97451bc9bbe5]]]Now let's implement the sin function based on equation (27.3). Here we use the math.factorial function from Python's math module to compute the factorial.

[[[00000000000000001431---d4bda20014b26b3e90235be48fec6d76cfd61124d162f99b8f84f06c12fe0b10]]]As above, in the for statement, the term to be added in the i-th time is t, and the implementation based on the expression (27.3) is performed. At this time, specify the threshold with threshold, and exit the for statement when the absolute value of t is below it. The value of this threshold controls the accuracy of the approximation (the smaller the threshold, the more accurate the approximation).

[[[00000000000000001432---5e4987c580d7f0fa28bd8927041741cd4462bb50a13963acfe06c65d4f1d402d]]]Now, let's perform a calculation using the my_sin function implemented above.

[[[00000000000000001433---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001434---662f2e23110f84c4cc7fe8e699267f12ad05a44101c4a3837040997c875fe804]]]This result is almost identical to the original sine function - the sine function we implemented at the beginning of this step. In fact, the error is so small that it can be ignored. Furthermore, this error can be further reduced by decreasing the value of threshold.

[[[00000000000000001435---e9d91ca84a61dd6f670d90262249c39976f3d59b96519df19e6a123883cf2951]]]Theoretically, the smaller the Taylor expansion threshold, the better the accuracy of the approximation. However, computer calculations do not necessarily match the theory because 'significant loss' and 'rounding errors' occur.

[[[00000000000000001436---f5364b9011a9f0c9dd3d9e81e5379a9684a4f17f98c1e8ae49e9a51e1c888937]]]Visualization of computational graphs

[[[00000000000000001437---6d1fea2046cccc8a779e873cb4c0fcbf8f5b65ebebb8ce20524122a0ad20e43f]]]Let's see what kind of computational graph is created when the above code is executed. To do this, we will use the visualization function we implemented in the previous step - the plot_dot_graph function in dezero/utils.py. First, let's visualize the computational graph of the my_sin function when threshold=0.0001. The result should look like Figure 27-1.

[[[00000000000000001438---2f3325c3ccd8571a53507a76002ddbbd3fead5a29c99b9ddb02f4f8aaf591196]]]
Figure 27-1 Computation graph of my_sin function when threshold=0.0001


[[[00000000000000001439---d5376912f44ec978cf44ace5038a18c8908c1fc92493586133d29d2878d67933]]]Figure 27-1 is a computational graph made to approximate the sine function. An interesting point here is that we control this 'computation graph complexity' by the threshold value. Let's try to visualize the computation graph with threshold=1e-150 (150 0s of 0.00...1). The result should look like Figure 27-2.

[[[00000000000000001440---1f5929cfb5baf69b90e351edd0e71913027d2134095bba402e2aae8732858b8c]]]
Figure 27-2 Computation graph of my_sin function with threshold=1e-150


[[[00000000000000001441---65c7bc6095930e3aedfdd367c9837aef42dc84241ebc2256cc41d2086d170aa8]]]By decreasing the value of threshold, the number of iterations of the for statement increases. This resulted in a 'deep' computational graph as shown in Figure 27-2. We created such a computational graph using Python's for and if statements. By using Python's control syntax, you can write code as you would any other programming. Here you can see the Define-by-Run 'ease of use' that DeZero has achieved.

[[[00000000000000001442---32fc73b5e8a21feaf62eb4d13e90fd25b6c3be6fe78467ce82453f7c3c554b65]]]step 58

[[[00000000000000001443---eaf18e2d591e20c320f51c356b413b03c4fa71619cdc5a58def17c03cc305fa5]]]Representative CNN (VGG16)

[[[00000000000000001444---7f3f0e7b8bf7accc907bbfd7c8b59388aa81f954b295ae8545fad0cfe2933c23]]]In the previous step, we have implemented the Conv2d layer and the pooling function. Here, we will use them to implement a famous model called VGG16. We will also use the learned weights to perform inference processing.

[[[00000000000000001445---a6da1cb05ee9df589564943dcdab0bcda4efc840dc096b45f697de31e197ef58]]]Implementation of VGG16

[[[00000000000000001446---5bb95c5887689dda774cafe35cd3710d50232db6af6124c02993809099d40d3e]]]VGG[36] is the second-place model in the 2014 ILSVRC competition. The paper [36] proposes some variations depending on the number of layers used in the model. Here, we implement a model called 'VGG16' among them. Its network configuration looks like Figure 58-1.

[[[00000000000000001447---77146d7a265072e4504ac3b0c45ab255471c7f79633e911148592f19c2063978]]]
Figure 58-1 Network configuration of VGG16 (activation function ReLU omitted)


[[[00000000000000001448---1c8d6427ddc0cbb363e1804fa4a811596cfdad60698d78d47e5ea9a76eba088f]]]'3x3 conv 64' in Figure 58-1 means that the kernel size is 1 and the number of channels in the output is 64. Also, 'pool/2' is a pooling and 'linear 4096' is a fully connected layer with an output size of 4096. The features of this VGG16 include the following points.

[[[00000000000000001449---ea4bc67e5a42445c41fd3652762e5794bff63150b1e1828bae1e4817c987713f]]]use a convolutional layer of (the padding is)

[[[00000000000000001450---feb2f61e40f68d2cfc82d889c633cf3b6599704261c577244e95a8682afb4f06]]]The number of channels in the convolutional layer is (basically) doubled with pooling (64 → 128 → 256 → 512)

[[[00000000000000001451---933b64cc785a2a0d205350a4b401f427c395065be479f50b88f1ff2cc9dfa10d]]]Use Dropout in Fully Connected Layers

[[[00000000000000001452---5de50fbf09777ab92e06c2e7015c4f39123e4220182088df4a86c81ed94471fd]]]Activation function uses ReLU

[[[00000000000000001453---db31f89e2b875ee58ebeae33eb11c4400f90bb53797010b6768840b5e26d7bcb]]]Now, implement VGG16 with reference to Figure 58-1. That code looks like this:

[[[00000000000000001454---7369e020da42350499dd4e7cb173453dd2b4dccffd3702abbec35b33a064d309]]]# (1) Specify only the number of output channels

[[[00000000000000001455---8c9f9a0712091c68e71046f9ab0a4220f31e99ed62a424e61c231a0ae2be6a83]]]# ②Specify output size only

[[[00000000000000001456---851366c9173e95d6d73831a1c816e1ad4f898743789cfff0692e0997f6356cab]]]# ③ Shaping

[[[00000000000000001457---8a33894d032dfb26172c03ce6e69fe7ba0d8e48ff5128d09f09b56c8d9eb23aa]]]Although the code is large, the configuration is simple. Initialization creates the necessary layers. And in the forward method, we use layers and functions to do the processing. Here, I will supplement the three points of the above code.

[[[00000000000000001458---988597f72f2a872fc5b0084bef2ae6acaecf29496370bd11713caf8543e3b78d]]]First, let's talk about point 1. There, we did not specify the number of channels of the input data to generate the convolutional layers. The number of input data channels is obtained from the data flowing during forward propagation, and the weight parameters are initialized at that timing. Similarly, L.Linear(4096) in (2) specifies only the output size. The input size is automatically determined by the data that actually flows, so you can simply specify the output size.

[[[00000000000000001459---f2cf6cbe1b5106a0a49e54d34ae48bf015bb35add05de57c35063aaaeb003524]]]Next, at point ③, we shape the data in order to switch from the convolutional layer to the fully connected layer. Convolutional layers deal with 4th order tensors, while fully connected layers deal with 2nd order tensors. Therefore, before feeding the data to the fully connected layer, we use the reshape function to shape it into a 2nd order tensor. The above is the implementation of the VGG16 class.

[[[00000000000000001460---2ba8b3aa1bcb6fccde473f4dd643cc69d55f29597a0063adb66816de2d4bd192]]]learned weight data

[[[00000000000000001461---8a3bc5e556ad3e8e573065112e6375d2d70ddf6754c7027cc4e469c2daae0580]]]VGG16 is trained using a huge dataset called ImageNet. And the learned weight data is open to the public. Here, we will add a function to read the learned weight data to the VGG16 class implemented earlier.

[[[00000000000000001462---f09b56fba51196d353fc0cb867c5cfa92d1ae24ad69daf9c18df88cdf91c8c61]]]The VGG16 model is available at http://www.robots.ox.ac.uk/~vgg/research/very_deep/ under the Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/) It is open to the public. Also, a weight file with minor modifications that allows DeZero to read the original weight data is available at https://github.com/koki0702/dezero-models.

[[[00000000000000001463---950c34ee785ee7c8afb235e11a9824cfbf21fb01d88511b0b654952fc971329a]]]So here are the additions to the VGG16 class above.

[[[00000000000000001464---cfd3f873fd26b596dba2e449335c33bc91d2a2cd3b302b0e6ee15ca821930db0]]]As above, in the VGG16 class initialization add pretrained=False to the arguments. If this is True, download the weights file (converted weights file for DeZero) from the specified location and load the weights file. Note that reading the weight file is a function added in step 53.

[[[00000000000000001465---b3b264208e9a930aa9cd2aa833f85d51e4d9662d283f363851f86fedd09102f8]]]dezero/utils.py has a get_file function. This function downloads a file from the specified URL. And it will return the absolute path (on your PC) of the downloaded file. It also returns the absolute path of the already downloaded file if it is in the cache directory. DeZero's cache directory is ~/.dezero.

[[[00000000000000001466---b2d6ef40e9c3a6e222ae7362d395e525dd1ee53abb8252b893e2ea38c6ac2448]]]The above is the implementation of the VGG16 class. Add this VGG16 class to dezero/models.py. Now you can use the pretrained VGG16 like this:

[[[00000000000000001467---023d2831f9e10271ca31df04ede8f6c5e3ef9b334536ed8d52e94fd07f671eae]]]# dummy data

[[[00000000000000001468---b4267e8dbcf88af42d49d9f1f82eff14287520f2be8ebb6f22aaae27505337c5]]]Also, the above code visualizes the VGG16 computational graph. The result looks like Figure 58-2.

[[[00000000000000001469---2c35a076bce9a60c2fcd0dc978e1dfb8a0085c8ca594a567498364cdb3f452ea]]]
Figure 58-2 Computation graph for VGG16


[[[00000000000000001470---9c0e210316a5732b90761a83f84493eec807bf165d5d60927d703f66926d8d3d]]]Use trained VGG16

[[[00000000000000001471---e9378afed8144e286968a24f864958de6fe8bc818ff4340703f0133acf2b88f8]]]Now, let's use the trained VGG16 to perform image recognition. Start by loading a sample image.

[[[00000000000000001472---6fd0342b9ac056cddec6f4c7b1c0b733f6ba3787e5a0dc42ec4c36a7df3f77ae]]]Here, we use the dezero.utils.get_file function explained earlier to download the image file. Then load the downloaded image using the PIL package. After running the code above, you should see something like Figure 58-3.

[[[00000000000000001473---dd9faf30278c54a2a9de2485e6a1e7f9322e657c6d755e5bc2a1f17c4927bf4f]]]
Figure 58-3 Sample Image Loaded Using PIL


[[[00000000000000001474---71d96c0423a403050acdf2b6b4ab5ce4c0d10b6cad7ba6f55651806d50643930]]]PIL (Python Image Library) is an image processing library. A variety of functions are available, such as reading and writing images, and transforming images. You can install PIL with pip install pillow.

[[[00000000000000001475---2a79fada26de623774e393c0d012e993a02e96173c5b8a918599e88539acbc27]]]In the code above, I loaded the image with the code img = Image.open(img_path). At this time, the type of img is PIL.Image. Our DeZero on the other hand deals with ndarrays. So we need a function to do that conversion. DeZero provides preprocess as a static method in the VGG16 class. It is used like this:

[[[00000000000000001476---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001477---df428fbe2db6716c2aa5c54130521af878220ad15e1e2396c75382c63fe1e292]]]Since preprocess is a static method, it can be called from the class, not the instance. Give the data of type PIL.Image to the argument. The internal processing is resizing to a size of 224 height and 224 width, and converting to an ndarray instance. This size (224, 224) is the size of the VGG16 input image. The VGG16.preprocess method also performs preprocessing such as sorting the color channels in BGR order and subtracting a fixed value. They are preprocessing when VGG16 trains ImageNet.

[[[00000000000000001478---1f75a8f1f190e7877e7eda2d351f2998e30ac46fea5ad2519b31a6d6465ffc85]]]When inferring unknown data using trained weight data, we need to do the same preprocessing as when training the model. Otherwise, the data input to the model will be different, resulting in incorrect recognition.

[[[00000000000000001479---3d7e2bd6ef4fbe7ebbe2ffb8225efdd830f8968ac18c1bf900fb335f17ecdd1e]]]Now you are ready to go. Now, let's classify using the trained VGG16. Here is all the code in steps/step58.py.

[[[00000000000000001480---1e7b9c17adaa42930078264887472be189a81cb6b3c844a2691aa9344c1acaa2]]]# add axis for batch

[[[00000000000000001481---bb0cdd855d32eb324087f7eaf9cd44a26dba791efd923383237aa026cce6e75b]]]# Visualization of computational graph

[[[00000000000000001482---e1068522eba6efd46f675a95bfd8bf7e807e5b972b7cf842d6dda84e8e777266]]]# ImageNet labels

[[[00000000000000001483---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001484---2a84f32932f58701fe730612990665a70c2b37dbce91dc3d333795738629c113]]]First, the image is loaded and preprocessed. And prepend the axis for the batch. Now the shape of x goes from (3, 224, 224) to (1, 3, 224, 224). After that, we give data to VGG16 and let it make an inference. Here, the index with the largest value in the output layer (1000 classes) is the classification result of the model.

[[[00000000000000001485---b162cdee582194c847696352bf07ede3bf9bbd382061f4f4e1ec16960102010b]]]Also, in dezero/datasets.py, ImageNet labels (dictionaries where the key is the ID of the object and the value is the label name) are prepared. With it you can extract the label name from the id of the object. Looking at the result, it is 'zebra', and you can see that it is correctly recognized. This completes the implementation of VGG16.

[[[00000000000000001486---2b238dec37a193c758a26b6d6c21ea774d4fa9135a1cae1abcbe0304a99daf9d]]]In addition to VGG16, dezero/models.py has other famous models such as ResNet[37] and SqueezeNet[38]. Please refer to it if you are interested.

[[[00000000000000001487---005882b739e374671bd876838df1153aa5de85afe28dbd07c6df546633719188]]]3rd stage

[[[00000000000000001488---f976464a1c039e4357bc70d01d537f0e97a536b1a13906ad5ef76a9239c5de9b]]]Realize higher-order differentiation

[[[00000000000000001489---9edb29e1c05cb386d700c6fc17e4d2958c264564f35c6da476b793934bd6d97b]]]Our DeZero now works perfectly with backpropagation. No matter how complex the calculation, the correct logic is used to backpropagate. Today's DeZero should be able to solve many problems that require differentiation. But still sometimes you can't. One of them is higher order differentiation.

[[[00000000000000001490---a6bfd83e2c36e2f19a7fba32e3bc1446c50ac59a55fed4c7a9652cdda1bfe7cb]]]A higher-order derivative is a further derivative of a derivative. Specifically, it is an operation that repeats the first derivative, the second derivative, the third derivative, and so on. Modern deep learning frameworks such as PyTorch and TensorFlow can automatically find higher-order derivatives. More precisely, for backpropagation, we can do backpropagation (this principle will become clear in this stage).

[[[00000000000000001491---bbeaea4ef430e7edcca727a704ca1805bdad81dc832d5efa05ce8fab802be32f]]]We will now proceed to the third stage. In this stage, we will mainly extend DeZero so that we can find higher-order derivatives. That makes DeZero even more versatile. Let's move on!

[[[00000000000000001492---4b066ded165818e11a48d8a437673377c1a3e3d661231fab505262dfb6964802]]]step 48

[[[00000000000000001493---b3e408203286be5e127fe322057724a78cf1fdf5d2b949a4e00a911fe54155db]]]multi-value classification

[[[00000000000000001494---2a6d03797ff18ffaeaa87bcd4c30acb7684504d9f4ff68ff8008b6215ddf1fac]]]In the previous step, we implemented the softmax function and the cross-entropy error. Now we are ready to do multi-class classification. In this step, a small data set called 'spiral data set' is used for multi-value classification. We'll start by looking at the spiral dataset.

[[[00000000000000001495---1e94321feb9522c27c2d9243b6611b2442960eb6521293052132c6c3f4c8cc51]]]Spiral dataset

[[[00000000000000001496---57d380fb68ba735179c441ac90d5cd112d96f963a8af3ad093ef1e3abdb4231a]]]DeZero has a module (file) called dezero/datasets.py. This module contains dataset related classes and functions. We also have some representative datasets for machine learning. Here, we will read the 'spiral dataset' from among them. For that we use the function get_spiral. A simple usage example would look like this:

[[[00000000000000001497---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001498---ebec8ee62b1fe921a32457384f4885b4678f7ba3445a6012fb3ad45882b2cfe7]]]The get_spiral function takes a train of flags as an argument. Returns training data if train=True and testing data if train=False. The actual returned values are x and t, where x is the input data and t is the teacher data (label). where x is an ndarray instance of shape (300, 2) and t is an ndarray instance of shape (300,). The problem we are dealing with is a three-class classification, where the elements of t can be 0, 1, or 2. By the way, plotting this x and t results in Figure 48-1.

[[[00000000000000001499---87340e25d8be8e2a05419e952a14f6ef7e71ee137d0dfaba6ddf502ae48e01cd]]]
Figure 48-1 Spiral distributed dataset


[[[00000000000000001500---1373f5875ef8dfe57c0c89f7e76a57a1969036ef50655c359dd3454307d87712]]]In Figure 48-1, each class is plotted with different symbols such as ○△×. As you can see, it's a spirally distributed dataset. Now let's see if we can correctly classify this data using a neural network.

[[[00000000000000001501---4200569cacafbd71e8796095c521f3c93b90c9b671d7f1e9a794c748b4ab6028]]]code for learning

[[[00000000000000001502---2c69d920baa9a73511c9be9f7210db500cef6363fe0f955147131df1ac484dd2]]]Now, here is the code that does the multi-class classification. Due to the large amount of content, it will be posted in two parts, the first half and the second half. Here is the first half of the code.

[[[00000000000000001503---7749da34e1616777ac5b224b4b6f12cb268937565558817fa6c8708ed9010e1d]]]# (1) Setting hyperparameters

[[[00000000000000001504---71be7d326eb74f426f9f9f951b2d84a396ec01bbac02efa8c142636928595c8b]]]# ② Read data / generate model optimizer

[[[00000000000000001505---e17c524f35004990f7184fa945dfbe43b74336092b3f6c73512fa2139682c46b]]]The code shown here is almost identical to the code you've seen so far. First, set the hyperparameters in ① in the code. Hyperparameters are parameters determined by 'humans', such as the number of intermediate layers and learning coefficients. Then, at point ②, we load the dataset and generate a model and optimizer.

[[[00000000000000001506---62aa0b818ba92fd25f9f74435c5a4a4cf088ac94d775a82e6407434a31c03af6]]]Note that the code above sets max_epoch = 300. The word 'epoch' stands for unit of measure. One epoch is when you use all of the prepared datasets (when you 'see' all of them). Also, there is batch_size = 30, which means batch process 30 data at a time.

[[[00000000000000001507---94207481372d3036a42bd9e8c407274e6dfceec621b9b110f50905d4fa1aec49]]]There are a total of 300 pieces of data to be handled here, which is more than the examples we have seen so far. In real-world problems, you will most likely be dealing with much more data. In such cases, instead of processing all the data together, a random subset of the data is processed. This 'group' is called a mini batch.

[[[00000000000000001508---b5717fcb6b191b058d229beb3f8886b087295c67e4afbb41b131749327794b88]]]So here's the code for the second half:

[[[00000000000000001509---f1c92e93dcd5aa565a9920e2e6ec8395e55bf0f9aa9ad9ae498e696ecfc1f6df]]]# Round up the decimal point

[[[00000000000000001510---ff6efc56d21ab4da4a185c95608393b0be180ed85b695a540614e902b711e65a]]]# ③ Shuffle the index of the dataset

[[[00000000000000001511---ba5950f3f5efcfdf2043aa39994ae0bc3cfdfc57ee897c5967eb76acb091129d]]]# ④ Generate mini-batch

[[[00000000000000001512---20812a134928609be688878b50c6325aa2cfe703fd66e24193e0e63f7706dfe1]]]# (5) Gradient calculation / Parameter update

[[[00000000000000001513---30093dac359e17551016c6390e520cb7c165591ee976622f386ea831623d6894]]]# ⑥ Output learning progress every epoch

[[[00000000000000001514---6df6af019fa9c411050712c712885fce434c333ea6bf5d58e04e8c85b85907bc]]]Code ③ randomly rearranges the index of the dataset. You can use the np.random.permutation function for that. This function, for example, np.random.permutation(N), will give you a list of random integers from 0 to N-1. Here, we generate a new randomly permuted list of indices with index = np.random.permutation(data_size) for each epoch.

[[[00000000000000001515---e4c6248941242ce546871de2766a1d018a44244cc6b4f0987faa50ee37b054fa]]]Code ④ generates a mini-batch. This mini-batch index (batch_index) takes out the index generated earlier in order from the beginning. Note that DeZero functions take either Variable or ndarray instances as input. In the example above, the mini-batches batch_x and batch_t are both ndarray instances. Of course, even if it is explicitly converted to Variable like Variable(batch_x), the calculation will be performed correctly.

[[[00000000000000001516---ed3005c3a04d70562d1160cfd3adfaae020a4b7493374101982bd2301430235b]]]Code ⑤ finds the gradient and updates the parameters as usual. And in ⑥, we record the results of the loss function for each epoch. That's the code for training the spiral dataset.

[[[00000000000000001517---4c0b213ba52ef84ea8f6bb24d8988a1c1635cfd8914aefca68a4020b1ef552ff]]]Now let's run the above code. Then you can see that the loss is steadily decreasing. The results can be represented graphically as follows.

[[[00000000000000001518---66283bbe17bd434eded15c3c61541a2370d719a735bf75f25e901655f608d14b]]]
Figure 48-2 Loss graph (horizontal axis is epoch, vertical axis is average loss per epoch)


[[[00000000000000001519---ba13894f54b42d7fd83706ca11cb2da6d1334bbbd8d466db456d389db762a7b5]]]The loss decreases as training progresses, as shown in Figure 48-2. Our neural network seems to be learning in the right direction. Now, let's visualize how the neural network after training creates a separation region, which is called a 'decision boundary'. The result should look like Figure 48-3.

[[[00000000000000001520---30b6ee7b5dda32795eb282628adac6521325541ff45fd3585208f309f60ec59c]]]
Figure 48-3 Decision boundaries of the neural network after training (color-coded areas for each class identified by the neural network)


[[[00000000000000001521---0869f940cbf63f92027915377a49c12cd94656c94252299d93c1f1941c44ca93]]]As shown in Figure 48-3, the trained neural network correctly captures the 'vortex' pattern. In other words, we learned a non-linear separation region! In this way, neural networks can express complex expressions by having hidden layers. Furthermore, the feature of deep learning is that the power of expression becomes richer by layering.

[[[00000000000000001522---b9a624ca3ecdf5de3e368f70f61cc98ba923e7b00d5ce625f5da5544371f3bb3]]]step 55

[[[00000000000000001523---ed8c3a2c60a6fd7b741a1c64b8b0f7bd60e6437e060ac7db8c73818be6ae4090]]]CNN mechanism (1)

[[[00000000000000001524---6fe015185e03a7adad55ceebc71b5544b63df559b6156d2b2deb70db4eec02a6]]]From now on, the theme will be 'CNN' over several steps. CNN is an abbreviation for Convolutional Neural Network, and is called 'convolutional neural network' in Japanese. Neural networks are used in various fields such as image recognition, speech recognition, and natural language processing. Especially in the field of image recognition, many deep learning methods are based on CNN. Here, I will explain the mechanism of CNNs, especially those used in images.

[[[00000000000000001525---28c814f6603097f64fb7ff72c33445b05aba30219fb4f5b47f3b49032347baeb]]]In this document, we will only explain the mechanism assuming the implementation side of CNN. I will not explain why CNN can perform image recognition well and why it can extract image feature values. For more information, please refer to 'Deep Learning from Scratch'.

[[[00000000000000001526---8b333629c9087de56690653c29ed26881b7bcf0d1459f409b2f20a780af47b2b]]]CNN network structure

[[[00000000000000001527---5eefbe7c5517a84ea2b97bf442c68064a1a72e4c6f5c1c0c46b0c22641ca733e]]]Like the neural networks we have seen so far, CNNs are also made by combining layers. However, in the case of CNN, a new 'convolution layer' and 'pooling layer' will appear. We'll talk more about convolutional and pooling layers later, but first, let's look at what layers a CNN can combine to build. Figure 55-1 shows an example of a CNN model.

[[[00000000000000001528---15c9a2df19c65ceebbc8aa6180d587b69b8cd71e5cb50c342e051f3492caa22c]]]
Figure 55-1 An example of a network using CNN (here, the computation graph is drawn for each layer, and only the input and output variables are shown. Convolution layers are represented by Conv, and Pooling layers by Pool)


[[[00000000000000001529---5e6fe46314a6ad90d7aac31aab200353f6306c3b78f452007687cd1a2b5dd15c]]]As shown in Figure 55-1, CNN adds new Conv and Pool layers. And the connection order of CNN layers is 'Conv → ReLU → (Pool)' (Pool layer is sometimes omitted). It is thought that this is because the conventional “Linear → ReLU” connection has been replaced with “Conv → ReLU → (Pool)”. Also, in the layers close to the output in Figure 55-1, the conventional “Linear → ReLU” combination is used. The above is a configuration often seen in general CNN.

[[[00000000000000001530---a9d72488fe5236f7246394319af1ea4734f255a186c84e3809a3e04b50202676]]]convolution operation

[[[00000000000000001531---32ff459f4e1c2da6fa1b8b6583cedba7c0da3c7c01aadfa669ef5fcc38f5e80d]]]CNN uses convolutional layers. The processing performed in the convolution layer is a 'convolution operation'. This corresponds to 'filter operation' in image processing. Here, we use the example in Figure 55-2 to explain the convolution operation.

[[[00000000000000001532---936a9462ee4d18739446d0ba98805387afdc396117dad405b439547143d8cc46]]]
Figure 55-2 Example of convolution operation (convolution operation is represented by '')


[[[00000000000000001533---a01162841d19fc9752afd517c57849a1064b488f0b40be48ba0a32e87df9740c]]]The convolution operation applies a filter to the input data, as shown in Figure 55-2. In this example, the input data has vertical and horizontal dimensions, and the filter has vertical and horizontal dimensions as well. If we denote the shape in the order (height, width), then in this example the input has shape (4, 4), the filter is (3, 3), and the output is (2, 2). The convolution operation performed at this time is calculated according to the procedure shown in Figure 55-3.

[[[00000000000000001534---a27e1b0f970d0dd461917420e4186375a1a7ad1322477d6b0db40348335c8e4a]]]
Figure 55-3 Calculation procedure for convolution operation


[[[00000000000000001535---02a69ea7d4f7faad5568758fd1753d0a265e350d9af486ca4a178fb7f86817b4]]]A convolution operation is applied to the input data while moving the window of the filter at regular intervals. Multiply and sum the corresponding elements of the filter and the input, as shown in Figure 55-3. Then store the result in the corresponding location. Doing this process everywhere gives us the output of the convolution operation. Note that the term 'filter' mentioned here is expressed by the term 'kernel' in some literature. In this document, the terms 'filter' and 'kernel' are used interchangeably.

[[[00000000000000001536---133c7d68118f05937b83d3ee07902eafcb16ca63ee64cfc69bd533a6e46310d8]]]Note the movement of the filter in Figure 55-3, it moves in two directions, horizontal and vertical. Since this is a filter that moves in two dimensions, it is also called a 'two-dimensional convolutional layer'. According to this principle, if the filter moves in only one direction, it can be called a 'one-dimensional convolutional layer', and if it moves in three directions, it can be called a 'three-dimensional convolutional layer'. For images, we mainly deal with 2D convolutional layers. By the way, DeZero implements a 2D convolutional layer named Conv2d.

[[[00000000000000001537---2aaf49630a8f26ab2679eb7970a1aa9561873ebbb390bfa3ccaecb420513a3b8]]]In addition to the weight parameter, there was also a 'bias' in the fully connected layer neural network. Similarly, convolutional layers also have biases. Figure 55-4 shows the processing flow of convolution operation including bias.

[[[00000000000000001538---d19be4b8059eb3f77f9714de88638b41ea823976c6b0efc9d26a11a36170b2a1]]]
Figure 55-4 Convolution Operation Bias


[[[00000000000000001539---87155f58c8639616d7f9b42751ad3c4678e952f7104e468e6317ae8f7c4fa068]]]As shown in Figure 55-4, the addition of the bias term is done on the filtered data. Note that there is only one bias here (one bias for four filtered data in this example). That one value is added to all filtered elements.

[[[00000000000000001540---c376de5cc9148b86843e98b1bb9914148cc10acb46717fc831da3344e1c7c1c7]]]Next, I will explain the terms 'padding' and 'stride' in convolutional layers.

[[[00000000000000001541---9c0bee2f1a1e931c077c2670cc6249e93e5ddd6e51f557d10c50be9a9b1c5996]]]padding

[[[00000000000000001542---9650d127dd9b571bd6cbbace3f2bef7d21945918b6e2ed1f55ca552cdf953951]]]Before the main processing of the convolutional layer, the input data may be padded with fixed data (for example, 0). This process is called padding. For example, the example in Figure 55-5 applies padding of width 1 to input data of shape (4, 4).

[[[00000000000000001543---0ef1cab50e248c0c0205efe3329726367b1b3d2b520d638cc57f6a828b93d25b]]]
Figure 55-5 Padding for Convolution Operations


[[[00000000000000001544---42bc0ae316ce353bbcd520340d4dcc70ff1682d158773eb158b17b5bb8d5c9cb]]]Input data of shape (4, 4) is padded to shape (6, 6), as shown in Figure 55-5. And a filter of shape (3, 3) will output data of shape (4, 4). In this example I set the padding to 1, but you can set it to any integer, such as 2 or 3. You might also consider setting the vertical and horizontal padding separately.

[[[00000000000000001545---ec4604a91968485aa0808bf71a54422625f2fbbf894ba6aa8d807f6787fd0938]]]The main reason to use padding is to adjust the output size. For example, if you apply a filter of shape (3, 3) to input data of shape (4, 4), the output will be of shape (2, 2) and the output size will be reduced from the input size by two elements. will be This becomes a problem for deep networks that perform many iterations of convolution operations. This is because at some point you can't do any more convolutions if you're spatially shrinking each time you do a convolution. Use padding to avoid such situations. As for the previous example, setting the padding width to 1 makes the input shape (6, 6). That leaves the output with shape (4, 4), preserving the size.

[[[00000000000000001546---d1af7f2083febd2dee0a65da06742ebc315f6a8f0316b6275b188875fdc64330]]]stride

[[[00000000000000001547---2c2e9a5e41b5ac25b923dbc9f1c42aa4ef84c8f6d89dbb36333ce139e811ce32]]]The interval between positions where the filter is applied is called the stride. In the examples we have seen so far, the stride was 1, but a stride of 2 results in a filtered window spaced every 2 elements, as shown in Figure 55-6.

[[[00000000000000001548---cf2330220141459feaf47a2f5d36b9aecce58ed67adf12d326f46976f2acbb28]]]
Figure 55-6 Example of a Convolution Operation with a Stride of 2


[[[00000000000000001549---24b06fc4922da9d1c69c5b1aaf8e3fd23e9216a93fd98b0dddf8b16697cad8dc]]]The example in Figure 55-6 applies a filter with a stride of 2 for data with an input size of (7, 7). By setting the stride to 2, the output size will be (3, 3). Thus, stride specifies the interval at which the filter is applied. You might also consider setting vertical stride and horizontal stride separately.

[[[00000000000000001550---b5b26580105b450ea042796b05567184cd1a68a8f7775c2c035ef778af173b6f]]]How to calculate output size

[[[00000000000000001551---fd32a0dfdcfef9b133fb85cac98e5c8ef543c3579ad5f9915dcf6994dbdcdd99]]]As we have seen, increasing the stride decreases the output size. Also, more padding increases the output size. In other words, the output size is affected by stride and padding. Given the stride and padding, plus the input data and kernel (filter) size, the output size is unique. The calculation method is as follows.

[[[00000000000000001552---925eb11d84d594a64ddaa16fa5583689bd80c55b0bac30a41edeee5539dacac1]]]where all arguments are assumed to be of type int. input_size is the input data size, kernel_size is the kernel size, stride is the stride size, and pad is the padding size.

[[[00000000000000001553---0435ff45b9f937c52163dde98232138e9c46bee3b1ece7a82555732a4064c77a]]]The // operation in the above code is division. However, for non-divisible calculations, decimal places are truncated.

[[[00000000000000001554---b9897e8382dc13829ccaa15b0e1eed217b9dd2a857cae78ffc6aadf62c2d69d4]]]Let's try using the get_conv_outsize function implemented here. The result looks like this:

[[[00000000000000001555---15acc10ee3ab64cad7358c24dc93e178702402f964a8081dc32722f40f0366af]]]# stride (vertical stride, horizontal stride)

[[[00000000000000001556---cf97f87d3fc02b2f385a78de53d3fb27e20c5686cb5de7c90b0f935501cf51c2]]]# padding (vertical padding, horizontal padding)

[[[00000000000000001557---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001558---ef1ab6dabd22a87873088d81a1f01c7c3254bef3b98a8ee179868b08c35f6d56]]]I was able to calculate the output size as shown above. This get_conv_outsize function will be used later, so I will add it to dezero/utils.py.

[[[00000000000000001559---7c756073de4962176d084b5817e924e311463e77005f7749e8329219c305da88]]]Since the basic explanation of the convolution operation is over, this step ends here. In the next step, we'll talk about the remaining themes of CNN mechanics: channels and pooling.

[[[00000000000000001560---f44b31844f5238ae7d4c3db25aaf61db985a33a1927e11214e3dbf13855a6946]]]step 6

[[[00000000000000001561---1bd69345c3e9dfd099139ad649a5a2ba34875589faa3e9c590720b8f6672057d]]]Manual backpropagation

[[[00000000000000001562---dc3576b910ef192fc53ec3d7f0ba9b4240fb81388c7965ab252ff1ed849c480d]]]In the previous step, we explained how backpropagation works. In this step, we will extend the Variable and Function classes so far and implement them so that differentiation can be obtained using backpropagation. Let's start with the Variable class.

[[[00000000000000001563---fe17eee9c44fce2bddaa718a1f166dff445f873e6183532b7556a4e6aeff648a]]]Additional implementation of Variable class

[[[00000000000000001564---7c96caf01a3db66ed32da1082615fdf28077defada5158dc59653a65f6a6ec40]]]Now, let's implement a Variable class that supports backpropagation. To do so, we extend it to have a corresponding differentiated value (grad) in addition to the normal value (data). Here, the newly added code is shaded in blue.

[[[00000000000000001565---ec701d5b6718c798d4f4dde44339a24055a4441433d7790de4a3c2f56b5ef700]]]As above, we will have a new instance variable called grad. Both the instance variables data and grad are assumed to be NumPy multidimensional arrays (ndarrays). Also, grad is initialized to None and set to that value when backpropagation actually computes the derivative.

[[[00000000000000001566---c97c8dc1d058b3fbafcdcf1c0e4250caf98cff1dcb008ec4c3399d4f32e38242]]]Differentiation with respect to multiple variables, such as vectors and matrices, is called a gradient. Therefore, the Variable class has a variable called grad, which is an abbreviation for gradient.

[[[00000000000000001567---ce30e5368e03eda2687c30aac40b8a03278b580115f8d53b31bbb2ad70b062b9]]]Additional implementation of Function class

[[[00000000000000001568---e07f407dc3f2bca50283073847394109a59a112978eb8d53097c5ec4e5a8fcb9]]]Next is the Function class. Up to this point, the Function class has had forward propagation (forward methods) functionality for normal computation. In addition to this, we will add the following two new features:

[[[00000000000000001569---b75ab8547d5652695128e488531c0c3d46b5326c79f944785e7495d0e6c1add3]]]Backpropagation function to calculate differentiation (backward method)

[[[00000000000000001570---19d0c46d3659480e06bbce51e18bf765d7150f2181dda0d4b40f10d8d0cc3610]]]A function to retain the input Variable instance when calling the forward method

[[[00000000000000001571---76de570b57012a53228c162c4371cc8ccf7853ed6e71c3c8d5b54f8b6e4bc65c]]]The following code implements these two points.

[[[00000000000000001572---9780386095b37f6cb4eb53a1052b59e3c7482741e8f63c08cfd51450f80a64c2]]]# Remember input variables

[[[00000000000000001573---d96a3cd32ef6ca7be90709741c581bbc4b191554ee03d05de8d2c8d3c1530218]]]As shown in the code above, in the __call__ method, set the input that was entered to an instance variable. Now when the backward method is called, we can use the Variable instance we entered into the function as self.input.

[[[00000000000000001574---3a10aa372b9dbab66185bfbe748cdef663061922b9cb893abd4a28b64ecc6e63]]]Additional implementations of Square and Exp classes

[[[00000000000000001575---aec29be2f77d45391ea3db0cc4285b4d9f776ba6874320ee2e7ea671b7202f3a]]]Next, implement backward propagation for concrete functions. Let's start with the Square class, which does square calculations. This can be implemented as

[[[00000000000000001576---c3c805da2687868fb40f52dc39e77bf5a4477b5ad86b768d9a4f801dfc5b3884]]]Add a backward method for backpropagation as above. The argument gy of this method is an ndarray instance, and the derivative transmitted from the output side is passed. Returns the value obtained by multiplying the derivative passed in the argument by the 'differential of' as the result of the backward method. The returned result will be propagated further in the input direction.

[[[00000000000000001577---4dc6c726fc66f8a9c9ce840fc270cae35266f3b7b5e179d4dca268a79cd94a78]]]Next is the Exp class, which does the calculation for . This can be implemented as:

[[[00000000000000001578---f99f8e5d9f4e2446ba8d1f7aae96e84140ad839ea84fafb55a1e09ab4e5ba67c]]]Implementing backpropagation

[[[00000000000000001579---c31e112f883380b7c268ab1bd424fe0d2f8b5353e121c9412539b4b69f68decc]]]Everything is ready. Here, let's find the derivative of the calculation shown in Figure 6-1 by backpropagation.

[[[00000000000000001580---67906192014c8dc6a77bc325b8b08de0e566f8905dbe2fae30b615b29d2f4ee4]]]
Figure 6-1 Composite function for backpropagation


[[[00000000000000001581---360c4b94f0716bfa2ee6a246a3fe21920c3e4541648fac51ca78632466f530cb]]]First, here is the code for the forward propagation in Figure 6-1.

[[[00000000000000001582---257c69c95e66f04a2d4347055df7c9cbc58f6c3871008bd1e0ed008308e3c9e7]]]Then backpropagation finds the derivative of y. To do so, call the backward method of each function in the reverse order of forward propagation. Figure 6-2 shows the calculation graph of the backpropagation performed at this time.

[[[00000000000000001583---17f19239b72a04fc4c69c6f15c8c64a3609fd2aaeb91cdf56dc1d6b615c57377]]]
Figure 6-2 Backpropagation Computational Graph


[[[00000000000000001584---1ca5ccdee801eb59708f0f671a0a1f6091dcdcfd8a790ae50f3d520b0b5ce2a6]]]Looking at Figure 6-2, you can see which functions' backward methods should be called in what order. You can also see which variable grad should be set to the result of the backward method. So here's the backpropagation implementation:

[[[00000000000000001585---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001586---8953cc4114807bbbf82345d0679c62ce464a2f0dc871f421e8fbbcf8bdb9656e]]]Backpropagation starts from So I set the derivative of the output y to np.array(1.0). After that, just call the backward method in the order of C → B → A. This will give you the derivative with respect to each variable.

[[[00000000000000001587---ec3db90c70131168b6ac2702a9251c59bda83a5b400003964c05ace7b2776516]]]After running the code above, the x.grad result is 3.297442541400256. This is the derivative of y with respect to x. By the way, the result for the numerical differentiation in step 4 was 3.2974426293330694, so we can see that the two results are almost the same. From this, it can be inferred that backpropagation has been implemented correctly—more precisely, it has been implemented correctly with a high degree of probability.

[[[00000000000000001588---bc4d7ac3cf80dd0bcdae377f79d27a88dad3bd99a59e31afaa6667e5edcffa11]]]This is the implementation of backpropagation. It ran correctly, but we specified the order of backpropagation C → B → A manually—that is, by coding it ourselves. The next step is to automate this manual work.

[[[00000000000000001589---de0595f48c961e960e8357b30e88c895b166c26b9d555cd2b8bb8c786b5f6be4]]]step 49

[[[00000000000000001590---31ad665fc14bc731686025129f22b69814676e6bbd19f834562a93bba4149642]]]Dataset class and preprocessing

[[[00000000000000001591---2abfd09f43422882a2a25f642eb2c400bf0eb45e656c6dab064af0e5e6f87e1e]]]In the previous step, we performed multi-class classification using a spiral dataset. There, the data was read with the code x, t = dezero.datasets.get_spiral(). At this time, x and t were ndarray instances, whose shape was (300, 2) for x and (300,) for t. So we were holding 300 pieces of data together in one ndarray instance.

[[[00000000000000001592---d7c02fbdb7c16150d8676cb7a16214bdc2fc144f26f5b7968b31e55bb370aa3d]]]The spiral dataset was a small dataset of about 300, so it could be treated as a single ndarray instance. But when dealing with large datasets -- imagine a dataset with a million elements -- the data format becomes a problem. The reason is that if you deal with one huge ndarray instance, you have to expand all the elements into memory. Here, we will create a class dedicated to datasets, the Dataset class, so that we can deal with such problems. In addition, the Dataset class also provides a mechanism for preprocessing the data.

[[[00000000000000001593---8c40cab9670bc18f372e90893664e4798e181260b264dfc8b501e8ae2cd31205]]]Dataset class implementation

[[[00000000000000001594---70fde7f506fbe6fb6d6f229c1e4a186c194f11459ed0c2269923c2824e18bc65]]]Dataset class is implemented as a base class. The dataset actually used by the user will be implemented by inheriting the Dataset class. So here is the implementation of the Dataset class.

[[[00000000000000001595---5a22a4e95604102fa57629cf6873acf6a98ced19f267c974eda6a285aa1270e1]]]# index only supports integers (scalar)

[[[00000000000000001596---3d29a87bd86dd3612a5352c190263083f006ffcc2aa996981384b4359b054713]]]First, in the initialization, we receive an argument called train. This is a flag to distinguish between 'for learning' and 'for testing'. Also, as a basic configuration, the input data is held in the instance variable data, and the label is held in the instance variable label. Then, have the inherited class implement a method called prepare to prepare the data.

[[[00000000000000001597---d5a6ce3253af3463f0961b06d6060db700d73dd083e574e29818720f580fa0a8]]]Two methods of interest in the Dataset class are __getitem__ and __len__. Having those two methods (interface) is a requirement for being a 'DeZero dataset'. If you fix the interface like that, you can use different datasets interchangeably.

[[[00000000000000001598---b1959b79fe646f2941599772c8ee11553d4fd2b1b5adb072c979d7e1b8bd4918]]]__getitem__ is a Python special method. This special method allows you to define the behavior when accessed with square brackets (eg x[0], x[1], etc.). The __getitem__ method of the Dataset class simply retrieves the data at the specified index. If there is no label data, return the input data as self.data[index] and the label as None (this is the behavior assuming 'unsupervised learning'). Also, the __len__ method is called when you use the len function (for example, len(x)). This is used when checking the length of the dataset.

[[[00000000000000001599---05cb103578d8d9f11e6e515c8e8e9f809517997b57c8f917555b6805f8c7b7f1]]]The argument of the __getitem__ method originally corresponds to 'slice' in addition to int type. A slice is, for example, an operation like x[1:3]. However, DeZero's Dataset class does not support slicing operations, only int indices.

[[[00000000000000001600---af475fcc55eee79938c67c93c3f315ab23c0182954a7ac3181fc7260cebeeaa6]]]The above is the Dataset class. Now extend the Dataset class to implement the spiral dataset. Here we name it the Spiral class and implement it as follows:

[[[00000000000000001601---020aedfbb7f55d187baee8766950dda919627e846f1813f70788218eac2388cb]]]As above, just set data to the instance variables data and label in the prepare method. Now you can retrieve data using the Spiral class as follows: You can also get the length of the data.

[[[00000000000000001602---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001603---cb6f81e7db5ac31d530665499a1bdd030cb5dd43d50a43ccd36feb06318cef0b]]]Here we are accessing the data as train_set[0]. That will return the 0th input data and label as a tuple.

[[[00000000000000001604---d363c7aa67ecffac1a40178fb0d326460f98fcdb49f1819454df88c8b1b93dfe]]]For large datasets

[[[00000000000000001605---5d3c1358677e86935d2f63f2b91bfc4ab00faf0b57f0e1cab2f267d498694972]]]For small datasets like spiral datasets, you can directly hold the ndarray instances in the instance variables data and label of the Dataset class. However, if the dataset is large, that implementation method cannot be used. In that case, a possible implementation would be:

[[[00000000000000001606---efebac6ce61900cccf0159b1d75e42c76efae066d08f509cda9dc1b030cb7bab]]]Here, we assume that the data directory and the label directory each contain 1 million pieces of data (labels). In that case, do not read the data when the BigData class is initialized, but read it when the data is accessed. Specifically, when __getitem__(index) is called, the data in the data} directory is read (np.load function will be explained in step 53). Again, the requirement to be a 'DeZero Dataset' is that the two methods __getitem__ and __len__ are implemented. The BigData class above also meets that requirement. Therefore, the BigData class can be used in the same way as the Spiral class.

[[[00000000000000001607---2306cee660418361ad2176cd98e4f2b2739bf4d7ee180f0ec4848fa1f03b155f]]]Next, let's write a learning code using the Spiral class. It requires 'concatenation' of data.

[[[00000000000000001608---ca2bd5be3fc6a787e7683b374fa314210b1da9a6888b3ff7164467bdd563153a]]]data concatenation

[[[00000000000000001609---6e979dcb371455a4b375dc67f50774c370c1d616fa31cdbeeb2dff1533cd8e1c]]]In training a neural network, a portion of the dataset is taken out as a mini-batch. Using the Spiral class, the code for fetching as a mini-batch looks like this:

[[[00000000000000001610---29ad73b6f6e66f48b2dd94389f53bd72f05302ca0c07e6f4c27caa551eed225e]]]# get data from 0 to 2

[[[00000000000000001611---e30667184f0a7a9a12304e077eb3ac95f8c4d88f83ce2790c5a6ec7b10ff2f27]]]As above, we first retrieve multiple data (mini-batches) by indexing. Here batch is multiple data stored as a list. To input to DeZero's neural network, we need to convert them to a single ndarray instance. The following code does that.

[[[00000000000000001612---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001613---8e9db3095cbf714df45e32de1477e440cffe824a2a5e72bbc9acac0dc3db0dc5]]]Here, we extract only the data (or labels) from each element of batch and transform (concatenate) it into one ndarray instance. Now we can input to the neural network.

[[[00000000000000001614---4200569cacafbd71e8796095c521f3c93b90c9b671d7f1e9a794c748b4ab6028]]]code for learning

[[[00000000000000001615---e00db73c3a1a421798062aefd48f028967da074556de86b830227adbfbeadd21]]]Now let's learn using the Spiral class. The code looks like this (note that the Python imports are omitted here, and the changes from the previous step are shaded):

[[[00000000000000001616---e88154dc318ad0f52e900fb0b44030f6945513a4e8d9307aa22c941e32f026c5]]]# fetch minibatch

[[[00000000000000001617---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001618---3736918013277acb3fc3c3f64b505808c912f44ba8a518d82702535fa3d607b9]]]The change from the previous step is to use the Spiral class. Along with that, I modified the code where the mini-batch is created. Others are the same as the code in the previous step. If you run the above code, you can see that the loss is decreasing as in the previous step.

[[[00000000000000001619---c10d1c1981b1eab6228b884834f1b155dd624fac38ade6ce881c061360563b4e]]]Now you have trained a neural network using the Dataset class. The advantage of using the Dataset class can be realized when training on a different dataset. For example, if a dataset called BigData class satisfies the specifications of 'DeZero's dataset', the code can be executed simply by rewriting Spiral in the above code to BigData. By defining the dataset interface in this way, various datasets can be handled in common.

[[[00000000000000001620---0b6afd91909bf13e6aa6d76637d1429e8a62ebe908f1d48dea6d99b90ee7709c]]]Finally, let's add some preprocessing functionality to our Dataset class.

[[[00000000000000001621---37bc6853ad72d62997b6bf7cbfae204bbb007de4cc09db820265c30f76784f72]]]Dataset preprocessing

[[[00000000000000001622---92b7f506642e46f36d89316e45d342f6cd8b4a0b7107b834d36a0f3aeb4153e4]]]In machine learning, we often apply certain processing before we feed the data into the model. For example, the process of subtracting a certain value from the data or the process of transforming the shape of the data. A technique that artificially increases the amount of data by rotating or horizontally flipping an image--this is called data augmentation--is also often used. To support those preprocessing (and data augmentation), we add preprocessing functionality to the Dataset class. The code to add is:

[[[00000000000000001623---117e3ed87223804607aaab60d80c0a1d29822c341cfaa9eee3701acd6322de21]]]As above, it receives a new transform and target_transform on initialization. These arguments will take callable objects (for example, 'Python functions'). transform performs transformation processing for one input data, and target_transform performs transformation processing for one label. If it is None, set the preprocessing of lambda x:x to return the argument as-is (i.e. preprocessing does nothing). Having the ability to set this preprocessing allows you to write code like this:

[[[00000000000000001624---d26b2a9083a2362978573a14f45644a6c9be64dbefed7b61f53b79b1d57f2136]]]Here is an example of performing preprocessing to convert the scale to 1/2 for the input data. In this way, it is possible to add any preprocessing that the user likes to the dataset. DeZero also provides transformations commonly used in preprocessing in dezero/transforms.py. For example, data normalization and image data (PIL.Image instance) conversion processing are implemented. In fact, here's an example of how it's used:

[[[00000000000000001625---06b129aa6ab2b41eeb22f4c2455aa057bb338a2f0bd5dc7b63eea867644288ff]]]The above transforms.Normalize(mean=0.0, std=2.0) transforms (x - mean) / std, where x is the input. Also, if you want to perform multiple conversion processes in succession, you can write the following code.

[[[00000000000000001626---7fb7d9a6d71031ea78fd8bd1551d39433d3a55d213a80db66e85c0dd202bdca9]]]The transform.Compose class works through the list it is given. In the above example, normalization is performed first, and then the data type is converted to np.float64. As you can see, dezero/transform.py provides convenient transformation processing. The implementation of the contents is simple, so the explanation is omitted. Please refer to it if you are interested.

[[[00000000000000001627---1f00fe6d569d3e0ae3956a92221b6b0b7f53747f52d9e523a067285b4aae9ae0]]]step 29

[[[00000000000000001628---dcaf5ce20efffe419f08321c8beb24f892334e12cde749c157a7664ea58683a0]]]Optimization using Newton's method (manual calculation)

[[[00000000000000001629---fd3c64c5506926ec1fe39a2bdd0afebd358aeaa21ccb28fe80b6fde13d787213]]]In the previous step, we used gradient descent to find the minimum value of the Rosenbrock function. However, there, after nearly 50,000 repetitions of finding the gradient, I finally reached my destination. As the example shows, gradient descent generally suffers from slow convergence.

[[[00000000000000001630---0b36859326884852dc429caa77e779bd75c5ecb4addf3f4235d52969bd02d15d]]]There are several alternatives to gradient descent that converge faster. The most famous of these is optimization using Newton's method. Optimization by Newton's method increases the possibility of reaching the optimal solution in fewer steps. For example, for the problem we tackled in the previous step, we get the result shown in Figure 29-1.

[[[00000000000000001631---6ebbb27a6c526e7532a13fbac8625663eef77caccecb2c1cc885adf182612da4]]]
Figure 29-1 Updated path by gradient descent (left figure) and updated path by optimization method using Newton's method (right figure)


[[[00000000000000001632---33b794505a55285fb4877c290e38fb58404f572cdd92aa28c7e7e9e4d410f0eb]]]Looking at Figure 29-1, while the gradient descent method struggles in the “valley” and slowly moves toward the target value, the Newton method jumps over the valley and reaches the destination in one go. It has only been updated 6 times! The gradient descent method required nearly 50,000 updates, but the Newton method required only 6 updates, a dramatic difference.

[[[00000000000000001633---9fe3c923fb304d6f67ea8ce6bbb08a82a4ca27a461dc4d0e51606ee0a4e4bdbb]]]For the Rosenbrock function, there was a big difference in the number of steps between gradient descent and Newton's method. Of course, this number of times varies greatly depending on the initial value, learning rate, and other settings. In fact, there are many cases in which the difference is not so great. In general, Newton's method converges faster if the starting values are close enough to the solution.

[[[00000000000000001634---87542130cfb69beef7f634523e75b6b20408209e472e4d1ce147a5e62fbb0198]]]Theory of optimization by Newton's method

[[[00000000000000001635---0d14a7bdfb90c2ce5e463dc6914a4f0dba1a1512481a0e686d8e19fab3febc98]]]The goal of this step is to implement Newton's method optimization. Verify that using Newton's method instead of gradient descent actually converges faster. To keep things simple, let's consider a function that takes one variable as input (the Rosenbrock function was a function that took two variables as input).

[[[00000000000000001636---d59349e0557605b78ad214c937f936bc12e917901890f83197b974b659e35c7d]]]Now let's derive the optimization by Newton's method. Consider the problem of finding the minimum value for the function To derive the Newton's method optimization, transform by the Taylor expansion to

[[[00000000000000001637---75c0caa3b1e0aca4cdd44645dbd0f2853f6a95ee4a0846e2fd218eb260479e0e]]]Beginning at the point, can be expressed as a polynomial of by the Taylor expansion (Taylor expansion was discussed in step 27). At this time, the number of terms increases as the first derivative, the second derivative, the third derivative, and so on. Here, we will truncate with the second derivative as follows.

[[[00000000000000001638---1c02a873c2d9883e97a26ff0435221a852dca390847eaae8b67797606bf08308]]]As shown in equation (29.2), we approximated the function up to the second derivative term. Here, if we focus on the variables, we can see that it is a quadratic function of . In other words, 'some function' of is approximated by a quadratic function of . That's why it's called a second-order approximation. By the way, the work done here is shown in Figure 29-2.

[[[00000000000000001639---d2c3355db3782401cd09ecd0060755c56054c4ad330c5a25d45edf0d37195180]]]
Figure 29-2 Example approximated by second-order Taylor expansion


[[[00000000000000001640---edec2317cd3b6282e10bcd95a95aac556950f578abcd51f84f4718cb8e7e8220]]]The fitted quadratic function is a curve tangent at , as shown in Figure 29-2. Fortunately, the minimum of quadratic functions can be found analytically. To do so, simply identify where the derivative of the quadratic function is zero. If we write down the formula, we get:

[[[00000000000000001641---034f1b2aacde429214900caaeb7538f4fcf7114ae33188d209ff825a93e5bb58]]]From the results above, we can see that the minimum value of the fitted quadratic function is at . This means that you only need to update the position of , as shown in Figure 29-3.

[[[00000000000000001642---a1c09b6e90c6a2f93f62c1b53c119beea3e650ad1cb96ab06452668529b42cbc]]]
Update Position in Figure 29-3


[[[00000000000000001643---57e9908799fed68909d9688959dd1657b636b3f23e00825e6cb6720d88bcba9a]]]Update the position of as shown in Figure 29-3. Then repeat the same work in the updated location. This is Newton's method of optimization. This property of Newton's method becomes clearer when contrasted with gradient descent. Now let's look at the following formula:

[[[00000000000000001644---29b07e54e72c2406e3638883e27aa615d48c43a57a1dda58d3e43ec0e069e94d]]]Equation (29.4) is gradient descent and equation (29.5)†1 is Newton's method. As you can see, both techniques update, but in different ways. In the gradient descent method, the coefficient is set manually, and the value is updated by proceeding in the direction of the gradient (first derivative) by this value. Newton's method, on the other hand, uses the second derivative to automatically adjust what is called gradient descent. In other words, Newton's method can be regarded as a method with

[[[00000000000000001645---5f9a8326e04e0c87580188a380dd2804b1618ec6f9bb6ce6918897c634a4f7f5]]][†1] Equation (29.5) is written by changing (29.3) to , for comparison.

[[[00000000000000001646---8e3914894b45390729304afbfd288f996a230da30c631d822b1a2fcac4b5fe6e]]]Here, I explained Newton's method when the input of the function is 'scalar'. This theory naturally extends to the case where the inputs of the function are 'vectors'. The difference is that for vectors we use the gradient as the first derivative and the Hessian as the second derivative. For details, see 'Column: Newton's Method and Double Backprop Supplement'.

[[[00000000000000001647---f12ddee1d059cdf510119e41f2d8d33ce42bc1733f66ebe4ab242ac5b85f9976]]]To summarize, while gradient descent uses only information from the first derivative, Newton's method of optimization also uses information from the second derivative. Using a physics analogy, gradient descent uses only velocity information, while Newton's method uses both velocity and acceleration information. Newton's method can be expected to search efficiently with the added information of the second derivative, and as a result, the probability of arriving at the destination quickly increases.

[[[00000000000000001648---9b10c5916ebd3e5fd2e7c1fba04e4c8c38f2aa0399193ba76fccd53183108e3d]]]Let's use Newton's method to solve a specific problem. Here we are minimizing the expression The shape of this function has two 'dents', as shown in Figure 29-4. And that minimum is in two places: This time, with an initial value, we verify whether we can reach one of the minimum values.

[[[00000000000000001649---9f2949353d485e81e700b9b6c59267936f1539f701b59756f3c6707443e6fb8b]]]
The graph in Figure 29-4


[[[00000000000000001650---7cb8a8e64b556f34be4e35cf193d93e4fd235df8b9b06747ad53f5cb43c2d7e4]]]Implementation of optimization by Newton's method

[[[00000000000000001651---d14989f2089adf0c604e51a850f024c4e035336ff62f77db386fb4d11e9ad099]]]Now let's implement Newton's method. To implement Newton's method optimization, we simply implement equation (29.5). Unfortunately, our DeZero does not automatically calculate the second derivative. Therefore, here we will manually calculate the second derivative as follows.

[[[00000000000000001652---5cfce1a3e80ca014a6ee52137a26f0050f066bf1a13098aaae9865cf522394ed]]]Using this result, Newton's method optimization can be implemented as follows.

[[[00000000000000001653---dd2bed443021668dd28eef69a200f851cf134fdd145b66acd56abe03b99d03b3]]]As above, the first derivative is still obtained by backpropagation. And the second derivative is obtained by hand coding. After that, we will update x according to the update formula of Newton's method. Executing the above code will print the process of updating the value of x as follows.

[[[00000000000000001654---d18b7418e4879724c0672c40e8a99bebc05c5e0fcc2366b977de80810b3daa75]]]The answer (minimum value) for this question is 1. Looking at the results above, we can see that we have reached our destination in no time. In fact, it takes 7 updates to reach the minimum value. Gradient descent, on the other hand, requires many iterations to reach the optimal solution. A comparison of the update paths of the two techniques is shown in Figure 29-5.

[[[00000000000000001655---e0bdc897ed8b40668ff0bf76688f5426625ee43f6b2bcb386f9dc454f727e867]]]
Figure 29-5 Updated path by gradient descent (left figure) and updated path by optimization method using Newton's method (right figure)


[[[00000000000000001656---1fda5ea08d4fdda147c644df8494bce72ca3ffaa83df89412253eb2249eb4dd7]]]Gradient descent requires many updates, as shown in Figure 29-5. By the way, the result of the above gradient descent method is the result when the learning rate is set to 0.01. At this time, it took 124 updates until the absolute error of x = 1.0 became 0.001 or less. Newton's method, on the other hand, is only 7 times!

[[[00000000000000001657---6416da54df52715c4c6294510ac0b4c864db5742ac6b06e55a1f8e0415e33e02]]]The above is the theory and implementation of Newton's method. In this step, we implemented Newton's method optimization and solved a specific problem. And we were able to get really good results. But in that implementation, we found the second derivative by hand (we wrote down the formula by hand and hand-coded the result to find the second derivative). The next step, of course, is to automate that manual work.

[[[00000000000000001658---c3f861221a7f8d7f0e7b869d95e626e4514cdba75a96fbe68fdfda7a4bd16ef2]]]step 35

[[[00000000000000001659---d6af75263e6cf8ef9599bbc68db6a843f176d4ae89087e233c8a43e978de367e]]]Computational graph of higher derivatives

[[[00000000000000001660---8676fe6d1b4200975d6f4cb8ef3a03c02b18cb9aa1ec2dbaceb93379fcafec14]]]Following the previous step, add the DeZero function in this step as well. The function we add here is the tanh function. Tanh is called hyperbolic tangent or hyperbolic tangent in Japanese. The tanh function is expressed by Equation (35.1) and its shape is shown in Figure 35-1.

[[[00000000000000001661---6c2e2b7ab36bb999497e232fd1cbba6b72cdcec01ebdc2fc17e110631118f702]]]
Figure 35-1 Shape of the tanh Function


[[[00000000000000001662---a4ac08d15c86849fdda7e38c927b82c68fa729a92d89c52fdbe4c9a648053b40]]]The tanh function converts the input to values between and , as shown in Figure 35-1. First, find the derivative of equation (35.1).

[[[00000000000000001663---de587b246ddbd8acd40e5498045d4f646e8cdacad7bfac0de2c4a8023ac9080e]]]Differentiation of tanh function

[[[00000000000000001664---d3e7e73f3606c1aa9f6331561386ec71d0af8e10f486d098fb0b5b7fd27cc7e6]]]To find the derivative of the tanh function, use the following derivative formula.

[[[00000000000000001665---5ab3b3293a26c3b8fad56c4de268ad9999537f47c5e2eb6201a2c4aaba206c18]]]Equation (35.2) is the differential formula for fractional functions. Here, the derivative with respect to is expressed as , for clarity. At this time, with respect to Napier's number ( ), the differentiation of the function expressed by equation (35.1) can be obtained as follows.

[[[00000000000000001666---371d4cdba6e89374f316a9e7e77f03c8237d4a4fe8f40e0ef8fb20846cc2808e]]]As shown in Equation (35.3), the derivative of the tanh function can be obtained by simple formatting using 'differentiation of fractional functions'. And finally got the result.

[[[00000000000000001667---a675c8fa5637444969a513241efe39d6219bbc9d5a966b2210a0d83389e60180]]]tanh function implementation

[[[00000000000000001668---22cabc27169befbfa211e46a555d675588053d98e10f08e35c8b3e70b4660268]]]The derivative of the tanh function is when The Tanh class and tanh function can then be implemented as follows:

[[[00000000000000001669---254e636b793fdb3af1b695cd8406d9d83c29b3a0b88ae0824878dcdfd7745648]]]Forward propagation makes use of NumPy's np.tanh method. Backpropagation, on the other hand, implements gy * (1 - y * y) (which, of course, could also be written as gy * (1 - y ** 2)). The above is the implementation of DeZero's tanh function. Add this tanh function to dezero/functions.py for future use.

[[[00000000000000001670---dddca80dc59aa6e48b4ce83635e3bb55af01c0ae85a9ee5cf21a5e8e16b189b7]]]Visualization of Computational Graphs of Higher Derivatives

[[[00000000000000001671---4978e09d509be4b75a6f1ccc9778bc16c7d90e100a455a057662bfc248be14db]]]Now that we have DeZero's tanh function implemented, we can use it to do some interesting experiments. Specifically, we obtain higher-order derivatives of the tanh function and visualize its computational graph. Let's see what kind of computational graph we can create as we progress through the first derivative, second derivative, third derivative, and so on. The code for that experiment looks like this:

[[[00000000000000001672---e039f2f262863d50bc02e028508fd0afd9b52e5a0691a70abb48f5e9e639e7be]]]# draw computational graph

[[[00000000000000001673---ce6a50863ce2275bb9334019d3cccaf52cfee56daf9d070253c76960d6c3c3cc]]]This is almost identical to the code we've seen so far. Higher-order derivatives are obtained by repeatedly backpropagating in the for statement. Here, specify the number of iterations with the iters value. When iters=0, it is the first derivative, when iters=1, it is the second derivative, and so on. Then, visualize the computational graph done there.

[[[00000000000000001674---201520de06286e46b43531adfdd22550307f50d4c5d55a415f5164dd020d149a]]]To visualize the computational graph, use the plot_dot_graph function implemented in step 26. This function is located in dezero/utils.py.

[[[00000000000000001675---e37c08df4ce4450e7ee2398a854893bca766a702d943a06a1e4f1b95a45e94a5]]]Now let's run the code above. First, let's look at the computation graph when iters=0. The result is shown in Figure 35-2.

[[[00000000000000001676---4b07bf526fa7a785b7002fc7a7216a783515d6a3dade13111488c2888aca24f8]]]
Figure 35-2 Computation graph of the first derivative of y=tanh(x)


[[[00000000000000001677---09078d02338311d8051657164157cf23c135964ed0402d1310cf76e36cab597a]]]Figure 35-2 is a computational graph for obtaining the first derivative of y=tanh(x). You can see that Tanh, Mul, and Sub are used as DeZero functions. Then change the value of iters to get the 2nd derivative, the 3rd derivative, and so on. What kind of computational graphs can we create? The result looks like Figure 35-3.

[[[00000000000000001678---acec32ffd35ca902bef8274fe3957134bd1fed3fac65e68d0d8f3259c74a4ebc]]]
Figure 35-3 Computation graph of n-order differentiation (n=2, 3, 4, 5)


[[[00000000000000001679---ac00778bef0f8ef43791da0300491df8fe829d5c30dc48ed4746235d4071bfa2]]]As shown in Figure 35-3, the structure of the computational graph becomes more complicated as the number of derivatives increases from second to third, and so on. By performing backpropagation, a new computation graph is created for the previous computations, so the number of nodes increases exponentially. Looking at the figure, it feels like the computational graph is getting bigger, just like life is growing. Next, the results of the 6th and 7th derivatives are shown in Figure 35-4.

[[[00000000000000001680---08a94d2f43108ac5db002eafcfccc4e9c8ee88f99b280b911b24fd51e323a881]]]
Figure 35-4 Calculation graph of n-order differentiation (n=6, 7)


[[[00000000000000001681---e992a6581c8b11d48235504756f65eafa44c3267ec078c7a7462f2a742c615de]]]Figure 35-4 is a fairly complex computational graph. This complex computational graph is already nearly impossible for us to create. DeZero was indeed made by us, but DeZero created 'something beyond us'! There you will find the fun of computers and programming.

[[[00000000000000001682---ec65d36a8dc66cdf4db6aec968c5d72774359d4ac23213c8410422362cf3157b]]]More clear data for the computational graphs shown here can be found at https://github.com/oreilly-japan/deep-learning-from-scratch-3/tree/tanh. Please refer to it if you are interested.

[[[00000000000000001683---5696f53b443fe0011806d581ca093189ae9e38d4382f6cade018a5ea47da9c90]]]Finally, I would like to visualize the 8th derivative and finish the experiment in this step. The result should look like Figure 35-5.

[[[00000000000000001684---69c11e7aa3fbc75596f60efde0d8b08a97176276e7b0c40d7cdf2657fc1f7782]]]
Figure 35-5 Computation Graph for 8th Derivative


[[[00000000000000001685---d4056001706097861b075c61669e1af78f633f1cf3512ee64ecf0268d964e4dd]]]We now have a more complex computational graph, as shown in Figure 35-5. In this limited space, the node shape is too small to be seen. In order to give you an actual sense of how complicated the computational graph is, the next page shows an enlarged version of the area circled in red in Figure 35-5. This concludes the experiment for this step.

[[[00000000000000001686---41eb79ec1e0854e4c3c81d66c1f882356aeac4deb05582256117bd00ce6b2913]]]step 15

[[[00000000000000001687---cc856b143f406b3a68769581a953a96550eb5ecd51a6d284a76152d2896fc56a]]]Complex Computational Graph (Theory)

[[[00000000000000001688---caf7c299bc91d03219d6501e3cf8f1d1fe7fde07e36256cf4b7855ed1b68a7ae]]]So far we have been dealing with linear computational graphs like the one in Figure 15-1.

[[[00000000000000001689---cdc573e511d2d76575ac0c1a4a917e4c84d9790295eaae1cb4f2f03cd8e60da4]]]
Figure 15-1 Linear Computation Graph


[[[00000000000000001690---e193d9bf741baf9ab0f6b2d2503bf5fb88d794189a5eae57449c9fb16dd7cc45]]]However, the 'connection' of variables and functions is not limited to such a series of computational graphs. Our DeZero is making steady progress. With the current DeZero, for example, it is possible to create a computational graph as shown in Figure 15-2.

[[[00000000000000001691---a4580cecabfeaabbb6c8b5451e45098fbbb79d65797ed4099c6a404dcea1d625]]]
Figure 15-2 More Complex 'Connections' Example


[[[00000000000000001692---5c1a0f3e18747bd761e9d9b38a6e9d8df0a26cf3491bdb93147eef66627e6f10]]]You can calculate by using the same variable repeatedly or by using a function that inputs multiple variables, as shown in Figure 15-2. This allows you to create more complex 'connections'. Unfortunately, our DeZero cannot find the derivative of such a computation correctly. More precisely, it fails to backpropagate such complex 'connections' correctly.

[[[00000000000000001693---087bbc2b9646eb49051c635c50d939db86f782ee51f5ddf7ee95f2f962d7f570]]]The “connection” of graphs is called topology. The goal of this step is to support computational graphs with various topologies. We will introduce a new idea so that the differentiation can be obtained correctly regardless of the 'connection' of the computational graph.

[[[00000000000000001694---2d6f358c2f86fe2fc8f55957156b85a7e670821fda1c9f501f01dd4b077c47a7]]]Correct Order of Backpropagation

[[[00000000000000001695---d28b462d72230471154178104445d94ef27390a8548a1ac4ff340b286bc4f92c]]]What's wrong with our DeZero? To find out, consider the relatively simple computational graph depicted in Figure 15-3. The calculation graph also calculates wrong differentiation in the current DeZero.

[[[00000000000000001696---c6371e06bbf54a505009ece3c416b12f2187e66bfcd7d695b4a3f5aa0143383c]]]
Figure 15-3 Example of a computation graph that branches and merges in the middle


[[[00000000000000001697---9a7cf6c05d48422d9de9c0e521eefe1a297809f3b2e890fd47a50959c170be22]]]The point of interest in Figure 15-3 is the variable a in the middle of the calculation. As mentioned in the previous step, when we used the same variable repeatedly, the backpropagation needed to add up the derivatives coming from the output side. So to find the derivative of a (in mathematical terms) we need two derivatives propagating from the output side of a. After the two derivatives are propagated, we can propagate the derivative from a to x. With that in mind, the flow of backpropagation looks like Figure 15-4.

[[[00000000000000001698---4f8355da0cd8bf2b60130be372703d915a8fcfe1a81026efef76221321ad18ed]]]
Figure 15-4 Correct Order of Backpropagation


[[[00000000000000001699---4524b721f3cdf7a174df0206014bab0d38bfafb1b76d1fa0b05f74c0ac381550]]]Figure 15-4 shows the flow of the derivative propagating from variable y to x. Again, what I want to notice here is that after the two derivatives are propagated to the variable a, the derivative from a to x is propagated. At this time, from the point of view of the function, we are backpropagating in the order D, B, C, A. However, the order of B and C is arbitrary, and the order of D, C, B, and A is also correct. The trick is to finish backpropagating functions B and C before backpropagating function A.

[[[00000000000000001700---f787c014c71466c47e5776abaad67f308777a2e40db2b12e87413ec4d1eadf84]]]Current DeZero

[[[00000000000000001701---2daafb3fdcf8f7bd347ab9d7c7330359bf8c94a539016d8eb406c01ac0fb59ea]]]Next, let's take a look inside our DeZero. Is backpropagation performed in the order shown in Figure 15-4? By the way, the current Variable class is implemented as follows. Let's focus only on the shaded areas.

[[[00000000000000001702---bee7eaf2bdcf8de30b0beb09e5649b660d0a2f71cf4ec07f426e77aa8a529ede]]]The part I want to focus on here is the funcs list. In the while loop we append candidate functions to the end of the funcs list (funcs.append(x.creator)). Then, the next function to be processed is taken from the end of the list (funcs.pop()). Following this process, backpropagation will occur as shown in Figure 15-5.

[[[00000000000000001703---9f8c96b73ca02968f151419171a0b472a5b76b3b877ce73edcee2d2154a5b399]]]
Figure 15-5 Current flow of backpropagation in DeZero


[[[00000000000000001704---5b9c749ebbbb7055b7c1526788a1a9fe396fcb7668a1cc71eb479554c2999db7]]]The order of processing functions is D, C, A, B, A, as shown in Figure 15-5. The problem is that C is followed by A. It is also a problem that the backpropagation of function A is called twice. Let's see why such a problem occurs while comparing it with the previous code.

[[[00000000000000001705---a1ed248189b9bd19e098f69ea3c647833df838ecd15c86f0318fe2a092c804c2]]]First, D is added to the funcs list, and it starts from the [D] state. From there the function D is taken, then the parents of D's input variables (D.inputs), B and C, are added to the funcs list (Figure 15-6).

[[[00000000000000001706---3881234cecca6e6272a0906b98f5a41b726fedd2bb7f22e784da4d2fd5a857da]]]
Figure 15-6 Backpropagation of function D (code for funcs list shown on the right)


[[[00000000000000001707---ec3c057e7c30b5eaa10f70c95893cdd64fc96eae23cb8b70dff965335d953827]]]At this point the funcs list is [B, C]. Then the C at the end of the list is taken. Then A, the parent of C's input variable, is added to the list. At this point, the funcs list is [B, A] (Figure 15-7).

[[[00000000000000001708---5bd09597c8898fc15984eef15dbe3a037fc8015a00878f45bbf02f9e18247d03]]]
Figure 15-7 Backpropagation of Function C and Corresponding Code


[[[00000000000000001709---adea6e8fa988c5036fa95e6d9704de60469618f1acfd8674ab9a3a085854dc18]]]Then the last A is taken out. This is where the problem lies. Originally, I took out A when I should have taken out B here.

[[[00000000000000001710---d4298b56660489e9d5e3bd9e65650eaa431fb154831f1804f04d831cb34f61ae]]]So far, we've been dealing with linear computational graphs. In such cases, retrieving elements from the list can be done without regard to the order in which the functions process them. Because when I took it out of the list, it always contained only one element.

[[[00000000000000001711---cd2dbc634292e39e8dc92fece09ec9fcb3e00b8e2fc46841ad64d9a6c76d3ad0]]]function priority

[[[00000000000000001712---281f07f84230c560be5a142e36f0469d5f22670bfc9f399a656c977b8082766e]]]The funcs list contains candidates for the next function to process. But now it's (thoughtlessly) picking out the 'last' element of the candidate list. Of course, here we are expected to pick the appropriate function from the funcs list. In the previous example, from the list of [B, A], we want to extract B, which is positioned more toward the output side. One way to solve this is to give functions a 'priority'. For example, if B has a higher priority than A, B can be retrieved first accordingly.

[[[00000000000000001713---6504d57ca206abfb6fc824a869b838a8b3efa17f27eaaacf1a9c85c1e432186a]]]So how do we set the priority? One way is to 'parse' and examine a given computational graph. For example, if you use a type of algorithm called 'topological sort', you can 'sort' nodes based on how they are connected. The order of the 'order' is the priority. But there are easier ways without resorting to such algorithms. In fact, we have already 'witnessed' the answer.

[[[00000000000000001714---b3fff9a853c8bd2661fc3c3b9843b06c2d0b8046e7c4dea0f96de0f30febd015]]]We 'witness' the process of 'functions' producing 'variables' when performing ordinary computations -- forward propagation. So you're already seeing the 'parent-child' relationship of which function produced which variable. That relationship allows us to record 'generations' of functions and variables, for example, as shown in Figure 15-8.

[[[00000000000000001715---8dbf6f05dd9f9aafa17891cd8a10e4bd97d3eee908e7b9d4bba95c513f4fb4ef]]]
Figure 15-8 'Generations' of Functions and Variables in Forward Propagation


[[[00000000000000001716---0e25f63f78b283bc7f615f90fcd3f9922d6eb3158208edb1ef9424fb1e48c2a5]]]The 'generations' in Figure 15-8 just correspond to priorities. During backpropagation, processing the higher number of generations first ensures that the 'children' are retrieved before the 'parents'. In the example of Figure 15-8, when choosing between functions B and A, we can pick up B first because it has the largest number of generations. That's how you do the backpropagation in the correct order. The next step is to implement it.

[[[00000000000000001717---8688b760d0105b91173aecef6afabb9f9f662636cf0465fc13e0ca43a276bedf]]]step 38

[[[00000000000000001718---e552fec42688fe47266529c051cb79e35bb47ee4691d51d27ffb489497920076]]]functions that change shape

[[[00000000000000001719---28e06d4f3d898f281b4da4fabb9c2a68ed7a0cc3321e2cde7fd2f49b818523aa]]]In the previous step, we discussed backpropagation in computations with tensors. Here's what I learned:

[[[00000000000000001720---b18fabad4701023756ee66935c2dc91e2a9f6eeca0891e79c2fd7bb44f5ecfba]]]Functions that perform element-wise operations, such as the add and sin functions, can implement forward and backpropagation assuming the input and output data are scalars.

[[[00000000000000001721---e6f1fb208edf4a38bdac4ec0aed28be8b5d57ff9d8e073407df7c8222d41f417]]]In that case, backpropagation holds correctly even if you input a tensor

[[[00000000000000001722---faeffbcb02141afedf98368c670a5a4b74c2feee12b8887261ef37eec416b66b]]]We will now look at functions that do not perform element-wise calculations. Here, as a first step, we implement two functions. The first is the reshape function, which reshapes the tensor. Another is the transpose function, which transposes a matrix. These two functions are functions that change the shape of the tensor.

[[[00000000000000001723---b7d8560b49f2104dc3dc146404927c66e454352f51e1a4df84b3e903e5d9defa]]]Implementing the reshape function

[[[00000000000000001724---d6ff95bf2b73b56fe79b34b6e329499da276879fba06f5434022d9fce53ee960]]]Now implement a function to reshape the tensor. First, as a preparation, let's check how to use NumPy's reshape function. This converts x to the shape of shape by writing np.reshape(x, shape). An example of its use would look like this:

[[[00000000000000001725---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001726---4c280dd16ff618fa09901323ee85204a82c1cfe4eb92707006c9a9be3fa7b814]]]Here we transformed the shape of x from (2, 3) to (6,). The tensors have the same number of elements, only their shape changes. Now let's implement the DeZero version of the reshape function. The problem here is how to implement that backpropagation.

[[[00000000000000001727---ce61a03736e96328cf70f1f19871899944cbab15a8622fa3e63de1cbe57a2988]]]For functions that do not perform element-wise computation, the backpropagation implementation becomes clear when considering the shape of the tensor. Specifically, we check that the data in the variables and the shapes of the gradients match. For example, when x is a Variable instance, implement backpropagation to satisfy x.data.shape == x.grad.shape.

[[[00000000000000001728---21b860796e588484ea2946a3780500bc1770b8e3f6ebbb11011daf1e29160d35]]]The reshape function simply transforms the shape. That is, no concrete calculations are performed there. Therefore, in backpropagation, the gradient transmitted from the output side is passed to the input side without any modification. However, transform it so that the shape of its gradient matches the shape of the input, as shown in Figure 38-1.

[[[00000000000000001729---ead59ffdae003970526cdf79277601a2486ca4b7abefe7235cc9d261356f80f3]]]
Figure 38-1 Computation graph for forward and backpropagation of the reshape function (a function that performs backpropagation is denoted by reshape' and uses dummy gradients of (a, b, c, d, e, f))


[[[00000000000000001730---5e7ae8d2997f2c136e10fe4922637550082e77972c6daf21809f9301a8adac35]]]In Figure 38-1, backpropagation propagates the gradient from the output side. Transform that gradient so that x.data.shape and x.grad.shape match. Specifically, the gradient of shape (6,) is transformed to shape (2, 3). In other words, it shapes to the shape of the input variable. This is the backpropagation of the reshape function. Based on the above points, we will implement the reshape function for DeZero.

[[[00000000000000001731---88b21740e387dabc191c0adc0e83f4320871a253b805c57254282afceb365024]]]First, when the Reshape class is initialized, it receives the shape to be transformed as shape. And in the forward method of forward propagation, we use NumPy's reshape function to transform the shape. Then remember the shape of the input x as self.x_shape = x.shape. Then you can convert it to the input shape (self.x_shape) with the backward method of backpropagation.

[[[00000000000000001732---04f63218923256a44af40a16cc30b37949afe0cc9924b772f3656a0a586f5475]]]gy in the argument of backward(gy) is a Variable instance. Therefore, calculations in backward(gy) must be calculated using DeZero functions for Variable instances. To do that, we use the reshape function that we're implementing right now.

[[[00000000000000001733---33ff2cd5ce8f46e8420d87032d4465ffa7a987ac871f0619660623e34511d353]]]Then implement the reshape function as follows:

[[[00000000000000001734---df9d5904ec48ce27bd700e8fb5c17285ae809c42f11d17023e09fcb2524683ee]]]Here the argument x is assumed to be either an ndarray instance or a Variable instance. If x.shape == shape, do nothing and return x. However, to ensure that the reshape function returns a Variable instance, we use the as_variable function to convert it to a Variable instance. By the way, the as_variable function was implemented in step 21. as_variable(x) converts x to a Variable instance and returns it when x is an ndarray instance, and returns x as it is when x is a Variable instance.

[[[00000000000000001735---55b127598f5a6d24d5356d3f92da2eb905db497871b3d5f2cea60d41c0477bae]]]The input to the DeZero function should be a Variable or an ndarray instance. And the output is a specification that returns a Variable instance. If the function inherits the Function class (for example, Reshape), the ndarray instance is automatically converted to a Variable instance in the __call__ method of that function class.

[[[00000000000000001736---3b41574b65f473e7ca089ccdafd57ab5c6e28e129b5ddc7eb12e3923b6d901d1]]]Now let's use the reshape function we just implemented.

[[[00000000000000001737---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001738---83e947a8628618b4c39f2bb2ecdd1f7836514e18b06c1fd42ebfca3bec1b9377]]]As above, we reshape the shape using the reshape function. Then call y.backward(retain_grad=True) to find the gradient of x. At this time, the gradient of y is automatically interpolated. The imputed gradient is a tensor with the same shape as y (y.grad.shape == y.shape) and whose elements are all ones. Now let's see what kind of data actually flows. The result should look like Figure 38-2.

[[[00000000000000001739---c095d7d930208037c51f753a64459712fc03775e2dfc6b281d8224daf0d10911]]]
Figure 38-2 Calculation example using the reshape function


[[[00000000000000001740---cf77077444b048177560dc78008b50cca05289030bb2d0f7d328514610d40f0f]]]Forward propagation changes the shape of the tensor from (2, 3) to (6,), as shown in Figure 38-2. And in backpropagation the shape of the gradient changes from (6,) to (2,3) in the opposite direction to forward propagation. At this time, you can confirm that the data and grad shapes of each variable match. The above is the implementation of DeZero's reshape function. Next, we'll add some tricks to make this function more convenient to use.

[[[00000000000000001741---cb01b394d2df0f29ce5da971cc15878f950d8d72864c6482786ad58e4ae51f56]]]Using reshape from Variable

[[[00000000000000001742---1e3e4df6b6f881863dee06f7b5fdc556d9d1b51e6c0c0f9dd9a60f15ce05c6ab]]]Our next goal is to bring DeZero's reshape function closer to NumPy's. With NumPy, you can also use:

[[[00000000000000001743---d25cae7b1e8bea2c4e7d5a5d04982e5f8ed4c5615a6cd72649d09be504269150]]]# give a tuple

[[[00000000000000001744---3f3733f159d66531eecc06262412028fd75c23ccfab99923716692ca2c91bd7a]]]# give a list

[[[00000000000000001745---99cc09126a0a2a11d32d22fa9c76fe2aabba54a34be8e5cb349255d0113a14ad]]]# Give arguments as they are (expanded)

[[[00000000000000001746---d5cf827b9a8222d7be17405207f4699fe1b482252b662c32b2ae7542374d7169]]]As above, you can use it as a method of ndarray instance. You can also give variable length arguments as arguments like x.reshape(2, 3). We would like to realize this kind of usage with our DeZero. To do so, add the following code to the Variable class†1.

[[[00000000000000001747---1a6617a1c8c9348cade6b23141274f3d7f4d5c33be0c515165c0315e8d4ae22f]]][†1] Here we use dezero.functions.reshape instead of F.reshape to avoid circular imports.

[[[00000000000000001748---9947899b278753f82d2dc4f357cf85d73f5fe7c1d4cf5047b76d61ef0f9963a7]]]Implement reshape as a method of the Variable class as shown above. Its arguments are received as 'variable length arguments'. It then calls DeZero's reshape function that we implemented earlier, adjusting the given arguments. Now you can use it like this:

[[[00000000000000001749---98e097f160c78ceed09745cf3183ec963f927954a9c77bb1abccbd1da379faa0]]]As above, you can call the reshape function as a method on your Variable instance. This makes it easier to reshape Variables. This completes the implementation of the reshape function.

[[[00000000000000001750---83b8a04721cf75cccf6967960fa68fddca12c6443335d15f1703f6a4d329b66c]]]matrix transpose

[[[00000000000000001751---a414d234039361b81dd39eb95ae666a8c804a79035539fac831a7ec2639fd9ac]]]Next, implement a function that transposes the matrix. Transposing a matrix transforms the shape of the matrix as shown in Figure 38-3.

[[[00000000000000001752---67e4aef99339f329c433bd294702c3f74744e8a15c9f8b963eb6cfd03be356f5]]]
Figure 38-3 Matrix Transpose Example


[[[00000000000000001753---d994923c8a10ca8cb4c0e60facd94cfd8dbd1c5cbe448889aa9b450a77e36b9e]]]Transposing changes the shape of the matrix, as shown in Figure 38-3. From now on, we will implement this transposition as a function of DeZero.

[[[00000000000000001754---5ed1da0d059ac19620a4ff39ca157a35dd8d7077eaea07778e0b05c281632a88]]]Here, we will implement the transpose function only when the input variable is a matrix (second order tensor). The actual DeZero's transpose function has a more general implementation, allowing the axis data to be transposed. This point will be explained at the end of this step.

[[[00000000000000001755---d3bd9d45272e39f1234242a225bc27b2fc50023df16f8abe4ae1f30f64388b19]]]Now, in NumPy you can use the transpose function to do the transposition. For example:

[[[00000000000000001756---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001757---b791b9ce5482fbab65f2bdc3e22a68716bf681ef00b40e56dae7c413348cf672]]]Here the shape of x is transformed from (2, 3) to (3, 2). The elements of the tensor themselves do not change, only their shape. So its backpropagation only changes the shape of the gradient coming from the output side. The way to change the shape is exactly the 'reverse' transformation of forward propagation. With that in mind, DeZero's transpose function can be implemented as follows:

[[[00000000000000001758---9e50c54081a89c12d1226f9df4f491676b82b6217c0471e0319ad6cc1ff9b184]]]As above, forward propagation uses the np.transpose function to transpose. Backpropagation transforms the gradient from the output side using the transpose function we are just implementing. Backpropagation now does the “backward” transformation from forwardpropagation. Now let's use this transpose function.

[[[00000000000000001759---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001760---86c3fcce0f06881058d50fb4954e92d85b57536ec58580ca7bf79a657611383c]]]As you can see above, it can be computed using the transpose function, and backpropagation can be performed correctly. Next, add the following code so that the transpose function can also be used from the Variable instance.

[[[00000000000000001761---a0ea3f063a37773a8b48668c6574fe704c0af3d69aa6adcbe339d39dc113bb86]]]Here we add two methods. The first is the code to use as the transpose method. And the second method is available as an instance variable by adding @property. Now you can use it like this:

[[[00000000000000001762---c8bcc846c9df21c67cea7bf9995dc7906930981c30798d765e10370d4da1a8c7]]]The above is the implementation of the transpose function that transposes. Here we have described a simple transpose function implementation limited to 'matrices'. The actual DeZero transpose function is a bit more code than the implementation shown here. I will supplement that point.

[[[00000000000000001763---fcaecbb1ba87895ac43a03ba554c65cf779865ef343dcee81d0a33a9f506f094]]][Supplement] Actual transpose function

[[[00000000000000001764---4eaddec2c5dd1b1258d0df69fd76235be42127f56fcd34aba4dcd7dcfb83c600]]]NumPy's np.transpose function has a more versatile use. It is an operation to change the order of data on the axis. Here's a working example:

[[[00000000000000001765---f8e230788dfe47e7b953931299b18e3da2885dc2ceb07cf9651fdd2060e69147]]]Here we have data with shape (A, B, C, D), and we transpose the axes of that shape using the np.transpose function (for simplicity, the shape values are variables such as A use). The argument given here is the order of the axes after transformation. This will become clearer if we look at Figure 38-4.

[[[00000000000000001766---8988ee71ae2daaba8e34d8dc0e1efa59e9614678311a9ccacc016933ffbd5096]]]
Figure 38-4 Concrete example of the np.transpose function


[[[00000000000000001767---7d21cb2e2cf3d91e04a47a91322d27efcaa3dca141f0b7726d3332c2be97cccb]]]As above, if you specify the order of the axes, the data axes will be swapped accordingly. If the argument is None here, the axes are sorted in reverse order. The default argument is None. Therefore, when x is a matrix, the data on the 0th and 1st axes are arranged in the order of the 1st and 0th axes by x.transpose(), so the matrix is just transposed. Become.

[[[00000000000000001768---de9abdd10ab61fa9df0ceb36b4c39aadcd23e41e90bf2e4a335231cfd6cd4016]]]DeZero's transpose function is also implemented so that the data on this axis can be transposed. Backpropagation simply swaps the “backwards” axes. I won't show that code here. If you're interested, check out the Transpose class code in dezero/functions.py.

[[[00000000000000001769---3eb347b7204b198dc1967c91715f7b9de1b87174ae65a3ba507c932e045d2364]]]step 32

[[[00000000000000001770---e93bed5aa4cfab3881c3f41ed2be0bf21fc2d4d6e6a028d48abe7bbad4b35c9e]]]Higher Order Differentiation (Implementation)

[[[00000000000000001771---334a17692a82d6444485b24f04ef45315440f0bb469ce1f903f05e609de7a044]]]In this step, we will modify DeZero so that higher-order differentiation can be achieved. The thing to do is to create a computation graph for the backpropagation computation as described in the previous step. For that reason, the calculations performed during backpropagation use Variable instances.

[[[00000000000000001772---18df1ef05cadec32141e95afc5fe1fe5c38b382c578362905958e941a5383658]]]We have so far implemented the Variable class in dezero/core_simple.py. Here we implement a new Variable class to replace them in dezero/core.py. In addition, dezero/core_simple.py also implemented overloading of functions and operators such as arithmetic operations. Those implementations are also newly done in dezero/core.py.

[[[00000000000000001773---b75e511e79c4d7c480290048df018327b312d75158391bb83f1dd1c35e63c4f5]]]To the new DeZero

[[[00000000000000001774---370ee72a24c1ec410570e1b9d774161204269afd15901ea328bec7b1b4e87ad1]]]The most important change to the new DeZero is grad, an instance variable of the Variable class. Until now that grad referred to an ndarray instance. In the new DeZero change it to refer to a Variable instance. Along with that, change the following parts of the Variable class.

[[[00000000000000001775---18c19a4d53889235a90d112eff8018c74549b5d6b9af529a3ad58409625ef62e]]]As you can see above, there is only one change. It modifies self.grad to be a Variable instance where it autocompletes the differentiation. This completes the modification of the Variable class.

[[[00000000000000001776---813a34f39537181fab2840448ad7119d3b4f9e819f4465e41a9383a33029e70d]]]Backpropagation of function classes

[[[00000000000000001777---72fac9ac9c69ce4111abf59c705a6381a504942338b9c177f934811b4667cd86]]]The only remaining task is to fix DeZero's backward methods for concrete functions (no fix for the Function class). So far we have implemented the following classes as DeZero functions in the dezero/core_simple.py file.

[[[00000000000000001778---181e62d8c515f15fad2b18ac6019892c083f40f600fc584214069191bac5c61c]]]Here, we will modify the backward method of the above class. and add them to dezero/core.py. Let's start with the Add class, but in fact there is no change in the Add class. By the way, the implementation of the Add class looks like this:

[[[00000000000000001779---4e91088825e4f3b464c0442ae0e79ceb871f4bba44c8d951c8f1b85341c0ecd9]]]In backpropagation of the Add class, only the derivative transmitted from the output side flows to the input side. Backpropagation doesn't do any calculations, so there's nothing to fix.

[[[00000000000000001780---4649590ee928484911db5265b01fb5c038116aab492c1b88b9feb4f888bda938]]]Next is the Mul class. Modify the backward method of the Mul class as follows.

[[[00000000000000001781---9efabd166d295aee2b01f02533728c8cf6126679249cce16fc95561ba27c068b]]]
Figure 32-1 Comparison of backward methods in Mul class (left: old code, right: new code)


[[[00000000000000001782---6726dbc90666b7238d108c9ea8a97f4dd8e03c0cde3a675382b73ca5b9623fe4]]]As shown in Figure 32-1, until now you had to retrieve the data (ndarray instance) contained within a Variable instance. On the other hand, the new Mul class uses Variable instances as is.

[[[00000000000000001783---0267b95ddb591e99245cbba52d127b5743d570ee61114b6d21595bb5bdb472ad]]]What I want you to notice in Figure 32-1 is the code gy * x1 that computes the backpropagation. Again, in the new DeZero, gy and x1 are Variable instances. We have already implemented the '* operator' overload for the Variable class. So behind the scenes where gy*x1 is executed, forward propagation of the Mul class is called. Then Function.__call__() is called and the computation graph is built in it.

[[[00000000000000001784---0a1ac51181a0c46957632e49026e219a969e8e07b4a033d530a2399cc5054363]]]Backpropagation calculations are performed on Variable instances. Therefore, it is necessary to calculate using the DeZero function for the Variable instance.

[[[00000000000000001785---2cf5904c8f286ac37894de90103274e40c19507bf10591572099ca060445c406]]]After that, modify the backward methods of the Neg, Sub, Div, and Pow classes in the same way. Since it is the same as the modification described here, it will not be described in this document.

[[[00000000000000001786---55e76c4ad3bdacb976e2968e03d74064f594102e0b6edb56fa2ef5679265cf2c]]]Towards more efficient backpropagation (addition of modes)

[[[00000000000000001787---d4f2b08cca28632ff12fac6e52233e631ea6a7a0b7bf11880aa3ac9dbd3be626]]]We introduced backpropagation enabled/disabled mode in step 18. Specifically, when backpropagation is unnecessary, it switches to 'backpropagation disabled mode' to omit the processing for backpropagation (creation of computational graph, retention of input variables, etc.). Here we use the same mechanism for the computations we do in backpropagation. In other words, for a computation that is backpropagated, if there is no further backpropagation after that -- if it is backpropagated only once -- then the backpropagation computation is an 'invalid mode of backpropagation'. to run it. To implement this mechanism, add the following code to the backward method of the Variable class.

[[[00000000000000001788---7d7db541472244d160f5e6b4c27f80d451fa9153f53df66914d2e3bff92f57c7]]]# main backward

[[[00000000000000001789---bc05d3174eb0032144f7f7f25fd06b398411d24a0794e7616c6c1c8edb99d6d6]]]# This calculation also applies

[[[00000000000000001790---d0c22877f5815295864620dcb0a880d70bcddb0b130028af80d6cc00f07a73bc]]]First, add create_graph to the arguments and set the default value to False. Then, the actual backpropagation is done inside with using_config(...) (the usage of the using_config function was explained in step 18, so I won't go into it here). This means that when create_graph is False, the computations done in backprop are done in 'backprop disabled mode'.

[[[00000000000000001791---a9c9e2105fb7eb218991b8c62caa2b65675d221b6fc27031af290b83c8db5ab3]]]Since the story is getting a little complicated, I will give a concrete example to supplement it. For example, when backpropagating the Mul class, its backward method computes gy * x1. Here the '* operator' is overloaded, so the code Mul()(gy, x1) is actually called, which in turn calls its parent class, Function.__call__(). In this Function.__call__ method, Config.enable_backprop is referenced to switch backpropagation enabled/disabled mode.

[[[00000000000000001792---18dc799f3d53a4b4d4bd76d9c0670ae79ddfbec002add8380a5e756849c47df6]]]The reason why create_graph=False is the default setting is that in practice, backpropagation is performed only once in overwhelming cases. Set the argument to True if you want, for example, the second derivative. In that case, the computation of backpropagation creates a new computational graph, so backpropagation can be continued.

[[[00000000000000001793---4e293e4d9890e39d1e563b5e9032147de293283a01ea71bba6aac9e098fbc763]]]Modifying __init__.py

[[[00000000000000001794---85fbe3e26d7a70864ab45380f87b14489d99eed30450b0b7346f310a35540b61]]]Now we have the core functionality of the new DeZero. The modifications made so far will be newly implemented in dezero/core.py. And from now on we will use dezero/core.py instead of dezero/core_simple.py. Therefore, change dezero/__init__.py, which performs initialization, as follows.

[[[00000000000000001795---f448460a546ae8bdef672816c97b94a7cffc8abf9772b4bb5c5fc5c53c3620bc]]]# use simple_core from step23.py to step32.py

[[[00000000000000001796---352f2c3b92810bbc30ae9e98c07ab4589c68f94b0cd24fea7a05cbd933eadf76]]]Change is_simple_core to False so that it is imported from dezero/core.py as above. Now, the core file corresponding to higher order differentiation will be imported.

[[[00000000000000001797---672311b1df0b50b2d2187f1f9c067684a6e82ba926658a55561666b861d07b2d]]]Dezero/core.py also copies unchanged classes and functions from core_simple.py. For example, the Function class and the using_config function.

[[[00000000000000001798---2864b54a712bdc1505e4b0f5159b13e4f1ae8e9e0c48114904ea63b0d032e385]]]This is the end of this step. In the next step, we will use the new DeZero to automatically find higher derivatives.

[[[00000000000000001799---8cf4ad8f4843637605dc9a116df07e84bf863d1d6a443aad61a31006ece6202f]]]step 34

[[[00000000000000001800---4819dbf57cbea84a37a1adbcaa7262b8fcf4c7ab4efb9913c904bd32c5b0e6b0]]]Higher derivative of sin function

[[[00000000000000001801---43a7e1a06135a908eee618b12f9b4a4f573c34774b1272a215710e91f416f007]]]We have so far implemented several functions that support higher derivatives. Right now they are all implemented in dezero/core.py (specifically the Add, Mul, Neg, Sub, Div and Pow classes). Here we will implement some new DeZero functions.

[[[00000000000000001802---98b70d003685500ee4e7ff3f1b5d45b15c80ea081e868571e0d049920468ca2c]]]From now on, we will add DeZero functions to dezero/functions.py. Then you can import DeZero's functions from other files like from dezero.functions import sin.

[[[00000000000000001803---fd2694943bac0698577365c6bea6f872cfb3692c78e8eed36ae428878f304170]]]Implementation of sin function

[[[00000000000000001804---8ebf89f9c480a9e28cd39a451ffead699647d4d3464b1ae7df5f6102db9086e2]]]First, we implement a new Sin class that supports higher-order derivatives. If you check it with a formula, it will be when . So the Sin class and the sin function can be implemented like this:

[[[00000000000000001805---b155d9be611fa899bf89d8759a5b81b57de55b3fae9368e8b910fc6091caabb8]]]Let's focus on the implementation of the backward method here. In particular, all variables in the backward method are Variable instances (while the variables in the forward method are ndarray instances). So cos(x) in the code above is DeZero's cos function. This means that to implement a Sin class, we need a Cos class and a cos function.

[[[00000000000000001806---14005761c035d110e2494cd8e6a1eed56ccf7da5f532fe451291bc1275f6d4da]]]Also, in the implementation of the backward method, all calculations should be done using DeZero functions. If you don't have a DeZero function, you'll have to implement that function anew. The multiplication of gy * cos(x) in the code above calls DeZero's mul function via operator overloading.

[[[00000000000000001807---8001abc673c26fad2f6e7fc4d09a0ba2b54f2fc726889043c8666f159bf72ffa]]]The code for the Sin class in dezero/functions.py has some differences from the code above. In the actual code, code is added for GPU support. In addition, the code for GPU support will be omitted in the same way for the code of functions that will be posted hereafter. GPU support is done in step 52.

[[[00000000000000001808---bdbb8254014fbe00700b6260f8a204163a259015afe91e23fc2bb49c75c65308]]]Implementation of the cos function

[[[00000000000000001809---79b666b63502e72a6c31cd1c929aa9e0360cc6680fed934b2b0ad66d72fedd7e]]]Next, implement the Cos class and cos function. In the formula, when . Now you can implement it like this:

[[[00000000000000001810---8513c330bc5828c9237f35e2d6fd66ef988d84da95a4bf5adcf5e68d36b1890b]]]Also of note here is the code for the backward method. The specific calculations performed there use the sine function. Luckily, we just implemented this sin function. Now we have the implementation of DeZero's sin and cos functions.

[[[00000000000000001811---4819dbf57cbea84a37a1adbcaa7262b8fcf4c7ab4efb9913c904bd32c5b0e6b0]]]Higher derivative of sin function

[[[00000000000000001812---d6a64b5c5a661b5933caa35d874f2cced9b10159edb3932fcfd64e01f4cfd79a]]]Now let's find the higher derivative of the sine function. This time, I will try to obtain not only the second derivative, but also the third derivative and the fourth derivative. The code looks like this:

[[[00000000000000001813---5aebcd4c8d9f6776f71da722798f5649720b2a422f0c36ecf0f3d548e640a9cb]]]# nth derivative

[[[00000000000000001814---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001815---6259c1ad37c7b6af667b6f44dde4d33ebdc4ca2c1fdcd79dae05b86da1f08084]]]Here, the for statement is used to iterate backpropagation. It gives the 2nd derivative, the 3rd derivative, ... and the 'nth derivative'. The code inside that for statement is business as usual. Specifically, take the derivative with gx = x.grad and backpropagate from gx. Then call x.cleargrad() to reset the derivative before doing the backpropagation. By repeating this work, the n-th order derivative can be obtained.

[[[00000000000000001816---a9d3fa892bcfed640a6b3819e2f91e371c90f875b3c9cb992b02268e3be82c59]]]In the code above, we import dezero.functions as F. By doing this, it can be used like F.sin() and F.cos(). In the future, various functions will be added to dezero/functions.py, so usage like F.xxx() will be convenient.

[[[00000000000000001817---e7bc7ed7d9b1836d198b66a42e2aa16daab3d127a8f745feed918e8931e4f7f0]]]Next, let's extend the above code a little and draw a graph. Here is the code:

[[[00000000000000001818---551fbf6def7d02d0e96307ddaee805a7a29d2bfc387c6d3f186b9ce52912d633]]]# draw the graph

[[[00000000000000001819---34d025171d585dabeed071ae8c990c197e63463f8c9f89a30c98290b3cb6bc99]]]The main difference from the previous code is that the input variable is x = Variable(np.linspace(-7, 7, 200)). This np.linspace(-7, 7, 200) creates an array that evenly divides -7 to 7 into 200 pieces. Specifically, it will be a one-dimensional array of [-7., -6.92964824, -6.85929648, ..., 7.]. In this code, we wrap this one-dimensional array with Variable.

[[[00000000000000001820---5714c5a571aa500c837c881ee2e714e75b46deb16b8f5cb0c6daf139d1f85d1c]]]The code for finding higher derivatives is exactly the same as before, except that the input variables are now one-dimensional arrays. And the DeZero functions we've implemented so far perform calculations independently for each element of a multidimensional array. Therefore, 200 elements can be calculated together in one calculation (forward propagation).

[[[00000000000000001821---19c010ca3317e41046c390912228dad8ed3f95d534e1fd4f21f7e05a0e44d70f]]]Many of NumPy's functions compute independently element by element when given a multidimensional array. The DeZero function uses NumPy functions to perform calculations on ndarray instances during forward propagation. So when you give DeZero's function a multi-dimensional array, it does the calculation element by element.

[[[00000000000000001822---5b21ca2d96ef55eca4440a2aa8f103fdeb3dad7e35478b33c37346b1030f3782]]]Now let's run the code above. Then you get the result in Figure 34-1.

[[[00000000000000001823---fb2055613c500cf537317df41fa7b7f3fef88d2c9f952fcb5e5da87500e4eaef]]]
Figure 34-1 Graph of y=sin(x) and Higher Derivatives (labeled 'y'' corresponds to first derivative, 'y''' to second derivative, and 'y'''' to third derivative)


[[[00000000000000001824---d174c616a4c7dcccc7e6305d647f0677c9e15114b4f22ccb2fdc2bcc8ba28bf6]]]We get a graph of y=sin(x) and its higher derivatives, as shown in Figure 34-1. Their graphs are functions with the waves 'out of phase'. y=sin(x) → y=cos(x) → y=-sin(x) → y=-cos(x ).

[[[00000000000000001825---395195b33a91e5b672e897fe7b5a3b6a7d7b81507a464ac2b2a920ea092a1e5d]]]This is the end of this step. In this step, we newly implemented the sine and cos functions of DeZero. We will continue to add new DeZero functions in the next step.

[[[00000000000000001826---16ece5c99ef2d04a3a8fb06d68224d95f42b71b3b66aaa60c93ec955200a90c8]]]step 21

[[[00000000000000001827---12b36415b66f34ef44e5c8e6ff548085d9079007f7b90259d71ee34b5ca01da5]]]operator overloading (2)

[[[00000000000000001828---525036f559ba3fe44fa08bac807655d7056aa594b9ffe0f4c581231799e907ff]]]Our DeZero just got more and more useful. Now you can write code like a * b or a + b where a and b are Variable instances. But unfortunately you can't use it with ndarray instances like a * np.array(2.0). Also, it cannot be used in combination with numeric data, such as 3 + b. DeZero will be even more useful if you can use ndarray instances and numeric data together. In this step, we will extend it so that Variable instances and ndarray instances, as well as ints and floats, can be used together.

[[[00000000000000001829---45c20d7b0c50f2295edda9c26871e68b93020938ea536546c8eefe2df22548cc]]]use with ndarray

[[[00000000000000001830---42dc761b7a4d8d34555688ffe0b15bae445fdd3dbb176e1733a14836354bebd4]]]First, let's make Variable usable with ndarray instances. The strategy for doing so is simple. For example, if a is a Variable instance and you run the code a * np.array(2.0), it converts the ndarray instance to a Variable instance (invisible to the user). In other words, once converted to Variable(np.array(2.0)), the rest of the calculation is the same as before.

[[[00000000000000001831---d512d5310bc9bb5daf543f71926ad1b1a5f4eb7abc7ff994c89ad81c23cad16b]]]As a preparation for that, we prepare a convenience function called as_variable. This function converts the object given as an argument to a Variable instance. Here's its implementation:

[[[00000000000000001832---2c1015d4e87ec5102b8737ff7c6143ba4c6dfcd74b26a52eed58d9d24881dded]]]Here, the argument obj is assumed to be either a Variable instance or an ndarray instance. If obj is a Variable instance, it is returned without modification. Otherwise, convert it to a Variable instance and return it.

[[[00000000000000001833---5884692bd5d174059fece2ea9aaf10040949e583ec89d6707dcb39b37c482464]]]Now, using the as_variable function, add the following shaded code to the beginning of the __call__ method of the Function class.

[[[00000000000000001834---b30b40f96cbd3b5d7ec06dc4399ad70dbea65485b35ee4d7c4a04cb3618dbe73]]]As above, for inputs given as arguments, convert each element x to a Variable instance. With this, if an ndarray instance is given, it will be converted to a Variable instance. After that, all variables will be Variable instances.

[[[00000000000000001835---f25761608d0d6ec502034efb7d304db2b4992f19bfeb658a894d5a56625ac8bd]]]All functions (operations) used in DeZero inherit the Function class. Then, when the actual operation is performed, the __call__ method of the Function class is called. Therefore, if you modify the __call__ method of the Function class as above, the modification will be applied to all functions used in DeZero.

[[[00000000000000001836---c577ef2086e6630942a317ae2dc5915a626e93ce32c3d4621f12a3c93124dd7a]]]Now let's do some calculations using the new DeZero. Try to write and run the following code.

[[[00000000000000001837---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001838---ed66704dbe6fbf67277d36f75f20b70f862aa2fa1554204bfc8c80bb9488c8b1]]]Here I ran the code y = x + np.array(3.0). Looking at the output, it's working correctly. Behind the scenes, the ndarray instance is converted to a Variable instance. Now ndarray and Variable can be used together.

[[[00000000000000001839---21837eaea5aede12ce1dd6dc9de9eca595b242b9cbb4e8e35c7d3735bb317229]]]use with float and int

[[[00000000000000001840---002b56ef3c18a82efd16fa1b1733513fa79508beea1fbfaa2a85107fd8660b3c]]]Next, change it so that it can be used in combination with Python's int and float, as well as types such as np.float64 and np.int64. So how can we execute code like x + 3.0, where x is a Variable instance? One possibility is to add the following shaded code to the add function.

[[[00000000000000001841---83800116c7f667f81777bb89876feadf16c531777ca433349303f68ccd3b2fdd]]]Here we use the as_array function. This is the function we implemented in step 9. With this function, if x1 is an int or float, it will be converted to an ndarray instance. And if it is an ndarray instance, it will be converted to a Variable instance in the Function class (after this). Now you can write code like this:

[[[00000000000000001842---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001843---fb2e3bc03edea0755090268b6c453ba6cd50ca15d4948385e65aba19eb3453a4]]]As above, I was able to calculate with a combination of float and Variable instances. I've shown the modification only for the add function here, but you can do the same modification for other functions, such as the mul function, in the same way. Then + and * can be calculated by combining Variable instances with ints and floats. However, there are two problems with the current implementation.

[[[00000000000000001844---d7d6c8ab2dffc78992fd91a669da9ebebc3215dd3f5c8e85073ce997f38333a8]]]Problem 1: When the left term is float or int

[[[00000000000000001845---b2eebfa9bedc14c25601ccb991d6908dd323437115c58eaad927a6083ae1bdc9]]]Currently DeZero can correctly execute the code x * 2.0 when x is a Variable instance. But the code 2.0*x gives an error. When I try, I get the following error message:

[[[00000000000000001846---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001847---3f7c8358ef37bda3d32b71317f7bef06beb71d7ad0170cf049eb57dcc44679e7]]]As you can see above, you will get an error. To see why, look at how the code 2.0 * x raises the error. When I run the code 2.0*x, the following steps take care of it:

[[[00000000000000001848---66c40f69b5bc8b416dba8b33ac93dab22087edcda38af9c7d830d490b0ebecc8]]]Attempting to call the 2.0 __mul__ method on the left side of the * operator

[[[00000000000000001849---a25dd6b2bae01db4e3a07f67d8216d098388f6c5374c7831f643ce83fcc449e4]]]2.0 is a float type, so the __mul__ method is not implemented

[[[00000000000000001850---cc82a239dac24ee37f4295b968abada5cbd8dd916b0db2fc22aa456104a4d42f]]]Next, try to call the special method of x (Variable) on the right side of the * operator

[[[00000000000000001851---5eace71a42b72d8eabe299608882553e936290307bdea69a3cc9ea3b3e6aceba]]]Try to call __rmul__ method (instead of __mul__) because x is the right term

[[[00000000000000001852---383740adc4d2dc9e7d0ab2ecf12508976e1d8cb7d98f489afd37dd506f758150]]]But Variable instance doesn't implement __rmul__ method

[[[00000000000000001853---dc00c937f71a84b97174cef32aa0ed4821f5ccca6a6d96f1dc37502a3a6d9713]]]The above is the process of generating the error. The point is that for operators with two terms such as *, different special methods are called depending on whether it is the right or left term. For multiplication, the __mul__ method is called for the left term and the __rmul__ method is called for the right term.

[[[00000000000000001854---c3ef3405605165bafbb44719b831f6dfa488b577dcde4697d6c189f234a954f1]]]Based on the above considerations, for this problem, we should implement the __rmul__ method. At this time, the arguments of the __rmul__ method are passed as shown in Figure 21-1.

[[[00000000000000001855---82916293f8e38fa3b934d96040ead70e5d65d6fb32d84aabad90c7c2f89e3277]]]
Figure 21-1 How Arguments Are Passed to the __rmul__ Method


[[[00000000000000001856---7cf589783160726f4ba5945a631c2fa2d7868be67d00b2dcc4ae0ff2bfd8c9d7]]]In the argument __rmul__(self, other), self corresponds to x, which is itself, and other corresponds to the other term, 2.0, as shown in Figure 21-1. However, in the case of multiplication, the result is the same even if the right and left terms are interchanged. For example, 2.0*x and x*2.0 are the same result. Therefore, no distinction between right and left terms is necessary. This is the same for addition, so for + and *, the following four special methods should be set.

[[[00000000000000001857---7cde71fba20fe209d3a9db2a6fb0b894295c439f83b306e48226098196d934d1]]]Now you can freely calculate by combining floats and ints. When I actually write the code and run it, it looks like this:

[[[00000000000000001858---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001859---d51a19d0b2ca7684afcc164955361d5446a338d458fe48422b4a481d847aa5cd]]]Now you can use Variable instances together with floats and ints. Finally, we will solve one more problem.

[[[00000000000000001860---203e012e0e2a95923f8870d58182eb0f76a37340ba21ca0e130eca0c0b42adad]]]Problem 2: When the left term is an ndarray instance

[[[00000000000000001861---16ae45f0f69a9b4a51167987a35bba8866c87f805dddb686b9005e63e02d47e4]]]The remaining problem is the calculation when the ndarray instance is the left term and the Variable instance is the right term. For example code like this:

[[[00000000000000001862---0fc71b1632b8d144fd8c9fada8da8ebf6f2a44abfcfde4f09718fa58f083c92c]]]In the example above, the left term is an ndarray instance and the right term is a Variable instance. At this time, the __add__ method of the ndarray instance of the left term is called. Of course, we want the __radd__ method of the Variable instance on the right to be called. Specify the 'operator precedence' for that. Specifically, add __array_priority__ to the attributes of your Variable instance and set its value to a large integer value. The actual code would look like this:

[[[00000000000000001863---80d0e5f7613ebded5f2c6bef9a73247d6c8a0e269c1809256bf1848cd45c1944]]]By adding to the Variable class as above, the 'operator priority' of the Variable instance can be raised compared to the 'operator priority' of the ndarray instance. As a result, even if the left term is an ndarray instance, the operator method of the right term Variable instance will be called with priority.

[[[00000000000000001864---940a62111432cb0ef1657676145e74ba0407b2ab29decba2d07d8ddcbcec493b]]]These are the points to be aware of when overloading operators. DeZero now allows the * and + operators to be used in conjunction with other types. In the next step, we will add other operators such as / and -.

[[[00000000000000001865---f984363648c5688e507e2094a3aa5b8c9412674d126700d18bba9ee9774a5327]]]step 33

[[[00000000000000001866---d5227a85d4da3bbf8f36e82345558ebbc9070a527ec7bf3d6f3f2e0916965503]]]Optimization using Newton's method (automatic calculation)

[[[00000000000000001867---8da8164a5d9469085205157c3de9148c44cd2391a1d3fdb850f12da1e19b2d28]]]We have so far calculated the second derivative by hand. Here, we will use the new DeZero to automatically calculate the second derivative. First, calculate the second derivative for a simple calculation. Then, after confirming that it works, we then optimize using Newton's method.

[[[00000000000000001868---380ba5fd54cd6b524843af5e18212ade3457264b7f64d346f2916f19ddcdc509]]]find the second derivative

[[[00000000000000001869---f5e6258ed76de8ea72a3c4cc97fe16513658a0ed9709480d8c8b04f597baddb3]]]Now let's find the second derivative. For now, let's focus on the computation we dealt with in step 29. With our DeZero, this can be implemented as follows (the implementation shown below actually has one problem).

[[[00000000000000001870---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001871---19b58cea7b2762cdc299c39f76e4a56332ce24b8e53e311d509ad496a8e58eef]]]First, y.backward(create_graph=True) performs the first backpropagation. Here, by specifying create_graph=True as an argument, a computation graph will be created for the backpropagation computation as well. Next, backpropagation is further performed on the computational graph of backpropagation. At this time, we want to find the second derivative of x, so we set gx = x.grad and take out the derivative of y with respect to x. Then, from the derivative gx, backpropagation is further performed. By doing so, we can find the derivative of gx with respect to x. This corresponds to the second derivative.

[[[00000000000000001872---cdea82dde124f9909ebaee9691df08ecf5bfa96dd1b3be02245b9d12a7429252]]]After running the code above, the first derivative is 24.0 and the second derivative is 68.0. Checking with the formula, the first derivative when is This agrees with our results. And the second derivative is in the formula, so when . Unfortunately, this differs from our results.

[[[00000000000000001873---12fcf65451223669f2d7e8020c6d92a072c775b8b8aebdff3a86251d27762f40]]]The erroneous result our code says is the result of the first derivative () plus the result of the second derivative (). In other words, since backpropagation was newly performed with the differential of Variable remaining, the new differential value was 'added'. To solve this problem, 'reset' the derivative of the Variable before doing a new calculation.

[[[00000000000000001874---240aa22c19ce911f4438555c9e3e0a287e8582b33b6f98d5ee6606cd27da992b]]]As a refresher, the backpropagation argument in DeZero has retain_grad, like x.backward(retain_grad=False). This retain_grad is the function added in step 18. When this is False (default), the differentiation (gradient) of the variable in the middle of the calculation is automatically reset. In that case, only the terminal variables--user-supplied variables--will have derivatives. In the computation above, only x retains the derivative after calling x.backward().

[[[00000000000000001875---d92dd96c9943387c7c3cea24387c1e9e4bc05346288d13e3b76605808024ce48]]]With the above points in mind, let's try to solve the previous problem again. Here is the code:

[[[00000000000000001876---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001877---7eaa2b98d00b5900e2c63820c2066bdc1c34fdc9c7b7cc6140d46d3304b5ee93]]]The change is that I added x.cleargrad() before calling gx.backward(). That resets the derivative of x. This will do the correct backpropagation. When the above code is actually executed, the result of the second derivative is 44.0, which matches the result confirmed by the formula.

[[[00000000000000001878---fa4aa9d3b0ee4d91a05b2acc3f1b9ffbf7fc1d986d6a3b8ef93e3bba712c99ba]]]Optimization by Newton's method

[[[00000000000000001879---3b47d7e479a117e828ce3e10a57d69d3e67a26674c7793d453ede4ddf68b6efe]]]Now, let's perform optimization using Newton's method. As a refresher, optimization by Newton's method is represented mathematically as follows.

[[[00000000000000001880---e7d678f591f76dc6ce056e72827131c578c025e943eefe2ac3c082ab6dfbe498]]]We update using the first and second derivatives of the function, as in equation (33.1). Let's use DeZero to solve it automatically.

[[[00000000000000001881---f72ec082b31498bb1966fe8cccb55a2311edfae58f6408c156e2cd12dd8f8e82]]]The code above is based on the code implemented in step 29. Last time, I coded the second derivative 'by hand', but this time, it is calculated automatically by executing the backward method twice. Running this code will print the process of updating the value of x as follows:

[[[00000000000000001882---413f946520e6d232f160d84710a2a4451bf989715a4f7dea5831a9b3c9f9095a]]]As the above results show, it takes only 7 updates to reach the minimum value. This result is the same as step 29. In other words, we were able to automatically implement Newton's method optimization correctly!

[[[00000000000000001883---153b113e44469168bca3697b0d7c6a13659766dad1115e1fa9df579ac6083c65]]]step 45

[[[00000000000000001884---f78b09c18e2c202489311ff5b14115c31e4ba6403a5295a0f57b5802e3ca1e47]]]Layers to keep layers together

[[[00000000000000001885---98011404bed14e6728f498599410ac966d23e7576e17a57599bed17af2df6001]]]In the previous step we created a Layer class. The Layer class has a mechanism for managing parameters, so if we use the Layer class, we don't have to deal with parameters ourselves. However, it is necessary to manage the layer instance itself. For example, to implement a 10-layer neural network, you have to manage 10 Linear instances (which is somewhat cumbersome). Here we extend the current Layer class to ease the burden of such work.

[[[00000000000000001886---1a7e8f792004e56c9ed248b95618761d6137ba3975185e28d5d0fc90f638d606]]]Extending the Layer class

[[[00000000000000001887---d8045d6616b71ce54c90ee20a822efc5e44fda5a1594ae5075f183b7285e80b9]]]The current Layer class can have multiple Parameters. Here, we further extend the Layer class so that it can also hold 'another layer'. A graphical representation of this relationship is shown in Figure 45-1.

[[[00000000000000001888---307c35f31cdf36fc1e6020f4678ca8766c062903e9893ffa4a71971b3adf42fa]]]
Figure 45-1 New Layer Class


[[[00000000000000001889---b83fa8630312927169635639e75d7a042a9ddd444fc43c9a16513e6a3c0e8206]]]As shown in Figure 45-1, this is a structure in which a layer contains another layer. In other words, it is a 'nested' structure. The goal of this step is to be able to extract all the parameters within the top Layer in Figure 45-1. To do that, change your current Layer class to:

[[[00000000000000001890---0970f7c2a1219b2c4ea923767c5eb51a189c2364b21b1d9bf7447999def05a09]]]# Add ①Layer

[[[00000000000000001891---30d9992c8859dba9e11ae5a725d5dd2513101fbb13cc36521a10200f0374898e]]]# ②Extract parameters from Layer

[[[00000000000000001892---fb9bf4cbcbf7849cee211fee867345561a07e473962b4dbc148812764cbbac3e]]]The first change is to add the name of the layer instance to _params when setting instance variables. This will append the names of the Parameter and Layer instances to _params.

[[[00000000000000001893---5fee0e0a60c805c513006e997697e2e9d07db6e338a0324de53bda34662fb7e6]]]The second change is the process of retrieving parameters. In the params method, extract the name (string) from _params and extract the object corresponding to that name as obj. Here, if obj is a Layer instance, call obj.params(). This allows you to recursively retrieve parameters from layers within layers.

[[[00000000000000001894---6c03a29527051aa9c5282c60307668f928d15f68cfa4ecdaf2b635d256619cda]]]A function that uses yield is called a generator. Use yield from to use a generator to create another new generator. This yield from is a feature introduced in Python 3.3.

[[[00000000000000001895---241bbc8be8066d97396e7ca33e62fb621ce32f5643e00bf3504627e4ceda0669]]]This completes the new Layer class. Using this Layer class, we can implement a neural network like this:

[[[00000000000000001896---b6bd8e2696aa545e452058562f219a3b4149f54406af311fa1b1ab2d4a5439c4]]]# specify output size only

[[[00000000000000001897---06d1c1c2c6ffdb969f43fa7d8ee41ecb845886c8d73505fd5755442ec3601e84]]]# the function that does the inference

[[[00000000000000001898---b59fbc9bf37426e67d4181a8dbdcef5fa6bdc24a56c040768c77d259a9f87bb7]]]# access all parameters

[[[00000000000000001899---94ccb16be068ab011cf1183dd7529d6d410a3fa977f7b7281b1b8da1016a6b88]]]# reset gradients for all parameters

[[[00000000000000001900---409a7d93384c6b8babfe96128a68ba52b68a4de824f3ae7485a3e61f2449bb1b]]]As above, instantiate with model = Layer(). Then add a Linear instance to the model instance variable. Then you can implement the inference function as predict(model, x). The important point here is that model.params() gives you access to all the parameters that exist in the model. You can also reset gradients for all parameters with model.cleargrads(). In this way, by using the Layer class, you can collectively manage the parameters used in the neural network.

[[[00000000000000001901---f905b9b1537291cb578b9b1e13f6207e24e81a7f7059f50bf821964e5aa61c1c]]]Also, while the Layer class can be used in the above way, it can also be used in more convenient ways. It is a method of inheriting the Layer class and defining a model as one 'class'. Here's an example in action:

[[[00000000000000001902---d2a92004a14affad7259cee28bfd00818b826930247501e9877db67dfada3ef2]]]Here we define a model with the class name TwoLayerNet. This class inherits from Layer and implements the __init__ and forward methods. In the __init__ method, create the layers you want and set self.l1 = ... Also, in the forward method, write the code that does the inference. We now have a single class called TwoLayerNet that contains all the necessary code for a neural network.

[[[00000000000000001903---ba24422f92b8df2533ce1e460036b68956f36fdb6f9eb045ea8ba958fcb6a6b8]]]The object-oriented method of model definition (a method of grouping models into units called classes) as shown here was first proposed by Chainer. It later became a commonly used style in many other frameworks such as PyTorch and TensorFlow.

[[[00000000000000001904---01a03c26c8bd383a065d5614cc984cfcfbcafb3ce381e7e6b274b583dbc90cdd]]]Model class

[[[00000000000000001905---31e2846c2c4302e0d9f7bdd4ee4fc5dfd23ba3ad9727aa794d2f1bc2e3b9318f]]]You've heard the word 'model' many times before. The word model has the meaning of 'a simplified representation of the essence of things'. The same applies to models used in machine learning. It is a simplified representation of a phenomenon that hides complex patterns and rules using mathematical formulas. A neural network is also a function represented by a formula, and is called a 'model'.

[[[00000000000000001906---98c921b9ff3d4a9e7e73d4929507dd613f3039270412f757757397bb86601bd3]]]Here, we'll create a new class for our model -- the Model class. This Model class will have the same functionality as the Layer class. Add one more method for visualization. Here's the code for it (add this code to dezero/models.py):

[[[00000000000000001907---6c788359a3d10a6167a63f5104cfdb9cbf6aac9fac3c1ee3dd52ef698daefdfe]]]As above, Model inherits from Layer. So the Model class can be used in the same way as the Layer classes we've seen so far. For example, you can write class TwoLayerNet(Model): Additionally, the Model class adds a plot method for visualization. The code gives the data passed in the argument *inputs to the forward method to perform calculations, and writes the generated computational graph as an image file. By setting verbose=True in the argument of the utils.plot_dot_graph function, the shape and type of the ndarray instance will also be displayed as a computational graph.

[[[00000000000000001908---f1f7ce0733e03851d8401312e9064b5a63a138c7023bfd3d1a6bd770ca714ee5]]]Finally, add the following line to dezero/__init__.py to simplify importing this Model class:

[[[00000000000000001909---e815f020188070d021f45a2ca3e648fffb78afff8ecd1d993b55f368e07675f1]]]Now you can write code like this:

[[[00000000000000001910---387a1a8812d83d97a929b7910d391c2a6c829207196d5c2957a7eeb9816299b1]]]As shown above, the Model class can be used in the same way as the Layer class so far. And it has methods for visualization of the computational graph. By the way, if you run the code above, you get Figure 45-2.

[[[00000000000000001911---604999b7afcd0f274d577e9c3bc15ea7294f92fddbd5881ea6fa91df501aa4cb]]]
Figure 45-2 TwoLayerNet Computational Graph


[[[00000000000000001912---fc619f4c2e5b10c37e77781dc136223cb031e7550167527156bc79c0af15eeac]]]Solve the problem using Model

[[[00000000000000001913---9f72603e8071aa637f50b0293e8903abfa346896c409f66deb0b5c3e6de74f9f]]]Now let's use the Model class to solve the problem we solved in the previous step - regression of the sine function on the data set. Here, the changes from the previous step are shaded.

[[[00000000000000001914---6d899f23a58765763821a8447872cc20e56e7c26a7590a09f97af22656e83c60]]]# Generate dataset

[[[00000000000000001915---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000001916---70a85ef7e8260ab49d37c68df541a4afecd7caf8a516d240632ac946525160ef]]]# model definition

[[[00000000000000001917---c9d0125c6272e5e4e945aec716e256ffb76021d5c5dd149413ea683aa0b427cc]]]# start learning

[[[00000000000000001918---de08aa67b1a95513492df5240c1c83e3ab21e83639417f2b149e7a80d0d89fca]]]As mentioned above, the neural network is implemented as TwoLayerNet that inherits from the Model class. This makes the code inside the for statement much simpler. And all parameters are accessible from model and resetting parameter gradients is also done by model.cleargrads().

[[[00000000000000001919---e91219abd96e02af9e74434402bd0f2971fdbc3b73b81a164194b05e8ecd0648]]]Now we are free from managing parameters! This is because no matter how complex the network is to be built, the parameters used in it can be managed by the Model class (or Layer class). This completes the main work of this step. Finally, we would like to finish this step by implementing a more versatile neural network model.

[[[00000000000000001920---0a57c3827cb7600f9fe396d8987de35efe8b5d353c1e50b5bf17e1c25615f31c]]]MLP class

[[[00000000000000001921---cff2210f053704740adc0fa33a54db2dba55d08efdf809576032dc8252962988]]]Well, we just implemented a model consisting of two fully connected layers. Here's an excerpt from that code:

[[[00000000000000001922---f56330ce9bd065caa8eb8678cd9155b4f32e1e5107686010c73008c3b3b81585]]]I have implemented a two-layer network as one class as above. Looking ahead, I would like to implement a more general-purpose network with fully connected layers. Its implementation is:

[[[00000000000000001923---256108ad62ff77af22c7e53c03b16a2eb4f0e415969c5bf275999c6b580a94af]]]I will explain the code briefly. First, in initialization, we receive fc_output_sizes and activation. This fc stands for full connect. fc_output_sizes specifies the output size of the fully connected layer as a tuple or a list. For example, (10, 1) creates two linear layers, the first with an output size of 10 and the second with an output size of 1. For (10, 10, 1), there will be one more linear layer. Also, the argument activation specifies the activation function (default is F.sigmoid function).

[[[00000000000000001924---eb11f3f6e6285779fbe203a5a1ecc7d0eb443b98ab0157602d10a9792041754c]]]Here we implement the model with the class name MLP. MLP is an abbreviation for Multi-Layer Perceptron, and it is called 'multilayer perceptron' in Japanese. MLP is another name for fully connected layer neural networks.

[[[00000000000000001925---28dae1f02e8880d6a0c8d269fc3cd2ac87e9679b1c7f92ae44ce94fb98784198]]]The MLP class implementation is a natural extension of the TwoLayerNet class shown earlier. Note that we use the setattr function here to set the instance variable. This is because you can't set like self.l2 = ... Note that DeZero puts the layer parameters under control by setting the layer to an instance variable of the model.

[[[00000000000000001926---f39c43eaed56a84f77ef0e438beee47aefaac5fd09d9d374ac906f851305161b]]]This is the MLP class. With this MLP class, it's easy to implement an 'N-layer' network like this:

[[[00000000000000001927---7b1af88ce1b3d88c61f1dac76d3878ac0c4672734459976bad0050e04e148df1]]]# 2 layers

[[[00000000000000001928---5290affc1e6fc0daf5abd43eb71b76087dc285f1348e31e17d8e7c68ade898d2]]]# 5 layers

[[[00000000000000001929---9c84f359347e304251cd1bbc6e7d8103da3a5f9e6647c2241f8e0ce1fce5b532]]]We will continue to use this versatile MLP class. So add the MLP class code to dezero/models.py. This is the end of this step.

[[[00000000000000001930---8e6777857899505af5cfda53f366b2c68a859ecfcf21e215121d775bf528e84e]]]step 50

[[[00000000000000001931---211e868243d2acef5fa8f0c139df78af269794bae991141beff4833bd19f2d66]]]DataLoader that retrieves the mini-batch

[[[00000000000000001932---0277797ac64ab09e5c9cb6ca542f66626a997c1771c76836aa1993218930df6f]]]In the previous step, we created a Dataset class and created a mechanism to access the dataset with the specified interface. In this step, we will implement a class that creates a mini-batch from the Dataset class as a DataLoader. Let this DataLoader class do the work, such as creating mini-batches and shuffling the dataset. Doing so will make the learning code you write simpler. Here, we will first explain the 'iterator', and then we will explain the flow of implementing the DataLoader class.

[[[00000000000000001933---e6e1ceaf98b02c4b3d9a8e7cb69992f91481a36481f0b90469a80ced51720257]]]What is an iterator

[[[00000000000000001934---03079a0fbfd8fe956d93662bc482b1a08dc13874cf3636a3d6b46f8454027365]]]Iterator is translated as 'iterator' in Japanese. As the name suggests, you can iterate over the elements. An iterator in Python provides a function for retrieving data in order for data types that have multiple elements such as lists and tuples. A concrete example is as follows.

[[[00000000000000001935---766160deab81302ffb05c82fc7736a7d62a86c7fd513e0f72466bc35aefd34c3]]]To convert a list to an iterator, use the iter function. Here we create an iterator from t to x of the list. The next function is used to sequentially retrieve data from an iterator. In the example above, you can see that the elements of the list are fetched in order each time the next function is executed. Note that the 4th execution will raise an exception called StopIteration because the next element does not exist.

[[[00000000000000001936---150f6a36513379af46034588f44925b7a4c6187923ea7753f714d0100414d6d0]]]The iterator functionality is used internally (out of sight of the user) when retrieving list elements in a for statement. For example, with t = [1, 2, 3], when you do for x in t: x, the list t is internally converted to an iterator.

[[[00000000000000001937---e3396ecfd146e4b3cfcf4c20c181eb70e01aa62353b594aa77813e522e41a910]]]You can also create your own Python iterators. For example, you can create your own iterator with the following code:

[[[00000000000000001938---30411aa10c0e6a1d6c58bf46ee169c6e307992b8f4d7c6bb52d320df58bbe79a]]]Here I have implemented a class called MyIterator. To use this class as a Python iterator, implement a special method called __iter__ that returns self. Then implement a special method called __next__ to return the next element. If there are no elements to return, then raise StopIteration(). Now the MyIterator instance is available as an iterator. In practice, it looks like this:

[[[00000000000000001939---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001940---942cfa43e39a7f903ec095c5c1a3178e02aa2378df861e2a541b8f95b29e6b19]]]In this way, we were able to extract the elements using the syntax for x in obj:. Now, using the iterator mechanism, implement a DataLoader class that retrieves DeZero's mini-batches. The DataLoader class fetches the given dataset from the beginning and shuffles the dataset if necessary. So here is the code for the DataLoader:

[[[00000000000000001941---0a610f4a3b1cfbfeea2e19ff296ddbe31f36469efd9ff69be90252e66accf613]]]The initialization of this class takes the following arguments:

[[[00000000000000001942---5974b5afe9001f4e281cdca8f6cdb312995e1e2368884324fea7613a9e59852e]]]dataset: an instance with an interface of Dataset†1

[[[00000000000000001943---fe9797212d3279057a2a199466766169037dfa9599f206494c54e6544f51a138]]]batch_size: batch size

[[[00000000000000001944---d6f1c072840292a86b548e9c86145acf7f9d5931e0e0cf5849826ada0d7f4ac3]]]shuffle: whether to shuffle the dataset every epoch

[[[00000000000000001945---1a17451baabdec8580869b1d147f12ec4dc620766a3c47a230f4ea298b120c0e]]][†1] An 'instance with a Dataset interface' is an instance created from a class that implements the __getitem__ and _len__ methods.

[[[00000000000000001946---cc1a83029efef5eaf0e762d38469dba677cf29f4a969e5fbd0130ed7efbb2095]]]The initialization code calls the reset method after setting the arguments to the instance variables. The reset method sets the iteration count of the instance variable to 0 and shuffles the data index if necessary.

[[[00000000000000001947---b0ca162df5a81ee62a6052280c343d955b5fa5e10eb08203218ce797828491eb]]]The all-important __next__ method takes the mini-batch and converts it to an ndarray instance. That code is the same code we've written so far. Therefore, the explanation is omitted.

[[[00000000000000001948---8406a4f8444ab0de978d1bdaffa268d8b78deeff744900f2c3927f65a6f4fff2]]]The DataLoader class in dezero/dataloaders.py also has a mechanism to transfer data to the GPU. The code shown above is an abbreviated version of the GPU-enabled code. GPU support is done in step 52.

[[[00000000000000001949---3b72bcbb299799e64d41253e33206ae215bf240d3b09cc16935fd10bd6224088]]]Finally, add the import statement from dezero.dataloaders import DataLoader to dezero/__init__.py. It allows the user to import the DataLoader as from dezero import DataLoader (no need to write from dezero.dataloaders import DataLoader).

[[[00000000000000001950---a4a083a4ce655ebd3f032ff31b17c848df140e1e324c0d4fcd8796e705b77c11]]]Use DataLoader

[[[00000000000000001951---ef494a72cc50027d7c0067573f70862459de73b3d202d76de248eb488dd8b550]]]Now let's use the DataLoader class. This DataLoader class makes it easy to retrieve mini-batches. As a trial, let's use DataLoader assuming neural network learning. The code looks like this:

[[[00000000000000001952---33e6501869a2265e2d4b7b2032c48f24f87a39beb7e28260b648e51c8379fa2d]]]# x, t are training data

[[[00000000000000001953---97777e3f1085e27ac31bc99d489b782895dcf431af8bbc418c983cb093d21793]]]# fetch the test data at the end of the epoch

[[[00000000000000001954---6b28b9e8964c7faf1efe1718b492c709f530d290c7bcdc7edc53b5bba323c9b3]]]# x, t are test data

[[[00000000000000001955---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001956---31d881666d17da8307dafe5edaad42304daea58681ea9ca527ae327ac260d160]]]Here we generate two DataLoaders, one for training and one for testing. The training data loader shuffles the data every epoch, so shuffle=True (default). On the other hand, the test data loader is only used for accuracy evaluation, so set shuffle=False. If you prepare the data loader like that, the data loader will take care of fetching mini-batches and shuffling the data.

[[[00000000000000001957---8cb7694e1463e18074f3a8752459ace7dfb93ae6802561a73c8f21fe97b951ce]]]Now let's train the spiral dataset using the DataLoader class. Before that, just add one convenience function.

[[[00000000000000001958---32c46b04d4fe74f43dcca640a58f83d5d2ec17312561bbef3d95f0033e098695]]]Implementing the accuracy function

[[[00000000000000001959---e88c218fd379fdeb89d9744b7cfe442443b651d240ae1d66554c03214397aaa7]]]Here we add a function called accuracy to evaluate the recognition accuracy. Here is the code for this function:

[[[00000000000000001960---9eb71e6d6dbb43d0d09a8d44d13d0eb468656586cd6beab128edb5a1c7bb6dc7]]]The accuracy function calculates the 'accuracy' of the arguments y and t. Here, y is the prediction result of the neural network and t is the correct data. These two arguments should be either Variable or ndarray instances.

[[[00000000000000001961---8f92610cfa2489a44ec8f188ecd8752de29f208271c41fed3770e05b17929457]]]In the implementation of the contents, first, the prediction result of the neural network is obtained as pred. To do so, we find the largest index of the neural network's prediction results and reshape it. Then, when pred is compared with the correct data t, the result is a True/False tensor (ndarray). If you find the percentage (average) of True in that tensor, that value corresponds to the accuracy rate.

[[[00000000000000001962---6a441928358cc2969fc9f99b19b5b2b3425684280dc3d14f7a4fde014c703a10]]]The accuracy function returns a Variable instance, but the calculation inside it is done on an ndarray instance. Therefore, the derivative cannot be obtained for the calculations performed by the accuracy function.

[[[00000000000000001963---146e9ffa691755efe44f0a5f63d7037bdb8e0ad8d68abae81abcb9b573ea2e50]]]Also, the last line of code above uses the as_array function to return Variable(as_array(acc)). This is because the data type of acc (= result.mean()) is np.float64 or np.float32. It is converted to an ndarray instance by using the as_array function (the as_array function was implemented in step 9).

[[[00000000000000001964---ce53031ce8f21419e254ee836d0945a4c0787f92b17bde3bc9956da8eb22a14b]]]Using this accuracy function, the accuracy rate (recognition accuracy) can be obtained as follows.

[[[00000000000000001965---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001966---62b8426b1c3c046d2a38c52a670e5467eaa916ff8c3575d8c8ec4bf03d96b6bd]]]where y is the neural network's prediction result for 3 sample data (that's 3-class classification). And in the training data t, the correct index is given to each sample data. At this time, the recognition accuracy is calculated as 0.66... by the accuracy function.

[[[00000000000000001967---8689b187641a4447d5fe09f6ba7b1f7718f9ae1dfbcc75a9b4b50780e6d0ac0c]]]Training code for spiral dataset

[[[00000000000000001968---00bf79cab73f05e2d4149daf19e8fd6fd7abe58fce389fb7b98bf245282be2ff]]]Now, let's use the DataLoader class and the accuracy function to train the spiral dataset. The code looks like this (I've omitted the imports):

[[[00000000000000001969---a08abc4ac9763f70f47d5e721396b75b66bb336c31aa7b037050524afa640a79]]]# (1) Mini-batch data for training

[[[00000000000000001970---19a11892c9f3be444d977d6872605d4ca76d3f8caf333f8ffa9694670a14b8ca]]]# ② Recognition accuracy of training data

[[[00000000000000001971---c7fe5e783903343b224aefefaeada02a126a2967228143db157fc2f70f46b4e7]]]# ③ Gradient unnecessary mode

[[[00000000000000001972---59c25c583e1a415c26467cebb259603509272d60d5b1fbb03c2e75611cd71876]]]# ④ Mini-batch data for training

[[[00000000000000001973---d7cf42144cfb5eb5060e814685920e2ed910733fd8de55b8363d6892cce15979]]]# (5) Recognition accuracy of test data

[[[00000000000000001974---d4e3868cda674acf5d7cbe37ab143227d28ca0822549ef35f3d90a073068d05a]]]In this section, I will explain the points 1 to 5 in the above code. First, at point ①, use the data loader to retrieve the mini-batch. And in ②, we calculate the recognition accuracy using the accuracy function. Next, in point ③, we evaluate using the test data set for each epoch. We don't need backpropagation when testing, so it goes into the scope of with dezero.no_grad():. Doing so saves processing and resources for backpropagation (the no_grad function was introduced in step 18).

[[[00000000000000001975---92ce527f94c9600a065d77907a3ec0be37a705ce6fce4acf1196dd641d6d8248]]]At point ④, the mini-batch data is retrieved from the test data loader and evaluated. Finally, at ⑤, we use the accuracy function to calculate the recognition accuracy.

[[[00000000000000001976---afd4a71f48363bc59eb0c09c4bcf0ff6a41e1272570ac54ceb07bd004bfa3205]]]Now let's run the code above. The result of graphing them is shown in Figure 50-1.

[[[00000000000000001977---b23bdfb6cf44f2c02a8a2d139882eb23cc56a53761768b6484858011a13ab266]]]
Figure 50-1 Changes in loss and recognition accuracy


[[[00000000000000001978---51b8f984ea58205191e04e0482cfc3f060bb5f44112f8cb60e708ccc7b6f8352]]]Looking at Figure 50-1, the loss decreases and the recognition accuracy increases as the epoch progresses. This is proof that learning is being done correctly. Also, from Figure 50-1, we can see that the difference between training (train) and testing (test) is small. From this, we can say that our model is not overfitting.

[[[00000000000000001979---52dce6b963d57b6338583b2e83bcbfad826dcfb7e87ccca6a3b73288d90f34f9]]]Overfitting is overfitting to the specific training data. Overfitting is a state that cannot handle unknown data, that is, a state that cannot be generalized. Because neural networks can create highly expressive models, overfitting is common.

[[[00000000000000001980---9f8a524f0d62c9e0d2cb2ab1782aec14c8bcb4d256f31e731018de2c629cbfb4]]]This is the end of this step. In the next step, we will use the MNIST dataset instead of the Spiral dataset.

[[[00000000000000001981---3ca446b3c19b448935df4c697ebc148e2bd6219dc74b2c7d9d3bd008a507e900]]]step 22

[[[00000000000000001982---0ff71d6acbd6fcdb32de0a2bdcbb1c4469b196590dc6a0d86971ae9353edf885]]]operator overloading (3)

[[[00000000000000001983---2039d33885bf321d351a6fd1676198bdb9a5daf5b3d10a63784ad0ec33a715b9]]]In the previous step we supported two operators, * and +. Of course, there are other operators as well. In this step, we will add the new operators in Table 22-1.

[[[00000000000000001984---413e98997c5f14dfd6edb7bc74379b9ec5ced0b2a4803456ba4141c17387e9c1]]]Table 22-1 Operators newly added in this step

[[[00000000000000001985---0f82bb54cc55a73a1cfa5c78ae22075042e0abd6afeeca33c403a4be9c1a1346]]]special method

[[[00000000000000001986---9a4632346963d7e188fd63da51ea5fc16d9b2274b4574bf371f7cbe50a15d61b]]]example

[[[00000000000000001987---9cb80cff3549049bdad1b5c6eb3795278b1751e04dbd75c9ee701f2cab441b20]]]The first __neg__(self) in Table 22-1 is the operator for negative numbers. This __neg__(self) is an operator for one term (such an operator is called a 'unary operator'). Therefore, the special method also has one argument. The remaining operations are subtraction, division, and exponentiation. As I said before, these are the operators you use on two terms (such as a - b or a / b). Therefore, it is divided into two special methods depending on whether the right or left term is targeted. However, for exponentiation we will only consider cases like x ** 3 where the left term is a Variable instance and the right term is a constant (an int such as 2 or 3).

[[[00000000000000001988---4a3e7ac8e56a9af1b734e6c5f0398d61f9a2a4b9f47e9078cd9a0ff745de8678]]]There are several other Python operators than those in Table 22-1. For example, a // b or a % b. There are also assignment operators such as a += 1 and a -= 2. Here, we will select and implement only the operators that will be used frequently. For other operators, please add them yourself if necessary. Note that this step is somewhat tedious, so feel free to skip it.

[[[00000000000000001989---04aac3a03844e2ce63cb1de076cb3ddff78d8fc8ce8534c723f46ca420fc2e46]]]So let's get to work. As a refresher, we followed the steps below to add a new operator.

[[[00000000000000001990---fcb982d68895f082a231fa50c8cc0c70fc02a029572b21f015ce94ff576490a8]]]Inherit the Function class and implement the desired function class (e.g. Mul class)

[[[00000000000000001991---0af8fd924a3a33b42eb66a827f8daf9b653488ff449f38559bfca1f39f42f02d]]]Make it available as a Python function (e.g. mul function)

[[[00000000000000001992---8bfd61a62cc21b2c8fa8e61e0a0d40df0257e0ac8bce351a239300fd0492d99d]]]Do operator overloading for Variable class (e.g. Variable.__mul__ = mul)

[[[00000000000000001993---0bbc43bb49fbc936cd6c56b8fab535f2a8d719781e87fbcca9420b1eaf9b70e1]]]Again, follow the steps above to add new operators. Let's start with 'negative numbers'.

[[[00000000000000001994---d52ddd6f0bd57856e3b9c2d13889b22de182a91f9051ff825523abb80f3a7b73]]]negative number

[[[00000000000000001995---68a50f041b809733486e2d81e1978632896a9252d2c06456d8908e9e32e96ff5]]]The negative derivative is found when . Therefore, in backpropagation, the derivative transmitted from the upstream (output side) is doubled and sent downstream. With that in mind, you could implement it like this:

[[[00000000000000001996---6963e672d010b938cc9c869e6c0a7b00a471b019de6ec6b9e90967286b7bdf7f]]]As above, implement the Neg class and implement the neg function as a function in Python. Then, assign neg to __neg__, which is a special method, and you're done. Now you can run the following code:

[[[00000000000000001997---23d1427f18939fc78ff410294328ffccc43d6f98e17acc81a0b59697ae37aca7]]]# find the negative number

[[[00000000000000001998---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000001999---9bc307c92a4d83bd0a25b72ddd3874e8d167a5a53713ec9e1d576c5b166c1b14]]]Next is 'subtraction'.

[[[00000000000000002000---2add90c2f134278794f302f1a61659c387c1d39462bd90b8b410e2bdb0bc1984]]]subtraction

[[[00000000000000002001---2ee46315cf32f7c064d05dfd9dc8254b81c123ab6a19b42c8fd27957a07ec991]]]The derivative of subtraction is when . Therefore, in backpropagation, the derivative is multiplied by the derivative transmitted from upstream, and the multiplication is the derivative. With that in mind, you could implement it like this:

[[[00000000000000002002---bc2f91c1d1a24d74e8b4f3b23e25f1be3803ad7d3cddfa19a6c983af639b30e3]]]Now you can perform the calculation y = x0 - x1 where x0 and x1 are Variable instances. But if x0 is not a Variable instance, then code like y = 2.0 - x will not work correctly. In that case, x's __rsub__ method is called, passing the arguments as shown in Figure 22-1.

[[[00000000000000002003---8b1317112e4a668f0485e22e9de824dc42d54c348157085fe4715f470f1d556f]]]
Figure 22-1 How Arguments Are Passed to the __rsub__ Method


[[[00000000000000002004---0d0ffb1874d72ac2aacb2801695faed85c8887c48dd69a9e819568b459c7c9b7]]]As shown in Figure 22-1, when __rsub__(self, other) is called, the x on the right side of the minus operator is passed to the argument self. With that in mind, __rsub__ should be implemented like this:

[[[00000000000000002005---cb0e4e6085d16e42a70b35e88189e283626822ac938526ea7fa34de792010391]]]# swap x1, x0

[[[00000000000000002006---bda45222c359bc7a83d4a0a6a4da327f0a8f10f09d5e31df90cbffc324854ec3]]]Here, prepare a function called rsub(x0, x1), change the order of x0 and x1 in it, and call it as Sub()(x1, x0). Then, assign the function rsub to the special method __rsub__.

[[[00000000000000002007---42eb4df01776e399809b0a90b556fb9c73b513e8386fdc91dce24569bf84b150]]]There is no need to distinguish between 'addition' and 'multiplication' because the calculation is the same even if the right and left terms are exchanged. For subtraction, however, it is necessary to distinguish between right and left terms (subtracting x0 from x1 or x1 from x0 yields different results). Therefore, as shown above, the function rsub(x0, x1) for the right term must be prepared separately.

[[[00000000000000002008---44aa27eccc4cf5b3c2426265d8215649f65a2ca4bb78dcc5749de7c1dab08687]]]Now you can do the subtraction. Now you can write code like this:

[[[00000000000000002009---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002010---9d94efef1caa6d2456f48ac6d0bda83841bbe812cbe807f00f2675b6d1c9b4eb]]]Next is 'division'.

[[[00000000000000002011---e8df17bed28d14ec63ad942c38e17f1ddacf960cecd0f3ae23771d6fe4f6369c]]]division

[[[00000000000000002012---b192550df8c522e5b4cfbd54dbd81adcccb2bba291087bc071a23366b28476bc]]]The derivative of division is when . With that in mind, you could implement it like this:

[[[00000000000000002013---cb0e4e6085d16e42a70b35e88189e283626822ac938526ea7fa34de792010391]]]# swap x1, x0

[[[00000000000000002014---1072baf53612ec02c23ed44284f418745799964f56cbc4320eb6023021e52e0a]]]As with subtraction, the function to be set differs depending on whether the right or left term is the target. After that, it won't be too difficult. Finally, there is power calculation.

[[[00000000000000002015---07fee5df7def20e13bc7da91c12a87043bb63bc6efd48a8e34acc80317357073]]]exponentiation

[[[00000000000000002016---e31e781adbed102e217eea1ccd2d6e0b84a2c9074aba2527473889eb74e28904]]]The exponentiation is expressed by the formula In this case, is called the base and is called the exponent. For differentiation of powers, it follows from the differentiation formula. , there are not many cases where it is needed in practice (although it is possible to ask for it). Therefore, in this book, we will seek differentiation only for the base. In other words, we treat the power exponent as a constant and do not calculate its derivative. With that in mind, a possible implementation would be:

[[[00000000000000002017---e0a56cf1463168ab4d1bc4be7491775f270b1e579a4343ea7441ae522e2ee7a6]]]Here, the power exponent c is given when the Pow class is initialized. And forward propagation forward(x) accepts only the base x - just one variable. After that, assign the function pow to the special method __pow__. Now you can use the ** operator to calculate powers. If you actually do the power calculation, it will look like this:

[[[00000000000000002018---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002019---4bdebf9a6fe6b0e4d0eeabd280c5e0c70c78ad34ecea47b76ea4e3fe2694fa9a]]]This completes the addition of operators. This step was somewhat tedious, but it made DeZero's usability much better. Now, you can freely calculate the four arithmetic operations with each operator. You can also calculate powers, so you can do pretty advanced calculations at this point! The next step is to wrap up what we've done so far as a Python 'package'. After that, I will verify the current DeZero's ability.

[[[00000000000000002020---70ca15c34a29931573190c83eedbd237af1725dace7f3a8d35e5c7d95b4216d3]]]5th stage

[[[00000000000000002021---42c116f5fec28a8650ee16b9d598ad1f14439cd2a9997b81b99116ca78156f56]]]Challenge with DeZero

[[[00000000000000002022---e49fbebcb2ca279794308bfbdf9064b849624d689f3440b588bc3bd0e0817012]]]In the fourth stage, we added dedicated functionality to DeZero for machine learning, especially neural networks. Specifically, classes such as Layer, Optimizer, and DataLoader. Thanks to them, DeZero can easily create models and learn them efficiently. DeZero already has basic functionality for developing neural networks.

[[[00000000000000002023---5fec143bff637a5913f454bd7a8ff2ba78bc54f5a15e725c62ff14a03c57c9f6]]]From now on, we will further evolve DeZero. Specifically, it adds new features such as running on the GPU and saving the model to an external file. It also supports layers that change behavior during training and testing (Dropout, etc.). With these functions, DeZero becomes a fine 'deep learning framework'.

[[[00000000000000002024---fcdbd6642d93b252b224101a354ed935431b314fb5e7b1f3465e25bd8b0e21a8]]]In the second half of this stage, we will implement convolutional neural networks (CNN) and recursive structured neural networks (RNN). They are networks of complex structure and appear not to be easily implemented. However, if you use DeZero, you can see that it can flexibly deal with such a complicated network structure. Now it's time to start the final stage.

[[[00000000000000002025---d3076c9ecabd5c8dfe5fe91d3fb0ad49efaf30da2df7e295d7c897ee2c9da150]]]in conclusion

[[[00000000000000002026---1ac45fd156f7721e62a7f25227a085800e9ea40af0692c96cb229a054e1cea53]]]Thank you for joining us on our journey to create DeZero. We hope that this book will help you learn new things about deep learning frameworks and deep learning itself. If so, as an author, I couldn't be happier. Finally, I would like to conclude this book by briefly describing the production process of this book.

[[[00000000000000002027---89d613302f1b6448cebbd8df72afee3ef3e7a7351b58bbf57b394401cb5c2ff1]]]The production of this book started in October 2018. As a result, the production of this book took about a year and a half. When I started working on it, I expected to write it in about half that amount of time, but each time I find myself unreliable in my predictions. You may have caused trouble to people around you. I would like to take this opportunity to apologize.

[[[00000000000000002028---5b32505375db555a112d369358e64ed8f2addf9abc985f70bb17e2771f951241]]]Now, the theme of this book is to create your own “mini-framework” for deep learning. There are three main reasons for choosing such a theme (in other words, why I wanted to write this book). The first was that modern deep learning frameworks had passed a transitional period, and common features such as the greatest common divisor had become established within the community. It felt like the right time to write this book.

[[[00000000000000002029---4883b0a8c07693a5d5a8cb61b56c557904000515373bb7973b6d343a46ccf32a]]]The second reason I wanted to write this book was that there were no books or literature to understand the inner workings of the framework at the implementation level. As for myself, I learned a lot from the code (under the hood of the framework) of Chainer, PyTorch, etc. I thought it would be worthwhile if I could convey those techniques correctly—and interestingly.

[[[00000000000000002030---d9011d17bd4ac3df1cf7ad39f7cfc6c594b2b32f8d0cbc108aeb26eae02be44d]]]The third reason is that Chainer's code was beautiful. Chainer was well designed and brilliantly implemented the pioneering idea of Define-by-Run. I was fascinated by the idea and the code. That was the driving force behind writing this book. However, the contents of Chainer are huge (for beginners) and complicated. So, based on Chainer, my goal was to create a framework that is as simple as possible while still having modern functionality.

[[[00000000000000002031---7a95d0dc9af2d5a20aeaebda1937ff7ba314c0c54a3553b555f6b3af1906b3e5]]]With all of this in mind, the direction of this book was easily decided. However, I didn't have any good ideas about how to structure this book - how to improve the reader's understanding and how to convey 'interestingness'. After that, I explored various things, and the period of trial and error continued for a while. Then, after about three months had passed since I started working on it, I began to think, 'There is no other way but to make this little by little.' From there, I spent a lot of time coming up with appropriate plots (step-by-step constructions). As a result, the book consists of 60 steps in total.

[[[00000000000000002032---96005669ce7479549e3b44d7d555fe01617a62e9511a9e64447ab620e6ef821f]]]As an aside, I bought Diagostini's 'Weekly Robi' around this time and made a bipedal robot 'Robbi' (there are 70 volumes in total). As I read through 'Lobi', I was thinking about how I could make the process of creating it an exciting experience (making Lobi was fun and a good change of pace).

[[[00000000000000002033---fe4181e403fe33a66c6f6129f2430a351801add7654b5fb2aaf54d769d1d9d6c]]]In that way, the structure of the book was solidified, and a rhythm emerged in the writing. So making this book was really fun. In particular, I was very excited to work on developing an original framework called DeZero. Writing code, thinking about design, researching other code, and forgetting the time I remember working. I realized again that the work of developing software is purely fun.

[[[00000000000000002034---6dfa58df3cc6c093871afc567bf2466365ba5383ffb0a2a4aa2ea1345df95736]]]Now that I have finished writing this book, I am both relieved and wanting to continue writing. As a result, 'Step 60' was the goal of this book. For example, I wanted to add a step to implement more advanced models (GAN, VAE, DQN, BERT, etc.) using DeZero. I once dreamed that if I did that, I would reach “Step 100”.

[[[00000000000000002035---880d6ae44065de68c76889add9bdff107d787ce000bb4f2de953559b32fd3359]]]However, this book has also grown, and I think that the goal of creating a framework has been achieved. Above all, DeZero has grown significantly beyond my initial expectations (the book itself has grown 1.5 times as much as I originally anticipated). 'Step 100' will be reserved for my own (or readers') future work, and I will write it here. Finally, I would like to thank you again for picking up this book. thank you very much.

[[[00000000000000002036---32ab2a02a0d4b95b0a2ededc4dfdc60d2b0051008128f92fbc152e0e8facf0b8]]]Acknowledgments

[[[00000000000000002037---44ed4454cb7a7a9c475c2df242e22c771b16c832da2bef926d936ef4ecdea48b]]]Many people have contributed to the production of this book. I would like to thank you for being here. I had various discussions about Chainer with Mr. Seiya Tokui of Preferred Networks. Shunta Saito provided advice on the structure and content of this book. In addition, the “opportunity” for writing this book in the first place is in conversations with Mr. Toru Nishikawa, Mr. Daisuke Okanohara, and Mr. Ryosuke Okuda. I am grateful to have had such an opportunity.

[[[00000000000000002038---8537b67757b92c6a47be93083a00b6086a074a6f54431e2719db1d664368e938]]]In addition, this book was proofread by a method called 'public review'. For this public review, we have published the manuscript of this book on the web for anyone to view and comment on. As a result, we received more than 2,000 comments from over 100 people. We would like to thank everyone who participated in the review. It is precisely because of the help of such reviewers that I was able to polish this book even further. Of course, any deficiencies or errors in this document are the responsibility of the authors, not the reviewers.

[[[00000000000000002039---73acf2d941fce121063bdfe499511e4cc2c7a0191963f55e76153f076548865f]]]As always, this book was edited by Naoki Miyagawa, Mio Iwasa, and Sara Koyanagi from O'Reilly Japan. The production of this book (mainly typesetting and page design) was handled by Kenji Muto and Moe Masuko of Top Studio Co., Ltd. It is because of their help that this book exists. Finally, I would like to thank my family for always sharing and supporting me. thank you very much.

[[[00000000000000002040---e196c210c5aca71f09475a94ca1750789fda03bb1b9bf85bd1fe22554073460b]]]peer review

[[[00000000000000002041---458e96dce38a0096fb0c3d9f3b5a3a80ca87c77dad6ea88658a617f5cd53dc0a]]]Michio Saito Ryosuke Sato Ryo Ozawa Yuichi Masumiya Yuta Yamazaki　

[[[00000000000000002042---c1ac3b764d9772e295b0fdaaf9338b80e4751af59085fb974cf428e720001de3]]]Yasuyoshi Hirata Shotaro Ishihara Yuichiro Nakamura Teruya Kato Kazuaki Ishizaki　

[[[00000000000000002043---f7455f1b7e57876e1abdcb02657b3b8e8e9c6d175d7f0e3a6b693dd0100a2be4]]]Takara Endo Hideki Takahashi Naoya Kumagai Junya Suzuki Takumi Iida　

[[[00000000000000002044---5793f314fe9135f82abfa728627a791adbb3bdb29d0e31ed080febf55c2049a4]]]Naoto Nagahama Atsushi Shimizu Yutaka Takahara Taiga Sasakawa Masahiro Goto　

[[[00000000000000002045---a1d27765047a0dba23a73f9a1da6ef2a0dae470c02a7000619189f8661a54828]]]Kenta Yamauchi Muneyuki Noguchi Kodai Takashima Shogo Nagano Masaya Hikuma　

[[[00000000000000002046---5cec34be046f44e5fe41ff965eacecf5a260d7e16db306b4a5d38f2e3785a040]]]Yuta Ura Yudai Yamamoto Makoto Takenaka Katsuyuki Toyoda Ikuyu Sakaguchi　

[[[00000000000000002047---195a5ebdd57ec295c0ffa54b96d15089ac98924996deebc999c6177489c333da]]]Yuki Kashiwada Kotaro Yamashita Takaaki Inada Makoto Morinaga Tomoki Ishikawa　

[[[00000000000000002048---c2d37555c0e428390aa5ed684968ce72052ae708eac342eee63baf24c39541f1]]]Masaru Shibata Toshiaki Takenouchi Takumi Ando Takeo Imai Yu Miyabayashi　　

[[[00000000000000002049---72d3ea9f464f828ecf34b46f3a105be8573386895e7bd46b0805aec919a194f5]]]Ryoma Murai Akira Mitsuhashi Satoshi Kanechika Yasushi Fujinami Toshihiro Shunsaku　

[[[00000000000000002050---a19f946c0c07615c76d667716cbcf804455f6f5cdc5a1c92ce5ce80eff39a166]]]Eri Kabayama Shinya Matsuhashi Keisuke Tamenasu Hideo Yanagi Munetaka Mizutani　

[[[00000000000000002051---dd6dba047711e8624631a6232a4a4b90a1a5a90070bb619ef585858fd294704b]]]Shinji Ogishima Tomoko Furuki Naoto Hori Yusuke Hamada Go Shida　　

[[[00000000000000002052---a6b576ceea4f0229a19a77c10c2b436a89de46d5c8238af32422358029fa9d0c]]]Yosuke Seki Satoshi Okiyama General Tabuchi Toshiki Shimizu Takayuki Inadome　

[[[00000000000000002053---5d6b96d00454186d620106053a56864be2d50c531dcb8ecf93730df48eb9cd7d]]]Shunsuke Fukasawa Satoshi Yoshimura Masaru Mita Yutaro Ishikawa Hiromi Sano　

[[[00000000000000002054---0f463b666186e9d89186a9335fbb7b7042bb4e220e19e149ae50fea764e72797]]]Shinnosuke Kouchi Koki Saito Kumiko Kobayashi Rinko Niisato Shintaro Suzuki

[[[00000000000000002055---03c05239264b365c7ec3f2ea03f22451cd16c677fc1fa8648ea1dc6aa2f3e28f]]]Keita Watanabe Hajime Ogino Keisuke Ohtaki Tetsuya Honkawa Atsuya Shibata　

[[[00000000000000002056---6ae04fbbcff4affa5d565233d8c2ecc0088e4df36c5fec51616a4302f8e22af9]]]Takahiro Kato, Go Takano, Hirotaka Iehiro, Yasuhiro Kashima, Ei Nagasaka　　

[[[00000000000000002057---bd6f9590607e902fe6e6a46dffe9bfafab2f8a9a66f9fd943040c2710b4fb0c1]]]Takayoshi Kawata Masataka Nishihira Kenji Takase Toshikazu Karube Koichi Imamura　

[[[00000000000000002058---5239aa4db5cad3291f7cbfdd8d92d2cf8757eb99f5a1dcc991df5e78b0ded95d]]]Tomoaki Ando Katsuya Ogata Kentomo Ohmae Masanao Muramatsu Shunsuke Tsuzuki　

[[[00000000000000002059---3fc6e182ba8ff8a95f36ff94436a7a25ec3e564dbf90b69609f56be9a08e2546]]]Yusei Saito Liu Chao Ryoi Horii kk2170 　　

[[[00000000000000002060---da2dadcd3ff9c5bc455a1fb290f57841ab906e42d94613ffa9e978a8770e3c9c]]]production

[[[00000000000000002061---316b62b31be496c93aa368122b16d3102451dd5ff29861bf5c5389a630d0e9fb]]]Takeshi Mutoh

[[[00000000000000002062---fad57992d6fd1dd6c738c1cb96193c1656aebb97f28bd57db816e4ff5273569f]]]Moe Masuko

[[[00000000000000002063---11f9049ddab593adbbeccbad6d070f959cb4d60f41351acce2a9ead4281028b8]]]edit

[[[00000000000000002064---2ed590e436542778edbed46bcadd8fe8162eaa0de4a6abe060a194f3fc863e0e]]]Naoki Miyakawa

[[[00000000000000002065---c882f036e3715fc8ab51e4cfa1cad2f96b626cc3ecc42252e2f4c4b1ebeb920d]]]Mio Iwasa

[[[00000000000000002066---7e30891d89bcfa73947df871d7de8cfcd15bf2b2cfd998167599e638f6e1288a]]]Sara Koyanagi

[[[00000000000000002067---2e5cb39b9a62c5cb86a97eedf5d5fa99f15a60bb00860e7d98586ddb3d7fefe5]]]1st stage

[[[00000000000000002068---bb7e7a8aa80bb7e176bb20055c6f67f10f1c4aa3803be55fd7b25785f99f74f1]]]Automatically find the derivative

[[[00000000000000002069---e4ca6dc802d2b2512e1c645f0aee48ca3cc5c04d1f78fa0cee76e4eebea02f19]]]Differentiation is used in various fields in modern science and technology. In particular, differentiation plays a central role in each field of machine learning, including deep learning. Deep learning frameworks are, in a sense, tools for finding derivatives. Therefore, the subject of this book is naturally “differentiation”. In other words, the main theme is how to obtain differentiation using a computer.

[[[00000000000000002070---5deaf75057929257b9d8498c6d58510819a8d94c4b5999bb5a10ebe01458d62a]]]The first stage, which starts now, consists of 10 steps in all. At this stage, we create a framework for automatically finding derivatives. Here, 'automatic differentiation' means that a computer (not a human) determines the differentiation. Specifically, it refers to a system in which a computer automatically obtains the derivative of a calculation (function) by coding it.

[[[00000000000000002071---9e5cba5abe6e220f3829d8c47185b0a5a68ace16385e9df33a3d345bf63647f9]]]In this stage, two classes (Variable class and Function class) representing 'variable' and 'function' are created to automatically obtain differentiation. Surprisingly, those two classes provide the foundation for automatically finding derivatives. By the end of the first stage, the differentiation of simple calculations (functions) will be automatically obtained. Let's take the first steps with DeZero!

[[[00000000000000002072---066c6a41a8ed489b43ec07285dc093bd8202eacf454506394df85a55c7652bd4]]]Appendix A

[[[00000000000000002073---5a40276e2b3e9214b8dc1ab6ce39a9f53210ece2a0093261944716f25ea17eac]]]In-place operations (complementary to step 14)

[[[00000000000000002074---28340ac092426178810cb4ad61fc17cb02ec2688792f0452095ffa2ad5fb298d]]]This appendix is a supplemental page for Step 14. Here, I will explain 'Why not use += for addition of derivatives' mentioned in step 14.

[[[00000000000000002075---3263dbe2bfe0a993b6622b66dd7fd2452d9b3cdbc98621f7d1c68bac3c0f763f]]]Confirmation of the problem

[[[00000000000000002076---ad2b2398aaad69ef38082f7bfdada90ccf9e49ca753b66d6eaf97ad25fb1b237]]]Let's sort out this issue first. In step 14, I modified the backward method of the Variable class as follows to allow the same variable to be used repeatedly.

[[[00000000000000002077---9ad450e7244d5a453f0fdbd516e292e4ba6784f7852e4a39bfe04bd411724d27]]]The points to be corrected are shaded. Simply put, when the derivative (gradient) is propagated the first time, x.grad = gx and 'substitution', after that, x.grad = x.grad + gx and 'addition' . By the way, gx is an ndarray instance. The point I pointed out in step 14 was that writing this addition as x.grad += gx would cause problems. Here's why.

[[[00000000000000002078---e6a1cd0f7415091f5168bbe6ab42f34fb2c371604ad7f0be9602a6c07a43e898]]]About copying and overwriting

[[[00000000000000002079---6573e5952ebe87f0b1bc632730022c678ffd2abeef790acbbb5e508800d76672]]]First of all, as prerequisite knowledge, you need to know about 'copy' and 'overwrite' of ndarray instances. Now let's look at the following code.

[[[00000000000000002080---dc1dbd8ff83b1c29725949766ee0d1646310c24373def8c29b107ca8ea7995ab]]]# overwrite

[[[00000000000000002081---6e4e18e94833baeeacff2d3dfb93590704fce4c0f3851c9345aa11f68ae5e661]]]# copy (newly generated)

[[[00000000000000002082---5af80e0591969308d49a862aaa2268ffb4e8fc4515a4afea8594ba1385c2e2d3]]]Here, by looking at the result of id(x), we can see whether x (the ndarray instance) is overwritten in memory or is newly created. From the result above, if we used += and the accumulating assignment operator, only the value would be overwritten, since the object id of x would not change--that is, the memory location would be the same. Such operations that directly overwrite values in memory without copying are called in-place operations.

[[[00000000000000002083---cc6833212f201a25cda645009ca1bc2cdd270ecb68084d6de3cded4c43e9e9dc]]]On the other hand, if you write x = x + x, the object ids are different. Therefore, you can see that a new ndarray instance was generated (copied) in another location. In terms of memory usage, it is preferable to use in-place operations (if in-place operations are acceptable).

[[[00000000000000002084---ba51c6d84e091e388b2d6047926296adc84b406b0bc98e0935ef4fc465587159]]]The function id returns the ID of a Python object. The value returned by the id function varies depending on the execution timing and environment.

[[[00000000000000002085---2a9b8d3de4288ab6cc4f94e7da1c4d06b89b18be3ae8aaf8055655cc6bcaf71f]]]In DeZero's backpropagation

[[[00000000000000002086---9133ce1432118273c1c64695cb906f61b151a2a21247282192e3724fb951c61f]]]In DeZero's backpropagation, derivatives are propagated as ndarray instances. Here, we consider the case of overriding the second and subsequent backpropagation derivatives with x.grad += gx and 'in-place operations'. Let's actually rewrite the code in steps/step14.py to 'in-place operation'. And then try running the following code:

[[[00000000000000002087---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002088---7de0bfc2d39f4c29473b799a2eea384a8d815dfa691b7cbb2b4350fedfe2f0bb]]]After running the code above, the derivatives of x and y are both 2 and the ndarrays have the same ID. In other words, they refer to the same ndarray. And the problem here is that the derivative of y is not correct. The derivative of y should be 1 by nature.

[[[00000000000000002089---4a6a3cee5233e6f1c2596a1343214cff9862eb52fa6266c40d75b25e95694f22]]]The situation above is caused by overwriting values with in-place operations. y.grad gives wrong results because y.grad and x.grad refer to the same value. So change it to do a copy as x.grad = x.grad + gx. If we then run the same code again, we get the following result:

[[[00000000000000002090---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002091---2692695446b531c2cd2c072effabced4ab54a97aa67da9c91cf58740d79863d3]]]Now y and x are referencing another ndarray and the referenced values are correct. This fixed the issue. That's why we didn't use += (in-place operation) in step 14.

[[[00000000000000002092---6ea0bb329c5893f5f61327af9e11d27f83cd5dd93662a15ac2fb98cab7fbf8f2]]]step 20

[[[00000000000000002093---c6a95c5e55a7fbd9c4618a0bb337e0d1358647fa0e90090c026bf64a4b2deb6f]]]operator overloading (1)

[[[00000000000000002094---a684479522331e27f7a55649ea8d4ab04eef7b526876b0a68246b92c01bb426b]]]In the previous step, we made a mechanism to make Variable a 'transparent box'. However, the gimmick is not yet complete. The remaining challenge is to support operators such as + and *. For example, it is convenient to write y = a * b, where a and b are Variable instances. Such extension is the goal of this step.

[[[00000000000000002095---7190235b6937409fca0cf6259cd5017a89e8c533bbc578a73a71c781b8d96406]]]Our ultimate goal is to make our Variable instances 'look like' ndarray instances. By doing so, you can use DeZero as if you were writing NumPy code. For those familiar with NumPy, DeZero can be expected to greatly reduce learning costs.

[[[00000000000000002096---72355aa0ca20ebb384ec49d25fcc42c8b3a07818eb48a11f8f4ad5511e6f8968]]]From now on, we will extend Variable to handle + and * operators. Before that, we need to implement the function that does the multiplication (addition was implemented in step 11). So, first, implement a class that performs multiplication as Mul. By the way, Mul stands for Multiply.

[[[00000000000000002097---0017195b80505591b49734350257e62d9570e59669fb1033493888cf010eb4a8]]]Mul class implementation

[[[00000000000000002098---16af89b4abd09ac6e651ae74af4f7bba1b062a73c2341187f45048033d8bf755]]]The derivative of multiplication is obtained when . From this result, we can see that backpropagation should be performed according to the procedure in Figure 20-1.

[[[00000000000000002099---dc998843276536a7c3602978899133ce99f381fdbebc13f44004d0fa792374a1]]]
Figure 20-1 Forward propagation (top) and backpropagation (bottom) of multiplication


[[[00000000000000002100---cdd5bb060cf473b6ef0d8799a06ba4db6b1e27d811d79f58d657b3ed47aa8589]]]As shown in Figure 20-1, backpropagation propagates the final output, the derivative of , or more precisely, the derivative with respect to each variable of . Then the derivatives with respect to the variables are , respectively.

[[[00000000000000002101---84b6b8a458a8ed2c4389c7a16d942796f642e7b8fdb1af0cf47e26cf07b604c6]]]We are interested in a composite function that outputs a scalar. Therefore, in Figure 20-1, we assumed a computation that eventually outputs a scalar in a composite function.

[[[00000000000000002102---bda2fb39181daf895b04bf12a553e379d917322e20baa4b41cec957360064e18]]]Now for the implementation of the Mul class. Referring to Figure 20-1, the Mul class can be implemented as follows.

[[[00000000000000002103---ba30df5aac1cef8d8b0f74174a580f4b30278acf6f7fd96c5938f6bcbb76ef16]]]Next, use the Mul class to implement it so that it can be used as a Python function. Here is the code:

[[[00000000000000002104---6a58c17d2af925a65ce8bd666d4d55bfbcdd75e7a105fc276e7390d471ac2b8e]]]Now we can 'multiply' using the mul function. For example, you can write code like this:

[[[00000000000000002105---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002106---9ec2e6530b0208ed0f66c3a63e7e2e2df281121c0621972d4af2821b82c2b227]]]As above, it can be calculated using the add and mul functions. And the derivative is also calculated automatically. However, writing y = add(mul(a, b), c) feels somewhat cumbersome. Of course this would be better written as y = a * b + c. Next, we extend Variable so that it can be calculated using the + and * operators. To achieve that, we do operator overloading.

[[[00000000000000002107---d2ec90c42279044b04a513a8b0a6473e87d08d63de8bef27b87427e99995ebe7]]]Operator overloading allows operators such as + and * to call user-defined functions. In Python, user-specified functions can be called by defining special methods such as __add__ and __mul__.

[[[00000000000000002108---4be983176b6d39d78a75c429bfc921b1457e03b4745f8f8565ff7d59b00c16b0]]]operator overloading

[[[00000000000000002109---86154fbc7bc1368b34fa130e4ad9525f9f13a1c707d59844c24c5377dabe8a44]]]Here we start by overloading the * operator for multiplication. The special method for multiplication is __mul__(self, other) (arguments self and other are explained shortly). By defining (implementing) this __mul__ method, the __mul__ method will be called instead when * is used for calculation. Let's try implementing the __mul__ method of the Variable class as follows.

[[[00000000000000002110---5295b1d900767aa977f29f4aefcaae777a940b80a1362b453e0ffc8dc7dff5a9]]]As above, add the __mul__ method to the Variable class we've implemented so far. Now, if you do a calculation with *, the __mul__ method will be called instead, and the mul function will be called inside it. Let's actually calculate using the * operator.

[[[00000000000000002111---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002112---d37635726a80c591e1bddc9ed75cef8a1e56f4a10d71a76153ce44b850c39aa7]]]As you can see above, I was able to run the code y = a * b. When a * b is executed, the __mul__(self, other) method of instance a is called. At this time, as shown in Figure 20-2, a on the left side of operator * is passed as self, and b on the right side is passed as other.

[[[00000000000000002113---8937ccbb96cf2033ddd7e2d834a9afdb64b0bb0c88ecb15e2ba0ce6e7628ab8b]]]
Figure 20-2 How Arguments are Passed to the __mul__ Method


[[[00000000000000002114---8bd403a504ee2bf1576d26b31894400d59531c34fdc21b3806fce74f1aee49fa]]]In the above example, when the code of a * b is executed, first the __mul__ method, which is a special method of instance a, is called. If a does not implement the __mul__ method, the special method of the * operator on instance b is called. In that case, b is on the right side of the * operator, so the special method __rmul__ is called.

[[[00000000000000002115---a8e7e0939f654a6c0460a959d55763a78a19b7e38804578ecab3a29664201efa]]]Now we have the * operator overloaded. We implemented the __mul__ method of the Variable class to do overloading of the * operator. The same thing can be achieved with the following code.

[[[00000000000000002116---cf4a7f6e763cc0ca1e338fd3ad945530436f8de7345ec73a18f3ab42fd18fc96]]]As above, after defining the Variable class, we write Variable.__mul__ = mul. In Python, functions are also objects, so you can assign functions (themselves) as above. By doing so, calling the __mul__ method of the Variable instance will call the mul function.

[[[00000000000000002117---b63aa8f5debe92b1390da64d0f44bcb79c7625359f8acf7ceaf76db037567a93]]]The code above also sets the __add__ which is a special method of the + operator. It does the overloading of the + operator. Now let's do some calculations using + and *.

[[[00000000000000002118---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002119---69e42351deb5fbbf165cc7f0cee9dcd87e0a2b3144680d810d79667bb6b9440a]]]As above, we could write y = a * b + c. Now you are free to do calculations with + and *. Other operators (such as / and -) can then be implemented in the same way. We will continue implementing operator overloading in the next step.

[[[00000000000000002120---d743c12fbc424bb91f37f923c0f5fce9f1aac31a33e360c2da2e6ff471e6d3d1]]]step 28

[[[00000000000000002121---384879cd92b05ecda35523e233f29c742fdfc82c33c44d5d19358cda723208c5]]]Function optimization

[[[00000000000000002122---c9cb85367e18d7da9ade8de18a4e6dcb29e133f2235253724ab3fedd767847be]]]Our DeZero can now find the derivative automatically. This differentiation can be used for many purposes. One use - and the most important one - is function optimization. Here, we will try to optimize for a concrete function.

[[[00000000000000002123---1fc077d295da4472cd774ae7b48d7024b48bc9ee5eed43798f29b69529e9e5cd]]]Optimization is finding the 'function argument (input)' that takes the minimum (or maximum) value of a given function. Training neural networks is another optimization problem. The goal is to find parameters that minimize the output of the loss function. Therefore, what is done in this step can be applied to neural network training as it is.

[[[00000000000000002124---04abf9dd754e88f7a1c2e6b578b6519900846fd75d034242b95e4752894e0ff9]]]Rosenbrock function

[[[00000000000000002125---895f4f44c1b7742cee24d4dff6830ee10ad04694b5b51d5138e9bd8f6a475158]]]This step deals with the Rosenbrock function. This function is expressed by the following equation (28.1) and has the shape shown in Figure 28-1.

[[[00000000000000002126---f3ebdd7b048e76e7638490c1acac82a95c5431d0a6797f111b9b71f515a4d114]]]
Figure 28-1 Shape of Rosenbrock function (image is taken from reference [19])


[[[00000000000000002127---cc20244c50bec6ac935b4337f7543163a5cc2f57cc8e79d37a48af90c3882927]]]Looking at Figure 28-1, we can see that there is a parabolic valley and the valley is long. By the way, if you draw the contour lines of the 'mountain' in Figure 28-1, the shape resembles a banana, so the Rosenbrock function is also called the 'banana function'.

[[[00000000000000002128---1f6ae2c3771e929b1a3165a8ab673937163dca8ebffd69eadb02c68114af674b]]]The goal of this step is to find the minimum output of the Rosenbrock function. To answer first, the minimum value of the Rosenbrock function is at In this step, we use DeZero to see if we can actually find that minimum.

[[[00000000000000002129---55c8e02faeaff76d1925651c1c89a212280daeec704dab97a22c9e8594e23467]]]The correct definition of the Rosenbrock function is , as a constant. The above example corresponds to the Rosenbrock function for . The Rosenbrock function is often used as a benchmark function for optimization problems. Normally, when used as a benchmark, the value of is used.

[[[00000000000000002130---f13d94023146e5ad6dcfcfe86dc03ef731ce73c77141c5adbd9aab8d0a1fdeee]]]find the derivative

[[[00000000000000002131---6d1b6d20d2e37cf84e6fa26848f71c11053a7b57121fb3f229ffeafd42d95948]]]As a start, let's find the derivative -- and -- at the Rosenbrock function. With DeZero, this can be implemented like this:

[[[00000000000000002132---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002133---029385af9ec43ab4aca5186ae81bd1f05707fcce4aaa36d19cf57558bcf78890]]]As above, if you first wrap the numerical data (ndarray instance) in Variable, you just code according to the formula. After that, just call y.backward() and the derivative will be automatically obtained.

[[[00000000000000002134---cb94ccad94a105c162a8595be81fb9c21591c5514a94cdf241b94b0b250c10bb]]]After running the code above, the derivatives of x0 and x1 are -2.0 and 400.0 respectively. The sum of these two derivatives -- (-2.0, 400.0) and the vector form -- is then called the gradient or gradient vector. This gradient indicates the direction that will increase the output of the function the most at each point. In the example above, it means that (-2.0, 400.0) is the direction that increases the y value the most at (x0, x1) = (0.0, 2.0). This also means that the (2.0, -400.0) direction, which is the negative multiplication of the slope, is the direction that reduces the y value the most.

[[[00000000000000002135---166e5b076edfde96d1e4ba45d48bba911b51ae2dbba2d91db6219bf6e4f433d6]]]Gradient descent implementation

[[[00000000000000002136---9aba7c3a2e61cbb61f5ad8ef667ac8f87a7ccdd5cb278f9051ebc969a6983557]]]Functions with complex shapes often do not have a maximum value in the direction the gradient points (or a minimum value in the direction opposite the gradient). However, when confined to local points, the gradient points in the direction that maximizes the output of the function. Therefore, if you repeat the work of moving a certain distance in the direction of the gradient and finding the gradient again at that point, you can expect to gradually approach the target location (maximum value or minimum value). This is gradient descent. With gradient descent, if you start from a good point (given a good initial value), you can reach the target value efficiently.

[[[00000000000000002137---33fcb0793ee3256cdc7a8031f60417c8456c5ab495721ce4f242a3ea428118dc]]]Now let's apply gradient descent to our problem. The problem here is to find the 'minimum' of the Rosenbrock function. Therefore, it advances in the direction that multiplied the gradient direction by minus. With that in mind, you can implement it like this:

[[[00000000000000002138---e08811e546931f69c6555b954091aeaa55ff7dfb8a3032fc9790ff2e5d17f65d]]]# learning rate

[[[00000000000000002139---d44b9bc77be7c69e8fea9c4d0136ffc73437d96fe4abfea71f4132782012f8ec]]]# number of repetitions

[[[00000000000000002140---27df0a2172659159ea2a57c2438ad5166250594f83c082668abd29ff4b5faf9b]]]As above, set the number of iterations to update as iters. This iters stands for iterations. Also, set the value to be multiplied for the gradient in advance. In the example above, set lr=0.001. This lr stands for learning rate and means learning rate.

[[[00000000000000002141---2991b9c9025e2febf1bb7edd6e1eec51c365b6226c9083cc834ddb47f12e080b]]]In the for statement of the code above, the variable instances x0 and x1 are repeatedly used to find the derivative. At this time, the differential values are added to x0.grad and x1.grad one after another, so when obtaining a new differential, it is necessary to reset the differential that has been added so far. So before doing backpropagation, we call the cleargrad method on each variable to reset the derivative.

[[[00000000000000002142---252526aa0270b1db45c2409edbf85eb745b402583fb9e21a231fed093ebd93bf]]]Now let's run the code above. Then you can see how the values of (x0, x1) are updated. In the terminal it actually prints something like this:

[[[00000000000000002143---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002144---1dd9361e4918adb5523be8293c13a8bc374d7192ddc3c1e6f64ba2e31b8faa0a]]]Starting from the starting point (0.0, 2.0), you can see how the location is updated one after another. Plotting this result looks like Figure 28-2.

[[[00000000000000002145---f5f633e58d44b977545b5bc99af64ccc06736b66330284792ebf5fdfd7668966]]]
Figure 28-2 Update Path (The locus of red dots indicates the update process, and the asterisk indicates the location of the minimum value.)


[[[00000000000000002146---abbc98db68b83788286c46e5b5162094a860b532fded88e6ba9fc5899850a224]]]Looking at Figure 28-2, you can see that we are gradually approaching the starred destination. However, as you can see, it's still on the way. So, let's increase the number of iterations and set iters=10000 all at once. Plotting the results at this time looks like Figure 28-3.

[[[00000000000000002147---66d04fd2bd2a4158411a61ab2b7adcb7d0b4c43b8822e50579ded358f881c4a6]]]
Figure 28-3 Results when iters=10000


[[[00000000000000002148---f8408ca198d2e314ade767f0917a7a5e63eae2e84fbe267a093da5c2dfd690a9]]]We are closer to our destination, as shown in Figure 28-3. The value of (x0, x1) at that time is (0.99449622, 0.98900063). If you repeat it more times, say iters=50000, you will actually reach the location of (1.0, 1.0).

[[[00000000000000002149---3dc315413b24b99b55ea479e53aac15fe5dfca2873686e52db76d1f15174a377]]]This is the end of the work for this step. Here I used DeZero to implement gradient descent and was able to find the location of the minimum value of the Rosenbrock function. But 50,000 iterations is probably too many. In fact, the gradient descent method is not good at functions with long valleys such as the Rosenbrock function. The next step is to introduce another optimization technique and its implementation.

[[[00000000000000002150---61053bf45ad7afa6ae088827596b3fa9e83c63288b16ea32a7f1818a8191eaed]]]step 24

[[[00000000000000002151---1b57c6de80fc634d5f7d8459c4bfbe603cd6ad0c313e8beff6a14695155fcb18]]]Differentiation of Complex Functions

[[[00000000000000002152---fe24f9e5ad107623a145896929a0a5b827426900d7a0bf84e3f8a6609b9195fa]]]Our DeZero now supports common operators (+, *, -, /, **). So you can code like you would any normal Python programming. Its value comes when coding complex formulas. Here, as a result of our efforts so far, we will try to find some derivatives of complex mathematical expressions.

[[[00000000000000002153---6fc6fcaa09e5283ee377e9e63ebfd10483e601ca296a59ab6ce79af66b8b692b]]]The functions covered in this step are test functions often used in optimization problems. A test function for an optimization problem is a function used to evaluate the 'goodness' of various optimization methods. It's a 'benchmark' function, so to speak. There are several candidates for such functions. For example, the page 'Test functions for optimization' on Wikipedia[13] summarizes them in a table like Figure 24-1.

[[[00000000000000002154---96816d23b7b042ab3196bb83c17d94a1d6dd54373e07a0ee2e6ab8ac0eeef354]]]
Figure 24-1 Benchmark function list used for optimization problems (quoted from Wikipedia[13])


[[[00000000000000002155---23f48ca83ce91d2b3cbef96ffcb0d2fb06e055d341fd3c215de5917c9a10f0e7]]]Here, we will select three functions from Figure 24-1 and actually find their derivatives. By doing so, you will know the power of DeZero. We start with a simple function called the Sphere function.

[[[00000000000000002156---4c8070c96d4fa0fccee04cf38dfdb70dfc58342a2b69165c9f709e50a783587b]]]Sphere function

[[[00000000000000002157---f9ae0f81461960e1a704399743a6763d0c861f0282dee746ee9573e2480e75bf]]]The Sphere function is represented by the formula It's a simple function that just squares the input variables and adds them. Our task is to find - and - its derivative. Now, let's find the derivative at Here is the code:

[[[00000000000000002158---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002159---f294133e6895fd76b917af2678d57cb7d085b97eac190993637a81a439b760a4]]]As above, the desired computation can be written as z = x**2 + y**2. And the result is that both x and y derivatives are 2.0. If you check it with the formula, it will be, so the derivative at (x, y) = (1.0, 1.0) will be (2.0, 2.0). This is consistent with the above execution result.

[[[00000000000000002160---869f500aeee5e7ecec1f5a92def86614964b154ee57841c59aed24bd4db4b2f8]]]matyas function

[[[00000000000000002161---0a98ea3194688e0bc1f9acacf68c66e8ad78e88de9eb3f8001aa42e323ac8fdf]]]We then deal with a function named matyas†1. This function is represented in the formula as For our DeZero this could be implemented as follows:

[[[00000000000000002162---9c202a9e8f136176894fb58b1a15d5337fd337434ceeb1a1c485f66c4853089b]]][†1] matyas is pronounced 'marchas' or 'matyas'.

[[[00000000000000002163---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002164---26abbf71d7ebd3a3dc626f6119a056cc7c5bb59b2971055e2648374659f5510f]]]As above, the formulas can be easily transferred here as well. Since the operators of the four arithmetic operations can be used freely, their work is easy. By the way, assuming you can't use those operators, the matyas function should be written like this:

[[[00000000000000002165---98ba4cba6da7589e4386247afcef53e26b49c7b4b5e973119292365c2ab552a3]]]The above code is hard for humans to read. With this, you will feel the waste of being able to use operators such as + and **. By using those operators, the amount of typing will be reduced, and you will be able to read and write in a notation close to normal mathematical formulas. Finally, let's tackle a complex formula called the Goldstein-Price function.

[[[00000000000000002166---e0978b9760df977561c2204eab3cd9a03c26e1f02dde316a5687e596fccdac59]]]Goldstein-Price Function

[[[00000000000000002167---c02d4e60aeffdf2762dceadb51758b5a839055268d324071819b971f857ad2c0]]]The Goldstein-Price function is represented in the formula as:

[[[00000000000000002168---2c2e33255665889ba3088bcb191e37f085e390d398b0ca7f5b1b1ab8b6fc69fa]]]It looks like a fairly complicated calculation, but with DeZero it's not difficult to code. In practice, you can implement it like this:

[[[00000000000000002169---2707d9277ef8286ca0a64a8fdeebfc1e33b969ae72f0e648764677bfb7d4fa91]]]If you code against the formula, the above work should be completed quickly. It's impossible for a normal person to write this without using operators. Now let's actually find the derivative of the Goldstein-Price function.

[[[00000000000000002170---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002171---c2ea73d9541ee43b3cdb995201b06f60bfdc8267747911ab54d3a8a95070491e]]]After running the code above, the derivative of x is -5376.0 and the derivative of y is 8064.0. This is the correct result. DeZero did a great job of finding the derivative of even a complex calculation like the Goldstein-Price function! The correctness of the above result can be verified by checking the gradient. This concludes the second stage.

[[[00000000000000002172---25310925e803de42e72749c4bd83ec0a0cd1d733deffc7dd371cac651f542b73]]]DeZero has grown a lot in this second stage. At the beginning of this stage, DeZero could only do simple calculations, but now it can do complex calculations (more precisely, any complex 'connected' computational graph , can be backpropagated correctly). In addition, operator overloading allows you to write code just like regular Python programming. Still, since the differentiation can be obtained automatically, from a different perspective, DeZero can be said to make 'ordinary programming' 'differentiable'.

[[[00000000000000002173---a5566b8a9480728781d61eaa3b96a25e7d2d996af8192d8653cea2580e448fab]]]The basics of DeZero are now complete. From the next stage, we will extend DeZero so that it can perform even more advanced calculations.

[[[00000000000000002174---ac7c916bf3c8571532b62c75f5da00a585a341a819c5c0369b724ac1b293ad4a]]]Column: Define-by-Run

[[[00000000000000002175---90866567b22318ae0f90a6f71704988f38fc58f4686690bef06bfebf66ae1727]]]Deep learning frameworks can be broadly divided into two types. One is a method called 'Static Computation Graph' or 'Define-and-Run'. The other is a method called 'dynamic computation graph' or 'Define-by-Run'. In this section, we will discuss these two methods and discuss the advantages and disadvantages of each.

[[[00000000000000002176---e5859ec8b6a793aebb3c0d85521b0f34470331f271c77cb44c772f92adc01251]]]Define-and-Run (Static Computation Graph)

[[[00000000000000002177---c62ff6f2e42564a75fa89d538ac71f2b372fceef0ca17060f5a1c1cd37004592]]]The literal translation of the term Define-and-Run is 'define a computational graph, then run the data'. The definition of the computational graph is given by the user, and the framework transforms the computational graph so that data can flow. This processing flow can be represented graphically as shown in Figure B-1.

[[[00000000000000002178---6d00e0d93c43ebae44e10fe6a1d615c9d137e8ef40bf2aeb1c18083c6a560930]]]
Figure B-1 Processing Flow in the Define-and-Run Expression Framework


[[[00000000000000002179---00b4bea7355757f1e9f4f6e8bf523bf2353eea555a310399cee62cae20377548]]]As shown in Figure B-1, the framework transforms the computation graph definition. For convenience, we will refer to this conversion as 'compilation'. This 'compilation' unfolds the computational graph in memory and prepares it for the actual data to flow. The important point here is that the processes of 'defining the computational graph' and 'flowing data' are separated. This will become clearer if you look at the following pseudocode.

[[[00000000000000002180---4cdf30c8cee6c3828f3118e5a29f81581cd349fd09e68b60b24e1c31cd1ebec9]]]# Example code for a hypothetical Define-and-Run expression framework

[[[00000000000000002181---40642733367215d9fafabade184c5a309ea1e260b6503e23f6336ba9942cb197]]]# definition of computational graph

[[[00000000000000002182---298260d2a86a9ce3fcce2ab075a46c1f4a4def1082244d3cedd9cdd42532ea49]]]# Compile the computational graph

[[[00000000000000002183---6f3c1c9a83febe7281eeecfe26e73367d51f93aad173b6f79329eeed16e09aec]]]# send data

[[[00000000000000002184---369f16fc83bf2a252298a9963c4f563726ca275e7eeee6f7bbcec1643fe44d98]]]In this code example, the first four lines define the computational graph. Note that in those four lines of code, no actual calculations are performed. There, programming is done not for 'numerical values' but for 'symbols'. By the way, such programming is called 'symbolic programming'.

[[[00000000000000002185---4c8ad51f6e0c612dc419a7ccde269fdc3b3d12daf6c717d95305438918fcd2af]]]Thus, the Define-and-Run expression framework requires you to code abstract computational procedures using symbols rather than actual data. And that code should be written according to a 'domain-specific language'. A 'domain-specific language' here is a language that consists of framework-specific rules. In the example above, we need to follow rules like 'store constants in Constants'. In addition, for example, when you want to switch processing with an if statement, use a special operation for that purpose. That's also a domain-specific language rule. By the way, for TensorFlow, the if statement uses the operation tf.cond. For example:

[[[00000000000000002186---7d61add8d2821adc962632cae24af627115c117c5ff959f2086c0ade618d83c0]]]TensorFlow creates a computational graph over a tf.placeholder that stores data as above. Here, the operation tf.cond is used to switch processing depending on the value of flg at runtime. In other words, the operation tf.cond implements a Python if statement.

[[[00000000000000002187---3567eeea90e5e9009ad418675c3f56c94d85390f665eac4f690daf3ac2feef18]]]Many Define-and-Run expression frameworks use domain-specific languages to define computations. This domain-specific language is, in other words, a 'new programming language' that runs on top of Python (given that it has its own instructions for flow control, such as if and for statements, it's a 'new programming language'). ”). And it is also a language designed for taking derivatives. Against this background, deep learning frameworks are also called Differentiable programming [14] [15] these days.

[[[00000000000000002188---5bfe53f316c8df5aeb828dfb90fde1ff3dbebab6a7868bc99024cce1cba51031]]]That's the overview of Define-and-Run. In the early days of deep learning, frameworks corresponding to Define-and-Run dominated. Typical examples are TensorFlow, Caffe, CNTK, etc. (TensorFlow also later adopted the Define-by-Run style). Then came the next generation, Define-by-Run, which is also used by our DeZero.

[[[00000000000000002189---87d399079d705351f5af9196e15455ad225438568d7b6f282d22d45a38225b0c]]]Define-by-Run (dynamic computation graph)

[[[00000000000000002190---e46763023dd4eb9fd94679f6cf277a6e8bae6e2abce418a4cdc5459771e5381f]]]The term Define-by-Run means 'flowing data defines the graph of computation'. The feature is that 'flowing data' and 'building a computational graph' are performed at the same time.

[[[00000000000000002191---fe8d5b8dcbc4418cd784c60d6ef2a7421f149c425022f2a1a11f7c875d007414]]]In our DeZero case, when the user feeds the data--performs normal numerical calculations--behind the scenes, it creates a 'link (reference)' for the computational graph. That link is equivalent to the computational graph in DeZero. It is represented by a 'linked list' data structure. With that linked list, you can follow the link backwards after the computation is done.

[[[00000000000000002192---9213203fca356307d5eedeb9bb3a7aa88657b13770e6b304c68cb272263b1d26]]]The Define-by-Run framework allows you to code in the same style as normal programming with NumPy. In fact, with our DeZero you can write code like this:

[[[00000000000000002193---1fe5f006f4ae84e07f1f5ed6bd6701be8d979bf46af99eeb85de1f232fc514fb]]]The above program is almost the same as a general program using NumPy. The only difference is that the NumPy data is wrapped in a class called Variable. Otherwise, it is the same as normal code using NumPy, and its value is also obtained immediately when the code is executed. And behind the scenes, 'connections' for the computational graph are created.

[[[00000000000000002194---faee06c8025ba9df9fd06bef38bb5d36e1cb034b779d301dd43adf2b0201cce0]]]This Define-by-Run paradigm was first proposed by Chainer in 2015. Since then it has been adopted by many frameworks. Major examples include PyTorch, MXNet, DyNet, and TensorFlow (default since ver.2.0).

[[[00000000000000002195---8d6b0fd997b9a148fe356197bcf25bcc110fe6e65741f7901235057cfbd5ce3d]]]Advantages of dynamic computation graphs

[[[00000000000000002196---4b2cc294d07f9abc67f86d7406b3a78956e0f54c82d0ffd13808ad1d601fdc81]]]In the dynamic computation graph framework, numerical computation can be performed in the same manner as when using normal NumPy. So you don't have to learn a framework-specific 'domain-specific language'. Also, there is no need to 'compile' the computational graph into a proprietary data structure. This means you can build and run computational graphs as normal Python programming. Therefore, computational graphs can be constructed using Python's if and for statements. In fact, with our DeZero, we can write code like this:

[[[00000000000000002197---f15a9dbe0ecb20bfe35a2c6a6091be1f4c84884291896e0a62832df27531db94]]]As shown above, it can be calculated using while and if statements. And behind the scenes, a computational graph -- in DeZero's case, a link for the computational graph -- is created. Here, we used while and if statements, but other programming techniques such as 'closures' and 'recursive calls' that can be used in Python can also be used in DeZero as they are.

[[[00000000000000002198---bc1b91a106f73e6ed5f4e2ec7c783fed313df07f21898c4925ef8d0ba5939f4f]]]A static computation graph (Define-and-Run expression) framework requires the computation graph to be defined before data can flow. Therefore, the structure of the computational graph cannot be changed while data is flowing. Also, in static computation graph frameworks, you have to learn to use special operations like tf.cond for if statements. Such domain-specific languages require programmers to learn new things.

[[[00000000000000002199---377d8645267b2c90ff9cde3edb452b2db06c2b4c39d33f4d76ffc5ad125eec7c]]]The advantage of dynamic computation graphs can also be seen when debugging. It runs the computational graph as a Python program, so debugging can be done as a regular Python program. Of course, Python debuggers such as pdb are also available. A static computation graph framework, on the other hand, is transformed by compilation into a representation that only the framework can understand and execute. Of course, Python (Python processing system) cannot understand its own expression format.

[[[00000000000000002200---06460bf9ab9d6a75432f2e88844252b2744a6450714413f7291e22aaef6f7b62]]]The essential reason why static computation graphs make debugging difficult is that the tasks of 'defining the computation graph' and 'flowing data' are separated. This is because problems (bugs) are often discovered when 'flowing data', but most of the time the cause of the problem lies in 'defining the computational graph'. In the case of static computation graphs, it is often difficult to identify the location of the cause of a problem because the location and cause of the problem are often separated.

[[[00000000000000002201---28ba463abcc2bb51da3b0af9429f31028bf159cca2a604b14447fb3b7eabc8a7]]]Advantages of static computation graphs

[[[00000000000000002202---620356927fb2c730f1776d1e98ca71f5090d51af7d06776baa72b73820b3f4b7]]]The biggest advantage of static computation graphs is performance. One way to improve performance is by optimizing the computational graph. Computational graph optimization is done by transforming the structure of the computational graph and the operations used into efficient ones. A simple example is the case shown in Figure B-2.

[[[00000000000000002203---22f66f94c42fe9d78b7b63dd38d286b5c6e06cf495e9004a59f48b7d12d60b62]]]
Figure B-2 Example of Computational Graph Optimization


[[[00000000000000002204---35a9a416d8d820fd33a2e56b0271000ad908596a9194293b8b94360c3aeec34d]]]Figure B-2 shows the computational graph a * b + 1 and the optimized computational graph. Here we are using an operation that can be multiplied and added at once (many hardware has instructions that do both addition and multiplication at the same time). This conversion allows you to 'pack up' 'two operations' into 'one operation', reducing computation time.

[[[00000000000000002205---36cc333bf4920ad36a981ed96f20d6403790c6d87c9efc18d77e0a2ccb2935de]]]In addition to fine-grained optimization as above, optimization can also be performed after grasping the entire computational graph. In the Define-and-Run framework, the entire computational graph is obtained before data is sent, so optimization can be performed considering the entire computational graph. For example, if there is an operation that is repeated in a for statement, etc., it may be possible to perform efficient calculation by 'collecting' it into one.

[[[00000000000000002206---93a3e6d95f07aa624a5181e1ddb5ba69499a13e5d5adc060776f2d4b10e1b462]]]In neural network learning, it is common to take the flow of 'define a network only once and send data to that network many times'. At this time, even if it takes some time to build and optimize the network, there is a high possibility that the time spent will be recovered in the phase of repeatedly streaming data.

[[[00000000000000002207---578bf1f6398758e5beb78694855ff38be64117161f14693e5b34d855afe780a6]]]Another advantage of the Define-and-Run expression framework is that the act of compilation translates it into another executable. This makes it possible to flow data without going through Python. The main benefit of Python independence is the elimination of Python overhead. This is especially important in resource-poor edge-oriented environments.

[[[00000000000000002208---f7c900902b8b9c5d282c65b8dd85e1c3e09b34883666462ac49ec0bd75ddb08b]]]Also, when performing distributed learning on multiple machines, the Define-and-Run method may be advantageous. In particular, when dividing the computational graph itself and distributing it to multiple machines, the entire computational graph is required in advance. That's why the Define-and-Run expression framework comes into play.

[[[00000000000000002209---7f7ad40ba94a2507fb7fe55ea8aff92c6ece28879685607e5eb7cc261a4ecd72]]]summary

[[[00000000000000002210---736a20d846719784625c7995b5aa24954587b18c99cb2075b98da30301f38136]]]So far, I have explained that Define-and-Run and Define-by-Run each have advantages and disadvantages. To summarize, it looks like this:

[[[00000000000000002211---a2d8874bcf7b5f3544ff75eb3d25df80c3ee6ac4e0198ab27e6ed43287859c14]]]Table B-1 Comparison of Static and Dynamic Computation Graphs

[[[00000000000000002212---d049729eea77e16baa59455bf13726c386784e2cc65b762accf39a48da59cddb]]]merit

[[[00000000000000002213---2d0f65b5f1a2da59943f38af22475776a14dcc924c57940ebb0809d561964499]]]・High performance

[[[00000000000000002214---a6f07942017f6abb5be50d7edcc4813e72349fcd354d0290084e06b6e0e5e4a2]]]・Easy to optimize the network structure

[[[00000000000000002215---5ee5d1f2c3339773c8b40881ff171cb384010e6b4ff28fefcc32e89cf01f6392]]]・Distributed learning is easy

[[[00000000000000002216---084203f3e3e7a395253548066b1b089e31a442b390cd9e16075f7d381986eb62]]]・Computation graph can be controlled by Python

[[[00000000000000002217---6c6d4fad2ce297b977b8813e87936738baf90b6ca2d00fc39421f747ed1392d8]]]・Easy debugging

[[[00000000000000002218---8b63cc3a182be05893a025ff302512cf92092e8d41867deb66a48fc296ef9782]]]・ Good at dynamic calculation

[[[00000000000000002219---769a1ca315491bbf4d627e896e155a49f658f91e990c3fa2ddb242bb0ee4f328]]]Demerit

[[[00000000000000002220---31708036d0db8e45de08768c727bfc103dc6cdbe8ae045113eff16e9bc883d3b]]]・You need to learn your own language (rules)

[[[00000000000000002221---6f0535fa3d64ecd1a6d4ae0134679e418521d4c08e1216fe777cbbae8c58b1e4]]]・Difficult to create dynamic computational graphs

[[[00000000000000002222---b4ad7d23bfc86cf8166ef88a2adbce2f43266cef1a0ddcae9f70ff84385856dd]]]・Debugging work is difficult

[[[00000000000000002223---f8829db87d869bf14aadff4fca82eaa91744cbded44fc2bf2a276703565ecc7a]]]- Poor performance (can be low)

[[[00000000000000002224---8a22c11aea247091cd568f9f0832e1940ff0f1b15b8d089c62319de5b5f1b32a]]]Each of the two methods has advantages and disadvantages, as shown in Table B-1. In short, Define-and-Run has an advantage when performance is required. Conversely, when it comes to ease of use, Define-by-Run has a big advantage.

[[[00000000000000002225---5649eeba7bf2f43585138c5607b9da3ed40f7b49583989f073c747a5af1d60c9]]]In addition, both static computation graphs and dynamic computation graphs have advantages and disadvantages, so there are many frameworks that combine the two modes. For example, PyTorch is primarily a dynamic computation graph mode, but also has a static computation graph mode (see TorchScript[16] for details). Similarly, Chainer is basically Define-by-Run, but can be switched to Define-and-Run mode. Also, in the case of TensorFlow, since version 2, a dynamic computation graph mode called Eager Execution is standard, and you can switch to a static computation graph.

[[[00000000000000002226---8b84baaac8552bc028535b5e8ad292c394e35e4c34b867a0ffa92337fb43910e]]]More recently, there have been attempts to equip the programming language itself with automatic differentiation functions. A famous example is Swift for TensorFlow[17]. This is an attempt to extend the general-purpose programming language Swift—that is, modify Swift's compiler—and incorporate an automatic differentiation mechanism into it. By equipping the programming language itself with the function of automatic differentiation, it is expected to combine both performance and usability.

[[[00000000000000002227---801fe8f7c7cecdf2595af25ca0216d29e790b8733a30cab244ef0740a97f2330]]]step 59

[[[00000000000000002228---54969dffa2a9fa72a97e82cb47e5181e92d0805be57ab1edbf42fac186eb7f90]]]Time series data processing by RNN

[[[00000000000000002229---b3c8f99450a37fcb2dcf385cdac936311122545186dd0662c2a6945d681a96e2]]]The neural networks we have seen so far are networks with a structure called feedforward. Feedforward means that the signal goes in one direction. Its peculiarity is that the output depends only on the input. On the other hand, a recurrent neural network (RNN) has a looping structure as shown in Figure 59-1.

[[[00000000000000002230---5f6647143d8e4cc9786f6c93a9773a3215c498c4c38b30d18af670d50b6ee5f1]]]
Figure 59-1 Structure of RNN


[[[00000000000000002231---b4811dbef995294ac1ef4fae703bc96985c8ac8f0ee9ccac5d5340abdd804ff1]]]A looping structure like that in Figure 59-1 feeds the RNN's output back into itself. It allows RNNs to have 'state'. In other words, when data is input to the RNN, the 'state' is updated, and the output is determined according to the 'state'.

[[[00000000000000002232---bfd60282239c368fcd309b8508a0c9570ef7e46c867de6a0eb999822de5821af]]]The theme of this step is RNN. RNNs are computationally complex compared to feedforward networks. However, with our DeZero, even such complex calculations can be implemented simply. Here, we will explain the principle of RNN while showing the implementation of RNN.

[[[00000000000000002233---b2d628192fca8c9df479fb10fe01dd4aa7f37f51b0061b5671c0956070a83f10]]]RNN layer implementation

[[[00000000000000002234---42706739d5a7c5ef778f949a1dc414cffbe0ed9c131e0a680a9e96d9f630a80e]]]First, let's explain RNN using mathematical formulas. Here, we consider an RNN that has input as time-series data and outputs hidden states. Here means the time (or number) of the time series data. Also, since the state of the RNN is called the 'hidden state', we use the formula to represent the state of the RNN. Next, we express the forward propagation of the RNN with a formula.

[[[00000000000000002235---de281f443bcb1bd34d311ae850343f5fca014b8ba1b2dbae470cd0cce6959c29]]]First, let's explain the symbols in equation (59.1). RNN has two weights. One is weights for transforming inputs into hidden states. And the other is the weight for converting the previous RNN output to the next time output. There is also a bias as Note that here, is a row vector.

[[[00000000000000002236---3acfeeeee1afab32861c93ccf7e46d6559c038b4565ff0f5925dce0347537255]]]In equation (59.1) we perform matrix multiplication and transform their sum by the tanh function (hyperbolic tangent function). The result is the time output. This will be used by another layer as well as by the next RNN layer (myself).

[[[00000000000000002237---9163fd4e60fe0aceed530f58e1208ab7da308ba98f0cb37404c128604b8a72b8]]]Now, let's implement DeZero's RNN layer. Following the conventions we have seen so far, we also inherit the Layer class and write forward propagation in the forward method. So here is the code for the RNN layer (add this code to dezero/layers.py):

[[[00000000000000002238---5c5276d5ac4c8fe9e8e9398ba01c04e2dca9ccbf0b23a2dc6c013c77ddab72a1]]]First, receive hidden_size and in_size in the __init__ method of initialization. If in_size is None, you are specifying only the size of the hidden layer. In that case, the input size is automatically determined from the data flowing during forward propagation. And inside the __init__ method, we create two Linear layers:

[[[00000000000000002239---6aba5483d9c1897226de8c876e87c4f4a37eb1fb8b2dce3e4081bdb69d6b5e5e]]]x2h: fully connected layer that transforms input x to hidden state h

[[[00000000000000002240---b0a1a74fbe1888be9eb38e7db82e8cf5d162e25ebbc39ad42d38a64fa7bef859]]]h2h: a fully connected layer that transforms from the previous hidden state to the next hidden state

[[[00000000000000002241---ce16a028f0f8825b156b84bab431ba071b174ea28abfe371dc2a109cd992185b]]]Next, in the implementation of the forward method, processing is switched depending on the presence or absence of self.h (hidden state). The first time self.h == None, so we only want the hidden state from the input x. From the second time onwards, a new hidden state is calculated using the previously stored hidden state (self.h). The RNN layer also provides a method called reset_state. This is the method for resetting the hidden state.

[[[00000000000000002242---3ecb8bf013a775d5b41b76dffca2ba5d5b675f8df48b0195903ca4c0a7dc3c32]]]The RNN has one bias, as shown in equation (59.1). Therefore, we will only use the x2h bias and omit the h2h (Linear layer) bias (h2h is initialized with nobias=True in the code above).

[[[00000000000000002243---ea70ffe20c40b5f4d83fac06164a2f4c2aa0b887c51edbb94752e8423c054526]]]Now, let's actually send data to the RNN layer above. Now try running the following code:

[[[00000000000000002244---d1989bf4528ad88f44777d292df859a48bf32a8d023b2afbdcf6b6e8c6d777a4]]]# specify only the size of the hidden layer

[[[00000000000000002245---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002246---4adda179fe94b851625eb9a4058424a430232409100baeb3fd73fb30f5ce573f]]]As dummy data, I made an x of shape (1, 1). This means that the batch size is 1 (i.e. there is 1 piece of data) and the dimension of the data is 1. Giving this input x to rnn gives us the hidden state h. The calculation graph created at this time looks like Figure 59-2.

[[[00000000000000002247---88bdbc03bebb4d2b73be242ca498a8beb6edd5fa7886f2bfa6bcff41c244ffaf]]]
Figure 59-2 Computation graph when x is given first (x2h is a linear layer)


[[[00000000000000002248---04a014031c3a087f1df9032e9a230d7c42c03f492b6101931ae633feb546cffe]]]Now let's continue with the data. Now consider the case of executing y = rnn(np.random.rand(1, 1)) following the code above. At this time, the calculation graph in Figure 59-3 is created.

[[[00000000000000002249---5a8c43a8ea5a2474d86dc9c4fb34183e91366cdc5b763bf3cc5d02da11c68f92]]]
Figure 59-3 Computation Graph After Second Input Data Has Been Processed


[[[00000000000000002250---e955c8e188cfd81a2157857bb2d2b74253b4bc09ac87056fb08a0e5bef3dbc25]]]As shown in Figure 59-3, a larger computation graph is created by 'growing' from the computation graph in Figure 59-2. It is the RNN's hidden state that enables the 'growth' of this computational graph. By using the previously held hidden state, the RNN's computational graph has a 'connection' with the previous computational graph.

[[[00000000000000002251---20f63164f766eab9cbc5ad493810a134b226a51851f395a5eb6de8c7406c93ce]]]The RNN builds a computational graph containing all the input data, as shown in Figure 59-3. As such, it can also learn about 'relationships' between input data. Note that in Figure 59-3, there are two x2hs, but they are the same Linear instance and use the same weights.

[[[00000000000000002252---4d0ed65946cde10ab6219da9b69f886ceb1544fcf68113885b9eed383ea93d55]]]RNN model implementation

[[[00000000000000002253---2be98f9a66ac01bea40ba99bb252ddb4368b6d0b6ee63b2981683ff4b94c4533]]]Next, we will implement a neural network (model) using RNN layers. For that, we use a Linear layer that transforms the hidden state of the RNN layer into the output. Here we implement it as SimpleRNN as follows:

[[[00000000000000002254---c571d76b999f327cfa694556e08892f9b3921b4f175ec7dd870a793bdea5e159]]]Add a Linear layer to the instance variable fc as above. That Linear layer receives the hidden states of the RNN layer and computes the output. Also in the model above, the reset_state method resets the hidden state of the RNN layer. Now let's use this model for training. Here, we will use the mean squared error (mean_squared_error function) as the loss function. In that case, you can find the gradient as follows:

[[[00000000000000002255---6e488c0eb6fec3dd33484bb7f04c0b60a439dba6db3ef2bde094ef352032f491]]]# Dummy time series data

[[[00000000000000002256---09e079448a674ade1f4682db5e101f14bd95024d8d73832502fe76430f6409b1]]]# data one step ahead of xs

[[[00000000000000002257---a6e4dcd6313614f2429ef9c28f35ab50225eb9ed8c007bf125ccf31442e15e8e]]]First, generate seq_data as dummy time series data. Here, we want to train a model that predicts the data one step ahead for that time series data. For that purpose, the teacher data holds the data one step ahead of the input data.

[[[00000000000000002258---31b0818e1b58f50655ae19ba959a497c3bd6bbbc17d49245a7a3c9c8ab584332]]]Next is backpropagation, which is the key, but for illustration purposes, backpropagation is performed when the second input data arrives. By the way, the calculation graph when the second input data is given is shown in Figure 59-4.

[[[00000000000000002259---53e7ed3999203a3df099e31045479487f76dc00321ab4ca7a6b109a6fc81a7e9]]]
Figure 59-4 Computation Graph After Applying Loss Function


[[[00000000000000002260---ccfd97d57bb310da7c1ae8f8a26220a53673db4c1b660b4e3016ba3c45e873bc]]]Once the computation graph is created as shown in Figure 59-4, the gradient of each parameter is obtained by loss.backward(). Backpropagation for a computation graph consisting of such a set of input data is called Backpropagation Through Time (BPTT). This means backpropagating backwards in time.

[[[00000000000000002261---44cdd828ac32616266945039e8816017b4ccaaede6045c490220b52f1811c628]]]RNNs can learn about patterns of 'arrangement' of input data. The 'order' of data arrangement corresponds to 'time' in time-series data. That's why Backpropagation Through Time uses the term Time.

[[[00000000000000002262---4e262f8ff443bc387208a2500d89989dbabfb76ff7c07be15b4720a4af3736a8]]]Figure 59-4 is a computation graph when two pieces of input data are given. Of course, you can give as many input data as you like, whether it's 10 or 100. Then, according to the number, the computation graph will grow longer. However, in order to perform backpropagation successfully, it is necessary to 'truncate' the computational graph at a certain length. That is Truncated BPTT (truncate means 'to discontinue' or 'to cut off'). In the above example, we truncated at two input data.

[[[00000000000000002263---cbd648e8344fea7ceb6f6715483f83274ac586333e344b1a0efefa093c87b219]]]When doing truncated BPTT, care must be taken to preserve the hidden state of the RNN. For example, given the following input data after backpropagating the computation graph in Figure 59-4: In that case, the hidden state of the RNN should start from the previous hidden state, as shown in Figure 59-5.

[[[00000000000000002264---1798f5476d70f5f8a6bc8d3e632382c56fcfb6d20ddcb87a522bab88db2f7ac8]]]The first hidden state starts from the previous hidden state, as shown in Figure 59-5. And for that hidden state variable, we need to break the computational 'tie'. As a result, gradients no longer flow into the computational graph used in the previous study (this is a 'truncated BPTT').

[[[00000000000000002265---a7544eff43fee062cf9bf970a365c03a33a624574df4790b56cc04dd8e737e25]]]
Figure 59-5 Computation Graph Created by Next Iteration


[[[00000000000000002266---a24e633c07c018133041a8fe3743023fed23456738c52c06c172a648eaefa57b]]]A method for cutting the 'connection'

[[[00000000000000002267---50001b28468e79f20a1c6bb00e9175a48507d1ab3a9f8237c0961ac789cc6dec]]]Now, let's add a method to cut the 'connection' to the Variable class. Our Variable class is located in dezero/core.py. Here, add the following unchain method to the Variable class.

[[[00000000000000002268---0696d8701c0fcda1105911ec40332752bd3395760fa4b27b5b126a60f6796c28]]]The unchain method simply sets the creator self.creator to None. It breaks the 'tether' to the function that created it.

[[[00000000000000002269---3b2b88e80387afc911a59741935d11e4fbecdd71ef4d08b863383d132bb50a5b]]]Add one more convenience method to cut the 'tether'. It's a method called unchain_backward. When this method is called, it calls the unchain method of all the variables appearing there while tracing the computation graph backwards from the called variable. Its implementation looks like this:

[[[00000000000000002270---e4a00f805fecbf4b1e6879cbf13696a3ae8e25e9a51db75b876ad5191ae126d4]]]This implementation calls the unchain method of variables, walking backwards through variables and functions. This logic is the same as the backward method of the Variable class. However, it is simpler than the code of the backward method, because you don't have to consider the order in which variables are traversed (variable 'generations').

[[[00000000000000002271---039a1919ffb76a0dcc074c05a59a1e6c28e44150a50e545192478c101717d047]]]Predicting a sine wave

[[[00000000000000002272---b5478449eefbca59c8029223956e899c336f5840cf53d927d6530cbe80d496e0]]]Based on the above, let's try to learn RNN. We will use a noisy sine wave as our data set. It can be read using the SinCurve class found in dezero/datasets.py as follows:

[[[00000000000000002273---7d720d6924c55b1318cd3506ab43f06b02f1a841f7b20497f6bbc8c3d43baf98]]]# draw a diagram

[[[00000000000000002274---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002275---890b3405118b7aaaf405b766e8acaa2e3afe22b0021ce4f53f85087599a0988e]]]Here we are outputting the 0, 1 and 2 data of train_set. Each data is a tuple, the first element is the input data and the second element is the training data (label). And the code above gives us the following picture:

[[[00000000000000002276---2fc0de4669c8ec2e13069428043811b9f15ecf4f590cd02de18cbf5ed6bd2a63]]]
Figure 59-6 Plot Diagram for Sinusoidal Dataset


[[[00000000000000002277---2b2039e4a31bfd5b7039e863306bab4b30f44563af9c643ba4f276542fbb7276]]]It turns out to be a noisy sine wave, as shown in Figure 59-6. Now I have drawn two data, xs and ts, but I see only one curve. Actually, ts is the data one step ahead of xs. Therefore, in Figure 59-6, the two curves are drawn almost overlapping.

[[[00000000000000002278---c4e00a90a504605836d67bf8971264b2cf9c61adcefb611bc30c974da0905f33]]]The training data of the sine wave dataset is one step 'future' data of the input data. In the above code xs[1:] == ts[:-1]. Such datasets are used in the problem of forecasting time series data - the problem of predicting the next given data.

[[[00000000000000002279---6bfe0da30c499faf1b2c630c3d4c2ad623131715edacc56c5e24381e7304718e]]]Now, let's learn a sine wave dataset by RNN. The code looks like this (omitting the import part):

[[[00000000000000002280---9023f3814efeebe1f8b4ff64e7539fe1a26ec5468baeff7e130f21eaa84551af]]]# set hyperparameters

[[[00000000000000002281---ed2e55f03abbc7d039bc7b75f18cd2822c486af27f0730e4496a0294a41f0dcf]]]# Length of BPTT

[[[00000000000000002282---c9d0125c6272e5e4e945aec716e256ffb76021d5c5dd149413ea683aa0b427cc]]]# start learning

[[[00000000000000002283---df804e2edc05bddc70bd8522793fb6f3892794700a3e05f98d9ec2603a69a83d]]]# ① Convert shape to (1, 1)

[[[00000000000000002284---ce41438719d2781e51c0105b54bdf3e084485efc3f2ed8275d22469bb35f5d7f]]]# ②Adjust the timing of Truncated BPTT

[[[00000000000000002285---d37ca3d29bed2670144caa0568650418e131e9259d18276ebaa9911c18c6654a]]]# ③ Disconnect

[[[00000000000000002286---3069c257ef033ff72b9bb512241d43830b7a708b6409ef15289dbc6c45bc138f]]]Here are 3 additional points about the above code. First, at point ①, the shape of x is converted to (1, 1). In DeZero's neural network, the input data should be given as a 2nd order tensor or 4th order tensor (in case of CNN). Therefore, even if there is only one input data, it must be formatted as (1, 1).

[[[00000000000000002287---03b2888c4c827692234daa09ecaff04332e047115b3ac17b2b4c244677140653]]]Next, at point ②, decide when to call the backward method. The timing is either when the data has finished flowing 30 times or when the end (end) of the data set is reached. Finally, at point ③, the unchain_backward method cuts the 'connection' of the hidden state of the RNN.

[[[00000000000000002288---4d5299fed4815794a26b7c683ab8e2934278b840309449b03a7e7268ab8415e4]]]By calling loss.backward_unchain(), backwards from loss all variables appearing there are 'unchained'. It also cuts the 'tie' of hidden states in the RNN.

[[[00000000000000002289---d48f4e0a16060d7fd36b9025ff857659c53d8f0552fd8328c741a4d5c0b3ab42]]]After running the above code, the loss will drop steadily. Now let's use the model after training. Now let's input a new (noiseless) cosine wave and let it predict the value one step ahead. The code is as follows and the result of its execution is shown in Figure 59-7.

[[[00000000000000002290---c52c192074c29fba87fe1b13a462d3b293e4a77d09b0a0f82ce4ed1f28011f19]]]# reset the model

[[[00000000000000002291---35e77d1352f93bf8c3d0be476e7cf271346d4e0231625570d94ca9172fd3333c]]]
Figure 59-7 Model Prediction Results for New Data (y=cos(x))


[[[00000000000000002292---ac91d1f5b047f4081f8665fa7f2642d00fa865c5d0f2e3851a1c0bd5b36f2d2a]]]As shown in Figure 59-7, the predictions are generally good. However, in the current implementation, the data is processed one by one (because the batch size is 1), so it takes a lot of processing time. A larger batch size will result in a shorter processing time for one epoch. The next step is to fix the data so that it can be processed together as mini-batches. Additionally, we implement a better model using LSTM layers.

[[[00000000000000002293---06aa282b523f2f95cce1268e0b318a0cd92b5e5a6724259b324b31e771c7a14b]]]4th stage

[[[00000000000000002294---abc55283501d7f8cf42a98b7ec104dde0444a8acb1784051c54c2cb1343ebca5]]]create a neural network

[[[00000000000000002295---24992bae94843496b0d49b2f2a0e90a8e87344799ed108cf02dead825381353c]]]So far we have mainly dealt only with 'scalars' as variables. However, in machine learning, 'tensors (multi-dimensional arrays)' are the main players. The goal of this fourth stage is to extend DeZero towards machine learning, specifically towards neural networks. To do so, we first extend DeZero so that it can be computed using tensors.

[[[00000000000000002296---69137d2f17c578801f07704231c0ab4aebb50d574e61938a2a9807b70ff538df]]]Finding derivatives in machine learning can be complicated. But our DeZero already has the foundation for automatic differentiation. So what we're going to do isn't technically difficult. After that, my main job is to add functions and features necessary for machine learning on top of DeZero's automatic differentiation. Once that's done, I'd like to test out DeZero's capabilities by solving some machine learning problems.

[[[00000000000000002297---c629d3f5889218a41005b1ae423b8a51f97bf17efcb470185dd1ad62a4654b51]]]This stage is where DeZero, which has been built over a long period of time, blossoms in the field of deep learning (neural networks). By the end of this stage, DeZero will grow to be called a 'deep learning framework.' Let's move on to the 4th stage!

[[[00000000000000002298---6d409a808698085894c3bd2b364fab9e7b350f741174f1958f891db93247ab7e]]]step 4

[[[00000000000000002299---a47ec98c2be9ac989d67216a4b382bd965c8d6d1429e358eb466728dc2e674b1]]]Numerical differentiation

[[[00000000000000002300---c45e13ff4654ceb6963a5e1905da50c064278455a481417e426683f464b3e8f5]]]So far we have implemented a Variable class and a Function class. In fact, the reason why we have bothered to implement such a class is to obtain the differentiation automatically. Here, we will first review differentiation, and try to obtain differentiation using a simple method called numerical differentiation. The next step is to implement a more efficient algorithm to replace numerical differentiation - backpropagation.

[[[00000000000000002301---aad728fc56a0333a5ed86792d50243bf5cac6b4e7b8cfc6dce26fb98793b17f9]]]Differentiation is important in many fields, not just machine learning. Differential calculations are required in many fields such as fluid dynamics, financial engineering, meteorological simulations and engineering design optimization. And in such a variety of fields, functions that automatically obtain differentiation are actually used.

[[[00000000000000002302---2efd92288f9b4a1973d463a788c33030123d0dc90038c028a705cab05dc9d279]]]What is Differentiation

[[[00000000000000002303---6958152b6ec2b2b9762c824688ec67ba6081e0b9dbab5ec377c35aa2f0bd023b]]]What is differentiation? A derivative is simply a measure of the rate of change. For example, the rate of change of an object's position with respect to time—the derivative of position—is its velocity. Also, the rate of change of velocity with respect to time—the derivative of velocity—corresponds to acceleration. Thus, the derivative represents the rate of change. And it is defined as the amount of change in 'extremely small time'. Expressed mathematically, given the function, the derivative at is defined by the following equation.

[[[00000000000000002304---3c10bf34aa9d602e40a326fb86d8370f6939a91fb2f6dd5b51540dfae0775636]]]In Eq. (4.1) represents the limit, which means that is approaching infinitely. Here, in formula (4.1) is the slope of a straight line passing through two points, as shown in Figure 4-1.

[[[00000000000000002305---b70a8488f20d93ea1274102af9170f57ea2122d1f2aea5d534edb6e43fd93575]]]
Figure 4-1 A curve and a straight line passing through the two points


[[[00000000000000002306---f6f30e23ac2b07aca8fac19294f0225f73ae484282cdc1707bb3e310939e4a7f]]]As shown in Figure 4-1, the rate of change of the function at the two points is . Here, by bringing the width of , as close to 0 as possible, the rate of change in is obtained. This is the derivative of Also, if is a differentiable interval, then equation (4.1) holds for ``any'' within that interval. (4.1) is therefore also a function, which is called the derivative of

[[[00000000000000002307---07e0ae3bbfb26d3e4ffff42744962529bbf75183624cf6697afd7a7ed527e760]]]Implementation of numerical differentiation

[[[00000000000000002308---202e75cb0510fb2f27412cf5be9e649358d90b0e4e3f32d7a6cc5a102f4005e5]]]Now, let's implement the differentiation according to formula (4.1), which is the definition of differentiation. The caveat here is that computers cannot handle limits. Therefore, we express it by approximation. For example, use a small value such as (= 1e-4) to calculate equation (4.1). The method of finding the amount of change in a function using such minute differences is called numerical differentiation.

[[[00000000000000002309---317dc76a9644e9c6cc98e79b2062b54992e656d804b110fd9310f3d7ecf8dee8]]]Numerical differentiation approximates the 'true derivative' using fractional values. Therefore, the value includes error. As a device to reduce the approximation error, there is a method called 'central difference approximation'. Instead of diffing from , the central difference approximation diffs from . In a diagram, it looks like the blue line in Figure 4-2.

[[[00000000000000002310---c32ad70748fe3d383cf55bfec71a80fcaa9e7967955eb2970cc950e74f0362c0]]]
Figure 4-2 Comparison of 'true differentiation', 'forward difference approximation', and 'central difference approximation'


[[[00000000000000002311---1cd649c7a14b00ca2a45d56a7066b27324dacb5435e2cac43777a23844ae5720]]]As shown in Figure 4-2, the method of finding the slope at two points is called 'forward difference approximation'. And the case of and is called 'central difference approximation', and it is known that this has less error. I won't prove it in this book, but you can intuitively see it from the slope of the straight line in Figure 4-2. Note that the slope of the straight line in the central difference approximation is (note that the denominator is ).

[[[00000000000000002312---76ee4979ef31febf98fe4fa8aced9329b573d84625d5b48d16aa2b39a3144b3e]]]Taylor expansion proves that the central difference approximation is closer to the true derivative than the forward difference approximation. For the proof, please refer to [1].

[[[00000000000000002313---842078d10e132dfb136c19258cb1815a098f074f0aa0a99120086278a8ebeb71]]]Now, let's implement a function named numerical_diff(f, x, eps=1e-4) that finds the numerical derivative using the central difference approximation. Here, the argument f gives the function to be differentiated as a Function instance. Argument x gives the variable to be differentiated as a Variable instance. The argument eps represents a fractional value and defaults to 1e-4†1 (eps is an abbreviation for epsilon). Then numerical differentiation can be implemented as

[[[00000000000000002314---702922613171be64b73beac4d4e215025d0bcc161aaf23822c147d0963d253c5]]][†1] 1e-4 represents 0.0001.

[[[00000000000000002315---77af832afe4990557dacd541f46689fc76b8c18050f7c3f70268743157ee57bb]]]As above, there is nothing else to worry about except that the instance variable data of Variable has the actual data. Now, let's actually find the derivative of the Square class implemented in step 3.

[[[00000000000000002316---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002317---a84d5be7f386c859817077f930dd3e7607726062029786b129df7f694f3171ac]]]As the above results show, the derivative at is The result of this run can be said to be approximately correct, as it gives the exact differential value without any error.

[[[00000000000000002318---59f68a3ba703511434991f7c744f67e6d12288a930248c3876ce397d414c6b4c]]]The computation of the derivative can also be solved analytically. Solving analytically means deriving the answer only by transforming the formula. In the above example, from the differentiation formula, when ( is the symbol for differentiation with respect to ). So the derivative at becomes This value is the exact value without error. The result of numerical differentiation above is not exact, but you can see that the error is quite small.

[[[00000000000000002319---f349ab0b3eb1729d258dcfd20064a69ab045889389cd30b5b624266396f77857]]]Differentiation of Composite Functions

[[[00000000000000002320---ab4f43b1770a8ea7a2a9db940bced47dd8e726488b10a0ee531cf8ff3ab96e49]]]So far we've dealt with a simple function called Next, let's find the derivative of the composite function. Here, let's find the derivative for the calculation. The code looks like this:

[[[00000000000000002321---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002322---b7dbfb311bbb9307837227e0cf879b0c4b4859a2a0708e74752e51b1ab12a1b8]]]In the above code, I've grouped a series of calculations into a function called f. Functions are also objects in Python, so you can pass functions as objects as arguments to other functions. In the example above, we actually pass f, which is a function, to the numerical_diff function.

[[[00000000000000002323---c3305edf3856c577c31df42839bf1860ac27392f029fb76ca321a1fe9f2c5e26]]]Looking at the result above, the derivative is 3.297... This means that if you change x by a small amount from 0.5, the value of y will change by 3.297... times that small amount.

[[[00000000000000002324---3a311846f9de5ce7896328341146c12dae2b69809da7ed7c5c0b442cc03abdcf]]]With the above, we have succeeded in obtaining the derivative 'automatically'. Once the desired calculation was expressed in code (in the example above, we defined the function f), the program automatically calculated the differentiation. If you follow this method, you can automatically find the derivative, no matter how complicated the functions are combined! After that, if we increase the types of functions, we will be able to find the value of the derivative of any calculation—if it is a differentiable function. Unfortunately, numerical differentiation is problematic.

[[[00000000000000002325---085daf4c93e5378f73ff2fa9761e94f27a1facd0e2066fc4d1427abe548a94d5]]]Problems of numerical differentiation

[[[00000000000000002326---2af049d33acd53b23b661011eaf523ba40a98a2c3007d9daf490523a19c3a1f9]]]Numerical derivative results contain errors. In many cases the error is very small, but some calculations can have large errors.

[[[00000000000000002327---3d993939d7f709fd4705654303e93db069eed158d276263da80f69ced60119e0]]]The reason why the result of numerical differentiation tends to contain errors is mainly due to 'significant loss'. In calculations to find 'differences' such as central difference approximation, the difference is taken from numerical values of the same size, but the number of significant digits in the calculation results decreases due to cancellation of significant digits. For example, consider the calculation -- subtraction of close numbers -- when there are four significant digits. The result will be 1 significant digit. Originally, the result might have been like this, but it can be considered that it is because of precision loss. By the same principle, computation of the difference of numerical derivatives is prone to errors due to cancellation of significant digits.

[[[00000000000000002328---5c772f4e70f3621f6c06eab2c21682410a3192e5f90f6b310508c6822e68a11b]]]A more serious problem of numerical differentiation is its high computational cost. More specifically, if you want to take the derivative with respect to multiple variables, you have to take the derivative for each variable. In neural networks, there are cases where differentiation is required for more than several million variables (parameters), and it is not realistic to obtain the differentiation of that number by numerical differentiation. In its place is backpropagation. From the next step, we finally move on to backpropagation.

[[[00000000000000002329---64fa20fba1d066b28f00e800e69f2dea91fe4b924ea5a4c929994d9a65d6f79c]]]Note that numerical differentiation is easy to implement and gives approximately correct values. Backpropagation, on the other hand, is a complex algorithm and its implementation can easily introduce bugs. Therefore, to check the correctness of the implementation of backpropagation, we use the result of numerical differentiation. This is called gradient checking, and is a technique for comparing the results of numerical differentiation with the results of backpropagation. Slope checking is implemented in step 10.

[[[00000000000000002330---dc716b4c25d9e657f599acdd98668c9fda9c014a5005850ba4889aefdf9ab744]]]step 57

[[[00000000000000002331---407982be20fb33ef06ee2644a34dc5a68ba7ee6fcca6612ea614225ade41d957]]]conv2d and pooling functions

[[[00000000000000002332---232fc844a007ce8ce13c12bcfda7236e35f6d031d329534ab9efaa2bdb2cac1a]]]Steps 55 and 56 discussed convolutional and pooling layers. Seriously, the implementation of the convolution operation would be a code with multiple layers of for statements. Such an implementation is cumbersome, and NumPy slows things down with for statements. Here, the implementation using the for statement is not performed, and a simple implementation using a convenient function called im2col is performed. By the way, the name im2col is an abbreviation for 'image to column', which means 'image to column' in Japanese.

[[[00000000000000002333---cf16a8d52d5b42f3d405fc7e076c54744a0afa3d8ad990c8f9d67b3f2adb8cfe]]]In DeZero, neural network conversion processing is implemented as a 'function'. Layers with parameters inherit the Layer class to manage parameters. Here, the processing performed in the convolution layer is implemented with a function called conv2d (or conv2d_simple). Then implement a Conv2d layer that inherits from the Layer class. Also, since the pooling layer has no parameters, it implements only the pooling function.

[[[00000000000000002334---0be5a43ec3d7dd76127ec6289839f01c66b9deb9890dea650d1ddd594daa3324]]]Expansion by im2col

[[[00000000000000002335---20fd73729f3d95aa5292969138091ce89fc20f91035052edad1df166bedc91a3]]]im2col is a function that 'expands' input data. Expands the input data in a way that is convenient for the kernel of the convolution operation. As shown in Figure 57-1, for the input data of the 3rd order tensor, extract the area to apply the kernel (more precisely, extract the kernel area for the 4th order tensor including the batch number).

[[[00000000000000002336---b0e9c2e040ae1c1b766430fe98a00f7680197305e4b09b42563ea72fd6bd5d4e]]]
Figure 57-1 Expanding Kernel Coverage Space


[[[00000000000000002337---678b7496c96dcf8ae27437036c621aa5737b7fc33c10a4b73d797593d43922f8]]]Pick out the region where you want to apply the kernel, as shown in Figure 57-1. Then, format the extracted area into a single column, and finally convert it to a 'matrix (second-order tensor)'. This is what the im2col function does.

[[[00000000000000002338---ab2bf63400cbd8ef9b83c695ae36cec9781031ee4338761df7a084afd3eadc64]]]Chainer's im2col performs the processing up to the first stage in Figure 57-1 (not including the processing of the reshape part). This is because once the kernel region is taken out, the desired calculation can be performed by the tensor product†1. In this book, we need to process the reshape part because we use matrix multiplication. In addition, the argument of the im2col function used by DeZero has a flag called to_matrix, and only when this is True, the processing of the reshape part in Figure 57-1 is also performed.

[[[00000000000000002339---88e14dc1e5c1b46f15a616d2696782a63088f91ae8ae6cdf7a47b4ca6d80abc2]]][†1] The tensor product is simply an extension of the matrix product. It performs the multiply-accumulate operation between arbitrary tensors by specifying the axes of the tensors. NumPy allows you to compute tensor products using np.tensordot and np.einsum.

[[[00000000000000002340---ae4d59a81fece331a1da1e066fbfa1c4a0dab3b025039943a457a86249320f9f]]]After expanding the input data by im2col, the kernel (filter) of the convolutional layer is expanded to one column. Then compute the product of the two matrices, as shown in Figure 57-2.

[[[00000000000000002341---47d1e010d91150f19472da1450f1abb02e21b12e09edfcf721974ce77df6c9c4]]]
Figure 57-2 Calculation by Matrix Product of Input Data and Kernel


[[[00000000000000002342---de062f0b0861b57782664ef5722348e53b7263c101211e0eb7c5344a3ddc0ba7]]]Compute the matrix product, as shown in Figure 57-2. The output of this matrix product is a matrix (second order tensor). Finally, format the output into a 3rd order tensor (more precisely, a 4th order tensor including the batch number). The above is the flow of implementation of the convolutional layer.

[[[00000000000000002343---0ae5d4461cf2b0ab7a4e1f87d17212e71a738568a371772e77260eeb989e3481]]]Expanding with im2col during a convolution operation will in most cases result in more elements than the original volume. Therefore, implementation using im2col requires a lot of memory area. But since it can be computed as a product of matrices, it can benefit from optimized functions in the matrix library.

[[[00000000000000002344---cbf281378688d011a5c25edd35eb81d8572374da4103af20d435e78bc9b93acf]]]Implementation of the conv2d function

[[[00000000000000002345---05e90a64fa11cacb290d5b38fe5b3bfcdd9f6aa8b809d4ba764746773fe60be7]]]In this document, it is assumed that DeZero's im2col function is used as a black box (without worrying about the implementation inside). This im2col function is a DeZero function. You can perform calculations by giving a Variable instance, and its derivative can also be obtained backward.

[[[00000000000000002346---4b10eafb1a6b3ab0ce70a08f9bbbaddb4276ac47077e2289767020f591c74264]]]Since the function code used in CNN is large, it is stored in dezero/functions_conv.py, which is a separate file from dezero/functions.py. DeZero's im2col function is also in dezero/functions_conv.py. The DeZero functions implemented by dezero/functions_conv.py are imported by dezero/functions.py. It allows users to import all functions from dezero/functions.py.

[[[00000000000000002347---dcc08ad9e6b668404a44c15785b08f6b988535024e2f6175ba671f9f45bad19c]]]Now let's take a look at DeZero's im2col function. It has the following interface: Its arguments are described in Table 57-1.

[[[00000000000000002348---d447f7d937eb97943e975d2d32bcf0f390fac7f7be78aa47038d3c901cb7de1c]]]Table 57-1 Arguments for the im2col Function

[[[00000000000000002349---7d341bb44cea118093264bfe2372d6f57d3b45fed41f8b5197de111ba1a85a00]]]argument

[[[00000000000000002350---850e16d958cea72a95707148f5dc4ad7e984134f35b4c9ac7289facfeeca78ba]]]type

[[[00000000000000002351---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000002352---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002353---e7c31dccd8f58cb2ca6fa30e828f1e69312d05747f8c094e33b3e85b884b3731]]]Input data

[[[00000000000000002354---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002355---266dd86f83c6384c2e339c5dbbc3f5e1924a27e3bba9afc7c88c10ca51cf8791]]]kernel size

[[[00000000000000002356---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002357---d1af7f2083febd2dee0a65da06742ebc315f6a8f0316b6275b188875fdc64330]]]stride

[[[00000000000000002358---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002359---9c0bee2f1a1e931c077c2670cc6249e93e5ddd6e51f557d10c50be9a9b1c5996]]]padding

[[[00000000000000002360---014616f024086c52ba473f65f8fcd5dbf84a551ee4a8a5460559d0880db96b67]]]whether to format into a matrix

[[[00000000000000002361---e0c5b123b2765d64a19ce43643f5461ba918ead0519cca9a3a2572e1078f3b54]]]Argument kernel_size gives int or (int, int) (tuple). If (int, int) then the first element corresponds to height and the second element corresponds to width. If only an int is given, the height and width will be the same value. Arguments stride and pad are given in the same format. The final argument to_matrix is a flag. If True, after extracting the region to apply the kernel, shape it into a 'matrix' (so that it can be computed with matrix multiplication).

[[[00000000000000002362---9984c12fb707e0ca7ec0a7847c8ce4c042469e42c786cf5136c492a87ee4babd]]]Now let's actually use this im2col function.

[[[00000000000000002363---ec87e1401673d7034f5a95e8a6d19ff0aa70facb377f5dd9fdabfc3805383430]]]# 10 data

[[[00000000000000002364---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002365---ae795c4ec617ba5000239d7c3f4e565d3e1dd03b5b4f28eb93e75f96c196fd1e]]]Here are two examples. The first is data of shape (1, 3, 7, 7). This is data with a batch size of 1, channels of 3, height of 7, and width of 7. The second is when the batch size of the first example is increased to 10. After applying the im2col function respectively, the number of elements in the second dimension is 75 in both cases. This matches the number of elements in the kernel (3 channels, size (5, 5)). Also, if the batch size is 1, the im2col result is of size (9, 75). On the other hand, the second example has a batch size of 10, so (90, 75) and 10 times more data will be stored.

[[[00000000000000002366---3b09df10561f273fd16ed3f61750b2007e11eb40b1f8d73ef75c107ca5c51ad1]]]Now, use the im2col function to implement the 'DeZero function' that performs the convolution operation. Before that, we implement a convenience function called pair(x).

[[[00000000000000002367---87d022b3fc1d69adfc0d95db983cfa3a23e5f3f5334b5e43ebaa5ce2ada8d1ca]]]The function pair(x) returns in the form (x, x) if the argument x is an int. Also, if x is a tuple with two elements, it is returned as is. With this function you can get a tuple with two elements for both int and (int, int) forms, like this:

[[[00000000000000002368---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002369---beffc157e1c9861c7b96d7593f106c2a90199423605ec8d34d3df02f421ae018]]]Now implement a function called conv2d_simple that does the convolution operation (add the following code to dezero/functions_conv.py, not dezero/functions.py).

[[[00000000000000002370---f47c221927483f5ae8b4b785dc327f6b6ebef0eb67fbc0c1d85941c013658e9e]]]# to avoid W conflicts (Width and Weight)

[[[00000000000000002371---ebb28b30dab8c19f2350cc3b4ef0981bfaf5d4b6c764577bea81d24442250c7c]]]In this code, the important parts are shaded. First, at point ①, expand the input data with im2col. Then, in (2), the kernels (Weight) are expanded and arranged in a row as shown in Figure 57-2. There, -1 is specified as in Weight.reshape(OC, -1), which is a convenience feature of the reshape function. If you specify -1 for the argument of the reshape function, the elements will be put together so that the number of elements in the multidimensional array is consistent. For example, an array of shape (10, 3, 5, 5) has 750 elements. increase.

[[[00000000000000002372---549613039d9afb4d91a407aeaa82fc6ca7aa6e8a925bac23fd483791c61c7549]]]Then, in ③, the matrix product is calculated. There, the calculation including the bias is performed using the linear function, which is a linear transformation. Finally, in ④, shape the output size into an appropriate shape. For this formatting, we use DeZero's transpose function. As explained in step 38, the transpose function can change the order of the tensor's axes. Now change the order of the axes as shown in Figure 57-3.

[[[00000000000000002373---24b1c50c4e912f31b09113371040927eaf072fcdcd1d0e27b549541c7b93fca7]]]
Figure 57-3 Changing the order of the axes with the transpose function


[[[00000000000000002374---ed547915f90def34b4764cc2e88920d0c22a4fb05bf6388ac125de82d239e60d]]]The above is the conv2d_simple function. We have implemented the convolution operation using the DeZero functions we have implemented so far, so the backpropagation can also be done correctly. For example, you can use the conv2d_simple function like this:

[[[00000000000000002375---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002376---4c698c9ed8d5efae6f83f09340022ce177636d2b371a509113bb10f074ecebae]]]As you can see above, I was able to do the convolution operation. The implementation of the convolution operation shown here is a simple implementation method (for that reason, the function name is also conv2d_simple). A better implementation scheme is to extend the Function class and implement the Conv2d class. dezero/functions_conv.py has a Conv2d class and a conv2d function. Please refer to it if you are interested.

[[[00000000000000002377---47433f5d24c297cc778da708e60df38757bc17eb291742bed56bbbcbcd364a73]]]In the Conv2d class, calculation is performed by im2col method in forward propagation, and calculation is performed by tensor product. Also, the 2D tensor unfolded by im2col--here we call it col--is deleted from memory as soon as it's used (col is huge, so memory consumption is high). And backpropagation is calculated by 'transposed convolution†2'.

[[[00000000000000002378---7ec0cc3254d253acaee0f7da1895bd16ec3b1f78141ca7615a927ef3306627da]]][†2] Transposed convolution is also called “deconvolution”. It does the inverse transformation of convolution.

[[[00000000000000002379---59197ed22996eb44630ac224c5a3d0380cfc7cd3a12a498e3b74ae482bf6ae27]]]Conv2d layer implementation

[[[00000000000000002380---a8a5938d4223a47972caea8cbb3938fbe6045be92dd32ec85a325a6af3d94c09]]]Now it's time to implement the Conv2d class as a 'layer' (rather than a function). This can be implemented like this:

[[[00000000000000002381---89b53d42dfa8f26891dd5b9726d3920803ce8247f6da1d977051af2f2a3c761b]]]# or y = F.conv2d(x, self.W, self.b, self.stride, self.pad)

[[[00000000000000002382---c6d93b21f376b20de9e8bc1494807537ba7cc93e54f3855a4cb18af99692401b]]]First, inherit the Layer class and implement it as a Conv2d class. Initialization takes the arguments in Table 57-2.

[[[00000000000000002383---b452ad98ee22a2a137451eede19bd7978a4960aeabceceec0aa6a9c9c3b150f0]]]Table 57-2 Conv2d Class Initialization Arguments

[[[00000000000000002384---7d341bb44cea118093264bfe2372d6f57d3b45fed41f8b5197de111ba1a85a00]]]argument

[[[00000000000000002385---850e16d958cea72a95707148f5dc4ad7e984134f35b4c9ac7289facfeeca78ba]]]type

[[[00000000000000002386---9683551a843c71d8b84ba930126021ef40831ee11bd5e69b06a3ea589148bba6]]]explanation

[[[00000000000000002387---b12cfa29f176f8b29a5441a77fda4bac4a416da38fc8b048c0f6c879672fdbf1]]]Number of channels of output data

[[[00000000000000002388---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002389---266dd86f83c6384c2e339c5dbbc3f5e1924a27e3bba9afc7c88c10ca51cf8791]]]kernel size

[[[00000000000000002390---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002391---d1af7f2083febd2dee0a65da06742ebc315f6a8f0316b6275b188875fdc64330]]]stride

[[[00000000000000002392---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002393---9c0bee2f1a1e931c077c2670cc6249e93e5ddd6e51f557d10c50be9a9b1c5996]]]padding

[[[00000000000000002394---d8ab1fa70709c4eecfa0208b8fed8cb22a4f1e1a0700e5228334779f37eace44]]]whether to use bias

[[[00000000000000002395---21f29f29db61e2c0ca4f0382e50bc876069627c6a48dc55c3022c520a5ccae66]]]the data type of the weights to initialize

[[[00000000000000002396---8ce927f7de53d81f4afbcebe4a9f8b3a0b725290a88a1a9ec1b40629baaa4ac9]]] or 

[[[00000000000000002397---3e2f533db09ad17ad8868f7c8861a1f326b6311dbcb3b95ffbc212cd8ca324eb]]]Number of channels of input data

[[[00000000000000002398---66b20ababffc23b8d544c0baab6425013bd34ca70451e1769faaae90adf8fe98]]]Note that in Table 57-2, in_channels specifies None by default. If None, get the value of in_channels from the shape of x in forward(x) and initialize the weight data at that timing. This convention is the same as for Linear layers in fully connected layers.

[[[00000000000000002399---f5faaaff09d431d91f7e6453fec2ca919aff6dd5eb83e75a55a03d307f35d45c]]]The main process uses the function conv2d_simple (or conv2d) implemented earlier. The above is the implementation of the Conv2d layer.

[[[00000000000000002400---77b02e1badb193f9f1fcc237ed96832912a0646c3a348ef1d50f1725a4c3235d]]]Implementation of pooling function

[[[00000000000000002401---6a7b19785f572988c52d6541b72854efec0f8173ae8b1651a63e10401e2c6aca]]]Finally, implement the pooling function. As with the conv2d_simple function, this also expands the input data using im2col. However, pooling differs from convolutional layers in that it is independent in the channel direction. Specifically, the pooling coverage expands independently for each channel, as shown in Figure 57-4.

[[[00000000000000002402---20a492bde0a08af4faa082f19768633dc7153d16fb8faa8101371ea88f7e8ef5]]]
Figure 57-4 Expand pooling coverage for input data (example of 2x2 pooling)


[[[00000000000000002403---c9a7b188cf0bb4898ffa052b85489a6a48ea296932277234552da18325c0e99e]]]Once expanded like this, all that's left to do is find the maximum value for each row in the expanded matrix and shape it into an appropriate shape. Graphically, this looks like Figure 57-5.

[[[00000000000000002404---f7a003b733976f055a14a18935d27aff8f9e739aead82a88a0e697135481113e]]]
Figure 57-5 Implementation flow of the pooling function (the element with the maximum value in the pooling applicable area is drawn with an orange background)


[[[00000000000000002405---a969b16101feb2828a913aaf5275f63f775f7895317c2c062572e7b99f42afd8]]]The above is the flow of implementation of the pooling function. So here's the code:

[[[00000000000000002406---682c1dc59ca8aa07f462880541273fef6676a84bd99ea6d7261339238a16bf58]]]# ① Unfold

[[[00000000000000002407---aad73a6343726f8150dea8a719269dd6d68aeedaa1388418076e1b5af27d9d10]]]# ② Maximum value

[[[00000000000000002408---851366c9173e95d6d73831a1c816e1ad4f898743789cfff0692e0997f6356cab]]]# ③ Shaping

[[[00000000000000002409---ce651f3e5854d1979551861fc2d8625b1694c2b7b3486a76d7b161ec508ea868]]]Implementing the pooling function (more precisely, the pooling_simple function) is a three step process. (1) expands the input data, (2) finds the maximum value for each row, and (3) formats the data into an appropriate output size.

[[[00000000000000002410---b8e3fb63229e7ee8b74b53a876146dfd477a77c6de5edfc02c4f85a2c2eedffa]]]To calculate the maximum value, use DeZero's max function. This max function accepts the same arguments as NumPy's np.max. In the above code, by specifying axis as an argument, the maximum value is obtained for each specified axis.

[[[00000000000000002411---20ae98ec6276bdb15721d9de01086166bfb6389032751837d0983a68ef372d23]]]The above is the implementation of the pooling function. As shown here, once the input data has been expanded into a form that facilitates pooling, the rest of the implementation becomes very simple.

[[[00000000000000002412---123f40c2370e64ede3fd6206b756b43dfc0a29fa6858779ef9520734b6b3e287]]]step 18

[[[00000000000000002413---611a4e6fde09c4d6b57e567f671a94b41b51afeef1438fb91375650fa749247e]]]Modes that reduce memory usage

[[[00000000000000002414---0b731a0fb7b0261644d2de7ed0d02b508bb80664254beb66c2fcb34effd2bca6]]]In the previous step, we learned about memory management in Python. Here we introduce two mechanisms that can improve DeZero's memory usage. The first improvement is about the memory usage consumed by backpropagation. There, we prepare a mechanism to immediately eliminate unnecessary derivatives. And the second improvement is to prepare a 'mode when backpropagation is not needed'. Try to omit unnecessary calculations in that mode.

[[[00000000000000002415---b2eeaba170d6a292c02dd94f71ccdc73cfa63c4a5002269fc9faa13362a4f627]]]Do not keep unnecessary derivatives

[[[00000000000000002416---7373b8f975e8df114ec4f4b8e2435cddb26925001c7872e00e21ce89c8d3b27c]]]The first improvement is DeZero's backpropagation. Now in our DeZero all variables keep their derivatives. For example, let's look at the following example.

[[[00000000000000002417---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002418---a4bfeace44d8d98182d8566cb73eff49b377680db1a7e001dffc2804a3cef36a]]]Here, the user-provided variables are x0 and x1. Other variables t and y are generated by computation. At this time, when the derivative is obtained by y.backward(), all variables hold the derivative. However, in many cases—especially in machine learning—the only differentials we want to find by backpropagation are the terminal variables (x0, x1). In the example above, intermediate variables such as y and t are not often needed to be differentiated. So we add a mode that eliminates derivatives for variables in between. To do so, add the following shaded code to the backward method of the current Variable class.

[[[00000000000000002419---660ab8ab2f26b47c6f40199ff797306dbc3956c63a7eac2dd3b544f923d15d93]]]# y is weakref

[[[00000000000000002420---c0b69229d001044a4ac00384efe83efe31cdbd77104d922583007b813e675dd9]]]First, add retain_grad to the method arguments. If this retain_grad is True then all variables retain their derivatives (gradients) as before. If retain_grad is False (default is False), all intermediate variable derivatives are reset. The principle is to set y().grad = None at the end of the backward method's for statement so as not to retain the differentiation of the output variables of each function, as shown in the code above. By doing so, all but the terminal variables will not preserve derivatives.

[[[00000000000000002421---d3c44f17a7109cb7eb8596a72524668cfc097c8ad6ed0abab2e4413344ccff53]]]In the code above, y().grad = None, but this y is a weak reference, so it must be accessed as y() (we introduced the weak reference mechanism in the previous step). Also, with y().grad = None, the reference count goes to 0 and the derivative data is cleared from memory.

[[[00000000000000002422---a97e2be0afd86a2674de6d70ecec8b96c101f7a3690e70df6e5473c2d16c43b0]]]Now let's run the same code as before again.

[[[00000000000000002423---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002424---a69b016f21fc3bb8f2173fbe687eefef9771ced7b72a7338c7f64559e363c49c]]]As above, the derivatives of the intermediate variables y and t have been eliminated. That memory space is being freed immediately. This is the first improvement in DeZero's memory usage. This is the second improvement. In doing so, we will start by reviewing the current Function class.

[[[00000000000000002425---725a7f1b0c6aed38fb5fc2d4e89c942436d367502f7b8892188f4308ebefbabd]]]Function class review

[[[00000000000000002426---d6cd546729188a786e73e5c5eea6e702aad778140956f89290f6b1a376ab96d8]]]DeZero finds the derivative by forward propagation followed by back propagation. Backpropagation needs the result of calculation done in forwardpropagation, so it is necessary to store (remember) the result. The following shaded part of the Function class actually holds the calculation results.

[[[00000000000000002427---49754a04b0fd1d298279247b06e45247fa30c744c78f9920dd23417744a5996a]]]As above, we reference the inputs to the function with an instance variable called inputs. Variables referenced by this input will have their reference count increased by one. This ensures that the variables referenced by inputs remain in memory even after the __call__ method is called. If you don't reference the inputs here, the reference count goes to 0 and they are cleared from memory.

[[[00000000000000002428---821a1efaa514ebd0fa9f112af29c7522e4952bc3c24a82c56f105f6dc9df6e9a]]]The instance variables inputs are used when computing backpropagation. Therefore, when performing backpropagation, it is necessary to leave the variables referenced by the inputs. However, sometimes differentiation is unnecessary. In such cases, there is no need to store intermediate calculation results. Furthermore, there is no need to create a “connection” of calculations.

[[[00000000000000002429---82fd86f2edd45e799d901068c2a2b8f36422823c78fd0f35a774df684a22e1aa]]]Neural networks have two phases: training and inference. During training, we need to find the differentiation, but during inference, we simply do forward propagation. When only forward propagation is used, memory usage can be greatly reduced by 'throwing away' intermediate computation results.

[[[00000000000000002430---5da21909ae0270b902a050c802d6c55f89c1ec9a449c740d19c014c803973552]]]Switching by Config class

[[[00000000000000002431---c59a961db72c964f08e1f9b46a2210effbc730ec34664d1527c6909b64e589d2]]]Now we extend our DeZero in case we only do forward propagation. First, we need a mechanism to switch between two modes: the 'backpropagation enabled mode' and the 'backpropagation disabled mode'. Here, the following Config class is used to switch between them. By the way, Config is an abbreviation for Configuration and means 'setting'.

[[[00000000000000002432---d86be254c67600db49ed8c6ccded97218202078c1b18f627802304b17554aad5]]]As you can see above, the Config class is a simple class. The class attributes of the Config class only have a boolean enable_backprop (so far). This enable_backprop means 'whether to enable backpropagation', and when it is True, it is 'backpropagation enabled mode'.

[[[00000000000000002433---d1f6e795d20b29dbac4b37aee351d2b22abdc85281e78aee8f5591d5dd2b4704]]]Only one data exists for DeZero's 'settings'. Therefore, we will use the Config class as a 'class' without instantiating it. Only one class exists at any given time, but multiple instances can be created. So in the code above, the Config class has been configured to have a 'class attribute'.

[[[00000000000000002434---803e6fc1f2cdb58e3d6006cd473fb6aabb3789871b8b020c20b9f71cbfd23f25]]]After defining the Config class, the function can refer to it to switch modes. In actual code, it looks like this:

[[[00000000000000002435---a474a7b445343ae368a14343722669b787327376bcc137dcefdfd52072981275]]]# ① Generation setting

[[[00000000000000002436---6e28e1b733c66b3a3ae176f5c7835dbe3144811f7822693517f1ee5bd6484305]]]# ② Connection setting

[[[00000000000000002437---db4d7aa3957b873ea4d3c3b441a25cc2a3910718d91a859018c5074267254c25]]]As above, the code for backpropagation is executed only if Config.enable_backprop is True. Here, the ① part of the code above determines the value of 'generation', which is used to determine the order in which nodes are traced during backpropagation. Therefore, it is unnecessary in 'backpropagation disabled mode'. Also, the output.set_creator(self) code in (2) creates the 'connection' of the calculation, which is also unnecessary in the 'disabled mode of backpropagation'.

[[[00000000000000002438---5395ec0709be77d71757dec61f082a741eb5f3a0360c7f85e6af8f324f7dacac]]]switch mode

[[[00000000000000002439---5a7e5ddbcaaa077ec3d7016d0f716f6010c153bd6318e12470783258219b9af8]]]Now we have a mechanism to enable/disable backpropagation. Using this mechanism, you can switch the mode as follows.

[[[00000000000000002440---51ce31ca1e4675d279481797bc3e41bf9f497724ae3565efb7d99cd8a961ac99]]]Here, I purposely prepared a large multi-dimensional array. A tensor of shape (100, 100, 100). We apply the square function to the tensor three times in succession (this time the square is computed element by element). Here, if Config.enable_backprop is True, the intermediate computation results will be retained (at least until the end of backprop). And that much memory will be occupied. On the other hand, if Config.enable_backprop is False, intermediate computation results are cleared immediately after use (more precisely, they are cleared from memory when there are no more references from other objects).

[[[00000000000000002441---5fa54070fb3a7242ae388e30a032d48d0f4ecffb69167770832dbeacd3fb9a04]]]With the above, the mechanism to switch the mode of backpropagation is completed. Next, we will create a mechanism that makes it easier to switch between modes.

[[[00000000000000002442---9b1ebae59284c5dcf45c88d93d3ef70a246f5bb3cde3d659f5558e385dbba5b6]]]Switching with the with statement

[[[00000000000000002443---347df2ce6b3159eeb2eb78b25d0baa31556e5daedbb85a8b65834cfea220abb5]]]Python has the with syntax. This syntax can be used when you want to do post-processing automatically. A typical use case is opening and closing files. For example, to write something to a file, without a with statement, you would have to write code like this:

[[[00000000000000002444---8b3be0b79431a6161d4d5e9fcbbb07a492a8cc5cd3f4ed7b195a96b71e0d1576]]]As above, open() the file, write something to it, and close() the file. At this time, writing a routine close() every time is a little troublesome. And you may forget to do it. To prevent that, you can use the with statement like this:

[[[00000000000000002445---fd1f66e6b0feb9aaf75b2afeababbb1d55a93c13718990e11a8cde7fbb9ebe89]]]This code opens the file when entering the with block. The file is left open inside the with block, and closed (invisibly to the user) when the with block is exited. In this way, by using the with statement, 'processing when entering the with block (preprocessing)' and 'processing when exiting the with block (postprocessing)' can be automatically performed.

[[[00000000000000002446---fd671de9f2a30743094b40ae23a4f9ee41267e3f624c565db9ffb50c3ea34c9c]]]Use this with statement mechanism to switch to 'backpropagation disabled mode'. Specifically, we assume the following usage (implementation of the using_config method will be discussed shortly):

[[[00000000000000002447---b412bed2707ac4f0d3e47a1c23c6172a26266da46ffb79769b98046f325add4d]]]As above, only inside with using_config('enable_backprop', False): is 'backpropagation disabled mode'. Then, when the with statement is exited, it returns to the normal mode (= 'backpropagation enabled mode').

[[[00000000000000002448---96176bfba4f07287f0bf4649699f9865ce274cf4e605f3d8e20e536ff46ffc53]]]Temporarily switching to 'backpropagation disabled mode' is a common practice. For example, it is common in neural network training to use gradient-less modes for model evaluation (mid-training).

[[[00000000000000002449---557cc8d9a69272b8d78928def4cdfeabf09f4e14189447ca00d62f69eefe4cc4]]]Now, let's implement mode switching using the with statement. The easiest way to do that is with a module called contextlib. First, I will explain how to use the contextlib module. It can be used like this:

[[[00000000000000002450---cbf568f150a7bf185d18361b34b0e89d930b8ef5383a22d7059fd7eea63d4db6]]]# Preprocessing

[[[00000000000000002451---43e5b35ed2fef74db98b3821dfce41a173ef96223f58f72bc82db0f41f22bd84]]]# Post-processing

[[[00000000000000002452---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002453---94f1e6510b1e5366d7515257d8d4e2ae6cce6672d086bbd2ea088bcd85cda861]]]As above, by attaching the decorator @contextlib.contextmanager, a function that determines the context is created. And in that function, write the preprocessing before the yield and the postprocessing after the yield. Then you can use the with config_test(): syntax. With this, the preprocessing is called when entering the scope of the with block, and the postprocessing is called when exiting the scope of the with block.

[[[00000000000000002454---8b02e8ce8409eb23816ce1f382cc90a68276603aa78fad02ddf5df7a0224c1fa]]]Inside the with block, exceptions can occur. If an exception is thrown inside the with block, the exception will also be sent to where the yield is executed. Therefore, yield must be enclosed in try/finally.

[[[00000000000000002455---25750e1a3c0dba3a67180831e46fec0efdcbd9eda3e686cb6ce6e3da7f812868]]]Given the above, the using_config function can be implemented as follows:

[[[00000000000000002456---5b348e295c5b2a764c67300430cbff0cd874f3b776ab8d99ff8336e297cf9957]]]The type of the argument name of using_config(name, value) is str, and the Config attribute name (class attribute name) is specified there. Get the specified name from the Config class using the getattr function. Then set the new value with the setattr function.

[[[00000000000000002457---ac69856e293e9c5bfb7b98a0e8a263af35aa476eb5cb5c3da57ffcbf1a4973e1]]]Now when you enter the with block, the attribute specified by name in the Config class will be set to value. Then, when exiting the with block, it will be restored to its original value (old_value). Now let's see the using_config function in action.

[[[00000000000000002458---e45e72532581dc5e29cdbb8b049b7c5f4fc19ee1ae136f9beb1870468342ac6c]]]As above, when backpropagation is not needed, only forward propagation code is executed inside the with block. This eliminates unnecessary calculations and saves memory. However, writing with using_config('enable_backprop', False): every time is a bit tedious. Therefore, we prepare a function called no_grad as follows.

[[[00000000000000002459---bf03e662d2caed817e6c494f708c0e643977528412bdb54feadedc69f15474e1]]]The implementation of the no_grad function simply calls using_config('enable_backprop', False) (and returns it). Now when you don't need the gradient, just call the no_grad function. This is the end of this step. From now on, when we don't need to calculate gradients -- when we just need to calculate forward propagation -- let's use the 'mode switching' implemented here.

[[[00000000000000002460---c636279da96e2feb6aa3bc0c9d95b0cba1ddb7cc66ff665e07689c658720d7b3]]]step 42

[[[00000000000000002461---34772eaafb96acf5a61bc203e40981e67a6b9dbd9d0d79d39dc1db508430c98c]]]linear regression

[[[00000000000000002462---a8fa3ef4a7826abe381fdab771d0b2c8f2affdd43690b1344de80c7ac0717a1a]]]Machine learning uses data to solve problems. Instead of people thinking about how to solve problems, we let computers find (learn) how to solve problems from the collected 'data'. In this way, finding solutions from 'data' is the essence of machine learning. From now on, we will use DeZero to tackle machine learning problems. Here, we will implement the most basic of machine learning, 'linear regression'.

[[[00000000000000002463---fc93ca7dadd436c7a79d570773edb396d8b7f84e0d5efdd37f9f26e08a09225b]]]toy dataset

[[[00000000000000002464---a94bcdebaa5ae14bf3cd8cf6b032ad11766c673bffb390d102c9550a7b3f1086]]]In this step, we will create a small dataset for the experiment. Such small datasets are called 'toy datasets'. In consideration of reproducibility, the random number seed is fixed and the data is generated as follows.

[[[00000000000000002465---becddb3b041b7d4894332dd86734ce36b1ecc0f17cb29b08605fd589f2d51658]]]Create a dataset consisting of two variables, x and y, as above. At this time, the point cloud is on a straight line, and random numbers are added to y as noise. Plotting this (x, y) point results in Figure 42-1.

[[[00000000000000002466---894d46631cbcc6ed7336c446114a44a6eb71931e041ee104f9b6611c151d8a0f]]]
Figure 42-1 Dataset used in this step


[[[00000000000000002467---c04a76ac93ef6c80b4e1b8d0d51c52d8d05edeb0f29909be9d40e48199ae2fe1]]]As shown in Figure 42-1, x and y have a 'linear' relationship, but it contains noise. Our goal is to create a model (mathematical formula) that can predict y values from x values.

[[[00000000000000002468---2f73aa2cdb6f3054a8f5ff768d8df69700e9c7d5cd0c59e9314f67cc3fb671c1]]]Predicting real-valued y from the value of x is called 'regression'. Furthermore, when that predicting model predicts as 'linear' (as a straight line), it is called 'linear regression'.

[[[00000000000000002469---71d36bbe6893ba144d34954f3e6709ee7dcf23c030ec3b54962c144734e68a78]]]Theory of Linear Regression

[[[00000000000000002470---8c9ddb2c7e20ffe69cd197123cc5fe3344c9b90e767f857480058aed562bb525]]]Our goal is to find a function that fits the given data. Here, we assume that the relationship between is linear, so we can express it as This straight line is represented as shown in Figure 42-2.

[[[00000000000000002471---36e2741fc464984d5a60c4e762b88798917f49074ba8998cc4abfda1a63675ee]]]
Figure 42-2 Linear Regression Example


[[[00000000000000002472---4293c270c28ab099c9aaf061573c071bb49c10e278ebbc0e179a52fc53c57c45]]]Our goal is to find a straight line that fits the data, as shown in Figure 42-2. To do this, we need to reduce the difference between the data and the predicted value, called the 'residual', as much as possible. Here we define an indicator of the error between our predictions (model) and the data by the following formula:

[[[00000000000000002473---ed6db02ac3e07ebb16c2acfb9878f43758e527e3dac9a3390d9fcc640fc67ef4]]]In equation (42.1), if there are a total of points, find the squared errors at each point and add them together. and multiply by to find its average. This formula is called the mean squared error. In addition, in formula (42.1), there is also a case where it is defined as. However, in both cases, adjusting the value of the learning rate results in the same problem setup when solving with gradient descent.

[[[00000000000000002474---e2f6a2609710bd7bcaae8102d9bd0b66e759eb15edcdbb89ea41cb6c7c17e004]]]A function that evaluates the 'badness' of a model is called a loss function. Therefore, linear regression can be expressed as ``using the mean squared error as the loss function''.

[[[00000000000000002475---6f7ddf6937400e44bf4d8a0b70cbc09959327a9d76d373122784ba708adaee8b]]]Our goal is to find the output minimization of the loss function given by (42.1). This is a function optimization problem. And we have already (in step 28) solved such a problem using gradient descent. Again, we use gradient descent to find the parameters that minimize Eq. (42.1).

[[[00000000000000002476---696b2c2b92f9edba76d978ad927b92ecba6d0d6534e54a2456039367407788a6]]]Implementing linear regression

[[[00000000000000002477---c5c18c5b08149f77c0dc31d5943a84173a207b9308368d1f8b9f0fb4b0a5d9b5]]]Now let's use DeZero to implement linear regression. Here, the code is divided into two parts, the first half and the second half. Here is the code for the first half.

[[[00000000000000002478---37a43e14eb6a75bde7ebddcf5825f979ce93b39913ddd309e40949717793a998]]]# toy dataset

[[[00000000000000002479---fd28e81223ab81be7830f60b3ede70ab38848a5448c2f1debdf6481857415020]]]# optional

[[[00000000000000002480---f77e22d8a83109945e3348049cb325ee54d4bd0d7b4a054ddd4f1f6f2b1024f9]]]Here, W and b are created as Variable instances as parameters (W is capitalized). For geometry, let W be (1, 1) and b be (1,).

[[[00000000000000002481---0832c57b33b664711d840a804673c0e495cb82e0e81dd6c8f529e83d36192918]]]DeZero functions can process ndarray instances as they are (in which case DeZero converts them to Variable instances). Therefore, the data sets x and y in the above code can be treated as ndarray instances without being explicitly converted to Variable instances.

[[[00000000000000002482---986f656a990cc813419012e3d26363b1ff6857622c875b961060da70d57799a9]]]Also, the code above defines a predict function. There, we use the matmul function of matrix multiplication to do the calculation. By using matrix multiplication, you can perform calculations on multiple data (100 data in the example above) at once. The shape transition at this time is shown in Figure 42-3.

[[[00000000000000002483---398df80b8adb232204a7b22833d8c3988b10cf1199ad62b27061ed1167e74d60]]]
Figure 42-3 Changes in the shape of the product of matrices (addition of b omitted)


[[[00000000000000002484---0cc550e1318ef6c898ad46d98bfe81cda8896487cff1547420eeb56ba9c1b40a]]]You can see that the number of elements in the corresponding dimensions match, as shown in Figure 42-3. And the shape of the resulting y will be (100, 1). In other words, for x with 100 data points, all the data points have been multiplied by W. Now, in one calculation, we get the predicted values for all the data. Note that the dimension of the x data here is 1, but if this is D, the correct calculation can be performed if the shape of W is (D, 1). For example, if D=4, the matrix multiplication is performed as shown in Figure 42-4.

[[[00000000000000002485---3cbe85f8824251e6f8b358aa641c8f211d84c44da589ab743648db3a54f6db26]]]
Figure 42-4 Transition of the shape of the matrix product (when the x data dimension is 4)


[[[00000000000000002486---7754824a5483c64e19ace91308250f43042ca2aa9bd9d34c4f85e94012e1e1cf]]]Matching x.shape[1] and W.shape[0] as shown in Figure 42-4 ensures correct matrix multiplication. At this time, for each of the 100 pieces of data, the calculation of the 'vector inner product' by W is performed.

[[[00000000000000002487---626f4d452f4b3d35d1e95a46b71220c36ae526b5d8eab838a1012cb922627605]]]In the code above, adding y = F.matmul(x, W) + b causes a broadcast. Specifically, b has shape (1,), but it is element-copied to shape (100, 1), and then elementwise added. Note that we are responding to broadcasts in step 40. Therefore, even if a broadcast occurs, backpropagation is performed correctly.

[[[00000000000000002488---b5717fcb6b191b058d229beb3f8886b087295c67e4afbb41b131749327794b88]]]So here's the code for the second half:

[[[00000000000000002489---c104e83bc4c1b205f86c8c80d151fd469d5d63b9dfc3dddabe180816ff12551c]]]Here, we implement the function to find the mean squared error as mean_squared_error(x0, x1). It just implements equation (42.1) using DeZero's function. Then update the parameters by gradient descent. We already implemented that in step 28. The caveat here is that when you update the parameters, you do calculations on the instance variable data, like W.data -= lr * W.grad.data. Updating parameters simply updates the data, so there is no need to create a computational graph.

[[[00000000000000002490---3ca39db93fe95e3fe5decb22d433833eb82490495f2019abc1a33ecfaedae1b0]]]Now let's run the code above. Then you can see that the output value of the loss function is decreasing. And finally we get the values W = [[2.11807369]], b = [5.46608905]. For reference, the straight line graph obtained by this parameter looks like Figure 42-5.

[[[00000000000000002491---8dec8ca06f7ac1a51fffc730eeec971865e73b62802d617a71bcf01482d7cb66]]]
Figure 42-5 Model After Training


[[[00000000000000002492---cdc9ae306f7eb2ca672b1e7a0a94aeffb65797cdd77bc8000aa0a3754e0b9114]]]As shown in Figure 42-5, we were able to obtain a model that fits the data. We've successfully implemented linear regression using DeZero! This completes the linear regression implementation. Finally, I will add DeZero's mean_squared_error function.

[[[00000000000000002493---1d881b958bc627ec2f3258551c5f1c2b09fccae612e5612585d965b41977c3cb]]][Supplement] Mean_squared_error function of DeZero

[[[00000000000000002494---67121139f18c356fa39ffe1345fe6a2fe7a15caf78ed4bc733c0a0ea5a4b1737]]]Earlier we implemented a function to find the mean squared error. An excerpt from that code looks like this:

[[[00000000000000002495---ab387e12bb34ac044ea02f82dd95e57fa5f57c626e9a023884d027f219b46769]]]This function does the math correctly. And since it's calculated using DeZero's function, you can also find the derivative. However, there are improvements to this method of implementation. To illustrate, let's look at the computational graph in Figure 42-6.

[[[00000000000000002496---5ad2c543b234f490d08e304c8d759f705a458af1acaf61ccd7a903c31ac3d4d8]]]
Figure 42-6 Computational graph of mean_squared_error function (len(diff) in code is represented by N) 


[[[00000000000000002497---c000cd659dca4230c4e7ebcb062fc8260567065b3a210484355a1b986c2a79c6]]]Figure 42-6 is the computational graph produced by the mean_squared_error function above. What I want you to notice here is the variables in the middle. There are three unnamed variables. They are recorded in the computational graph, so they live in memory as long as the computational graph exists. And the data in those variables (ndarray instances) keep alive as well.

[[[00000000000000002498---b99ba4a76e08e5eee06d4f70b3e7511e78ea37f26a2e98dccd1c595db625d674]]]DeZero finds the derivative by forward propagation followed by back propagation. The variables that exist in Figure 42-6 (and the data they refer to) remain in memory during forward and backward propagation.

[[[00000000000000002499---705093afff36c56cea6bb5345c568abaa322def1e61192796c1bcd0ebde6f95e]]]If memory usage is not a problem here, the above implementation method is fine. However, a better implementation method can be considered for a function that is used by a third party. It is a method of inheriting and implementing the Function class. That is, implement DeZero's function class called MeanSquaredError. Here's the actual code:

[[[00000000000000002500---1891ca3dafe198e3ca002616f53bb4473037117babc1ef4bc4700da9f25b8f56]]]First, implement forward propagation for ndarray instances. The code is almost identical to the code we implemented earlier in the DeZero version of the function. Put this processing unit together and implement the backpropagation code backward. Backpropagation implementations take the derivative in a mathematical formula and code it. We omit the explanation here.

[[[00000000000000002501---15e982234faf9cdb5b484796a3b2ba6661190c12812f689fd93e52850819e660]]]This new implementation of the mean_squared_error function gives the same result as the old one. However, it results in a memory efficient implementation. Because the new mean_squared_error function produces a computation graph like Figure 42-7.

[[[00000000000000002502---f7c667c899b231c1af23e15fbbb2737f6220167ca812ce7e544b9bb4a6972026]]]
Figure 42-7 Computational Graph of New mean_squared_error Function (Len(diff) in Code is N)


[[[00000000000000002503---9a7b15a3116edd4295fa0ae13b4006363d08888da207ab09c9dc6c77d0a6201a]]]Comparing Figure 42-7 with the old computational graph (Figure 42-6), this time there are no intermediate variables in the computational graph. The data in between is used only in the forward method of the MeanSquaredError class. More precisely, they are used as ndarray instances and are erased from memory as soon as they exit the scope of the forward method.

[[[00000000000000002504---16b3d352c59998bdcdd6d37e587c568a0c18dba1e95aaaab69505bbe8caeeddf]]]For this reason the mean_squared_error function in dezero/functions.py is implemented in a new way. For reference, the obsolete implementation is named mean_squared_error_simple -- with _simple at the end -- and is added to dezero/functions.py. The above is a supplement about DeZero's mean_squared_error function.

[[[00000000000000002505---da14299534ba28d101cb9da012463647af73411d71f4a60c134d52f8f1fa9369]]]Appendix C

[[[00000000000000002506---34e20e3b5197c5d91429b09627b3ff39ac61b5d3acf96b2c1ee764cc749fa17a]]]Run with Google Colaboratory

[[[00000000000000002507---587288af7a8086bb24cf037eabf21080739e43695a5eeae05fe65dbdebf13974]]]Google Colaboratory (hereafter referred to as 'Google Colab') is a Jupyter notebook environment that runs in the cloud. No environment is required, and anyone with a browser can use it. Also, not only CPU but also GPU can be used.

[[[00000000000000002508---d9a76601e6d70cc67d4369f902f823403fa0d1352b72419f782a5540f109b5a6]]]Here, I will introduce an example of running DeZero using Google Colab. As an example, let's run the code used in step 52 (MNIST training code). First, let's access the following link.

[[[00000000000000002509---310e15359ed198e97f46849a670dbfc4eb3cb8af700ad165bb52ffdc1f02e5b5]]]If you open the above link in your browser, you will see a screen like Figure C-1. The contents of a notebook are divided into 'cells', as shown in Figure C-1. In addition to text, this 'cell' can write and execute code in Python and other languages. To run the code in a cell, click the cell to select it and click the play button to the left of the code. Alternatively, you can use the keyboard shortcuts 'command + return' or 'Ctrl + Enter'.

[[[00000000000000002510---19711616cb145055a9300eb27c2f9f282099282d3c99924bd9f7d92955aed45b]]]
Figure C-1 Google Colab Screen


[[[00000000000000002511---1d86e311a1e2ff269ca4b8074fa30dba8da037f01e09e7ff852a93c4991fa5a7]]]The data displayed in Figure C-1 is examples/mnist_colab_gpu.ipynb in the book's GitHub repository. The ipynb file on GitHub can be opened from Google Colab. Just replace 'github.com' in the URL with 'colab.research.google.com/github'.

[[[00000000000000002512---db0aa5300d07687e849cbc7ce86c1b1b81ae82988352aa73134e0cac7796d5dc]]]The following description reproduces the contents of the notebook in Figure C-1 in this book. Readers are encouraged to read while running on Google Colab. Also, when you run the cell for the first time, a warning like 'Warning: This notebook is not created by Google.' will appear, but you can proceed by selecting 'Run anyway'.

[[[00000000000000002513---e2e1c64f9c44a46b3dc339d1eed3345aba1b9f71d28a369416413c26cf15c8f6]]]For this notebook, we will install DeZero first. DeZero is registered on PyPI, so you can install it with the command pip install dezero.

[[[00000000000000002514---ca08e4776a6736c3266a55a7be397834671f72cbdaa8225167eb7a3f9ca63d3a]]]After the installation is completed, I will check whether the GPU can be used with DeZero.

[[[00000000000000002515---ab9a348b6a75cbcea09b27553a163793f4588f90bf93c412d9412871c63d5077]]]If True, the GPU is ready to use. If False, Google Colab's GPU settings are required. Follow these steps:

[[[00000000000000002516---07ccc2d21509b4993878e0f3a6338b2e4fb719aacdc17ed8db68ebcf99cf4773]]]Select 'Change runtime type' from the 'Runtime' menu

[[[00000000000000002517---c184a47360806ec6bb1e4378ba2448a1021aa028a7529d2dc05a8531afce4bf0]]]Select 'GPU' from the 'Hardware Acceleration' drop-down menu

[[[00000000000000002518---dfc07185f9c2c06edac77ef5cff89657fcf63416343eafcda5544d63402b4767]]]Now, let's try learning MNIST with DeZero. Let's run it on the CPU first.

[[[00000000000000002519---f6eb128035bc3aebdf0cdf63ac6962f094974ecbe972d6c8023ff08174a67d34]]]Next, let's calculate using the GPU.

[[[00000000000000002520---8fdca3329747f7685a6126b1ecf5a35e6bbbdaa69ac8807c50a491e5fc4c2824]]]For reference, when I do a speed comparison between CPU and GPU, the results are:

[[[00000000000000002521---ffa2539743ab62447ca7a377432c04c4e070755763d67460249cea2af7b2931f]]]That's the explanation from Google Colab. In addition to the example shown here, you can run other DeZero sample code. Of course, you can also run original code written using DeZero. Please make use of Google Colab and use it for various purposes such as experiments with DeZero.

[[[00000000000000002522---1d1ffdd5460132848c7b9f65c9cef69e5ff004af14ac2502062bb09a989f8a73]]]step 41

[[[00000000000000002523---ff3fd12e3cde230f92432b6ceff6a6a310741ebdb16bd0fe8f9fec2354fb29c9]]]matrix product

[[[00000000000000002524---1b89fd84ef0048d27f191a444854322a33133859730103b06852660f7c9807c0]]]The theme of this step is 'vector inner product' and 'matrix product'. First, we show the two calculation methods and implement them as DeZero functions. After this step, you will have a minimal set of functions for working with tensors. Then you can finally solve practical problems.

[[[00000000000000002525---0062a75e9438467a1d36bd6abea07ba65bc70f7f79caead34a7218ff77370f04]]]Inner product of vectors and product of matrices

[[[00000000000000002526---95491c40bff618a27bde92db0bfd45ed8183c25c9c771ba6769b2163f943a2a3]]]Now let's talk about 'vector inner product' and 'matrix product'. First, let's talk about inner product of vectors. Here we assume that we have two vectors In this case, the vector inner product is defined as Equation (41.1).

[[[00000000000000002527---b190091823e07b58dac302049ba426ffdcfc7ab94a0870f6be82844e4404f5db]]]The dot product of vectors is the sum of the products of corresponding elements between two vectors, as shown in equation (41.1).

[[[00000000000000002528---1563617c72896d61cc70a9c18a07e82a664e5d5d4703caac22ced5f12373539b]]]When notating symbols in mathematical expressions, in the case of scalars, they are notated like this. On the other hand, for vectors, matrices, etc., it is written in bold.

[[[00000000000000002529---745beb6b9b99059f69f5ed6260891b5ae5a700543141ae079340e29d3d4352ae]]]Next is 'matrix multiplication'. Matrix products are calculated according to the steps in Figure 41-1.

[[[00000000000000002530---8a91068d18429ad1bf5110c40c82b6aa6ee2873be50dbd6742af861b0c3ffaca]]]
Figure 41-1 How Matrix Products Are Computed


[[[00000000000000002531---1cd16fb586cc0dd6fcb8f4e6abf82305116a0abd609f972178d922e0e96967bd]]]As shown in Figure 41-1, matrix multiplication computes the dot product between the horizontal vectors of the left matrix and the vertical vectors of the right matrix. The result is then stored in the corresponding element of the new matrix. For example, row 1 and column 1 results in the element at row 1 and column 1, row 2 and column 1 results in the element at row 2 and column 1, and so on.

[[[00000000000000002532---68b950d80dec5db076be3454940c4f4c80178e4dc6791d50d720eff04514f101]]]Now, let's implement vector inner product and matrix product using NumPy. You can use the np.dot function for that.

[[[00000000000000002533---4fe2c5043a1cc0ec95bacbf6dd20de4019b8bbbf2ecae741c618e8e9a47b342a]]]# Dot product of vectors

[[[00000000000000002534---5de35d6711aab57c6425824f4b87616644a3686a4b31b9794605871f93722ad6]]]# matrix product

[[[00000000000000002535---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002536---53234f8ad19a97a4aff538cf0d3d451cfcb04b253440b731b454135af4ed9f3a]]]As shown here, the np.dot function can be used to both calculate vector dot products and matrix products. If both arguments of np.dot(x, y) are 1D arrays, it will compute the dot product of the vectors, and if the arguments are 2D arrays it will compute the matrix product.

[[[00000000000000002537---1b251f3328e2f2364911e0ed0aa224dc460ae10c40ee326d2cc53934f8be2b0a]]]Matrix shape check

[[[00000000000000002538---8e8663de521219b949e0b152b63cfa44f5f7ced17bf066f7b05cfe25abd4138d]]]When working with matrices and vectors, it is important to pay attention to their 'shape'. For example, matrix multiplication produces transitions in shape, as shown in Figure 41-2.

[[[00000000000000002539---1f2fd687b73d9356923b7cb4229b19fcbcbd08800f8a3e2718b80b5c38571fda]]]
Figure 41-2 Matrix Multiplication Matches the Number of Elements in Corresponding Dimensions (Axis)


[[[00000000000000002540---c73b7bbea453f61bbb9148bf3f230a2d826a56d5abf30fcd0c5f2dd8342bf1ca]]]In the example in Figure 41-2, the matrix multiplication with the matrix produces the matrix . At this time, as the figure shows, the number of elements in the corresponding dimension (axis) of the matrix must match. The resulting matrix then consists of the number of matrix rows and the number of matrix columns.

[[[00000000000000002541---3bc52651a1a9353a918b424a49a0050a8608f47c662146defe30a78df1bee715]]]In calculations such as matrix multiplication, it is important to pay attention to the shape of the matrix and see its transition. In this document, we will call such confirmation work focusing on the shape as 'shape check'.

[[[00000000000000002542---eae287fc33ba9a4c513598e4f5fbf89eae9f5085133a68dfa0ecdf12ee1105de]]]Backpropagation of matrix product

[[[00000000000000002543---87d63ea74c88380f0db69c3d43c16a0e450c9f114272965dc85ff01b9e1b2254]]]Next, we will discuss backpropagation of matrix products. Backpropagation of matrix products is a bit complicated, so I'll give a straight-forward explanation here, followed by an intuitive explanation. By the way, matrix multiplication is implemented as MatMul class and matmul function in DeZero. matmul is an abbreviation for 'matrix multiply'.

[[[00000000000000002544---bff732f8d972b80bc80f4a3b773107ea9d933b65528154ba7103add392ff6e1b]]]Now, to explain the backpropagation of the matrix product, let us consider the calculation. Here, the shapes of , and are assumed to be , respectively. At this time, the calculation graph looks like Figure 41-3.

[[[00000000000000002545---dc74192b650e1297a2d63011f0ead5236b0631551ed46f628b4795adcfe92f75]]]
Figure 41-3 Forward Propagation of Matrix Products (Shapes Above Each Variable)


[[[00000000000000002546---aaba008487d7d6039e24f12a69ce4446583373299709e55dfa2606b58a29649f]]]Again, we're dealing with computations that end up as scalars. Therefore, here we consider a calculation that finally outputs a scalar (derivative of this with respect to each variable is obtained by backpropagation). At this time, the derivative with respect to the th element is obtained as follows.

[[[00000000000000002547---3ba2d438265b9174a3b3b13011e41ea9cf1c7e407d76b692cf29f773b31c8702]]]Equation (41.2) expresses the 'percentage of change' of how much changes when is changed (slightly). When you change here, all the elements of the vector will also change. And through the change of each element of , it will eventually change. Therefore, there are multiple paths of chain rules from to to, and the total is .

[[[00000000000000002548---a0e7561a1eab82aa833553a56d5139bd6e18be31f228668bade4e3a154d4fb84]]]Now for formula (41.2), this is still easy. To do so, we use the fact that †1 holds. Substituting it into formula (41.2), we can derive the following formula (41.3).

[[[00000000000000002549---dd31140c520c4b7405c8aaab8dca7e44c82714c14eba455fb55b6e996ff99a6e]]]Writing down the th element of [†1] gives Then it turns out that

[[[00000000000000002550---d6cc1e841c4a5464fcb994b352f74ecf919b409bf808eb63dfd83a2a2a176891]]]From equation (41.3), we can see that is determined by the inner product of ``vector'' and ``vector in the row of''. From this relationship, the following formula can be derived.

[[[00000000000000002551---d514cca57644c2abaef1571ecaec21149f864bb9fe4f80e074e8da167fc81a26]]]is obtained at once by matrix multiplication, as shown in equation (41.4). At this time, if we focus on the shape of the matrix (and vector), it will look like Figure 41-4.

[[[00000000000000002552---9f261331869a048b2e5ad9311df139c9056562b64e2769bb5c073f2bdf833cbb]]]
Figure 41-4 Shape Check for Matrix Product


[[[00000000000000002553---b3c5c5091779de0775b4f3a13ede311c026dd99403401aead2162a7b290c6d82]]]Looking at Figure 41-4, we can see that we have cleared the 'shape check' of the matrix. This confirms that formula (41.4) holds as a matrix calculation. It is also conceivable to work backwards--that is, to make sure that consistency holds--and to derive a formula (implementation) for backpropagation†2. To illustrate how to do this, consider the matrix multiplication computation again. But now suppose that the shape of is . That is, the shapes of , and are respectively . At this time, the computational graph of backpropagation looks like Figure 41-5.

[[[00000000000000002554---9a402f8d9669b7290c8ac502bb855696dcc41883e2b9508bba81c1db40ca51b3]]][†2] For matrix multiplication, it means that the backpropagation formula can be derived by 'matrix check', and this 'matrix check' method does not always lead to the correct backpropagation formula.

[[[00000000000000002555---547d90142f7ff7eeb500f143d3b234a90569cdf6b80f29060b311ec234916c14]]]
Figure 41-5 Forward Propagation (Top) and Back Propagation (Bottom) of Matrix Product


[[[00000000000000002556---8c98ffe2fb4ab88a4db69a9011cb0b81b0a56cf3cceee267817e61fc0a8392fa]]]So let's derive . We will focus on the shape of the matrices and construct the matrix product so that their integrity is preserved. This leads to the formula in Figure 41-6.

[[[00000000000000002557---c3d7351f753d3763d6645522e19283467b53a698ea7b180c50416144d8e33c77]]]
Figure 41-6 Backpropagation for Matrix Product


[[[00000000000000002558---8743b3f25427eb62f66c9f15fd582f7c3e0434fa5ed46ed280b199498d0e5156]]]The equation in Figure 41-6 can be derived by calculating the elements of each matrix and comparing both sides, just like the equation in Figure 41-4. You can also check that it also satisfies the matrix product shape check. Given this formula, it is trivial to implement 'matrix multiplication' as a function in DeZero. It can be implemented like this:

[[[00000000000000002559---8df4ae647fa62861f5e007083ef55c7bdb3736803b7f15400d689f43da01e66f]]]Backpropagation follows the formula in Figure 41-6 and codes it as a function of DeZero. In addition, forward propagation is implemented as x.dot(W) instead of np.dot(x, W). As such, it is also available as a method for ndarray instances.

[[[00000000000000002560---5ccb1fc178eb301b159c64d9a33960653fb1e2e1fa9728a06688ed995dc4a70c]]]The matmul function used in backpropagation in the code above is the function we are implementing right now. Also, operations that transpose (WT and xT) call DeZero's transpose function (which we implemented in step 38).

[[[00000000000000002561---f2e5ddbd61030f87530311d55dd885f0052f2a14c948930faf173400e2d8f36c]]]Now we can compute it using DeZero's matmul function as follows: And you can also find the derivative.

[[[00000000000000002562---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002563---528139147f1aa4b6b7fb6000e5f01dca2aa6fddb2ca60cf6e73798a9d6940a1e]]]Here, we randomly generate a NumPy multi-dimensional array and use it for computation. The code above runs without error. We can also confirm that x.grad.shape and x.shape are equal, and that w.grad.shape and W.shape are equal. This completes the DeZero version of matrix multiplication.

[[[00000000000000002564---31ff4669ee43f805791a0add306727770f905cbfd1cc414590d66a6c558e1a9e]]]step 46

[[[00000000000000002565---77a7dc6dc8bd4f73c1f476fe5a139818d744676b802f03a5d038cffdec486792]]]Parameter update by Optimizer

[[[00000000000000002566---8db2630f7d7a6c05ba5fd47ab2ad378e2d03e16e4cab9ce7f8985b0595d14dee]]]We have so far updated the parameters by gradient descent. In the field of deep learning, various optimization methods other than gradient descent have been proposed. In this step, we will modularize the parameter update work (update code) and create a mechanism that allows them to be easily changed.

[[[00000000000000002567---194d53a7f0911c0e4b508ebe57919ca925d35938876ac6962e330b04eadd4af6]]]Optimizer class

[[[00000000000000002568---ad6f2d6b4c578ff732a2372966524cdf03ae821d58d5ba80d6ffb835cfc5d0ff]]]Here, we implement the underlying class as an Optimizer for updating the parameters. The Optimizer class is the base class for optimization. The specific optimization method will be implemented after inheriting the Optimizer class. So here is the implementation of the Optimizer class:

[[[00000000000000002569---6396e091f6e55e4db0508d1ca71349cddbaf4fbfd0f5951f94f87a98bc49b077]]]# Collect parameters other than None into a list

[[[00000000000000002570---251d294caf3164b0103074339aa42a89920982a74839a37ba6450e477846e4b2]]]# Preprocessing (optional)

[[[00000000000000002571---38f43c19da8bdd7e5866eb1f7c6ecbd90942f9b37066fd1c3a57f64ff215bc8c]]]# update parameters

[[[00000000000000002572---783117e4ada44f18b78c92b3b5be2e4e630a3ee76cf5cf5ccdc3f960bed4c51b]]]Initializing the Optimizer class initializes two instance variables: target and hooks. Then, the setup method sets the class (Model or Layer) with parameters to the instance variable target.

[[[00000000000000002573---721bf28d24012703dfc7fad379657eeadd224fae6d61120a4bb90abcd20145af]]]The update method of the Optimizer class will update all parameters. However, for parameters whose instance variable grad is None, the update work is omitted. In addition, the update of concrete parameters is performed by the update_one method. This update_one method is implemented by overriding the class that inherits the Optimizer class.

[[[00000000000000002574---a57eae1f4580da58447df631892c0d7d660d51c9bfaf09de78fd565b67d96bef]]]The Optimizer class also provides the ability to do some pre-processing of the entire parameter before updating it. To do so, add a preprocessing function using the add_hook method. This mechanism can be used for things like weight decay and gradient clipping [26] (an example implementation can be found in example/mnist.py).

[[[00000000000000002575---0df25031c85b05fcbde881cd3dbca2883ff6b2559a5254113cacf77de5117076]]]SGD class implementation

[[[00000000000000002576---9d01683e45fd3a25473e122990892b3aa03191c236f22000c9e5d1d2c221dcdc]]]Now let's implement a class that updates the parameters via gradient descent. Here, in a class named SGD, I implement:

[[[00000000000000002577---9d87e984a92813352143a8a408e8f0d0a00b43259fa41ecd5c13393c66b8f496]]]The SGD class inherits from the Optimizer class. In the __init__ method of initialization we receive the learning factor. And in the update_one method implement the code to update the parameters. Now you can let the SGD classes update the parameters. Note that this SGD class is implemented in dezero/optimizers.py. From an external file it can be imported as from dezero.optimizers import SGD.

[[[00000000000000002578---40153fbe788dcd66bd2ece554ff4b446aa965915014c2e8dadcc36479bdceed9]]]SGD stands for Stochastic Gradient Descent. In Japanese, it translates as stochastic gradient descent. 'Stochastic' here means to select data randomly (probabilistically) from the target data and apply the gradient descent method to the selected data. In the field of deep learning, it is common to randomly select such data from the original data and perform gradient descent on that data.

[[[00000000000000002579---add01698219ad1efe7e91e2d0d1e85b82b1b20ac71d7a485741a3ed7594fb8ad]]]Solve the problem using the SGD class

[[[00000000000000002580---6d87837af86a49289e9631f7ba7b641722b8db18351ce14fa7f27ac705a176a6]]]Now let's solve the same problem as the previous step using the SGD class. Again, the changes from the previous step are shaded.

[[[00000000000000002581---4406b107986da29a63bc068b8990147ebc18dafdccdfbfb1b8e3f5667e64c38f]]]# or you can combine them into one line:

[[[00000000000000002582---9cb9a058ebb21835b8fca6fc84394dd06950f1b17af39cda686b177191de77c8]]]Here we use the MLP class to generate the model (we used TwoLayerNet in the previous step). Then the SGD class does the parameter update. The code that updates the concrete parameters is in the SGD class. Therefore, simply calling optimizer.update() completes the parameter update.

[[[00000000000000002583---bc6ef2c87fea930c368df10fd555215244e69f4ca956aecfbd412c2824bda235]]]The setup method of the Optimizer class returns itself as the return value. It also allows you to write my_optimizer = SGD(...).setup(...) in one line.

[[[00000000000000002584---83a27918ffa9dc180efbe6913f6620495246555f1b2498d5901da256d765dec0]]]Non-SGD Optimization Techniques

[[[00000000000000002585---2fc9a3413e391f688f2fbbd3322cfa41681b3a9c0a5607fd68bc9c994b5da798]]]There are various methods of optimization using gradients. Typical methods include Momentum, AdaGrad[27], AdaDelta[28], and Adam[29]. The primary purpose of introducing the Optimizer class is to easily switch between its various optimization techniques. So, extend the base class Optimizer to implement other optimization techniques. Here, we will implement Momentum as a non-SGD optimization technique.

[[[00000000000000002586---8eb14f994b0999f604b0411155f31e96b6d8363f9cc54b8aa50b6f484a2861ba]]]For optimization methods such as AdaGrad, AdaDelta and Adam there are implementations in dezero/optimizers.py. This document does not describe these methods. If you are interested, please refer to '6.1 Updating parameters' in 'Deep Learning from Scratch'.

[[[00000000000000002587---d6d1538e50213d26285ebf38c9a92394a9df1ed9ef1096c5a2f40af8ee684d0c]]]Now, about the technique called Momentum, this is represented by a formula as follows.

[[[00000000000000002588---e0433123db555f0736507029da92a71bbcb18f4cdeb8383852e56546e9e179bc]]]where is the weight parameter to update, is the gradient (the gradient of the loss function with respect to), and is the learning factor. Also, corresponds to 'velocity' in physics. Equation (46.1) expresses the law of physics that an object receives a force in the direction of the gradient, and that force accelerates the object's velocity. Then, the position (parameter) moves by that amount of speed according to equation (46.2).

[[[00000000000000002589---75c5d0044a536d734902d11e4769a37d94f5e80f52d11b2131213b5f43341932]]]Equation (46.1) has the term This term plays the role of gradually decelerating the object when no force is applied to it (set a value such as ).

[[[00000000000000002590---74b62c1b60a184df0561152765a5c675624e7a5bceced7eb60b68d21dbad23ae]]]So let's implement Momentum under the name MomentumSGD. The code looks like this:

[[[00000000000000002591---cef398b91918d3362696a843e53a07c06fd3fbac186dd9dc4f9d87bfe12b2af3]]]Here, each parameter has data corresponding to 'velocity'. For that I keep an instance variable of the dictionary as self.vs. The contents of vs are empty at the time of initialization, but when update_one() is first called, it generates data of the same shape as the parameter. All that remains is to replace formulas (46.1) and (46.2) with code.

[[[00000000000000002592---070cffb81434e6d1cb079fffac29c48924f229ad032270088bd4dedcee1a8577]]]The MomentumSGD class code in dezero/optimizers.py has some differences from the code above. Actually, in order to support GPU, the np.ones_like method calls CuPy's cupy.ones_like method according to the data type. GPU support is done in step 52.

[[[00000000000000002593---3edbd47e6677f1aa4991c573d12c4e154cf21b0e548d269ef3fb5cb6cec33455]]]The above is the implementation of Momentum. Now you can easily switch to Momentum with the training code you just implemented. To do so, simply rewrite optimizer = SGD(lr) as optimizer = MomentumSGD(lr). No other modifications are required. Now you can easily switch between different optimization techniques and try them out!

[[[00000000000000002594---080dd6fd1246ca2edb6ce1b6f7b08bb964f2915a7419a6ce93356c8b0c7066a9]]]step 11

[[[00000000000000002595---6cb7197373c0fb90f1e3ed2925ca4885921e0bc340feb9321a07fd2a71cbf6e2]]]Variable-length Arguments (Forward Propagation)

[[[00000000000000002596---c69d1331d92872ddc1198bcdb20b2c9d8a6dc5854a5c73e90bac43606380dbff]]]So far we have only considered input and output to functions with a single variable. For example, y = square(x) and y = exp(x) have one variable for both input and output. However, some functions may take multiple variables as inputs. For example, 'addition' and 'multiplication' as shown in Figure 11-1.

[[[00000000000000002597---e5b6221906f2052b7d681669082863bbdbb178ce33727fc367556d5e380b7ba1]]]
Figure 11-1 Calculation graph for 'addition' and calculation graph for 'multiplication' (multiplication operations are indicated by *)


[[[00000000000000002598---da3d6a3b1247d58699a615467266f48f9f293cc02958a446707547bfaf84753f]]]Also, some functions may have multiple outputs. For example, a function like Figure 11-2.

[[[00000000000000002599---56fd4c415b13a66eb046738e04cfad814e2eacc608d0e44c88b6a9da0984c955]]]
Figure 11-2 Example of a computation graph with multiple outputs (a function that partitions a multidimensional array)


[[[00000000000000002600---f147b6b17caebe5dacef118a85204dab8b1d1585db6633f4adeefcb3099c9075]]]Considering the above points, we extend our DeZero to handle variable-length input/output. Variable length means that the number of arguments (or return values) can vary - that number can be 1, 2, 3, .... Here we modify the Function class to accommodate variable length arguments and return values.

[[[00000000000000002601---f6c596300f9791f21611413230c67b24b3d696f2008b3f428663ff8e6dc1caf7]]]Modifying the Function class

[[[00000000000000002602---b05800aa8460d46ff8f69a8615c01acfe1d3e1846e76c58ded40e33d083afbfd]]]Let's modify the Function class so that it can handle multiple inputs and outputs. One way to do this is to process the variables in a list (or tuple). In other words, the Function class will continue to take 'one argument' and return 'one value'. However, change its arguments and return value to a list and modify it to put the necessary variables in that list.

[[[00000000000000002603---904649857829d490db25b7b4cb008a14e1407bb05924c28740c79349979542b6]]]Python lists and tuples hold multiple pieces of data side by side. Lists are enclosed in [], such as [1, 2,3], and tuples are enclosed in (), such as (1, 2, 3). The main difference between lists and tuples is that once a tuple is created, its elements cannot be changed. For example, if you have x = (1, 2, 3), you cannot overwrite x[0] = 4, etc. Lists, on the other hand, can change their elements.

[[[00000000000000002604---08fb461046b301d0ee67e8814af1fa4d3d42271bf13e2ec6d17af1c601e66406]]]Let's start by checking the current implementation of the Function class. Currently we implement the Function class as follows:

[[[00000000000000002605---0c13ca194b4afc3ff1e63347cad0c7dadb8b0822ec89477c33b83f61c1ab53d5]]]The function's __call__ method retrieves the actual data from the variable 'box' and the forward method performs concrete calculations. And wrap the result in Variable, and let them remember that they are the 'creators'. With that in mind, let's change the arguments and return value of the __call__ method to a list.

[[[00000000000000002606---30d6a08c33f2b1041d7b2dd3a44a4d61e41db147ddb8a23454bc6d003e5b3603]]]As above, I changed the arguments and return value to lists. The processing logic is the same as before, except that variables are handled in lists. Note that the code above uses a list comprehension to create a new list.

[[[00000000000000002607---a23957d0b37c8d75c43a4611e4e9def1b56a3c7ba0331d40d90e7a656000a091]]]A list comprehension is written like xs = [x.data for x in inputs]. In this case, for each element x in the inputs list, we take the respective data (x.data) and create a new list with that element.

[[[00000000000000002608---04cf2641e97ddd1c19d333a3975407e09c07dae4dff933b6dec467c01caa4f5e]]]The above is the new Function class. Then use this new Function class to implement concrete functions. First, let's implement the Add class that performs addition.

[[[00000000000000002609---2018d93703c548ea379ca7e3509c5b547059fbb05423d25ef1cb9b88faabe0ad]]]Add class implementation

[[[00000000000000002610---38d141df13e1574b65ea787b65bdb9cfd92055e3d82ff60f9f3d4f577425b119]]]Here we implement the forward method of the Add class. Note that the arguments and return values are lists (or tuples). With that in mind, the following implementations are possible:

[[[00000000000000002611---fa51ab3c2fc81e7a6a0859cb7627d4d3f1f75a8b5737075944a3d1270bcc2a16]]]The argument of the Add class is a list containing two variables. Therefore, by writing x0, x1 = xs, we can retrieve the elements of xs, which is a list. Then use that element to calculate. And when returning the result, write return (y,) to return a tuple (you can also write return y,). Now you can make use of the Add class like this:

[[[00000000000000002612---988065606e7b30723d2a3a51ce6c57b04fd739d28c10a0ea0f4a670f66f8c475]]]# prepared as a list

[[[00000000000000002613---32c10cb5d52dd253b97b3793aeaa67f16ce3f17afc6dea5c5e6089bb4beb42e6]]]# ys is a tuple

[[[00000000000000002614---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002615---7d04c99356287e66a48d9b41a94b2eb59445076f42bf6871aa8f5b091e6bd63a]]]As you can see above, DeZero was able to calculate 2 + 3 = 5 correctly. By making the input a list, it became possible to handle multiple variables, and by making the output a tuple, it was possible to handle multiple variables. For forward propagation, we now have support for variable-length arguments and return values. But looking at the code above, it seems a bit cumbersome. This is because it would be unnatural to have the person using the Add class prepare a list as an input variable or receive a tuple as a return value. The next step is to improve our current implementation so that we can write more natural code.

[[[00000000000000002616---b59c1339d0e40c615e13cb152882df3e8d02353b3f946ef7e5c383c00774dda3]]]step 44

[[[00000000000000002617---151781fb139f38e648881959d57d717920ec82f3c2754c9b720e4bc0cdbbfea9]]]A layer that organizes parameters

[[[00000000000000002618---8e072aa729ed2dc39f99c26efb43c54b5df3b388f66f04301c30ef52644517a7]]]In the previous step, we used DeZero to implement a neural network. It's a simple neural network, but it's a true neural network. Now, you could even call DeZero a 'neural network framework'. However, there are still some issues with DeZero's 'ease of use'. From now on, we will add functions for neural networks to DeZero. It makes implementing neural networks—and deep learning—simpler and more intuitive.

[[[00000000000000002619---57372a6912b7feb9b9500a4fb7f90fb5eabdf338384ce32054be65738aa84145]]]The issue addressed in this step is parameter handling. In the previous step, we had to write somewhat tedious code when resetting the parameter gradient (and also when updating the parameter). In the future, we will implement networks with more complex structures. This makes parameter handling even more complicated.

[[[00000000000000002620---1d90ebd2557734257176bd58644b7e8ee84a8cea69ee4bc30b4ed716808b2e27]]]Parameters are variables that are updated by optimization techniques such as gradient descent. In the example of the previous step, the 'weight' and 'bias' used for linear transformation correspond to the parameters.

[[[00000000000000002621---b68365ae5db289524fb7730b208c0b1c7c7b6d3f1005fb490d4ade404dbb3f99]]]In this step, we will create a mechanism for collecting parameters. For that, we will implement two classes: Parameter and Layer. By using these two classes, you can automate parameter management.

[[[00000000000000002622---164057b795ccfb78f8d48bc131b4bf06176b5fad033b54aa2d23304b39ff9329]]]Implementation of Parameter class

[[[00000000000000002623---f5c00c859981a87c52495fb9d2fb2ab0b5922a908316f4aeb8646bb788ce67cd]]]Let's start with the Parameter class. The Parameter class will have exactly the same functionality as the Variable class. So implement it like this:

[[[00000000000000002624---db201dfb03fa5ce79e5d599ba28e531601cde6529acd8069cafde187fc962235]]]This is the Parameter class. As you can see, it's just a class that inherits from the Variable class. So it has exactly the same functionality as the Variable class.

[[[00000000000000002625---844ca7e6dc09c3e68fd7c8e6ba1e4039d71f0b1b0e0bca4cbf018e5cc2c41196]]]We add the above Parameter class to dezero/core.py. Then add a line from dezero.core import Parameter to dezero/__init__.py. Now people using DeZero can import from dezero import Parameter.

[[[00000000000000002626---ad6d8cad0f9dc070b5f0a0ee45266fbbe2b3778e3403b0a75e4e41db7ea1bed1]]]Parameter and Variable instances have exactly the same functionality. However, they can be distinguished. A concrete example would be:

[[[00000000000000002627---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002628---2e1bdf8e44dc954f313df90b5505b328fb698078d3e04fdc0de1efc4a3675b04]]]As above, Parameter instances and Variable instances can be combined and computed. And the isinstance function allows us to distinguish between them. Using this, you can create a mechanism to collect only Parameter instances.

[[[00000000000000002629---9987d559cdf71af758853156a217770468069bf5f69396952e2691da706a481d]]]Layer class implementation

[[[00000000000000002630---28513c7b6960400c4ce31c18622dde2cbcd22e66ac52a2c62710217189b8fb8c]]]Next, implement a class called Layer. The Layer class is a class that transforms variables, just like DeZero's Function class. However, it differs in that it retains parameters. The Layer class is a class that holds parameters and performs conversion using parameters.

[[[00000000000000002631---0ed24ba8ff11a80a8afa73fd9b5a567da7bc9c60dec960db0b7d1e0bcd17988a]]]The Layer class will be implemented as a base class, and specific conversions will be implemented after inheriting the Layer class. For example, linear transformation is implemented with a class called Linear that inherits from the Layer class.

[[[00000000000000002632---3e5887142e363286a7c251946a4ce25cd4d0f5c642a1a6ba7cb4f02c365d1c33]]]Now let's look at the implementation of the Layer class. First, let's look at the initialization and the special method __setattr__.

[[[00000000000000002633---07c4ea7d272c2d2ef4a45ae29d5b53b6972414a528a471bf96f69fe9469f15d8]]]The Layer class has an instance variable called _params. This _params holds the parameters that the Layer instance has.

[[[00000000000000002634---66f0ff35a95ca0662293f5fe907e695344254afc0af64d62ede40f49c5107dd8]]]The type of instance variable _params is 'Set'. A set differs from a list in that its elements are not ordered. Also, collections do not hold duplicate objects with the same ID.

[[[00000000000000002635---9d5ea3338d0de97cc1cb3f66e9dc8e9110f6ee2e21bc8e9bd854ae791051be13]]]The __setattr__ method is a special method called when setting an instance variable. If you define __setattr__(self, name, value), the name of the instance variable is passed as name and the value of the instance variable as value. By overriding this method, you can add your own processing when adding instance variables.

[[[00000000000000002636---8766538306b5c3b9eafd74114e0b70d408b6d7a8803bf368c8a2f12efa322dba]]]Here we add name to self._params only if value is a Parameter instance†1. This allows you to collect the parameters of the Layer class into the instance variable _params. A real example would look like this:

[[[00000000000000002637---b6b713754c31134f7caeb8c2be7a119f97ab2c237f27df15454c09fa317b2d04]]][†1] Add name instead of value to self._params. The reason is that it is convenient to keep the name when saving the parameter to an external file. You will do that in step 53.

[[[00000000000000002638---5dbeb3cf90ff9bc35e8ed015609f0ce5a485cd7311c07714f70991cdab481125]]]Execution result

[[[00000000000000002639---b2d486f18cf98c6382947b0abbf0626c329dca7f1cbef8e6511f42c976c64591]]]As you can see here, setting an instance variable for layer only adds the instance variable name that holds the Parameter instance to layer._params. In addition, __dict__ of instance variables stores all instance variables in dictionary format, so you can use it to retrieve only Parameter instances.

[[[00000000000000002640---b9763b630985ad64fd14ab18907d007462ef5c915bf854879081a79d06d4ef1a]]]Next, add the following four methods to the Layer class.

[[[00000000000000002641---e1ec8c47f2b4cd6c675d4b8c93556f07f6f4519c4788e8f2efecea3bd6f19090]]]The __call__ method takes the input and calls the forward method. This forward method will be implemented in the inherited class. If the return value of the __call__ method has only one output, it should return that output directly instead of a tuple (this practice is the same as the implementation of the Function class). Also, for future proofing, I keep the input and output variables as weak references in the __call__ method.

[[[00000000000000002642---91f0911c3858a9edc119d5b96ef492f0fd44514d464821a70f5d4a3cf560a7ef]]]The params method retrieves the Parameter instance that the Layer instance has. Also, the cleargrads method resets the gradients for all parameters. Note that the name of this method is 'plural' by adding an s at the end of cleargrad. This is to explicitly call cleargrad (singular) for all parameters the layer has.

[[[00000000000000002643---8de569e51891b0ed0d9ea0157e5b6334157c82f1ec8f8b936e414ec62dbc7bee]]]The params method uses yield to return the value. yield can be used in the same way as return. However, return terminates processing and returns a value, whereas yield 'pauses' processing and returns a value. Therefore, you can use yield to resume processing. In the above example, every time you call the params method, the suspended process will resume. Now -- using yield in conjunction with the for statement -- you can retrieve the parameters sequentially.

[[[00000000000000002644---48b060c317c42d86327a03b92ce2a2c4a76b6a0aac222df98bbfc91be9b7bb32]]]This completes the implementation of the Layer class. We will inherit this Layer class and implement specific processing such as linear transformation.

[[[00000000000000002645---5e8ac81681aa37b72c70f6fc61626954dedc05716593a53d40ae5138cc3878cf]]]Linear class implementation

[[[00000000000000002646---979dcf4984f31511891b6f0178654ff7dc243088facc4acea5c9c53ed692dfcb]]]Then implement a Linear class that does the linear transformation (implement the Linear class as a layer, not as a function). I'll show the simple Linear class first, then the improved Linear class. Let's look at the following code.

[[[00000000000000002647---b53477140ba3f663e180dc43e569663c5db0db80e25d17e13e85906b6ea0c88a]]]The Linear class inherits and implements the Layer class. The __init__(self, in_size, out_size, nobias) argument of the initialization accepts 'input size', 'output size', and 'whether to use bias' flags. Omits the bias when nobias is True.

[[[00000000000000002648---2ff1fa29f2df54601bfa68d1ebb04d443c5190c6b559f4f6465a50c77fd34429]]]Initializing weights and biases sets Parameter instances in instance variables, such as self.W = Parameter(...) and self.b = Parameter(...). Doing so adds the instance variable names of those two Parameters to self._params.

[[[00000000000000002649---984962644ad455b10b15ddd9c1e5ef057e779e96ba9d9bb82e7c2b43f56417b8]]]Linear class weights should have random initial values. In the previous step we set the scale of that random initial value to 0.01 (0.01 * np.random.randn(...)). Here we set that scale to np.sqrt(1/in_size). This is the initial value setting method proposed in [25]. We also know that neural network calculations can be done with 32-bit floating point numbers without problems. Therefore, the parameter data defaults to 32-bit floating point numbers.

[[[00000000000000002650---f2d91edff33c8c636f2f7a70f0aa68cd56ed096cf21ee6101304ac8dee45cf1c]]]After that, implement the linear transformation with the forward method. In there, just call DeZero's linear function. The above is the implementation of the Linear class.

[[[00000000000000002651---8fe4c7b12eadf6e0cd459bd41b95afd3ede97e873c31aeb3b3da6b58b9384082]]]Now, as I said earlier, the Linear class has a better implementation. It is a method of delaying the timing of generating the weight W. Specifically, by generating weights in the forward method, the input size (in_size) of the Linear class can be automatically determined (the user does not have to specify it). So here's an improved Linear class implementation:

[[[00000000000000002652---911d21f97008828d5cd33e1b35208bb9914e9920363527aefec5bce6bf6203d0]]]# later if in_size is not specified

[[[00000000000000002653---17f82ca249752243b37a5396407e434f89cc8073a1d27f7ba2a61f9266adb8d5]]]# Initialize the weights when data is sent

[[[00000000000000002654---84685e790b9e844ede47b20cc4b0d9089c7e555edd5cef559791d819a574bc76]]]Here's an improved version of the Linear class. It's worth noting that you don't have to specify in_size in the __init__ method. The argument in_size defaults to None, which will 'defer' the initialization of self.W.data. Specifically, in the forward(self, x) method, the weight data is generated according to the size of the input x. Now you only need to specify the output size like layer = Linear(100), improving usability. The above is the implementation of the Linear class.

[[[00000000000000002655---68641c099731e0f588c756ecd98720c9741963e5ef1396ef2113eeecb9afc9cc]]]Implementing Neural Networks with Layers

[[[00000000000000002656---416baf9e6a2e58bb74a193bb769277f3e3657850a5b4218c283e4143d03e854a]]]Let's implement a neural network using the Linear class. Now let's re-solve the problem we solved in the previous step - regression of the sine function on a dataset. In the code below, the changes from the previous step are shaded.

[[[00000000000000002657---9c71f67d8eaff4a68bc844650e2021480d64dc9a2ef42801a29f4737b5d97dc7]]]# import as L

[[[00000000000000002658---e4381c42345ad5ca7e53cabd6ee7e91af8a0f019aede9388d49a6e12d3d003ee]]]# data set

[[[00000000000000002659---803a8ca7ea9c026ad2af7bf90002b545fcb3a56dd61134a51a51a2746cb644c0]]]# specify output size

[[[00000000000000002660---f89cf245a34cadb87275c25a7e7245c8102793930de37d543bbeda941343ef4e]]]The point to notice here is that the Linear instance is responsible for managing the parameters. As a result, the process of resetting parameter gradients and updating parameters has become cleaner than before.

[[[00000000000000002661---278b2363e8ef8a6dd6a195fb90b106f14c46a793a429ccb12a9f18e08f380c7e]]]But here we are dealing with the Linear class separately. Considering that we are going to “deepen” the network in the future, it would be difficult to deal with the Linear class separately. The next step is to manage multiple Layers together in one class.